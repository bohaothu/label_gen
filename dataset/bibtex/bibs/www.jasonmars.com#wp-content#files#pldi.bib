@inproceedings{Terauchi:2008:CRF:1375581.1375583,
 author = {Terauchi, Tachio},
 title = {Checking race freedom via linear programming},
 abstract = {We present a new static analysis for race freedom and race detection. The analysis checks race freedom by reducing the problem to (rational) linear programming. Unlike conventional static analyses for race freedom or race detection, our analysis avoids explicit computation of locksets and lock linearity/must-aliasness. Our analysis can handle a variety of synchronization idioms that more conventional approaches often have difficulties with, such as thread joining, semaphores, and signals. We achieve efficiency by utilizing modern linear programming solvers that can quickly solve large linear programming instances. This paper reports on the formal properties of the analysis and the experience with applying an implementation to real world C programs.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375583},
 doi = {http://doi.acm.org/10.1145/1375581.1375583},
 acmid = {1375583},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fractional capabilities, linear programming},
} 

@article{Terauchi:2008:CRF:1379022.1375583,
 author = {Terauchi, Tachio},
 title = {Checking race freedom via linear programming},
 abstract = {We present a new static analysis for race freedom and race detection. The analysis checks race freedom by reducing the problem to (rational) linear programming. Unlike conventional static analyses for race freedom or race detection, our analysis avoids explicit computation of locksets and lock linearity/must-aliasness. Our analysis can handle a variety of synchronization idioms that more conventional approaches often have difficulties with, such as thread joining, semaphores, and signals. We achieve efficiency by utilizing modern linear programming solvers that can quickly solve large linear programming instances. This paper reports on the formal properties of the analysis and the experience with applying an implementation to real world C programs.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375583},
 doi = {http://doi.acm.org/10.1145/1379022.1375583},
 acmid = {1375583},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fractional capabilities, linear programming},
} 

@article{Sen:2008:RDR:1379022.1375584,
 author = {Sen, Koushik},
 title = {Race directed random testing of concurrent programs},
 abstract = {Bugs in multi-threaded programs often arise due to data races. Numerous static and dynamic program analysis techniques have been proposed to detect data races. We propose a novel randomized dynamic analysis technique that utilizes potential data race information obtained from an existing analysis tool to separate real races from false races without any need for manual inspection. Specifically, we use potential data race information obtained from an existing dynamic analysis technique to control a random scheduler of threads so that real race conditions get created with very high probability and those races get resolved randomly at runtime. Our approach has several advantages over existing dynamic analysis tools. First, we can create a real race condition and resolve the race randomly to see if an error can occur due to the race. Second, we can replay a race revealing execution efficiently by simply using the same seed for random number generation--we do not need to record the execution. Third, our approach has very low overhead compared to other precise dynamic race detection techniques because we only track all synchronization operations and a single pair of memory access statements that are reported to be in a potential race by an existing analysis. We have implemented the technique in a prototype tool for Java and have experimented on a number of large multi-threaded Java programs. We report a number of previously known and unknown bugs and real races in these Java programs.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {11--21},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375584},
 doi = {http://doi.acm.org/10.1145/1379022.1375584},
 acmid = {1375584},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race detection, random testing},
} 

@inproceedings{Sen:2008:RDR:1375581.1375584,
 author = {Sen, Koushik},
 title = {Race directed random testing of concurrent programs},
 abstract = {Bugs in multi-threaded programs often arise due to data races. Numerous static and dynamic program analysis techniques have been proposed to detect data races. We propose a novel randomized dynamic analysis technique that utilizes potential data race information obtained from an existing analysis tool to separate real races from false races without any need for manual inspection. Specifically, we use potential data race information obtained from an existing dynamic analysis technique to control a random scheduler of threads so that real race conditions get created with very high probability and those races get resolved randomly at runtime. Our approach has several advantages over existing dynamic analysis tools. First, we can create a real race condition and resolve the race randomly to see if an error can occur due to the race. Second, we can replay a race revealing execution efficiently by simply using the same seed for random number generation--we do not need to record the execution. Third, our approach has very low overhead compared to other precise dynamic race detection techniques because we only track all synchronization operations and a single pair of memory access statements that are reported to be in a potential race by an existing analysis. We have implemented the technique in a prototype tool for Java and have experimented on a number of large multi-threaded Java programs. We report a number of previously known and unknown bugs and real races in these Java programs.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {11--21},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375584},
 doi = {http://doi.acm.org/10.1145/1375581.1375584},
 acmid = {1375584},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race detection, random testing},
} 

@inproceedings{Blackburn:2008:IMG:1375581.1375586,
 author = {Blackburn, Stephen M. and McKinley, Kathryn S.},
 title = {Immix: a mark-region garbage collector with space efficiency, fast collection, and mutator performance},
 abstract = {Programmers are increasingly choosing managed languages for modern applications, which tend to allocate many short-to-medium lived small objects. The garbage collector therefore directly determines program performance by making a classic space-time tradeoff that seeks to provide space efficiency, fast reclamation, and mutator performance. The three canonical tracing garbage collectors: semi-space, mark-sweep, and mark-compact each sacrifice one objective. This paper describes a collector family, called mark-region</i>, and introduces opportunistic</i> defragmentation, which mixes copying and marking in a single pass. Combining both, we implement immix</i>, a novel high performance garbage collector that achieves all three performance objectives. The key insight is to allocate and reclaim memory in contiguous regions, at a coarse block</i> grain when possible and otherwise in groups of finer grain lines</i>. We show that immix outperforms existing canonical algorithms, improving total application performance by 7 to 25\% on average across 20 benchmarks. As the mature space in a generational collector, immix matches or beats a highly tuned generational collector, e.g. it improves jbb2000 by 5\%. These innovations and the identification of a new family of collectors open new opportunities for garbage collector design.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375586},
 doi = {http://doi.acm.org/10.1145/1375581.1375586},
 acmid = {1375586},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, fragmentation, free-list, immix, locality, mark-region, mark-sweep, semi-space, sweep-to-free-list, sweep-to-region},
} 

@article{Blackburn:2008:IMG:1379022.1375586,
 author = {Blackburn, Stephen M. and McKinley, Kathryn S.},
 title = {Immix: a mark-region garbage collector with space efficiency, fast collection, and mutator performance},
 abstract = {Programmers are increasingly choosing managed languages for modern applications, which tend to allocate many short-to-medium lived small objects. The garbage collector therefore directly determines program performance by making a classic space-time tradeoff that seeks to provide space efficiency, fast reclamation, and mutator performance. The three canonical tracing garbage collectors: semi-space, mark-sweep, and mark-compact each sacrifice one objective. This paper describes a collector family, called mark-region</i>, and introduces opportunistic</i> defragmentation, which mixes copying and marking in a single pass. Combining both, we implement immix</i>, a novel high performance garbage collector that achieves all three performance objectives. The key insight is to allocate and reclaim memory in contiguous regions, at a coarse block</i> grain when possible and otherwise in groups of finer grain lines</i>. We show that immix outperforms existing canonical algorithms, improving total application performance by 7 to 25\% on average across 20 benchmarks. As the mature space in a generational collector, immix matches or beats a highly tuned generational collector, e.g. it improves jbb2000 by 5\%. These innovations and the identification of a new family of collectors open new opportunities for garbage collector design.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375586},
 doi = {http://doi.acm.org/10.1145/1379022.1375586},
 acmid = {1375586},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, fragmentation, free-list, immix, locality, mark-region, mark-sweep, semi-space, sweep-to-free-list, sweep-to-region},
} 

@article{Pizlo:2008:SCR:1379022.1375587,
 author = {Pizlo, Filip and Petrank, Erez and Steensgaard, Bjarne},
 title = {A study of concurrent real-time garbage collectors},
 abstract = {Concurrent garbage collection is highly attractive for real-time systems, because offloading the collection effort from the executing threads allows faster response, allowing for extremely short deadlines at the microseconds level. Concurrent collectors also offer much better scalability over incremental collectors. The main problem with concurrent real-time collectors is their complexity. The first concurrent real-time garbage collector that can support fine synchronization, STOPLESS, has recently been presented by Pizlo et al. In this paper, we propose two additional (and different) algorithms for concurrent real-time garbage collection: CLOVER and CHICKEN. Both collectors obtain reduced complexity over the first collector STOPLESS, but need to trade a benefit for it. We study the algorithmic strengths and weaknesses of CLOVER and CHICKEN and compare them to STOPLESS. Finally, we have implemented all three collectors on the Bartok compiler and runtime for C# and we present measurements to compare their efficiency and responsiveness.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1379022.1375587},
 doi = {http://doi.acm.org/10.1145/1379022.1375587},
 acmid = {1375587},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent garbage collection, garbage collection, lock-free computation, memory management, real-time},
} 

@inproceedings{Pizlo:2008:SCR:1375581.1375587,
 author = {Pizlo, Filip and Petrank, Erez and Steensgaard, Bjarne},
 title = {A study of concurrent real-time garbage collectors},
 abstract = {Concurrent garbage collection is highly attractive for real-time systems, because offloading the collection effort from the executing threads allows faster response, allowing for extremely short deadlines at the microseconds level. Concurrent collectors also offer much better scalability over incremental collectors. The main problem with concurrent real-time collectors is their complexity. The first concurrent real-time garbage collector that can support fine synchronization, STOPLESS, has recently been presented by Pizlo et al. In this paper, we propose two additional (and different) algorithms for concurrent real-time garbage collection: CLOVER and CHICKEN. Both collectors obtain reduced complexity over the first collector STOPLESS, but need to trade a benefit for it. We study the algorithmic strengths and weaknesses of CLOVER and CHICKEN and compare them to STOPLESS. Finally, we have implemented all three collectors on the Bartok compiler and runtime for C# and we present measurements to compare their efficiency and responsiveness.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375581.1375587},
 doi = {http://doi.acm.org/10.1145/1375581.1375587},
 acmid = {1375587},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent garbage collection, garbage collection, lock-free computation, memory management, real-time},
} 

@article{Wang:2008:CCA:1379022.1375588,
 author = {Wang, Xi and Xu, Zhilei and Liu, Xuezheng and Guo, Zhenyu and Wang, Xiaoge and Zhang, Zheng},
 title = {Conditional correlation analysis for safe region-based memory management},
 abstract = {Region-based memory management is a popular scheme in systems software for better organization and performance. In the scheme, a developer constructs a hierarchy of regions of different lifetimes and allocates objects in regions. When the developer deletes a region, the runtime will recursively delete all its subregions and simultaneously reclaim objects in the regions. The developer must construct a consistent</i> placement of objects in regions; otherwise, if a region that contains pointers to other regions is not always deleted before</i> pointees, an inconsistency will surface and cause dangling pointers, which may lead to either crashes or leaks. This paper presents a static analysis tool RegionWiz that can find such lifetime inconsistencies in large C programs using regions. The tool is based on an analysis framework that generalizes the relations and constraints over regions and objects as conditional correlations. This framework allows a succinct formalization of consistency rules for region lifetimes, preserving memory safety and avoiding dangling pointers. RegionWiz uses these consistency rules to implement an efficient static analysis to compute the conditional correlation and reason about region lifetime consistency; the analysis is based on a context-sensitive, field-sensitive pointer analysis with heap cloning. Experiments with applying RegionWiz to six real-world software packages (including the RC compiler, Apache web server, and Subversion version control system) with two different region-based memory management interfaces show that RegionWiz can reason about region lifetime consistency in large C programs. The experiments also show that RegionWiz can find several previously unknown inconsistency bugs in these packages.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {45--55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375588},
 doi = {http://doi.acm.org/10.1145/1379022.1375588},
 acmid = {1375588},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conditional correlation, error detection, memory management, program analysis, region},
} 

@inproceedings{Wang:2008:CCA:1375581.1375588,
 author = {Wang, Xi and Xu, Zhilei and Liu, Xuezheng and Guo, Zhenyu and Wang, Xiaoge and Zhang, Zheng},
 title = {Conditional correlation analysis for safe region-based memory management},
 abstract = {Region-based memory management is a popular scheme in systems software for better organization and performance. In the scheme, a developer constructs a hierarchy of regions of different lifetimes and allocates objects in regions. When the developer deletes a region, the runtime will recursively delete all its subregions and simultaneously reclaim objects in the regions. The developer must construct a consistent</i> placement of objects in regions; otherwise, if a region that contains pointers to other regions is not always deleted before</i> pointees, an inconsistency will surface and cause dangling pointers, which may lead to either crashes or leaks. This paper presents a static analysis tool RegionWiz that can find such lifetime inconsistencies in large C programs using regions. The tool is based on an analysis framework that generalizes the relations and constraints over regions and objects as conditional correlations. This framework allows a succinct formalization of consistency rules for region lifetimes, preserving memory safety and avoiding dangling pointers. RegionWiz uses these consistency rules to implement an efficient static analysis to compute the conditional correlation and reason about region lifetime consistency; the analysis is based on a context-sensitive, field-sensitive pointer analysis with heap cloning. Experiments with applying RegionWiz to six real-world software packages (including the RC compiler, Apache web server, and Subversion version control system) with two different region-based memory management interfaces show that RegionWiz can reason about region lifetime consistency in large C programs. The experiments also show that RegionWiz can find several previously unknown inconsistency bugs in these packages.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {45--55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375588},
 doi = {http://doi.acm.org/10.1145/1375581.1375588},
 acmid = {1375588},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conditional correlation, error detection, memory management, program analysis, region},
} 

@article{Amin:2008:AVM:1379022.1375590,
 author = {Amin, Ahmed M. and Thottethodi, Mithuna and Vijaykumar, T. N. and Wereley, Steven and Jacobson, Stephen C.},
 title = {Automatic volume management for programmable microfluidics},
 abstract = {Microfluidics has enabled lab-on-a-chip technology to miniaturize and integrate biological and chemical analyses to a single chip comprising channels, valves, mixers, heaters, separators, and sensors. Recent papers have proposed programmable labs-on-a-chip as an alternative to traditional application-specific chips to reduce design effort, time, and cost. While these previous papers provide the basic support for programmability, this paper identifies and addresses a practical issue, namely, fluid volume management. Volume management addresses the problem that the use of a fluid depletes it and unless the given volume of a fluid is distributed carefully among all its uses, execution may run out of the fluid before all its uses are complete. Additionally, fluid volumes should not overflow (i.e., exceed hardware capacity) or underflow (i.e., fall below hardware resolution). We show that the problem can be formulated as a linear programming problem (LP). Because LP's complexity and slow execution times in practice may be a concern, we propose another approach, called DAGSolve, which over-constrains the problem to achieve linear complexity while maintaining good solution quality. We also propose two optimizations, called cascading and static replication, to handle cases involving extreme mix ratios and numerous fluid uses which may defeat both LP and DAGSolve. Using some real-world assays, we show that our techniques produce good solutions while being faster than LP.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {56--67},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1379022.1375590},
 doi = {http://doi.acm.org/10.1145/1379022.1375590},
 acmid = {1375590},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid volume management, microfluidics, programmable lab on a chip},
} 

@inproceedings{Amin:2008:AVM:1375581.1375590,
 author = {Amin, Ahmed M. and Thottethodi, Mithuna and Vijaykumar, T. N. and Wereley, Steven and Jacobson, Stephen C.},
 title = {Automatic volume management for programmable microfluidics},
 abstract = {Microfluidics has enabled lab-on-a-chip technology to miniaturize and integrate biological and chemical analyses to a single chip comprising channels, valves, mixers, heaters, separators, and sensors. Recent papers have proposed programmable labs-on-a-chip as an alternative to traditional application-specific chips to reduce design effort, time, and cost. While these previous papers provide the basic support for programmability, this paper identifies and addresses a practical issue, namely, fluid volume management. Volume management addresses the problem that the use of a fluid depletes it and unless the given volume of a fluid is distributed carefully among all its uses, execution may run out of the fluid before all its uses are complete. Additionally, fluid volumes should not overflow (i.e., exceed hardware capacity) or underflow (i.e., fall below hardware resolution). We show that the problem can be formulated as a linear programming problem (LP). Because LP's complexity and slow execution times in practice may be a concern, we propose another approach, called DAGSolve, which over-constrains the problem to achieve linear complexity while maintaining good solution quality. We also propose two optimizations, called cascading and static replication, to handle cases involving extreme mix ratios and numerous fluid uses which may defeat both LP and DAGSolve. Using some real-world assays, we show that our techniques produce good solutions while being faster than LP.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {56--67},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375581.1375590},
 doi = {http://doi.acm.org/10.1145/1375581.1375590},
 acmid = {1375590},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid volume management, microfluidics, programmable lab on a chip},
} 

@inproceedings{Boehm:2008:FCC:1375581.1375591,
 author = {Boehm, Hans-J. and Adve, Sarita V.},
 title = {Foundations of the C++ concurrency memory model},
 abstract = {Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:<ul><li>We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work.</li> <li>We give no semantics to programs with data races. There are no benign C++ data races.</li> <li>We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock.</li></ul> This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375591},
 doi = {http://doi.acm.org/10.1145/1375581.1375591},
 acmid = {1375591},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {c++, data race, memory consistency, memory model, sequential consistency, trylock},
} 

@article{Boehm:2008:FCC:1379022.1375591,
 author = {Boehm, Hans-J. and Adve, Sarita V.},
 title = {Foundations of the C++ concurrency memory model},
 abstract = {Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:<ul><li>We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work.</li> <li>We give no semantics to programs with data races. There are no benign C++ data races.</li> <li>We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock.</li></ul> This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375591},
 doi = {http://doi.acm.org/10.1145/1379022.1375591},
 acmid = {1375591},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {c++, data race, memory consistency, memory model, sequential consistency, trylock},
} 

@article{Huang:2008:ESS:1379022.1375592,
 author = {Huang, Shan Shan and Smaragdakis, Yannis},
 title = {Expressive and safe static reflection with MorphJ},
 abstract = {Recently, language extensions have been proposed for Java and C# to support pattern-based reflective declaration</i>. These extensions introduce a disciplined form of meta-programming and aspect-oriented programming to mainstream languages: They allow members of a class (i.e., fields and methods) to be declared by statically iterating over and pattern-matching on members of other classes. Such techniques, however, have been unable to safely express simple, but common, idioms such as declaring getter and setter methods for fields. In this paper, we present a mechanism that addresses the lack of expressiveness in past work without sacrificing safety. Our technique is based on the idea of nested patterns that elaborate the outer-most pattern with blocking or enabling conditions. We implemented this mechanism in a language, MorphJ. We demonstrate the expressiveness of MorphJ with real-world applications. In particular, the MorphJ reimplementation of DSTM2, a software transactional memory library, reduces 1,107 lines of Java reflection and bytecode engineering library calls to just 374 lines of MorphJ code. At the same time, the MorphJ solution is both high level and safer, as MorphJ can separately type check generic classes and catch errors early. We present and formalize the MorphJ type system, and offer a type-checking algorithm.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {79--89},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375592},
 doi = {http://doi.acm.org/10.1145/1379022.1375592},
 acmid = {1375592},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aspect-oriented programming, class morphing, language extensions, meta-programming, object-oriented programming, structural abstraction},
} 

@inproceedings{Huang:2008:ESS:1375581.1375592,
 author = {Huang, Shan Shan and Smaragdakis, Yannis},
 title = {Expressive and safe static reflection with MorphJ},
 abstract = {Recently, language extensions have been proposed for Java and C# to support pattern-based reflective declaration</i>. These extensions introduce a disciplined form of meta-programming and aspect-oriented programming to mainstream languages: They allow members of a class (i.e., fields and methods) to be declared by statically iterating over and pattern-matching on members of other classes. Such techniques, however, have been unable to safely express simple, but common, idioms such as declaring getter and setter methods for fields. In this paper, we present a mechanism that addresses the lack of expressiveness in past work without sacrificing safety. Our technique is based on the idea of nested patterns that elaborate the outer-most pattern with blocking or enabling conditions. We implemented this mechanism in a language, MorphJ. We demonstrate the expressiveness of MorphJ with real-world applications. In particular, the MorphJ reimplementation of DSTM2, a software transactional memory library, reduces 1,107 lines of Java reflection and bytecode engineering library calls to just 374 lines of MorphJ code. At the same time, the MorphJ solution is both high level and safer, as MorphJ can separately type check generic classes and catch errors early. We present and formalize the MorphJ type system, and offer a type-checking algorithm.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {79--89},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375592},
 doi = {http://doi.acm.org/10.1145/1375581.1375592},
 acmid = {1375592},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aspect-oriented programming, class morphing, language extensions, meta-programming, object-oriented programming, structural abstraction},
} 

@inproceedings{Pouchet:2008:IOP:1375581.1375594,
 author = {Pouchet, Louis-No\"{e}l and Bastoul, C\'{e}dric and Cohen, Albert and Cavazos, John},
 title = {Iterative optimization in the polyhedral model: part ii, multidimensional time},
 abstract = {High-level loop optimizations are necessary to achieve good performance over a wide variety of processors. Their performance impact can be significant because they involve in-depth program transformations that aim to sustain a balanced workload over the computational, storage, and communication resources of the target architecture. Therefore, it is mandatory that the compiler accurately models the target architecture as well as the effects of complex code restructuring. However, because optimizing compilers (1) use simplistic performance models that abstract away many of the complexities of modern architectures, (2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of transformation sequences, they typically uncover only a fraction of the peak performance available on many applications. We propose a complete iterative framework to address these issues. We rely on the polyhedral model to construct and traverse a large and expressive search space. This space encompasses only legal, distinct versions resulting from the restructuring of any static control loop nest. We first propose a feedback-driven iterative heuristic tailored to the search space properties of the polyhedral model. Though, it quickly converges to good solutions for small kernels, larger benchmarks containing higher dimensional spaces are more challenging and our heuristic misses opportunities for significant performance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators that leverage the polyhedral representation of program dependences. We provide experimental evidence that the genetic algorithm effectively traverses huge optimization spaces, achieving good performance improvements on large loop nests.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {90--100},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375594},
 doi = {http://doi.acm.org/10.1145/1375581.1375594},
 acmid = {1375594},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affine scheduling, genetic algorithm, iterative compilation, loop transformation},
} 

@article{Pouchet:2008:IOP:1379022.1375594,
 author = {Pouchet, Louis-No\"{e}l and Bastoul, C\'{e}dric and Cohen, Albert and Cavazos, John},
 title = {Iterative optimization in the polyhedral model: part ii, multidimensional time},
 abstract = {High-level loop optimizations are necessary to achieve good performance over a wide variety of processors. Their performance impact can be significant because they involve in-depth program transformations that aim to sustain a balanced workload over the computational, storage, and communication resources of the target architecture. Therefore, it is mandatory that the compiler accurately models the target architecture as well as the effects of complex code restructuring. However, because optimizing compilers (1) use simplistic performance models that abstract away many of the complexities of modern architectures, (2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of transformation sequences, they typically uncover only a fraction of the peak performance available on many applications. We propose a complete iterative framework to address these issues. We rely on the polyhedral model to construct and traverse a large and expressive search space. This space encompasses only legal, distinct versions resulting from the restructuring of any static control loop nest. We first propose a feedback-driven iterative heuristic tailored to the search space properties of the polyhedral model. Though, it quickly converges to good solutions for small kernels, larger benchmarks containing higher dimensional spaces are more challenging and our heuristic misses opportunities for significant performance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators that leverage the polyhedral representation of program dependences. We provide experimental evidence that the genetic algorithm effectively traverses huge optimization spaces, achieving good performance improvements on large loop nests.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {90--100},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375594},
 doi = {http://doi.acm.org/10.1145/1379022.1375594},
 acmid = {1375594},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affine scheduling, genetic algorithm, iterative compilation, loop transformation},
} 

@inproceedings{Bondhugula:2008:PAP:1375581.1375595,
 author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
 title = {A practical automatic polyhedral parallelizer and locality optimizer},
 abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {101--113},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1375581.1375595},
 doi = {http://doi.acm.org/10.1145/1375581.1375595},
 acmid = {1375595},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affine transformations, automatic parallelization, locality optimization, loop transformations, polyhedral model, tiling},
} 

@article{Bondhugula:2008:PAP:1379022.1375595,
 author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
 title = {A practical automatic polyhedral parallelizer and locality optimizer},
 abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {101--113},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1379022.1375595},
 doi = {http://doi.acm.org/10.1145/1379022.1375595},
 acmid = {1375595},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affine transformations, automatic parallelization, locality optimization, loop transformations, polyhedral model, tiling},
} 

@article{Kudlur:2008:OES:1379022.1375596,
 author = {Kudlur, Manjunath and Mahlke, Scott},
 title = {Orchestrating the execution of stream programs on multicore platforms},
 abstract = {While multicore hardware has become ubiquitous, explicitly parallel programming models and compiler techniques for exploiting parallelism on these systems have noticeably lagged behind. Stream programming is one model that has wide applicability in the multimedia, graphics, and signal processing domains. Streaming models execute as a set of independent actors that explicitly communicate data through channels. This paper presents a compiler technique for planning and orchestrating the execution of streaming applications on multicore platforms. An integrated unfolding and partitioning step based on integer linear programming is presented that unfolds data parallel actors as needed and maximally packs actors onto cores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally overlapped with computation on the cores. To facilitate experimentation, a generalized code generation template for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming applications, a geometric mean speedup of 14.7x is achieved on a 16-core Cell platform compared to a single core.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {114--124},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375596},
 doi = {http://doi.acm.org/10.1145/1379022.1375596},
 acmid = {1375596},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell processor, multicore, software pipelining, stream programming, streamit},
} 

@inproceedings{Kudlur:2008:OES:1375581.1375596,
 author = {Kudlur, Manjunath and Mahlke, Scott},
 title = {Orchestrating the execution of stream programs on multicore platforms},
 abstract = {While multicore hardware has become ubiquitous, explicitly parallel programming models and compiler techniques for exploiting parallelism on these systems have noticeably lagged behind. Stream programming is one model that has wide applicability in the multimedia, graphics, and signal processing domains. Streaming models execute as a set of independent actors that explicitly communicate data through channels. This paper presents a compiler technique for planning and orchestrating the execution of streaming applications on multicore platforms. An integrated unfolding and partitioning step based on integer linear programming is presented that unfolds data parallel actors as needed and maximally packs actors onto cores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally overlapped with computation on the cores. To facilitate experimentation, a generalized code generation template for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming applications, a geometric mean speedup of 14.7x is achieved on a 16-core Cell platform compared to a single core.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {114--124},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375596},
 doi = {http://doi.acm.org/10.1145/1375581.1375596},
 acmid = {1375596},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell processor, multicore, software pipelining, stream programming, streamit},
} 

@article{Vechev:2008:DLF:1379022.1375598,
 author = {Vechev, Martin and Yahav, Eran},
 title = {Deriving linearizable fine-grained concurrent objects},
 abstract = {Practical and efficient algorithms for concurrent data structures are difficult to construct and modify. Algorithms in the literature are often optimized for a specific setting, making it hard to separate the algorithmic insights from implementation details. The goal of this work is to systematically construct algorithms for a concurrent data structure starting from its sequential implementation. Towards that goal, we follow a construction process that combines manual steps corresponding to high-level insights with automatic exploration of implementation details. To assist us in this process, we built a new tool called Paraglider. The tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability of algorithms. Starting from a sequential implementation and assisted by the tool, we present the steps that we used to derive various highly-concurrent algorithms. Among these algorithms is a new fine-grained set data structure that provides a wait-free contains operation, and uses only the compare-and-swap (CAS) primitive for synchronization.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {125--135},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375598},
 doi = {http://doi.acm.org/10.1145/1379022.1375598},
 acmid = {1375598},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, concurrency, data structures, linearizability, model checking},
} 

@inproceedings{Vechev:2008:DLF:1375581.1375598,
 author = {Vechev, Martin and Yahav, Eran},
 title = {Deriving linearizable fine-grained concurrent objects},
 abstract = {Practical and efficient algorithms for concurrent data structures are difficult to construct and modify. Algorithms in the literature are often optimized for a specific setting, making it hard to separate the algorithmic insights from implementation details. The goal of this work is to systematically construct algorithms for a concurrent data structure starting from its sequential implementation. Towards that goal, we follow a construction process that combines manual steps corresponding to high-level insights with automatic exploration of implementation details. To assist us in this process, we built a new tool called Paraglider. The tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability of algorithms. Starting from a sequential implementation and assisted by the tool, we present the steps that we used to derive various highly-concurrent algorithms. Among these algorithms is a new fine-grained set data structure that provides a wait-free contains operation, and uses only the compare-and-swap (CAS) primitive for synchronization.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {125--135},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375598},
 doi = {http://doi.acm.org/10.1145/1375581.1375598},
 acmid = {1375598},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, concurrency, data structures, linearizability, model checking},
} 

@inproceedings{Solar-Lezama:2008:SCD:1375581.1375599,
 author = {Solar-Lezama, Armando and Jones, Christopher Grant and Bodik, Rastislav},
 title = {Sketching concurrent data structures},
 abstract = {We describe PSketch, a program synthesizer that helps programmers implement concurrent data structures. The system is based on the concept of sketching, a form of synthesis that allows programmers to express their insight about an implementation as a partial program: a sketch. The synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria. PSketch is based on a new counterexample-guided inductive synthesis algorithm (CEGIS) that generalizes the original sketch synthesis algorithm from Solar-Lezama et.al. to cope efficiently with concurrent programs. The new algorithm produces a correct implementation by iteratively generating candidate implementations, running them through a verifier, and if they fail, learning from the counterexample traces to produce a better candidate; converging to a solution in a handful of iterations. PSketch also extends Sketch with higher-level sketching constructs that allow the programmer to express her insight as a "soup" of ingredients from which complicated code fragments must be assembled. Such sketches can be viewed as syntactic descriptions of huge spaces of candidate programs (over 10<sup>8</sup> candidates for some sketches we resolved). We have used the PSketch system to implement several classes of concurrent data structures, including lock-free queues and concurrent sets with fine-grained locking. We have also sketched some other concurrent objects including a sense-reversing barrier and a protocol for the dining philosophers problem; all these sketches resolved in under an hour.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {136--148},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1375581.1375599},
 doi = {http://doi.acm.org/10.1145/1375581.1375599},
 acmid = {1375599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, sat, sketching, spin, synthesis},
} 

@article{Solar-Lezama:2008:SCD:1379022.1375599,
 author = {Solar-Lezama, Armando and Jones, Christopher Grant and Bodik, Rastislav},
 title = {Sketching concurrent data structures},
 abstract = {We describe PSketch, a program synthesizer that helps programmers implement concurrent data structures. The system is based on the concept of sketching, a form of synthesis that allows programmers to express their insight about an implementation as a partial program: a sketch. The synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria. PSketch is based on a new counterexample-guided inductive synthesis algorithm (CEGIS) that generalizes the original sketch synthesis algorithm from Solar-Lezama et.al. to cope efficiently with concurrent programs. The new algorithm produces a correct implementation by iteratively generating candidate implementations, running them through a verifier, and if they fail, learning from the counterexample traces to produce a better candidate; converging to a solution in a handful of iterations. PSketch also extends Sketch with higher-level sketching constructs that allow the programmer to express her insight as a "soup" of ingredients from which complicated code fragments must be assembled. Such sketches can be viewed as syntactic descriptions of huge spaces of candidate programs (over 10<sup>8</sup> candidates for some sketches we resolved). We have used the PSketch system to implement several classes of concurrent data structures, including lock-free queues and concurrent sets with fine-grained locking. We have also sketched some other concurrent objects including a sense-reversing barrier and a protocol for the dining philosophers problem; all these sketches resolved in under an hour.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {136--148},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1379022.1375599},
 doi = {http://doi.acm.org/10.1145/1379022.1375599},
 acmid = {1375599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, sat, sketching, spin, synthesis},
} 

@article{Anderson:2008:SCD:1379022.1375600,
 author = {Anderson, Zachary and Gay, David and Ennals, Rob and Brewer, Eric},
 title = {SharC: checking data sharing strategies for multithreaded c},
 abstract = {Unintended or unmediated data sharing is a frequent cause of insidious bugs in multithreaded programs. We present a tool called SharC (short for Sharing Checker</i>) that allows a user to write lightweight annotations to declare how they believe objects are being shared between threads in their program. SharC uses a combination of static and dynamic analyses to check that the program conforms to this specification. SharC allows any type to have one of five "sharing modes" -- private to the current thread, read-only, shared under the control of a specified lock, intentionally racy, or checked dynamically. The dynamic mode uses run-time checking to verify that objects are either read-only, or only accessed by one thread. This allows us to check programs that would be difficult to check with a purely static system. If the user does not give a type an explicit annotation, then SharC uses a static type-qualifier analysis to infer that it is either private or should be checked dynamically. SharC allows objects to move between different sharing modes at runtime by using reference counting to check that there are no other references to the objects when they change mode. SharC's baseline dynamic analysis can check any C program, but is slow, and will generate false warnings about intentional data sharing. As the user adds more annotations, false warnings are reduced, and performance improves.We have found in practice that very few annotations are needed to describe all sharing and give reasonable performance. We ran SharC on 6 legacy C programs, summing to over 600k lines of code, and found that a total of only 60 simple annotations were needed to remove all false positives and to reduce performance overhead to only 2-14\%.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {149--158},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375600},
 doi = {http://doi.acm.org/10.1145/1379022.1375600},
 acmid = {1375600},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-race},
} 

@inproceedings{Anderson:2008:SCD:1375581.1375600,
 author = {Anderson, Zachary and Gay, David and Ennals, Rob and Brewer, Eric},
 title = {SharC: checking data sharing strategies for multithreaded c},
 abstract = {Unintended or unmediated data sharing is a frequent cause of insidious bugs in multithreaded programs. We present a tool called SharC (short for Sharing Checker</i>) that allows a user to write lightweight annotations to declare how they believe objects are being shared between threads in their program. SharC uses a combination of static and dynamic analyses to check that the program conforms to this specification. SharC allows any type to have one of five "sharing modes" -- private to the current thread, read-only, shared under the control of a specified lock, intentionally racy, or checked dynamically. The dynamic mode uses run-time checking to verify that objects are either read-only, or only accessed by one thread. This allows us to check programs that would be difficult to check with a purely static system. If the user does not give a type an explicit annotation, then SharC uses a static type-qualifier analysis to infer that it is either private or should be checked dynamically. SharC allows objects to move between different sharing modes at runtime by using reference counting to check that there are no other references to the objects when they change mode. SharC's baseline dynamic analysis can check any C program, but is slow, and will generate false warnings about intentional data sharing. As the user adds more annotations, false warnings are reduced, and performance improves.We have found in practice that very few annotations are needed to describe all sharing and give reasonable performance. We ran SharC on 6 legacy C programs, summing to over 600k lines of code, and found that a total of only 60 simple annotations were needed to remove all false positives and to reduce performance overhead to only 2-14\%.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {149--158},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375600},
 doi = {http://doi.acm.org/10.1145/1375581.1375600},
 acmid = {1375600},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-race},
} 

@article{Rondon:2008:LT:1379022.1375602,
 author = {Rondon, Patrick M. and Kawaguci, Ming and Jhala, Ranjit},
 title = {Liquid types},
 abstract = {We present Logically Qualified Data Types</i>, abbreviated to Liquid Types</i>, a system that combines Hindley-Milner</i> type inference with Predicate Abstraction</i> to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive run-time checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31\% of program text to under 1\%.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375602},
 doi = {http://doi.acm.org/10.1145/1379022.1375602},
 acmid = {1375602},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
} 

@inproceedings{Rondon:2008:LT:1375581.1375602,
 author = {Rondon, Patrick M. and Kawaguci, Ming and Jhala, Ranjit},
 title = {Liquid types},
 abstract = {We present Logically Qualified Data Types</i>, abbreviated to Liquid Types</i>, a system that combines Hindley-Milner</i> type inference with Predicate Abstraction</i> to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive run-time checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31\% of program text to under 1\%.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375602},
 doi = {http://doi.acm.org/10.1145/1375581.1375602},
 acmid = {1375602},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
} 

@article{Feng:2008:CLP:1379022.1375603,
 author = {Feng, Xinyu and Shao, Zhong and Dong, Yuan and Guo, Yu},
 title = {Certifying low-level programs with hardware interrupts and preemptive threads},
 abstract = {Hardware interrupts are widely used in the world's critical software systems to support preemptive threads, device drivers, operating system kernels, and hypervisors. Handling interrupts properly is an essential component of low-level system programming. Unfortunately, interrupts are also extremely hard to reason about: they dramatically alter the program control flow and complicate the invariants in low-level concurrent code (e.g., implementation of synchronization primitives). Existing formal verification techniques---including Hoare logic, typed assembly language, concurrent separation logic, and the assume-guarantee method---have consistently ignored the issues of interrupts; this severely limits the applicability and power of today's program verification systems. In this paper we present a novel Hoare-logic-like framework for certifying low-level system programs involving both hardware interrupts and preemptive threads. We show that enabling and disabling interrupts can be formalized precisely using simple ownership-transfer semantics, and the same technique also extends to the concurrent setting. By carefully reasoning about the interaction among interrupt handlers, context switching, and synchronization libraries, we are able to---for the first time---successfully certify a preemptive thread implementation and a large number of common synchronization primitives. Our work provides a foundation for reasoning about interrupt-based kernel programs and makes an important advance toward building fully certified operating system kernels and hypervisors.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {170--182},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1379022.1375603},
 doi = {http://doi.acm.org/10.1145/1379022.1375603},
 acmid = {1375603},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {certified system software, concurrency, hardware interrupts, preemptive threads, separation logic},
} 

@inproceedings{Feng:2008:CLP:1375581.1375603,
 author = {Feng, Xinyu and Shao, Zhong and Dong, Yuan and Guo, Yu},
 title = {Certifying low-level programs with hardware interrupts and preemptive threads},
 abstract = {Hardware interrupts are widely used in the world's critical software systems to support preemptive threads, device drivers, operating system kernels, and hypervisors. Handling interrupts properly is an essential component of low-level system programming. Unfortunately, interrupts are also extremely hard to reason about: they dramatically alter the program control flow and complicate the invariants in low-level concurrent code (e.g., implementation of synchronization primitives). Existing formal verification techniques---including Hoare logic, typed assembly language, concurrent separation logic, and the assume-guarantee method---have consistently ignored the issues of interrupts; this severely limits the applicability and power of today's program verification systems. In this paper we present a novel Hoare-logic-like framework for certifying low-level system programs involving both hardware interrupts and preemptive threads. We show that enabling and disabling interrupts can be formalized precisely using simple ownership-transfer semantics, and the same technique also extends to the concurrent setting. By carefully reasoning about the interaction among interrupt handlers, context switching, and synchronization libraries, we are able to---for the first time---successfully certify a preemptive thread implementation and a large number of common synchronization primitives. Our work provides a foundation for reasoning about interrupt-based kernel programs and makes an important advance toward building fully certified operating system kernels and hypervisors.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {170--182},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1375581.1375603},
 doi = {http://doi.acm.org/10.1145/1375581.1375603},
 acmid = {1375603},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {certified system software, concurrency, hardware interrupts, preemptive threads, separation logic},
} 

@article{Chen:2008:TCL:1379022.1375604,
 author = {Chen, Juan and Hawblitzel, Chris and Perry, Frances and Emmi, Mike and Condit, Jeremy and Coetzee, Derrick and Pratikaki, Polyvios},
 title = {Type-preserving compilation for large-scale optimizing object-oriented compilers},
 abstract = {Type-preserving compilers translate well-typed source code, such as Java or C#, into verifiable target code, such as typed assembly language or proof-carrying code. This paper presents the implementation of type-preserving compilation in a complex, large-scale optimizing compiler. Compared to prior work, this implementation supports extensive optimizations, and it verifies a large portion of the interface between the compiler and the runtime system. This paper demonstrates the practicality of type-preserving compilation in complex optimizing compilers: the generated typed assembly language is only 2.3\% slower than the base compiler's generated untyped assembly language, and the type-preserving compiler is 82.8\% slower than the base compiler.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {183--192},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375604},
 doi = {http://doi.acm.org/10.1145/1379022.1375604},
 acmid = {1375604},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {object-oriented compilers, type-preserving compilation},
} 

@inproceedings{Chen:2008:TCL:1375581.1375604,
 author = {Chen, Juan and Hawblitzel, Chris and Perry, Frances and Emmi, Mike and Condit, Jeremy and Coetzee, Derrick and Pratikaki, Polyvios},
 title = {Type-preserving compilation for large-scale optimizing object-oriented compilers},
 abstract = {Type-preserving compilers translate well-typed source code, such as Java or C#, into verifiable target code, such as typed assembly language or proof-carrying code. This paper presents the implementation of type-preserving compilation in a complex, large-scale optimizing compiler. Compared to prior work, this implementation supports extensive optimizations, and it verifies a large portion of the interface between the compiler and the runtime system. This paper demonstrates the practicality of type-preserving compilation in complex optimizing compilers: the generated typed assembly language is only 2.3\% slower than the base compiler's generated untyped assembly language, and the type-preserving compiler is 82.8\% slower than the base compiler.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {183--192},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375604},
 doi = {http://doi.acm.org/10.1145/1375581.1375604},
 acmid = {1375604},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {object-oriented compilers, type-preserving compilation},
} 

@inproceedings{McCamant:2008:QIF:1375581.1375606,
 author = {McCamant, Stephen and Ernst, Michael D.},
 title = {Quantitative information flow as network flow capacity},
 abstract = {We present a new technique for determining how much information about a program's secret inputs is revealed by its public outputs. In contrast to previous techniques based on reachability from secret inputs (tainting), it achieves a more precise quantitative result by computing a maximum flow of information between the inputs and outputs. The technique uses static control-flow regions to soundly account for implicit flows via branches and pointer operations, but operates dynamically by observing one or more program executions and giving numeric flow bounds specific to them (e.g., "17 bits"). The maximum flow in a network also gives a minimum cut (a set of edges that separate the secret input from the output), which can be used to efficiently check that the same policy is satisfied on future executions. We performed case studies on 5 real C, C++, and Objective C programs, 3 of which had more than 250K lines of code. The tool checked multiple security policies, including one that was violated by a previously unknown bug.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {193--205},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1375581.1375606},
 doi = {http://doi.acm.org/10.1145/1375581.1375606},
 acmid = {1375606},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, implicit flow, information-flow analysis},
} 

@article{McCamant:2008:QIF:1379022.1375606,
 author = {McCamant, Stephen and Ernst, Michael D.},
 title = {Quantitative information flow as network flow capacity},
 abstract = {We present a new technique for determining how much information about a program's secret inputs is revealed by its public outputs. In contrast to previous techniques based on reachability from secret inputs (tainting), it achieves a more precise quantitative result by computing a maximum flow of information between the inputs and outputs. The technique uses static control-flow regions to soundly account for implicit flows via branches and pointer operations, but operates dynamically by observing one or more program executions and giving numeric flow bounds specific to them (e.g., "17 bits"). The maximum flow in a network also gives a minimum cut (a set of edges that separate the secret input from the output), which can be used to efficiently check that the same policy is satisfied on future executions. We performed case studies on 5 real C, C++, and Objective C programs, 3 of which had more than 250K lines of code. The tool checked multiple security policies, including one that was violated by a previously unknown bug.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {193--205},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1379022.1375606},
 doi = {http://doi.acm.org/10.1145/1379022.1375606},
 acmid = {1375606},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, implicit flow, information-flow analysis},
} 

@article{Godefroid:2008:GWF:1379022.1375607,
 author = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
 title = {Grammar-based whitebox fuzzing},
 abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {206--215},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375607},
 doi = {http://doi.acm.org/10.1145/1379022.1375607},
 acmid = {1375607},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, grammars, program verification, software testing},
} 

@inproceedings{Godefroid:2008:GWF:1375581.1375607,
 author = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
 title = {Grammar-based whitebox fuzzing},
 abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {206--215},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375607},
 doi = {http://doi.acm.org/10.1145/1375581.1375607},
 acmid = {1375607},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, grammars, program verification, software testing},
} 

@article{Quintao Pereira:2008:RAP:1379022.1375609,
 author = {Quint\~{a}o Pereira, Fernando Magno and Palsberg, Jens},
 title = {Register allocation by puzzle solving},
 abstract = {We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {216--226},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375609},
 doi = {http://doi.acm.org/10.1145/1379022.1375609},
 acmid = {1375609},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {puzzle solving, register aliasing, register allocation},
} 

@inproceedings{Quintao Pereira:2008:RAP:1375581.1375609,
 author = {Quint\~{a}o Pereira, Fernando Magno and Palsberg, Jens},
 title = {Register allocation by puzzle solving},
 abstract = {We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {216--226},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375609},
 doi = {http://doi.acm.org/10.1145/1375581.1375609},
 acmid = {1375609},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {puzzle solving, register aliasing, register allocation},
} 

@inproceedings{Hack:2008:CCG:1375581.1375610,
 author = {Hack, Sebastian and Goos, Gerhard},
 title = {Copy coalescing by graph recoloring},
 abstract = {Register allocation is always a trade-off between live-range splitting and coalescing. Live-range splitting generally leads to less spilling at the cost of inserting shuffle code. Coalescing removes shuffle code while potentially raising the register demand and causing spilling. Recent research showed that the live-range splitting of the SSA form's \&#198;-functions leads to chordal</i> interference graphs. This improves upon two long-standing inconveniences of graph coloring register allocation: First, chordal graphs are optimally colorable in quadratic time. Second, the number of colors needed to color the graph is equal to the maximal register pressure in the program. However, the inserted shuffle code incurred by the \&#198;-functions can slow down the program severely. Hence, to make such an approach work in practice, a coalescing technique is needed that removes most of the shuffle code without</i> causing further spilling. In this paper, we present a coalescing technique designed for, but not limited to, SSA-form register allocation. We exploit that a valid coloring can be easily obtained by an SSA-based register allocator. This initial coloring is then improved by recoloring the interference graph and assigning shuffle-code related nodes the same color. Thereby, we always keep the coloring of the graph valid. Hence, the coalescing is safe, i. e. no spill code will be caused by coalescing. Comparing to iterated register coalescing, the state of the art in safe coalescing, our method is able to remove 22.5\% of the costs and 44.3\% of the copies iterated coalescing left over. The best solution possible, found by a colaescer using integer linear programming (ILP), was 35.9\% of the costs and 51.9\% of the copies iterated coalescing left over. The runtime of programs compiled with our heuristic matches that of the programs compiled with the ILP technique.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {227--237},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375610},
 doi = {http://doi.acm.org/10.1145/1375581.1375610},
 acmid = {1375610},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation, ssa form},
} 

@article{Hack:2008:CCG:1379022.1375610,
 author = {Hack, Sebastian and Goos, Gerhard},
 title = {Copy coalescing by graph recoloring},
 abstract = {Register allocation is always a trade-off between live-range splitting and coalescing. Live-range splitting generally leads to less spilling at the cost of inserting shuffle code. Coalescing removes shuffle code while potentially raising the register demand and causing spilling. Recent research showed that the live-range splitting of the SSA form's \&#198;-functions leads to chordal</i> interference graphs. This improves upon two long-standing inconveniences of graph coloring register allocation: First, chordal graphs are optimally colorable in quadratic time. Second, the number of colors needed to color the graph is equal to the maximal register pressure in the program. However, the inserted shuffle code incurred by the \&#198;-functions can slow down the program severely. Hence, to make such an approach work in practice, a coalescing technique is needed that removes most of the shuffle code without</i> causing further spilling. In this paper, we present a coalescing technique designed for, but not limited to, SSA-form register allocation. We exploit that a valid coloring can be easily obtained by an SSA-based register allocator. This initial coloring is then improved by recoloring the interference graph and assigning shuffle-code related nodes the same color. Thereby, we always keep the coloring of the graph valid. Hence, the coalescing is safe, i. e. no spill code will be caused by coalescing. Comparing to iterated register coalescing, the state of the art in safe coalescing, our method is able to remove 22.5\% of the costs and 44.3\% of the copies iterated coalescing left over. The best solution possible, found by a colaescer using integer linear programming (ILP), was 35.9\% of the costs and 51.9\% of the copies iterated coalescing left over. The runtime of programs compiled with our heuristic matches that of the programs compiled with the ILP technique.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {227--237},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375610},
 doi = {http://doi.acm.org/10.1145/1379022.1375610},
 acmid = {1375610},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation, ssa form},
} 

@inproceedings{Xin:2008:EPE:1375581.1375611,
 author = {Xin, Bin and Sumner, William N. and Zhang, Xiangyu},
 title = {Efficient program execution indexing},
 abstract = {Execution indexing uniquely identifies a point in an execution. Desirable execution indices reveal correlations between points in an execution and establish correspondence between points across multiple executions. Therefore, execution indexing is essential for a wide variety of dynamic program analyses, for example, it can be used to organize program profiles; it can precisely identify the point in a re-execution that corresponds to a given point in an original execution and thus facilitate debugging or dynamic instrumentation. In this paper, we formally define the concept of execution index and propose an indexing scheme based on execution structure and program state. We present a highly optimized online implementation of the technique. We also perform a client study, which targets producing a failure inducing schedule for a data race by verifying the two alternative happens-before orderings of a racing pair. Indexing is used to precisely locate corresponding points across multiple executions in the presence of non-determinism so that no heavyweight tracing/replay system is needed.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {238--248},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375611},
 doi = {http://doi.acm.org/10.1145/1375581.1375611},
 acmid = {1375611},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data race, execution alignment, execution indexing, semantic augmentation, structural indexing},
} 

@article{Xin:2008:EPE:1379022.1375611,
 author = {Xin, Bin and Sumner, William N. and Zhang, Xiangyu},
 title = {Efficient program execution indexing},
 abstract = {Execution indexing uniquely identifies a point in an execution. Desirable execution indices reveal correlations between points in an execution and establish correspondence between points across multiple executions. Therefore, execution indexing is essential for a wide variety of dynamic program analyses, for example, it can be used to organize program profiles; it can precisely identify the point in a re-execution that corresponds to a given point in an original execution and thus facilitate debugging or dynamic instrumentation. In this paper, we formally define the concept of execution index and propose an indexing scheme based on execution structure and program state. We present a highly optimized online implementation of the technique. We also perform a client study, which targets producing a failure inducing schedule for a data race by verifying the two alternative happens-before orderings of a racing pair. Indexing is used to precisely locate corresponding points across multiple executions in the presence of non-determinism so that no heavyweight tracing/replay system is needed.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {238--248},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375611},
 doi = {http://doi.acm.org/10.1145/1379022.1375611},
 acmid = {1375611},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data race, execution alignment, execution indexing, semantic augmentation, structural indexing},
} 

@article{Kahlon:2008:BTS:1379022.1375613,
 author = {Kahlon, Vineet},
 title = {Bootstrapping: a technique for scalable flow and context-sensitive pointer alias analysis},
 abstract = {We propose a framework for improving both the scalability as well as the accuracy of pointer alias analysis, irrespective of its flow or context-sensitivities, by leveraging a three-pronged strategy that effectively combines divide and conquer, parallelization and function summarization</i>. A key step in our approach is to first identify small subsets of pointers such that the problem of computing aliases of any pointer can be reduced to computing them in these small subsets instead of the entire program. In order to identify these subsets, we first apply a series of increasingly accurate but highly scalable (context and flow-insensitive) alias analyses in a cascaded fashion such that each analysis A<sub>i</sub></i> works on the subsets generated by the previous one A<sub>i-1</sub></i>. Restricting the application of A<sub>i</sub></i> to subsets generated by A<sub>i-1</sub></i>, instead of the entire program, improves it scalability, i.e., A<sub>i</sub></i> is bootstrapped</i> by A<sub>i-1</sub></i>. Once these small subsets have been computed, in order to make our overall analysis accurate, we employ our new summarization-based flow and context-sensitive alias analysis. The small size of each subset offsets the higher computational complexity of the context-sensitive analysis. An important feature of our framework is that the analysis for each of the subsets can be carried out independently of others thereby allowing us to leverage parallelization further improving scalability.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {249--259},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375613},
 doi = {http://doi.acm.org/10.1145/1379022.1375613},
 acmid = {1375613},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive analysis, demand-driven analysis, divide and conquer, steensgaard partitioning, summarization},
} 

@inproceedings{Kahlon:2008:BTS:1375581.1375613,
 author = {Kahlon, Vineet},
 title = {Bootstrapping: a technique for scalable flow and context-sensitive pointer alias analysis},
 abstract = {We propose a framework for improving both the scalability as well as the accuracy of pointer alias analysis, irrespective of its flow or context-sensitivities, by leveraging a three-pronged strategy that effectively combines divide and conquer, parallelization and function summarization</i>. A key step in our approach is to first identify small subsets of pointers such that the problem of computing aliases of any pointer can be reduced to computing them in these small subsets instead of the entire program. In order to identify these subsets, we first apply a series of increasingly accurate but highly scalable (context and flow-insensitive) alias analyses in a cascaded fashion such that each analysis A<sub>i</sub></i> works on the subsets generated by the previous one A<sub>i-1</sub></i>. Restricting the application of A<sub>i</sub></i> to subsets generated by A<sub>i-1</sub></i>, instead of the entire program, improves it scalability, i.e., A<sub>i</sub></i> is bootstrapped</i> by A<sub>i-1</sub></i>. Once these small subsets have been computed, in order to make our overall analysis accurate, we employ our new summarization-based flow and context-sensitive alias analysis. The small size of each subset offsets the higher computational complexity of the context-sensitive analysis. An important feature of our framework is that the analysis for each of the subsets can be carried out independently of others thereby allowing us to leverage parallelization further improving scalability.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {249--259},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375613},
 doi = {http://doi.acm.org/10.1145/1375581.1375613},
 acmid = {1375613},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive analysis, demand-driven analysis, divide and conquer, steensgaard partitioning, summarization},
} 

@article{von Dincklage:2008:EFP:1379022.1375614,
 author = {von Dincklage, Daniel and Diwan, Amer},
 title = {Explaining failures of program analyses},
 abstract = {With programs getting larger and often more complex with each new release, programmers need all the help they can get in understanding and transforming programs. Fortunately, modern development environments, such as Eclipse, incorporate tools for understanding, navigating, and transforming programs. These tools typically use program analyses to extract relevant properties of programs. These tools are often invaluable to developers; for example, many programmers use refactoring tools regularly. However, poor results by the underlying analyses can compromise a tool's usefulness. For example, a bug finding tool may produce too many false positives if the underlying analysis is overly conservative, and thus overwhelm the user with too many possible errors in the program. In such cases it would be invaluable for the tool to explain to the user why</i> it believes that each bug exists. Armed with this knowledge, the user can decide which bugs are worth pursing and which are false positives. The contributions of this paper are as follows: (i) We describe requirements on the structure of an analysis so that we can produce reasons when the analysis fails; the user of the analysis determines whether or not an analysis's results constitute failure. We also describe a simple language that enforces these requirements; (ii) We describe how to produce necessary and sufficient reasons for analysis failure; (iii) We evaluate our system with respect to a number of analyses and programs and find that most reasons are small (and thus usable) and that our system is fast enough for interactive use.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {260--269},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375614},
 doi = {http://doi.acm.org/10.1145/1379022.1375614},
 acmid = {1375614},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analyses, reasons},
} 

@inproceedings{von Dincklage:2008:EFP:1375581.1375614,
 author = {von Dincklage, Daniel and Diwan, Amer},
 title = {Explaining failures of program analyses},
 abstract = {With programs getting larger and often more complex with each new release, programmers need all the help they can get in understanding and transforming programs. Fortunately, modern development environments, such as Eclipse, incorporate tools for understanding, navigating, and transforming programs. These tools typically use program analyses to extract relevant properties of programs. These tools are often invaluable to developers; for example, many programmers use refactoring tools regularly. However, poor results by the underlying analyses can compromise a tool's usefulness. For example, a bug finding tool may produce too many false positives if the underlying analysis is overly conservative, and thus overwhelm the user with too many possible errors in the program. In such cases it would be invaluable for the tool to explain to the user why</i> it believes that each bug exists. Armed with this knowledge, the user can decide which bugs are worth pursing and which are false positives. The contributions of this paper are as follows: (i) We describe requirements on the structure of an analysis so that we can produce reasons when the analysis fails; the user of the analysis determines whether or not an analysis's results constitute failure. We also describe a simple language that enforces these requirements; (ii) We describe how to produce necessary and sufficient reasons for analysis failure; (iii) We evaluate our system with respect to a number of analyses and programs and find that most reasons are small (and thus usable) and that our system is fast enough for interactive use.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {260--269},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375614},
 doi = {http://doi.acm.org/10.1145/1375581.1375614},
 acmid = {1375614},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analyses, reasons},
} 

@article{Dillig:2008:SCS:1379022.1375615,
 author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex},
 title = {Sound, complete and scalable path-sensitive analysis},
 abstract = {We present a new, precise technique for fully path- and context-sensitive program analysis. Our technique exploits two observations: First, using quantified, recursive formulas, path- and context-sensitive conditions for many program properties can be expressed exactly. To compute a closed form solution to such recursive constraints, we differentiate between observable</i> and unobservable</i> variables, the latter of which are existentially quantified in our approach. Using the insight that unobservable variables can be eliminated outside a certain scope, our technique computes satisfiability- and validity-preserving closed-form solutions to the original recursive constraints. We prove the solution is as precise as the original system for answering may and must queries as well as being small in practice, allowing our technique to scale to the entire Linux kernel, a program with over 6 million lines of code.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375615},
 doi = {http://doi.acm.org/10.1145/1379022.1375615},
 acmid = {1375615},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {path- and context-sensitive analysis, static analysis, strongest necessary/weakest sufficient conditons},
} 

@inproceedings{Dillig:2008:SCS:1375581.1375615,
 author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex},
 title = {Sound, complete and scalable path-sensitive analysis},
 abstract = {We present a new, precise technique for fully path- and context-sensitive program analysis. Our technique exploits two observations: First, using quantified, recursive formulas, path- and context-sensitive conditions for many program properties can be expressed exactly. To compute a closed form solution to such recursive constraints, we differentiate between observable</i> and unobservable</i> variables, the latter of which are existentially quantified in our approach. Using the insight that unobservable variables can be eliminated outside a certain scope, our technique computes satisfiability- and validity-preserving closed-form solutions to the original recursive constraints. We prove the solution is as precise as the original system for answering may and must queries as well as being small in practice, allowing our technique to scale to the entire Linux kernel, a program with over 6 million lines of code.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375615},
 doi = {http://doi.acm.org/10.1145/1375581.1375615},
 acmid = {1375615},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {path- and context-sensitive analysis, static analysis, strongest necessary/weakest sufficient conditons},
} 

@article{Gulwani:2008:PAC:1379022.1375616,
 author = {Gulwani, Sumit and Srivastava, Saurabh and Venkatesan, Ramarathnam},
 title = {Program analysis as constraint solving},
 abstract = {A constraint-based approach to invariant generation in programs translates a program into constraints that are solved using off-the-shelf constraint solvers to yield desired program invariants. In this paper we show how the constraint-based approach can be used to model a wide spectrum of program analyses in an expressive domain containing disjunctions and conjunctions of linear inequalities. In particular, we show how to model the problem of context-sensitive interprocedural program verification. We also present the first constraint-based approach to weakest precondition and strongest postcondition inference. The constraints we generate are boolean combinations of quadratic inequalities over integer variables. We reduce these constraints to SAT formulae using bitvector modeling and use off-the-shelf SAT solvers to solve them. Furthermore, we present interesting applications of the above analyses, namely bounds analysis and generation of most-general counter-examples for both safety and termination properties. We also present encouraging preliminary experimental results demonstrating the feasibility of our technique on a variety of challenging examples.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1379022.1375616},
 doi = {http://doi.acm.org/10.1145/1379022.1375616},
 acmid = {1375616},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounds analysis, constraint solving, most-general counterexamples, non-termination analysis, program verification, strongest postcondition, weakest precondition},
} 

@inproceedings{Gulwani:2008:PAC:1375581.1375616,
 author = {Gulwani, Sumit and Srivastava, Saurabh and Venkatesan, Ramarathnam},
 title = {Program analysis as constraint solving},
 abstract = {A constraint-based approach to invariant generation in programs translates a program into constraints that are solved using off-the-shelf constraint solvers to yield desired program invariants. In this paper we show how the constraint-based approach can be used to model a wide spectrum of program analyses in an expressive domain containing disjunctions and conjunctions of linear inequalities. In particular, we show how to model the problem of context-sensitive interprocedural program verification. We also present the first constraint-based approach to weakest precondition and strongest postcondition inference. The constraints we generate are boolean combinations of quadratic inequalities over integer variables. We reduce these constraints to SAT formulae using bitvector modeling and use off-the-shelf SAT solvers to solve them. Furthermore, we present interesting applications of the above analyses, namely bounds analysis and generation of most-general counter-examples for both safety and termination properties. We also present encouraging preliminary experimental results demonstrating the feasibility of our technique on a variety of challenging examples.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375581.1375616},
 doi = {http://doi.acm.org/10.1145/1375581.1375616},
 acmid = {1375616},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounds analysis, constraint solving, most-general counterexamples, non-termination analysis, program verification, strongest postcondition, weakest precondition},
} 

@inproceedings{Flanagan:2008:VSC:1375581.1375618,
 author = {Flanagan, Cormac and Freund, Stephen N. and Yi, Jaeheon},
 title = {Velodrome: a sound and complete dynamic atomicity checker for multithreaded programs},
 abstract = {Atomicity is a fundamental correctness property in multithreaded programs, both because atomic code blocks are amenable to sequential reasoning (which significantly simplifies correctness arguments), and because atomicity violations often reveal defects in a program's synchronization structure. Unfortunately, all atomicity analyses developed to date are incomplete in that they may yield false alarms on correctly synchronized programs, which limits their usefulness. We present the first dynamic analysis for atomicity that is both sound and complete. The analysis reasons about the exact dependencies between operations in the observed trace of the target program, and it reports error messages if and only if the observed trace is not conflict-serializable. Despite this significant increase in precision, the performance and coverage of our analysis is competitive with earlier incomplete dynamic analyses for atomicity.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {293--303},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375618},
 doi = {http://doi.acm.org/10.1145/1375581.1375618},
 acmid = {1375618},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, dynamic analysis, serializability},
} 

@article{Flanagan:2008:VSC:1379022.1375618,
 author = {Flanagan, Cormac and Freund, Stephen N. and Yi, Jaeheon},
 title = {Velodrome: a sound and complete dynamic atomicity checker for multithreaded programs},
 abstract = {Atomicity is a fundamental correctness property in multithreaded programs, both because atomic code blocks are amenable to sequential reasoning (which significantly simplifies correctness arguments), and because atomicity violations often reveal defects in a program's synchronization structure. Unfortunately, all atomicity analyses developed to date are incomplete in that they may yield false alarms on correctly synchronized programs, which limits their usefulness. We present the first dynamic analysis for atomicity that is both sound and complete. The analysis reasons about the exact dependencies between operations in the observed trace of the target program, and it reports error messages if and only if the observed trace is not conflict-serializable. Despite this significant increase in precision, the performance and coverage of our analysis is competitive with earlier incomplete dynamic analyses for atomicity.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {293--303},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375618},
 doi = {http://doi.acm.org/10.1145/1379022.1375618},
 acmid = {1375618},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, dynamic analysis, serializability},
} 

@article{Cherem:2008:ILA:1379022.1375619,
 author = {Cherem, Sigmund and Chilimbi, Trishul and Gulwani, Sumit},
 title = {Inferring locks for atomic sections},
 abstract = {Atomic sections</i> are a recent and popular idiom to support the development of concurrent programs. Updates performed within an atomic section should not be visible to other threads until the atomic section has been executed entirely. Traditionally, atomic sections are supported through the use of optimistic concurrency, either using a transactional memory hardware, or an equivalent software emulation (STM). This paper explores automatically supporting atomic sections using pessimistic concurrency. We present a system that combines compiler and runtime techniques to automatically transform programs written with atomic sections into programs that only use locking primitives. To minimize contention in the transformed programs, our compiler chooses from several lock granularities, using fine-grain locks whenever it is possible. This paper formally presents our framework, shows that our compiler is sound (i.e., it protects all shared locations accessed within atomic sections), and reports experimental results.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {304--315},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1379022.1375619},
 doi = {http://doi.acm.org/10.1145/1379022.1375619},
 acmid = {1375619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomic sections, concurrency, static lock inference},
} 

@inproceedings{Cherem:2008:ILA:1375581.1375619,
 author = {Cherem, Sigmund and Chilimbi, Trishul and Gulwani, Sumit},
 title = {Inferring locks for atomic sections},
 abstract = {Atomic sections</i> are a recent and popular idiom to support the development of concurrent programs. Updates performed within an atomic section should not be visible to other threads until the atomic section has been executed entirely. Traditionally, atomic sections are supported through the use of optimistic concurrency, either using a transactional memory hardware, or an equivalent software emulation (STM). This paper explores automatically supporting atomic sections using pessimistic concurrency. We present a system that combines compiler and runtime techniques to automatically transform programs written with atomic sections into programs that only use locking primitives. To minimize contention in the transformed programs, our compiler chooses from several lock granularities, using fine-grain locks whenever it is possible. This paper formally presents our framework, shows that our compiler is sound (i.e., it protects all shared locations accessed within atomic sections), and reports experimental results.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {304--315},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375581.1375619},
 doi = {http://doi.acm.org/10.1145/1375581.1375619},
 acmid = {1375619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomic sections, concurrency, static lock inference},
} 

@inproceedings{Chugh:2008:DAC:1375581.1375620,
 author = {Chugh, Ravi and Voung, Jan W. and Jhala, Ranjit and Lerner, Sorin},
 title = {Dataflow analysis for concurrent programs using datarace detection},
 abstract = {Dataflow analyses for concurrent programs differ from their single-threaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise thread-interleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale. We present RADAR, a framework that automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. RADAR uses a race detection engine to kill the dataflow facts, generated and propagated by the sequential analysis, that become invalid due to concurrent writes. Our approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine. We describe the RADAR framework and its implementation using a pre-existing race detection engine. We show how RADAR was used to generate a concurrent version of a null-pointer dereference analysis, and we analyze the result of running the generated concurrent analysis on several benchmarks.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {316--326},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375620},
 doi = {http://doi.acm.org/10.1145/1375581.1375620},
 acmid = {1375620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interprocedural analysis, locksets, multithreaded programs, summaries},
} 

@article{Chugh:2008:DAC:1379022.1375620,
 author = {Chugh, Ravi and Voung, Jan W. and Jhala, Ranjit and Lerner, Sorin},
 title = {Dataflow analysis for concurrent programs using datarace detection},
 abstract = {Dataflow analyses for concurrent programs differ from their single-threaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise thread-interleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale. We present RADAR, a framework that automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. RADAR uses a race detection engine to kill the dataflow facts, generated and propagated by the sequential analysis, that become invalid due to concurrent writes. Our approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine. We describe the RADAR framework and its implementation using a pre-existing race detection engine. We show how RADAR was used to generate a concurrent version of a null-pointer dereference analysis, and we analyze the result of running the generated concurrent analysis on several benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {316--326},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375620},
 doi = {http://doi.acm.org/10.1145/1379022.1375620},
 acmid = {1375620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interprocedural analysis, locksets, multithreaded programs, summaries},
} 

@inproceedings{Wegiel:2008:XTT:1375581.1375621,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {XMem: type-safe, transparent, shared memory for cross-runtime communication and coordination},
 abstract = {Developers commonly build contemporary enterprise applications using type-safe, component-based platforms, such as J2EE, and architect them to comprise multiple tiers, such as a web container, application server, and database engine. Administrators increasingly execute each tier in its own managed runtime environment (MRE) to improve reliability and to manage system complexity through the fault containment and modularity offered by isolated MRE instances. Such isolation, however, necessitates expensive cross-tier communication based on protocols such as object serialization and remote procedure calls. Administrators commonly co-locate communicating MREs on a single host to reduce communication overhead and to better exploit increasing numbers of available processing cores. However, state-of-the-art MREs offer no support for more efficient communication between co-located MREs, while fast inter-process communication mechanisms, such as shared memory, are widely available as a standard operating system service on most modern platforms. To address this growing need, we present the design and implementation of XMem ? type-safe, transparent, shared memory support for co-located MREs. XMem guarantees type-safety through coordinated, parallel, multi-process class loading and garbage collection. To avoid introducing any level of indirection, XMem manipulates virtual memory mapping. In addition, object sharing in XMem is fully transparent: shared objects are identical to local objects in terms of field access, synchronization, garbage collection, and method invocation, with the only difference being that sharedto-private pointers are disallowed. XMem facilitates easy integration and use by existing communication technologies and software systems, such as RMI, JNDI, JDBC, serialization/XML, and network sockets. We have implemented XMem in the open-source, productionquality HotSpot Java Virtual Machine. Our experimental evaluation, based on core communication technologies underlying J2EE, as well as using open-source server applications, indicates that XMem significantly improves throughput and response time by avoiding the overheads imposed by object serialization and network communication.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {327--338},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375581.1375621},
 doi = {http://doi.acm.org/10.1145/1375581.1375621},
 acmid = {1375621},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class loading, garbage collection, interprocess communication, managed runtimes, parallel, shared memory, synchronization, transparent, type-safe},
} 

@article{Wegiel:2008:XTT:1379022.1375621,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {XMem: type-safe, transparent, shared memory for cross-runtime communication and coordination},
 abstract = {Developers commonly build contemporary enterprise applications using type-safe, component-based platforms, such as J2EE, and architect them to comprise multiple tiers, such as a web container, application server, and database engine. Administrators increasingly execute each tier in its own managed runtime environment (MRE) to improve reliability and to manage system complexity through the fault containment and modularity offered by isolated MRE instances. Such isolation, however, necessitates expensive cross-tier communication based on protocols such as object serialization and remote procedure calls. Administrators commonly co-locate communicating MREs on a single host to reduce communication overhead and to better exploit increasing numbers of available processing cores. However, state-of-the-art MREs offer no support for more efficient communication between co-located MREs, while fast inter-process communication mechanisms, such as shared memory, are widely available as a standard operating system service on most modern platforms. To address this growing need, we present the design and implementation of XMem ? type-safe, transparent, shared memory support for co-located MREs. XMem guarantees type-safety through coordinated, parallel, multi-process class loading and garbage collection. To avoid introducing any level of indirection, XMem manipulates virtual memory mapping. In addition, object sharing in XMem is fully transparent: shared objects are identical to local objects in terms of field access, synchronization, garbage collection, and method invocation, with the only difference being that sharedto-private pointers are disallowed. XMem facilitates easy integration and use by existing communication technologies and software systems, such as RMI, JNDI, JDBC, serialization/XML, and network sockets. We have implemented XMem in the open-source, productionquality HotSpot Java Virtual Machine. Our experimental evaluation, based on core communication technologies underlying J2EE, as well as using open-source server applications, indicates that XMem significantly improves throughput and response time by avoiding the overheads imposed by object serialization and network communication.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {327--338},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1379022.1375621},
 doi = {http://doi.acm.org/10.1145/1379022.1375621},
 acmid = {1375621},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class loading, garbage collection, interprocess communication, managed runtimes, parallel, shared memory, synchronization, transparent, type-safe},
} 

@inproceedings{Halbwachs:2008:DPA:1375581.1375623,
 author = {Halbwachs, Nicolas and P\'{e}ron, Mathias},
 title = {Discovering properties about arrays in simple programs},
 abstract = {Array bound checking and array dependency analysis (for parallelization) have been widely studied. However, there are much less results about analyzing properties of array contents</i>. In this paper, we propose a way of using abstract interpretation for discovering</i> properties about array contents in some restricted cases: one-dimensional arrays, traversed by simple "for" loops. The basic idea, borrowed from [GRS05], consists in partitioning arrays into symbolic intervals (e.g., [1,i</i> -- 1], [i</i>,i</i>], [i</i> + 1,n</i>]), and in associating with each such interval I</i> and each array A</i> an abstract variable A</i><sub>I</i></sub>; the new idea is to consider relational</i> abstract properties \&#968;(A</i><sub>I</i></sub>, B</i><sub>I</i></sub>, ...) about these abstract variables, and to interpret such a property pointwise on the interval I</i>: \&#8704;l</i> \&#8712; I</i>, \&#968;(A</i>[l</i>], B</i>[l</i>],...). The abstract semantics of our simple programs according to these abstract properties has been defined and implemented in a prototype tool. The method is able, for instance, to discover that the result of an insertion sort is a sorted array, or that, in an array traversal guarded by a "sentinel", the index stays within the bounds.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {339--348},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375623},
 doi = {http://doi.acm.org/10.1145/1375581.1375623},
 acmid = {1375623},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {"sentinel", abstract interpretation, arrays, invariant synthesis, program verification, sorting algorithms},
} 

@article{Halbwachs:2008:DPA:1379022.1375623,
 author = {Halbwachs, Nicolas and P\'{e}ron, Mathias},
 title = {Discovering properties about arrays in simple programs},
 abstract = {Array bound checking and array dependency analysis (for parallelization) have been widely studied. However, there are much less results about analyzing properties of array contents</i>. In this paper, we propose a way of using abstract interpretation for discovering</i> properties about array contents in some restricted cases: one-dimensional arrays, traversed by simple "for" loops. The basic idea, borrowed from [GRS05], consists in partitioning arrays into symbolic intervals (e.g., [1,i</i> -- 1], [i</i>,i</i>], [i</i> + 1,n</i>]), and in associating with each such interval I</i> and each array A</i> an abstract variable A</i><sub>I</i></sub>; the new idea is to consider relational</i> abstract properties \&#968;(A</i><sub>I</i></sub>, B</i><sub>I</i></sub>, ...) about these abstract variables, and to interpret such a property pointwise on the interval I</i>: \&#8704;l</i> \&#8712; I</i>, \&#968;(A</i>[l</i>], B</i>[l</i>],...). The abstract semantics of our simple programs according to these abstract properties has been defined and implemented in a prototype tool. The method is able, for instance, to discover that the result of an insertion sort is a sorted array, or that, in an array traversal guarded by a "sentinel", the index stays within the bounds.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {339--348},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375623},
 doi = {http://doi.acm.org/10.1145/1379022.1375623},
 acmid = {1375623},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {"sentinel", abstract interpretation, arrays, invariant synthesis, program verification, sorting algorithms},
} 

@inproceedings{Zee:2008:FFV:1375581.1375624,
 author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin},
 title = {Full functional verification of linked data structures},
 abstract = {We present the first verification of full functional correctness</i> for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higher-order logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions. Our Jahob verification system uses integrated reasoning</i> to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, first-order theorem provers, and, in the worst case, interactive theorem provers to prove each subformula. Techniques such as replacing complex subformulas with stronger but simpler alternatives, exploiting structure inherently present in the verification conditions, and, when necessary, inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into a single powerful integrated reasoning system. By appropriately applying multiple proof techniques to discharge different subformulas, this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {349--361},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1375581.1375624},
 doi = {http://doi.acm.org/10.1145/1375581.1375624},
 acmid = {1375624},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data structure, decision procedure, java, theorem prover, verification},
} 

@article{Zee:2008:FFV:1379022.1375624,
 author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin},
 title = {Full functional verification of linked data structures},
 abstract = {We present the first verification of full functional correctness</i> for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higher-order logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions. Our Jahob verification system uses integrated reasoning</i> to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, first-order theorem provers, and, in the worst case, interactive theorem provers to prove each subformula. Techniques such as replacing complex subformulas with stronger but simpler alternatives, exploiting structure inherently present in the verification conditions, and, when necessary, inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into a single powerful integrated reasoning system. By appropriately applying multiple proof techniques to discharge different subformulas, this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {349--361},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1379022.1375624},
 doi = {http://doi.acm.org/10.1145/1379022.1375624},
 acmid = {1375624},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data structure, decision procedure, java, theorem prover, verification},
} 

@article{Musuvathi:2008:FSM:1379022.1375625,
 author = {Musuvathi, Madanlal and Qadeer, Shaz},
 title = {Fair stateless model checking},
 abstract = {Stateless model checking is a useful state-space exploration technique for systematically testing complex real-world software. Existing stateless model checkers are limited to the verification of safety properties on terminating programs. However, realistic concurrent programs are nonterminating, a property that significantly reduces the efficacy of stateless model checking in testing them. Moreover, existing stateless model checkers are unable to verify that a nonterminating program satisfies the important liveness property of livelock-freedom, a property that requires the program to make continuous progress for any input. To address these shortcomings, this paper argues for incorporating a fair scheduler in stateless exploration. The key contribution of this paper is an explicit scheduler that is (strongly) fair and at the same time sufficiently nondeterministic to guarantee full coverage of safety properties.We have implemented the fair scheduler in the CHESS model checker. We show through theoretical arguments and empirical evaluation that our algorithm satisfies two important properties: 1) it visits all states of a finite-state program achieving state coverage at a faster rate than existing techniques, and 2) it finds all livelocks in a finite-state program. Before this work, nonterminating programs had to be manually modified in order to apply CHESS to them. The addition of fairness has allowed CHESS to be effectively applied to real-world nonterminating programs without any modification. For example, we have successfully booted the Singularity operating system under the control of CHESS.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {362--371},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1379022.1375625},
 doi = {http://doi.acm.org/10.1145/1379022.1375625},
 acmid = {1375625},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, fairness, liveness, model checking, multi-threading, shared-memory programs, software testing},
} 

@inproceedings{Musuvathi:2008:FSM:1375581.1375625,
 author = {Musuvathi, Madanlal and Qadeer, Shaz},
 title = {Fair stateless model checking},
 abstract = {Stateless model checking is a useful state-space exploration technique for systematically testing complex real-world software. Existing stateless model checkers are limited to the verification of safety properties on terminating programs. However, realistic concurrent programs are nonterminating, a property that significantly reduces the efficacy of stateless model checking in testing them. Moreover, existing stateless model checkers are unable to verify that a nonterminating program satisfies the important liveness property of livelock-freedom, a property that requires the program to make continuous progress for any input. To address these shortcomings, this paper argues for incorporating a fair scheduler in stateless exploration. The key contribution of this paper is an explicit scheduler that is (strongly) fair and at the same time sufficiently nondeterministic to guarantee full coverage of safety properties.We have implemented the fair scheduler in the CHESS model checker. We show through theoretical arguments and empirical evaluation that our algorithm satisfies two important properties: 1) it visits all states of a finite-state program achieving state coverage at a faster rate than existing techniques, and 2) it finds all livelocks in a finite-state program. Before this work, nonterminating programs had to be manually modified in order to apply CHESS to them. The addition of fairness has allowed CHESS to be effectively applied to real-world nonterminating programs without any modification. For example, we have successfully booted the Singularity operating system under the control of CHESS.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {362--371},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375581.1375625},
 doi = {http://doi.acm.org/10.1145/1375581.1375625},
 acmid = {1375625},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, fairness, liveness, model checking, multi-threading, shared-memory programs, software testing},
} 

@inproceedings{Guerraoui:2008:MCT:1375581.1375626,
 author = {Guerraoui, Rachid and Henzinger, Thomas A. and Jobstmann, Barbara and Singh, Vasu},
 title = {Model checking transactional memories},
 abstract = {Model checking software transactional memories (STMs) is difficult because of the unbounded number, length, and delay of concurrent transactions and the unbounded size of the memory. We show that, under certain conditions, the verification problem can be reduced to a finite-state problem, and we illustrate the use of the method by proving the correctness of several STMs, including two-phase locking, DSTM, TL2, and optimistic concurrency control. The safety properties we consider include strict serializability and opacity; the liveness properties include obstruction freedom, livelock freedom, and wait freedom. Our main contribution lies in the structure of the proofs, which are largely automated and not restricted to the STMs mentioned above. In a first step we show that every STM that enjoys certain structural properties either violates a safety or liveness requirement on some program with two threads and two shared variables, or satisfies the requirement on all programs. In the second step we use a model checker to prove the requirement for the STM applied to a most general program with two threads and two variables. In the safety case, the model checker constructs a simulation relation between two carefully constructed finite-state transition systems, one representing the given STM applied to a most general program, and the other representing a most liberal safe STM applied to the same program. In the liveness case, the model checker analyzes fairness conditions on the given STM transition system.},
 booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '08},
 year = {2008},
 isbn = {978-1-59593-860-2},
 location = {Tucson, AZ, USA},
 pages = {372--382},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1375581.1375626},
 doi = {http://doi.acm.org/10.1145/1375581.1375626},
 acmid = {1375626},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {model checking, transactional memories},
} 

@article{Guerraoui:2008:MCT:1379022.1375626,
 author = {Guerraoui, Rachid and Henzinger, Thomas A. and Jobstmann, Barbara and Singh, Vasu},
 title = {Model checking transactional memories},
 abstract = {Model checking software transactional memories (STMs) is difficult because of the unbounded number, length, and delay of concurrent transactions and the unbounded size of the memory. We show that, under certain conditions, the verification problem can be reduced to a finite-state problem, and we illustrate the use of the method by proving the correctness of several STMs, including two-phase locking, DSTM, TL2, and optimistic concurrency control. The safety properties we consider include strict serializability and opacity; the liveness properties include obstruction freedom, livelock freedom, and wait freedom. Our main contribution lies in the structure of the proofs, which are largely automated and not restricted to the STMs mentioned above. In a first step we show that every STM that enjoys certain structural properties either violates a safety or liveness requirement on some program with two threads and two shared variables, or satisfies the requirement on all programs. In the second step we use a model checker to prove the requirement for the STM applied to a most general program with two threads and two variables. In the safety case, the model checker constructs a simulation relation between two carefully constructed finite-state transition systems, one representing the given STM applied to a most general program, and the other representing a most liberal safe STM applied to the same program. In the liveness case, the model checker analyzes fairness conditions on the given STM transition system.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {6},
 month = {June},
 year = {2008},
 issn = {0362-1340},
 pages = {372--382},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1379022.1375626},
 doi = {http://doi.acm.org/10.1145/1379022.1375626},
 acmid = {1375626},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {model checking, transactional memories},
} 

@inproceedings{Subramanian:2009:DSU:1542476.1542478,
 author = {Subramanian, Suriya and Hicks, Michael and McKinley, Kathryn S.},
 title = {Dynamic software updates: a VM-centric approach},
 abstract = {Software evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of J<sc>volve</sc>, a DSU-enhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, just-in-time compilation, scheduling, return barriers, on-stack replacement, and garbage collection. J<sc>volve</sc>, is safe</i>: its use of bytecode verification and VM thread synchronization ensures that an update will always produce type-correct executions. Jvolve is flexible</i>: it can support 20 of 22 updates to three open-source programs--Jetty web server, JavaEmailServer, and CrossFTP server--based on actual releases occurring over 1 to 2 years. Jvolve is efficient</i>: performance experiments show that incurs no overhead</i> during steady-state execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542478},
 doi = {http://doi.acm.org/10.1145/1542476.1542478},
 acmid = {1542478},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, garbage collection, virtual machine technology},
} 

@article{Subramanian:2009:DSU:1543135.1542478,
 author = {Subramanian, Suriya and Hicks, Michael and McKinley, Kathryn S.},
 title = {Dynamic software updates: a VM-centric approach},
 abstract = {Software evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of J<sc>volve</sc>, a DSU-enhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, just-in-time compilation, scheduling, return barriers, on-stack replacement, and garbage collection. J<sc>volve</sc>, is safe</i>: its use of bytecode verification and VM thread synchronization ensures that an update will always produce type-correct executions. Jvolve is flexible</i>: it can support 20 of 22 updates to three open-source programs--Jetty web server, JavaEmailServer, and CrossFTP server--based on actual releases occurring over 1 to 2 years. Jvolve is efficient</i>: performance experiments show that incurs no overhead</i> during steady-state execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542478},
 doi = {http://doi.acm.org/10.1145/1543135.1542478},
 acmid = {1542478},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, garbage collection, virtual machine technology},
} 

@article{Neamtiu:2009:STU:1543135.1542479,
 author = {Neamtiu, Iulian and Hicks, Michael},
 title = {Safe and timely updates to multi-threaded programs},
 abstract = {Many dynamic updating systems have been developed that enable a program to be patched while it runs, to fix bugs or add new features. This paper explores techniques for supporting dynamic updates to multi-threaded programs, focusing on the problem of applying an update in a timely fashion while still producing correct behavior. Past work has shown that this tension of safety</i> versus timeliness can be balanced for single-threaded programs. For multi-threaded programs, the task is more difficult because myriad thread interactions complicate understanding the possible program states to which a patch could be applied. Our approach allows the programmer to specify a few program points (e.g., one per thread) at which a patch may be applied, which simplifies reasoning about safety. To improve timeliness, a combination of static analysis and run-time support automatically expands these few points to many more that produce behavior equivalent to the originals. Experiments with thirteen realistic updates to three multi-threaded servers show that we can safely perform a dynamic update within milliseconds when more straightforward alternatives would delay some updates indefinitely.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542479},
 doi = {http://doi.acm.org/10.1145/1543135.1542479},
 acmid = {1542479},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, multi-threading, update safety, update timeliness},
} 

@inproceedings{Neamtiu:2009:STU:1542476.1542479,
 author = {Neamtiu, Iulian and Hicks, Michael},
 title = {Safe and timely updates to multi-threaded programs},
 abstract = {Many dynamic updating systems have been developed that enable a program to be patched while it runs, to fix bugs or add new features. This paper explores techniques for supporting dynamic updates to multi-threaded programs, focusing on the problem of applying an update in a timely fashion while still producing correct behavior. Past work has shown that this tension of safety</i> versus timeliness can be balanced for single-threaded programs. For multi-threaded programs, the task is more difficult because myriad thread interactions complicate understanding the possible program states to which a patch could be applied. Our approach allows the programmer to specify a few program points (e.g., one per thread) at which a patch may be applied, which simplifies reasoning about safety. To improve timeliness, a combination of static analysis and run-time support automatically expands these few points to many more that produce behavior equivalent to the originals. Experiments with thirteen realistic updates to three multi-threaded servers show that we can safely perform a dynamic update within milliseconds when more straightforward alternatives would delay some updates indefinitely.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542479},
 doi = {http://doi.acm.org/10.1145/1542476.1542479},
 acmid = {1542479},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, multi-threading, update safety, update timeliness},
} 

@inproceedings{Hammer:2009:CCL:1542476.1542480,
 author = {Hammer, Matthew A. and Acar, Umut A. and Chen, Yan},
 title = {CEAL: a C-based language for self-adjusting computation},
 abstract = {Self-adjusting computation offers a language-centric approach to writing programs that can automatically respond to modifications to their data (e.g., inputs). Except for several domain-specific implementations, however, all previous implementations of self-adjusting computation assume mostly functional, higher-order languages such as Standard ML. Prior to this work, it was not known if self-adjusting computation can be made to work with low-level, imperative languages such as C without placing undue burden on the programmer. We describe the design and implementation of CEAL: a C-based language for self-adjusting computation. The language is fully general and extends C with a small number of primitives to enable writing self-adjusting programs in a style similar to conventional C programs. We present efficient compilation techniques for translating CEAL programs into C that can be compiled with existing C compilers using primitives supplied by a run-time library for self-adjusting computation. We implement the proposed compiler and evaluate its effectiveness. Our experiments show that CEAL is effective in practice: compiled self-adjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down a from-scratch run by a moderate constant factor. Compared to previous work, we measure significant space and time improvements.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {25--37},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1542476.1542480},
 doi = {http://doi.acm.org/10.1145/1542476.1542480},
 acmid = {1542480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation, control and data flow, dominators, performance, self-adjusting computation, tail calls, trampolines},
} 

@article{Hammer:2009:CCL:1543135.1542480,
 author = {Hammer, Matthew A. and Acar, Umut A. and Chen, Yan},
 title = {CEAL: a C-based language for self-adjusting computation},
 abstract = {Self-adjusting computation offers a language-centric approach to writing programs that can automatically respond to modifications to their data (e.g., inputs). Except for several domain-specific implementations, however, all previous implementations of self-adjusting computation assume mostly functional, higher-order languages such as Standard ML. Prior to this work, it was not known if self-adjusting computation can be made to work with low-level, imperative languages such as C without placing undue burden on the programmer. We describe the design and implementation of CEAL: a C-based language for self-adjusting computation. The language is fully general and extends C with a small number of primitives to enable writing self-adjusting programs in a style similar to conventional C programs. We present efficient compilation techniques for translating CEAL programs into C that can be compiled with existing C compilers using primitives supplied by a run-time library for self-adjusting computation. We implement the proposed compiler and evaluate its effectiveness. Our experiments show that CEAL is effective in practice: compiled self-adjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down a from-scratch run by a moderate constant factor. Compared to previous work, we measure significant space and time improvements.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {25--37},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1543135.1542480},
 doi = {http://doi.acm.org/10.1145/1543135.1542480},
 acmid = {1542480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation, control and data flow, dominators, performance, self-adjusting computation, tail calls, trampolines},
} 

@article{Ansel:2009:PLC:1543135.1542481,
 author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
 title = {PetaBricks: a language and compiler for algorithmic choice},
 abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542481},
 doi = {http://doi.acm.org/10.1145/1543135.1542481},
 acmid = {1542481},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, algorithmic choice, autotuning, compiler, implicitly parallel, language},
} 

@inproceedings{Ansel:2009:PLC:1542476.1542481,
 author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
 title = {PetaBricks: a language and compiler for algorithmic choice},
 abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542481},
 doi = {http://doi.acm.org/10.1145/1542476.1542481},
 acmid = {1542481},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, algorithmic choice, autotuning, compiler, implicitly parallel, language},
} 

@article{Chugh:2009:SIF:1543135.1542483,
 author = {Chugh, Ravi and Meister, Jeffrey A. and Jhala, Ranjit and Lerner, Sorin},
 title = {Staged information flow for javascript},
 abstract = {Modern websites are powered by JavaScript, a flexible dynamic scripting language that executes in client browsers. A common paradigm in such websites is to include third-party JavaScript code in the form of libraries or advertisements. If this code were malicious, it could read sensitive information from the page or write to the location bar, thus redirecting the user to a malicious page, from which the entire machine could be compromised. We present an information-flow based approach for inferring the effects that a piece of JavaScript has on the website in order to ensure that key security properties are not violated. To handle dynamically loaded and generated JavaScript, we propose a framework for staging information flow properties. Our framework propagates information flow through the currently known code in order to compute a minimal set of syntactic residual checks that are performed on the remaining code when it is dynamically loaded. We have implemented a prototype framework for staging information flow. We describe our techniques for handling some difficult features of JavaScript and evaluate our system's performance on a variety of large real-world websites. Our experiments show that static information flow is feasible and efficient for JavaScript, and that our technique allows the enforcement of information-flow policies with almost no run-time overhead.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {50--62},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1543135.1542483},
 doi = {http://doi.acm.org/10.1145/1543135.1542483},
 acmid = {1542483},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidentiality, flow analysis, integrity, set constraints, web applications},
} 

@inproceedings{Chugh:2009:SIF:1542476.1542483,
 author = {Chugh, Ravi and Meister, Jeffrey A. and Jhala, Ranjit and Lerner, Sorin},
 title = {Staged information flow for javascript},
 abstract = {Modern websites are powered by JavaScript, a flexible dynamic scripting language that executes in client browsers. A common paradigm in such websites is to include third-party JavaScript code in the form of libraries or advertisements. If this code were malicious, it could read sensitive information from the page or write to the location bar, thus redirecting the user to a malicious page, from which the entire machine could be compromised. We present an information-flow based approach for inferring the effects that a piece of JavaScript has on the website in order to ensure that key security properties are not violated. To handle dynamically loaded and generated JavaScript, we propose a framework for staging information flow properties. Our framework propagates information flow through the currently known code in order to compute a minimal set of syntactic residual checks that are performed on the remaining code when it is dynamically loaded. We have implemented a prototype framework for staging information flow. We describe our techniques for handling some difficult features of JavaScript and evaluate our system's performance on a variety of large real-world websites. Our experiments show that static information flow is feasible and efficient for JavaScript, and that our technique allows the enforcement of information-flow policies with almost no run-time overhead.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {50--62},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1542476.1542483},
 doi = {http://doi.acm.org/10.1145/1542476.1542483},
 acmid = {1542483},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidentiality, flow analysis, integrity, set constraints, web applications},
} 

@inproceedings{Roy:2009:LPF:1542476.1542484,
 author = {Roy, Indrajit and Porter, Donald E. and Bond, Michael D. and McKinley, Kathryn S. and Witchel, Emmett},
 title = {Laminar: practical fine-grained decentralized information flow control},
 abstract = {Decentralized information flow control (DIFC) is a promising model for writing programs with powerful, end-to-end security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: language-level and operating system-level DIFC. Language level solutions provide no guarantees against security violations on system resources, like files and sockets. Operating system solutions can mediate accesses to system resources, but are inefficient at monitoring the flow of information through fine-grained program data structures. This paper describes Laminar, the first system to implement decentralized information flow control using a single set of abstractions for OS resources and heap-allocated objects. Programmers express security policies by labeling data with secrecy and integrity labels, and then access the labeled data in lexically scoped security regions. Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using a modified Java virtual machine and a new Linux security module. This paper shows that security regions ease incremental deployment and limit dynamic security checks, allowing us to retrofit DIFC policies on four application case studies. Replacing the applications' ad-hoc security policies changes less than 10\% of the code, and incurs performance overheads from 1\% to 56\%. Whereas prior DIFC systems only support limited types of multithreaded programs, Laminar supports a more general class of multithreaded DIFC programs that can access heterogeneously labeled data.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542484},
 doi = {http://doi.acm.org/10.1145/1542476.1542484},
 acmid = {1542484},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {information flow control, java virtual machine, operating systems, security region},
} 

@article{Roy:2009:LPF:1543135.1542484,
 author = {Roy, Indrajit and Porter, Donald E. and Bond, Michael D. and McKinley, Kathryn S. and Witchel, Emmett},
 title = {Laminar: practical fine-grained decentralized information flow control},
 abstract = {Decentralized information flow control (DIFC) is a promising model for writing programs with powerful, end-to-end security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: language-level and operating system-level DIFC. Language level solutions provide no guarantees against security violations on system resources, like files and sockets. Operating system solutions can mediate accesses to system resources, but are inefficient at monitoring the flow of information through fine-grained program data structures. This paper describes Laminar, the first system to implement decentralized information flow control using a single set of abstractions for OS resources and heap-allocated objects. Programmers express security policies by labeling data with secrecy and integrity labels, and then access the labeled data in lexically scoped security regions. Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using a modified Java virtual machine and a new Linux security module. This paper shows that security regions ease incremental deployment and limit dynamic security checks, allowing us to retrofit DIFC policies on four application case studies. Replacing the applications' ad-hoc security policies changes less than 10\% of the code, and incurs performance overheads from 1\% to 56\%. Whereas prior DIFC systems only support limited types of multithreaded programs, Laminar supports a more general class of multithreaded DIFC programs that can access heterogeneously labeled data.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542484},
 doi = {http://doi.acm.org/10.1145/1543135.1542484},
 acmid = {1542484},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {information flow control, java virtual machine, operating systems, security region},
} 

@inproceedings{Livshits:2009:MSI:1542476.1542485,
 author = {Livshits, Benjamin and Nori, Aditya V. and Rajamani, Sriram K. and Banerjee, Anindya},
 title = {Merlin: specification inference for explicit information flow problems},
 abstract = {The last several years have seen a proliferation of static and runtime analysis tools for finding security violations that are caused by explicit information flow</i> in programs. Much of this interest has been caused by the increase in the number of vulnerabilities such as cross-site scripting and SQL injection. In fact, these explicit information flow vulnerabilities commonly found in Web applications now outnumber vulnerabilities such as buffer overruns common in type-unsafe languages such as C and C++. Tools checking for these vulnerabilities require a specification to operate. In most cases the task of providing such a specification is delegated to the user. Moreover, the efficacy of these tools is only as good as the specification. Unfortunately, writing a comprehensive specification presents a major challenge: parts of the specification are easy to miss, leading to missed vulnerabilities; similarly, incorrect specifications may lead to false positives. This paper proposes Merlin, a new approach for automatically inferring explicit information flow specifications from program code. Such specifications greatly reduce manual labor, and enhance the quality of results, while using tools that check for security violations caused by explicit information flow. Beginning with a data propagation graph, which represents interprocedural flow of information in the program, Merlin aims to automatically infer an information flow specification. Merlin models information flow paths in the propagation graph using probabilistic constraints. A naive modeling requires an exponential number of constraints, one per path in the propagation graph. For scalability, we approximate these path constraints using constraints on chosen triples of nodes, resulting in a cubic number of constraints. We characterize this approximation as a probabilistic abstraction, using the theory of probabilistic refinement developed by McIver and Morgan. We solve the resulting system of probabilistic constraints using factor graphs, which are a well-known structure for performing probabilistic inference. We experimentally validate the Merlin approach by applying it to 10 large business-critical Web applications that have been analyzed with CAT.NET, a state-of-the-art static analysis tool for .NET. We find a total of 167 new confirmed specifications, which result in a total of 322 additional</i> vulnerabilities across the 10 benchmarks. More accurate specifications also reduce the false positive rate: in our experiments, Merlin-inferred specifications result in 13 false positives being removed; this constitutes a 15\% reduction in the CAT.NET false positive rate on these 10 programs. The final false positive rate for CAT.NET after applying Merlin in our experiments drops to under 1\%.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542485},
 doi = {http://doi.acm.org/10.1145/1542476.1542485},
 acmid = {1542485},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {security analysis tools, specification inference},
} 

@article{Livshits:2009:MSI:1543135.1542485,
 author = {Livshits, Benjamin and Nori, Aditya V. and Rajamani, Sriram K. and Banerjee, Anindya},
 title = {Merlin: specification inference for explicit information flow problems},
 abstract = {The last several years have seen a proliferation of static and runtime analysis tools for finding security violations that are caused by explicit information flow</i> in programs. Much of this interest has been caused by the increase in the number of vulnerabilities such as cross-site scripting and SQL injection. In fact, these explicit information flow vulnerabilities commonly found in Web applications now outnumber vulnerabilities such as buffer overruns common in type-unsafe languages such as C and C++. Tools checking for these vulnerabilities require a specification to operate. In most cases the task of providing such a specification is delegated to the user. Moreover, the efficacy of these tools is only as good as the specification. Unfortunately, writing a comprehensive specification presents a major challenge: parts of the specification are easy to miss, leading to missed vulnerabilities; similarly, incorrect specifications may lead to false positives. This paper proposes Merlin, a new approach for automatically inferring explicit information flow specifications from program code. Such specifications greatly reduce manual labor, and enhance the quality of results, while using tools that check for security violations caused by explicit information flow. Beginning with a data propagation graph, which represents interprocedural flow of information in the program, Merlin aims to automatically infer an information flow specification. Merlin models information flow paths in the propagation graph using probabilistic constraints. A naive modeling requires an exponential number of constraints, one per path in the propagation graph. For scalability, we approximate these path constraints using constraints on chosen triples of nodes, resulting in a cubic number of constraints. We characterize this approximation as a probabilistic abstraction, using the theory of probabilistic refinement developed by McIver and Morgan. We solve the resulting system of probabilistic constraints using factor graphs, which are a well-known structure for performing probabilistic inference. We experimentally validate the Merlin approach by applying it to 10 large business-critical Web applications that have been analyzed with CAT.NET, a state-of-the-art static analysis tool for .NET. We find a total of 167 new confirmed specifications, which result in a total of 322 additional</i> vulnerabilities across the 10 benchmarks. More accurate specifications also reduce the false positive rate: in our experiments, Merlin-inferred specifications result in 13 false positives being removed; this constitutes a 15\% reduction in the CAT.NET false positive rate on these 10 programs. The final false positive rate for CAT.NET after applying Merlin in our experiments drops to under 1\%.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542485},
 doi = {http://doi.acm.org/10.1145/1543135.1542485},
 acmid = {1542485},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {security analysis tools, specification inference},
} 

@inproceedings{Tripp:2009:TET:1542476.1542486,
 author = {Tripp, Omer and Pistoia, Marco and Fink, Stephen J. and Sridharan, Manu and Weisman, Omri},
 title = {TAJ: effective taint analysis of web applications},
 abstract = {Taint analysis, a form of information-flow analysis, establishes whether values from untrusted methods and parameters may flow into security-sensitive operations. Taint analysis can detect many common vulnerabilities in Web applications, and so has attracted much attention from both the research community and industry. However, most static taint-analysis tools do not address critical requirements for an industrial-strength tool. Specifically, an industrial-strength tool must scale to large industrial Web applications, model essential Web-application code artifacts, and generate consumable reports for a wide range of attack vectors. We have designed and implemented a static Taint Analysis for Java (TAJ) that meets the requirements of industry-level applications. TAJ can analyze applications of virtually any size, as it employs a set of techniques designed to produce useful answers given limited time and space. TAJ addresses a wide variety of attack vectors, with techniques to handle reflective calls, flow through containers, nested taint, and issues in generating useful reports. This paper provides a description of the algorithms comprising TAJ, evaluates TAJ against production-level benchmarks, and compares it with alternative solutions.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {87--97},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542486},
 doi = {http://doi.acm.org/10.1145/1542476.1542486},
 acmid = {1542486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, program analysis, security, slicing, static analysis, taint analysis, web application},
} 

@article{Tripp:2009:TET:1543135.1542486,
 author = {Tripp, Omer and Pistoia, Marco and Fink, Stephen J. and Sridharan, Manu and Weisman, Omri},
 title = {TAJ: effective taint analysis of web applications},
 abstract = {Taint analysis, a form of information-flow analysis, establishes whether values from untrusted methods and parameters may flow into security-sensitive operations. Taint analysis can detect many common vulnerabilities in Web applications, and so has attracted much attention from both the research community and industry. However, most static taint-analysis tools do not address critical requirements for an industrial-strength tool. Specifically, an industrial-strength tool must scale to large industrial Web applications, model essential Web-application code artifacts, and generate consumable reports for a wide range of attack vectors. We have designed and implemented a static Taint Analysis for Java (TAJ) that meets the requirements of industry-level applications. TAJ can analyze applications of virtually any size, as it employs a set of techniques designed to produce useful answers given limited time and space. TAJ addresses a wide variety of attack vectors, with techniques to handle reflective calls, flow through containers, nested taint, and issues in generating useful reports. This paper provides a description of the algorithms comprising TAJ, evaluates TAJ against production-level benchmarks, and compares it with alternative solutions.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {87--97},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542486},
 doi = {http://doi.acm.org/10.1145/1543135.1542486},
 acmid = {1542486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, program analysis, security, slicing, static analysis, taint analysis, web application},
} 

@article{Anderson:2009:LAC:1543135.1542488,
 author = {Anderson, Zachary R. and Gay, David and Naik, Mayur},
 title = {Lightweight annotations for controlling sharing in concurrent data structures},
 abstract = {SharC is a recently developed system for checking data-sharing in multithreaded programs. Programmers specify sharing rules (read-only, protected by a lock, etc.) for individual objects, and the SharC compiler enforces these rules using static and dynamic checks. Violations of these rules indicate unintended data sharing, which is the underlying cause of harmful data-races. Additionally, SharC allows programmers to change the sharing rules for a specific object using a sharing cast</i>, to capture the fact that sharing rules for an object often change during the object's lifetime. SharC was successfully applied to a number of multi-threaded C programs. However, many programs are not readily checkable using SharC because their sharing rules, and changes to sharing rules, effectively apply to whole data structures rather than to individual objects. We have developed a system called Shoal</i> to address this shortcoming. In addition to the sharing rules and sharing cast of SharC, our system includes a new concept that we call groups</i>. A group is a collection of objects all having the same sharing mode. Each group has a distinguished member called the group leader</i>. When the sharing mode of the group leader changes by way of a sharing cast, the sharing mode of all members of the group also changes. This operation is made sound by maintaining the invariant that at the point of a sharing cast, the only external pointer into the group is the pointer to the group leader. The addition of groups allows checking safe concurrency at the level of data structures rather than at the level of individual objects. We demonstrate the necessity and practicality of groups by applying Shoal to a wide range of concurrent C programs (the largest approaching a million lines of code). In all benchmarks groups entail low annotation burden and no significant additional performance overhead.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542488},
 doi = {http://doi.acm.org/10.1145/1543135.1542488},
 acmid = {1542488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent programming, data races, multithreaded programming},
} 

@inproceedings{Anderson:2009:LAC:1542476.1542488,
 author = {Anderson, Zachary R. and Gay, David and Naik, Mayur},
 title = {Lightweight annotations for controlling sharing in concurrent data structures},
 abstract = {SharC is a recently developed system for checking data-sharing in multithreaded programs. Programmers specify sharing rules (read-only, protected by a lock, etc.) for individual objects, and the SharC compiler enforces these rules using static and dynamic checks. Violations of these rules indicate unintended data sharing, which is the underlying cause of harmful data-races. Additionally, SharC allows programmers to change the sharing rules for a specific object using a sharing cast</i>, to capture the fact that sharing rules for an object often change during the object's lifetime. SharC was successfully applied to a number of multi-threaded C programs. However, many programs are not readily checkable using SharC because their sharing rules, and changes to sharing rules, effectively apply to whole data structures rather than to individual objects. We have developed a system called Shoal</i> to address this shortcoming. In addition to the sharing rules and sharing cast of SharC, our system includes a new concept that we call groups</i>. A group is a collection of objects all having the same sharing mode. Each group has a distinguished member called the group leader</i>. When the sharing mode of the group leader changes by way of a sharing cast, the sharing mode of all members of the group also changes. This operation is made sound by maintaining the invariant that at the point of a sharing cast, the only external pointer into the group is the pointer to the group leader. The addition of groups allows checking safe concurrency at the level of data structures rather than at the level of individual objects. We demonstrate the necessity and practicality of groups by applying Shoal to a wide range of concurrent C programs (the largest approaching a million lines of code). In all benchmarks groups entail low annotation burden and no significant additional performance overhead.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542488},
 doi = {http://doi.acm.org/10.1145/1542476.1542488},
 acmid = {1542488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent programming, data races, multithreaded programming},
} 

@article{Joshi:2009:RDP:1543135.1542489,
 author = {Joshi, Pallavi and Park, Chang-Seo and Sen, Koushik and Naik, Mayur},
 title = {A randomized dynamic program analysis technique for detecting real deadlocks},
 abstract = {We present a novel dynamic analysis technique that finds real deadlocks in multi-threaded programs. Our technique runs in two stages. In the first stage, we use an imprecise dynamic analysis technique to find potential deadlocks in a multi-threaded program by observing an execution of the program. In the second stage, we control a random thread scheduler to create the potential deadlocks with high probability. Unlike other dynamic analysis techniques, our approach has the advantage that it does not give any false warnings. We have implemented the technique in a prototype tool for Java, and have experimented on a number of large multi-threaded Java programs. We report a number of previously known and unknown real deadlocks that were found in these benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {110--120},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542489},
 doi = {http://doi.acm.org/10.1145/1543135.1542489},
 acmid = {1542489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active testing, concurrency, deadlock detection, dynamic program analysis},
} 

@inproceedings{Joshi:2009:RDP:1542476.1542489,
 author = {Joshi, Pallavi and Park, Chang-Seo and Sen, Koushik and Naik, Mayur},
 title = {A randomized dynamic program analysis technique for detecting real deadlocks},
 abstract = {We present a novel dynamic analysis technique that finds real deadlocks in multi-threaded programs. Our technique runs in two stages. In the first stage, we use an imprecise dynamic analysis technique to find potential deadlocks in a multi-threaded program by observing an execution of the program. In the second stage, we control a random thread scheduler to create the potential deadlocks with high probability. Unlike other dynamic analysis techniques, our approach has the advantage that it does not give any false warnings. We have implemented the technique in a prototype tool for Java, and have experimented on a number of large multi-threaded Java programs. We report a number of previously known and unknown real deadlocks that were found in these benchmarks.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {110--120},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542489},
 doi = {http://doi.acm.org/10.1145/1542476.1542489},
 acmid = {1542489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active testing, concurrency, deadlock detection, dynamic program analysis},
} 

@inproceedings{Flanagan:2009:FEP:1542476.1542490,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {FastTrack: efficient and precise dynamic race detection},
 abstract = {\beginabstract Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads. This paper exploits the insight that the full generality of vector clocks is unnecessary in most cases. That is, we can replace heavyweight vector clocks with an adaptive lightweight representation that, for almost all operations of the target program, requires only constant space and supports constant-time operations. This representation change significantly improves time and space performance, with no loss in precision. Experimental results on Java benchmarks including the Eclipse development environment show that our <sc>FastTrack</sc> race detector is an order of magnitude faster than a traditional vector-clock race detector, and roughly twice as fast as the high-performance DJIT+ algorithm. <sc>FastTrack</sc> is even comparable in speed to <sc>Eraser</sc> on our Java benchmarks, while never reporting false alarms.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {121--133},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1542476.1542490},
 doi = {http://doi.acm.org/10.1145/1542476.1542490},
 acmid = {1542490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race conditions},
} 

@article{Flanagan:2009:FEP:1543135.1542490,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {FastTrack: efficient and precise dynamic race detection},
 abstract = {\beginabstract Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads. This paper exploits the insight that the full generality of vector clocks is unnecessary in most cases. That is, we can replace heavyweight vector clocks with an adaptive lightweight representation that, for almost all operations of the target program, requires only constant space and supports constant-time operations. This representation change significantly improves time and space performance, with no loss in precision. Experimental results on Java benchmarks including the Eclipse development environment show that our <sc>FastTrack</sc> race detector is an order of magnitude faster than a traditional vector-clock race detector, and roughly twice as fast as the high-performance DJIT+ algorithm. <sc>FastTrack</sc> is even comparable in speed to <sc>Eraser</sc> on our Java benchmarks, while never reporting false alarms.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {121--133},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1543135.1542490},
 doi = {http://doi.acm.org/10.1145/1543135.1542490},
 acmid = {1542490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race conditions},
} 

@article{Marino:2009:LES:1543135.1542491,
 author = {Marino, Daniel and Musuvathi, Madanlal and Narayanasamy, Satish},
 title = {LiteRace: effective sampling for lightweight data-race detection},
 abstract = {Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude. In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70\% of data races by sampling less than 2\% of memory accesses in a given program execution.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1543135.1542491},
 doi = {http://doi.acm.org/10.1145/1543135.1542491},
 acmid = {1542491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, dynamic data race detection, sampling},
} 

@inproceedings{Marino:2009:LES:1542476.1542491,
 author = {Marino, Daniel and Musuvathi, Madanlal and Narayanasamy, Satish},
 title = {LiteRace: effective sampling for lightweight data-race detection},
 abstract = {Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude. In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70\% of data races by sampling less than 2\% of memory accesses in a given program execution.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1542476.1542491},
 doi = {http://doi.acm.org/10.1145/1542476.1542491},
 acmid = {1542491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, dynamic data race detection, sampling},
} 

@inproceedings{Petrank:2009:PGP:1542476.1542493,
 author = {Petrank, Erez and Musuvathi, Madanlal and Steesngaard, Bjarne},
 title = {Progress guarantee for parallel programs via bounded lock-freedom},
 abstract = {Parallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lock-freedom, wait-freedom, and obstruction-freedom have been proposed in the literature to provide various levels of progress guarantees. In this paper we formalize the notions of progress guarantees using linear temporal logic (LTL). We concentrate on lock-freedom and propose a variant of it denoted bounded lock-freedom</i>, which is more suitable for guaranteeing progress in practical systems. We use this formal definition to build a tool that checks if a concurrent program is bounded lock-free for a given bound. We then study the interaction between programs with progress guarantees and the underlying system (e.g., compilers, runtimes, operating systems, and hardware platforms). We propose a means to argue that an underlying system supports lock-freedom. A composition theorem asserts that bounded lock-free algorithms running on bounded lock-free supporting systems retain bounded lock-freedom for the composed execution.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {144--154},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542493},
 doi = {http://doi.acm.org/10.1145/1542476.1542493},
 acmid = {1542493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounded lock-freedom, lock-freedom, model checking, parallel computation, progress guarantees},
} 

@article{Petrank:2009:PGP:1543135.1542493,
 author = {Petrank, Erez and Musuvathi, Madanlal and Steesngaard, Bjarne},
 title = {Progress guarantee for parallel programs via bounded lock-freedom},
 abstract = {Parallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lock-freedom, wait-freedom, and obstruction-freedom have been proposed in the literature to provide various levels of progress guarantees. In this paper we formalize the notions of progress guarantees using linear temporal logic (LTL). We concentrate on lock-freedom and propose a variant of it denoted bounded lock-freedom</i>, which is more suitable for guaranteeing progress in practical systems. We use this formal definition to build a tool that checks if a concurrent program is bounded lock-free for a given bound. We then study the interaction between programs with progress guarantees and the underlying system (e.g., compilers, runtimes, operating systems, and hardware platforms). We propose a means to argue that an underlying system supports lock-freedom. A composition theorem asserts that bounded lock-free algorithms running on bounded lock-free supporting systems retain bounded lock-freedom for the composed execution.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {144--154},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542493},
 doi = {http://doi.acm.org/10.1145/1543135.1542493},
 acmid = {1542493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounded lock-freedom, lock-freedom, model checking, parallel computation, progress guarantees},
} 

@article{Dragojevic:2009:STM:1543135.1542494,
 author = {Dragojevi\'{c}, Aleksandar and Guerraoui, Rachid and Kapalka, Michal},
 title = {Stretching transactional memory},
 abstract = {Transactional memory (TM) is an appealing abstraction for programming multi-core systems. Potential target applications for TM, such as business software and video games, are likely to involve complex data structures and large transactions, requiring specific software solutions (STM). So far, however, STMs have been mainly evaluated and optimized for smaller scale benchmarks. We revisit the main STM design choices from the perspective of complex workloads and propose a new STM, which we call SwissTM. In short, SwissTM is lock- and word-based and uses (1) optimistic (commit-time) conflict detection for read/write conflicts and pessimistic (encounter-time) conflict detection for write/write conflicts, as well as (2) a new two-phase contention manager that ensures the progress of long transactions while inducing no overhead on short ones. SwissTM outperforms state-of-the-art STM implementations, namely RSTM, TL2, and TinySTM, in our experiments on STMBench7, STAMP, Lee-TM and red-black tree benchmarks. Beyond SwissTM, we present the most complete evaluation to date of the individual impact of various STM design choices on the ability to support the mixed workloads of large applications.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542494},
 doi = {http://doi.acm.org/10.1145/1543135.1542494},
 acmid = {1542494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarks, software transactional memories},
} 

@inproceedings{Dragojevic:2009:STM:1542476.1542494,
 author = {Dragojevi\'{c}, Aleksandar and Guerraoui, Rachid and Kapalka, Michal},
 title = {Stretching transactional memory},
 abstract = {Transactional memory (TM) is an appealing abstraction for programming multi-core systems. Potential target applications for TM, such as business software and video games, are likely to involve complex data structures and large transactions, requiring specific software solutions (STM). So far, however, STMs have been mainly evaluated and optimized for smaller scale benchmarks. We revisit the main STM design choices from the perspective of complex workloads and propose a new STM, which we call SwissTM. In short, SwissTM is lock- and word-based and uses (1) optimistic (commit-time) conflict detection for read/write conflicts and pessimistic (encounter-time) conflict detection for write/write conflicts, as well as (2) a new two-phase contention manager that ensures the progress of long transactions while inducing no overhead on short ones. SwissTM outperforms state-of-the-art STM implementations, namely RSTM, TL2, and TinySTM, in our experiments on STMBench7, STAMP, Lee-TM and red-black tree benchmarks. Beyond SwissTM, we present the most complete evaluation to date of the individual impact of various STM design choices on the ability to support the mixed workloads of large applications.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542494},
 doi = {http://doi.acm.org/10.1145/1542476.1542494},
 acmid = {1542494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarks, software transactional memories},
} 

@article{Mehrara:2009:PSA:1543135.1542495,
 author = {Mehrara, Mojtaba and Hao, Jeff and Hsu, Po-Chun and Mahlke, Scott},
 title = {Parallelizing sequential applications on commodity hardware using a low-cost software transactional memory},
 abstract = {Multicore designs have emerged as the mainstream design paradigm for the microprocessor industry. Unfortunately, providing multiple cores does not directly translate into performance for most applications. The industry has already fallen short of the decades-old performance trend of doubling performance every 18 months. An attractive approach for exploiting multiple cores is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. However, despite decades of research on automatic parallelization, most techniques are only effective in the scientific and data parallel domains where array dominated codes can be precisely analyzed by the compiler. Thread-level speculation offers the opportunity to expand parallelization to general-purpose programs, but at the cost of expensive hardware support. In this paper, we focus on providing low-overhead software support for exploiting speculative parallelism. We propose STMlite, a light-weight software transactional memory model that is customized to facilitate profile-guided automatic loop parallelization. STMlite eliminates a considerable amount of checking and locking overhead in conventional software transactional memory models by decoupling the commit phase from main transaction execution. Further, strong atomicity requirements for generic transactional memories are unnecessary within a stylized automatic parallelization framework. STMlite enables sequential applications to extract meaningful performance gains on commodity multicore hardware.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {166--176},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542495},
 doi = {http://doi.acm.org/10.1145/1543135.1542495},
 acmid = {1542495},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, loop level parallelism, profile-guided optimization, software transactional memory, thread-level speculation},
} 

@inproceedings{Mehrara:2009:PSA:1542476.1542495,
 author = {Mehrara, Mojtaba and Hao, Jeff and Hsu, Po-Chun and Mahlke, Scott},
 title = {Parallelizing sequential applications on commodity hardware using a low-cost software transactional memory},
 abstract = {Multicore designs have emerged as the mainstream design paradigm for the microprocessor industry. Unfortunately, providing multiple cores does not directly translate into performance for most applications. The industry has already fallen short of the decades-old performance trend of doubling performance every 18 months. An attractive approach for exploiting multiple cores is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. However, despite decades of research on automatic parallelization, most techniques are only effective in the scientific and data parallel domains where array dominated codes can be precisely analyzed by the compiler. Thread-level speculation offers the opportunity to expand parallelization to general-purpose programs, but at the cost of expensive hardware support. In this paper, we focus on providing low-overhead software support for exploiting speculative parallelism. We propose STMlite, a light-weight software transactional memory model that is customized to facilitate profile-guided automatic loop parallelization. STMlite eliminates a considerable amount of checking and locking overhead in conventional software transactional memory models by decoupling the commit phase from main transaction execution. Further, strong atomicity requirements for generic transactional memories are unnecessary within a stylized automatic parallelization framework. STMlite enables sequential applications to extract meaningful performance gains on commodity multicore hardware.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {166--176},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542495},
 doi = {http://doi.acm.org/10.1145/1542476.1542495},
 acmid = {1542495},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, loop level parallelism, profile-guided optimization, software transactional memory, thread-level speculation},
} 

@article{Tournavitis:2009:THA:1543135.1542496,
 author = {Tournavitis, Georgios and Wang, Zheng and Franke, Bj\"{o}rn and O'Boyle, Michael F.P.},
 title = {Towards a holistic approach to auto-parallelization: integrating profile-driven parallelism detection and machine-learning based mapping},
 abstract = {Compiler-based auto-parallelization is a much studied area, yet has still not found wide-spread application. This is largely due to the poor exploitation of application parallelism, subsequently resulting in performance levels far below those which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach, resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection we overcome the limitations of static analysis, enabling us to identify more application parallelism and only rely on the user for final approval. In addition, we replace the traditional target-specific and inflexible mapping heuristics with a machine-learning based prediction mechanism, resulting in better mapping decisions while providing more scope for adaptation to different target architectures. We have evaluated our parallelization strategy against the NAS and SPEC OMP benchmarks and two different multi-core platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers, but comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96\% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning based parallelization for complex multi-core platforms.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {177--187},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542496},
 doi = {http://doi.acm.org/10.1145/1543135.1542496},
 acmid = {1542496},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {auto-parallelization, machine-learning based parallelism mapping, openmp, profile-driven parallelism detection},
} 

@inproceedings{Tournavitis:2009:THA:1542476.1542496,
 author = {Tournavitis, Georgios and Wang, Zheng and Franke, Bj\"{o}rn and O'Boyle, Michael F.P.},
 title = {Towards a holistic approach to auto-parallelization: integrating profile-driven parallelism detection and machine-learning based mapping},
 abstract = {Compiler-based auto-parallelization is a much studied area, yet has still not found wide-spread application. This is largely due to the poor exploitation of application parallelism, subsequently resulting in performance levels far below those which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach, resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection we overcome the limitations of static analysis, enabling us to identify more application parallelism and only rely on the user for final approval. In addition, we replace the traditional target-specific and inflexible mapping heuristics with a machine-learning based prediction mechanism, resulting in better mapping decisions while providing more scope for adaptation to different target architectures. We have evaluated our parallelization strategy against the NAS and SPEC OMP benchmarks and two different multi-core platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers, but comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96\% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning based parallelization for complex multi-core platforms.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {177--187},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542496},
 doi = {http://doi.acm.org/10.1145/1542476.1542496},
 acmid = {1542496},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {auto-parallelization, machine-learning based parallelism mapping, openmp, profile-driven parallelism detection},
} 

@inproceedings{Hooimeijer:2009:DPS:1542476.1542498,
 author = {Hooimeijer, Pieter and Weimer, Westley},
 title = {A decision procedure for subset constraints over regular languages},
 abstract = {Reasoning about string variables, in particular program inputs, is an important aspect of many program analyses and testing frameworks. Program inputs invariably arrive as strings, and are often manipulated using high-level string operations such as equality checks, regular expression matching, and string concatenation. It is difficult to reason about these operations because they are not well-integrated into current constraint solvers. We present a decision procedure that solves systems of equations over regular language variables. Given such a system of constraints, our algorithm finds satisfying assignments for the variables in the system. We define this problem formally and render a mechanized correctness proof of the core of the algorithm. We evaluate its scalability and practical utility by applying it to the problem of automatically finding inputs that cause SQL injection vulnerabilities.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {188--198},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542498},
 doi = {http://doi.acm.org/10.1145/1542476.1542498},
 acmid = {1542498},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decision procedure, regular language},
} 

@article{Hooimeijer:2009:DPS:1543135.1542498,
 author = {Hooimeijer, Pieter and Weimer, Westley},
 title = {A decision procedure for subset constraints over regular languages},
 abstract = {Reasoning about string variables, in particular program inputs, is an important aspect of many program analyses and testing frameworks. Program inputs invariably arrive as strings, and are often manipulated using high-level string operations such as equality checks, regular expression matching, and string concatenation. It is difficult to reason about these operations because they are not well-integrated into current constraint solvers. We present a decision procedure that solves systems of equations over regular language variables. Given such a system of constraints, our algorithm finds satisfying assignments for the variables in the system. We define this problem formally and render a mechanized correctness proof of the core of the algorithm. We evaluate its scalability and practical utility by applying it to the problem of automatically finding inputs that cause SQL injection vulnerabilities.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {188--198},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542498},
 doi = {http://doi.acm.org/10.1145/1543135.1542498},
 acmid = {1542498},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decision procedure, regular language},
} 

@inproceedings{Schwerdfeger:2009:VCD:1542476.1542499,
 author = {Schwerdfeger, August C. and Van Wyk, Eric R.},
 title = {Verifiable composition of deterministic grammars},
 abstract = {There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542499},
 doi = {http://doi.acm.org/10.1145/1542476.1542499},
 acmid = {1542499},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-aware scanning, extensible languages, grammar composition, language composition, lr parsing},
} 

@article{Schwerdfeger:2009:VCD:1543135.1542499,
 author = {Schwerdfeger, August C. and Van Wyk, Eric R.},
 title = {Verifiable composition of deterministic grammars},
 abstract = {There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542499},
 doi = {http://doi.acm.org/10.1145/1543135.1542499},
 acmid = {1542499},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-aware scanning, extensible languages, grammar composition, language composition, lr parsing},
} 

@article{La Torre:2009:ARP:1543135.1542500,
 author = {La Torre, Salvatore and Parthasarathy, Madhusudan and Parlato, Gennaro},
 title = {Analyzing recursive programs using a fixed-point calculus},
 abstract = {We show that recursive programs where variables range over finite domains can be effectively and efficiently analyzed by describing the analysis algorithm using a formula in a fixed-point calculus. In contrast with programming in traditional languages, a fixed-point calculus serves as a high-level programming language to easily, correctly, and succinctly describe model-checking algorithms While there have been declarative high-level formalisms that have been proposed earlier for analysis problems (e.g., Datalog the fixed-point calculus we propose has the salient feature that it also allows algorithmic</i> aspects to be specified. We exhibit two classes of algorithms of symbolic (BDD-based) algorithms written using this framework-- one for checking for errors in sequential recursive Boolean programs, and the other to check for errors reachable within a bounded number of context-switches in a concurrent recursive Boolean program. Our formalization of these otherwise complex algorithms is extremely simple, and spans just a page of fixed-point formulae. Moreover, we implement these algorithms in a tool called <sc>Getafix</sc> which expresses algorithms as fixed-point formulae and evaluates them efficiently using a symbolic fixed-point solver called <sc>Mucke</sc>. The resulting model-checking tools are surprisingly efficient and are competitive in performance with mature existing tools that have been fine-tuned for these problems.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542500},
 doi = {http://doi.acm.org/10.1145/1543135.1542500},
 acmid = {1542500},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstraction, logic, model-checking, mu-calculus, recursive systems, software verification},
} 

@inproceedings{La Torre:2009:ARP:1542476.1542500,
 author = {La Torre, Salvatore and Parthasarathy, Madhusudan and Parlato, Gennaro},
 title = {Analyzing recursive programs using a fixed-point calculus},
 abstract = {We show that recursive programs where variables range over finite domains can be effectively and efficiently analyzed by describing the analysis algorithm using a formula in a fixed-point calculus. In contrast with programming in traditional languages, a fixed-point calculus serves as a high-level programming language to easily, correctly, and succinctly describe model-checking algorithms While there have been declarative high-level formalisms that have been proposed earlier for analysis problems (e.g., Datalog the fixed-point calculus we propose has the salient feature that it also allows algorithmic</i> aspects to be specified. We exhibit two classes of algorithms of symbolic (BDD-based) algorithms written using this framework-- one for checking for errors in sequential recursive Boolean programs, and the other to check for errors reachable within a bounded number of context-switches in a concurrent recursive Boolean program. Our formalization of these otherwise complex algorithms is extremely simple, and spans just a page of fixed-point formulae. Moreover, we implement these algorithms in a tool called <sc>Getafix</sc> which expresses algorithms as fixed-point formulae and evaluates them efficiently using a symbolic fixed-point solver called <sc>Mucke</sc>. The resulting model-checking tools are surprisingly efficient and are competitive in performance with mature existing tools that have been fine-tuned for these problems.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542500},
 doi = {http://doi.acm.org/10.1145/1542476.1542500},
 acmid = {1542500},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstraction, logic, model-checking, mu-calculus, recursive systems, software verification},
} 

@inproceedings{Srivastava:2009:PVU:1542476.1542501,
 author = {Srivastava, Saurabh and Gulwani, Sumit},
 title = {Program verification using templates over predicate abstraction},
 abstract = {We address the problem of automatically generating invariants with quantified and boolean structure for proving the validity of given assertions or generating pre-conditions under which the assertions are valid. We present three novel algorithms, having different strengths, that combine template and predicate abstraction based formalisms to discover required sophisticated program invariants using SMT solvers. Two of these algorithms use an iterative approach to compute fixed-points (one computes a least fixed-point and the other computes a greatest fixed-point), while the third algorithm uses a constraint based approach to encode the fixed-point. The key idea in all these algorithms is to reduce the problem of invariant discovery to that of finding optimal</i> solutions for unknowns (over conjunctions of some predicates from a given set) in a template formula such that the formula is valid. Preliminary experiments using our implementation of these algorithms show encouraging results over a benchmark of small but complicated programs. Our algorithms can verify program properties that, to our knowledge, have not been automatically verified before. In particular, our algorithms can generate full correctness proofs for sorting algorithms (which requires nested universally-existentially quantified invariants) and can also generate preconditions required to establish worst-case upper bounds of sorting algorithms. Furthermore, for the case of previously considered properties, in particular sortedness in sorting algorithms, our algorithms take less time than reported by previous techniques.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542501},
 doi = {http://doi.acm.org/10.1145/1542476.1542501},
 acmid = {1542501},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint-based fixed-point, iterative fixed-point, predicate abstraction, quantified invariants, smt solvers, template invariants, weakest preconditions},
} 

@article{Srivastava:2009:PVU:1543135.1542501,
 author = {Srivastava, Saurabh and Gulwani, Sumit},
 title = {Program verification using templates over predicate abstraction},
 abstract = {We address the problem of automatically generating invariants with quantified and boolean structure for proving the validity of given assertions or generating pre-conditions under which the assertions are valid. We present three novel algorithms, having different strengths, that combine template and predicate abstraction based formalisms to discover required sophisticated program invariants using SMT solvers. Two of these algorithms use an iterative approach to compute fixed-points (one computes a least fixed-point and the other computes a greatest fixed-point), while the third algorithm uses a constraint based approach to encode the fixed-point. The key idea in all these algorithms is to reduce the problem of invariant discovery to that of finding optimal</i> solutions for unknowns (over conjunctions of some predicates from a given set) in a template formula such that the formula is valid. Preliminary experiments using our implementation of these algorithms show encouraging results over a benchmark of small but complicated programs. Our algorithms can verify program properties that, to our knowledge, have not been automatically verified before. In particular, our algorithms can generate full correctness proofs for sorting algorithms (which requires nested universally-existentially quantified invariants) and can also generate preconditions required to establish worst-case upper bounds of sorting algorithms. Furthermore, for the case of previously considered properties, in particular sortedness in sorting algorithms, our algorithms take less time than reported by previous techniques.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542501},
 doi = {http://doi.acm.org/10.1145/1543135.1542501},
 acmid = {1542501},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint-based fixed-point, iterative fixed-point, predicate abstraction, quantified invariants, smt solvers, template invariants, weakest preconditions},
} 

@inproceedings{Aftandilian:2009:GAU:1542476.1542503,
 author = {Aftandilian, Edward E. and Guyer, Samuel Z.},
 title = {GC assertions: using the garbage collector to check heap properties},
 abstract = {This paper introduces GC assertions</i>, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. By piggybacking on existing garbage collector computations, our system is able to check heap properties with very low overhead -- around 3\% of total execution time -- low enough for use in a deployed setting. We introduce several kinds of GC assertions and describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We report results on both the performance of our system and the experience of using our assertions to find and repair errors in real-world programs.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1542476.1542503},
 doi = {http://doi.acm.org/10.1145/1542476.1542503},
 acmid = {1542503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, garbage collection, heap properties},
} 

@article{Aftandilian:2009:GAU:1543135.1542503,
 author = {Aftandilian, Edward E. and Guyer, Samuel Z.},
 title = {GC assertions: using the garbage collector to check heap properties},
 abstract = {This paper introduces GC assertions</i>, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. By piggybacking on existing garbage collector computations, our system is able to check heap properties with very low overhead -- around 3\% of total execution time -- low enough for use in a deployed setting. We introduce several kinds of GC assertions and describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We report results on both the performance of our system and the experience of using our assertions to find and repair errors in real-world programs.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1543135.1542503},
 doi = {http://doi.acm.org/10.1145/1543135.1542503},
 acmid = {1542503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, garbage collection, heap properties},
} 

@article{Nagarakatte:2009:SHC:1543135.1542504,
 author = {Nagarakatte, Santosh and Zhao, Jianzhou and Martin, Milo M.K. and Zdancewic, Steve},
 title = {SoftBound: highly compatible and complete spatial memory safety for c},
 abstract = {The serious bugs and security vulnerabilities facilitated by C/C++'s lack of bounds checking are well known, yet C and C++ remain in widespread use. Unfortunately, C's arbitrary pointer arithmetic, conflation of pointers and arrays, and programmer-visible memory layout make retrofitting C/C++ with spatial safety guarantees extremely challenging. Existing approaches suffer from incompleteness, have high runtime overhead, or require non-trivial changes to the C source code. Thus far, these deficiencies have prevented widespread adoption of such techniques. This paper proposes SoftBound, a compile-time transformation for enforcing spatial safety of C. Inspired by HardBound, a previously proposed hardware-assisted approach, SoftBound similarly records base and bound information for every pointer as disjoint metadata. This decoupling enables SoftBound to provide spatial safety without requiring changes to C source code. Unlike HardBound, SoftBound is a software-only approach and performs metadata manipulation only when loading or storing pointer values. A formal proof shows that this is sufficient to provide spatial safety even in the presence of arbitrary casts. SoftBound's full checking mode provides complete spatial violation detection with 67\% runtime overhead on average. To further reduce overheads, SoftBound has a store-only checking mode that successfully detects all the security vulnerabilities in a test suite at the cost of only 22\% runtime overhead on average.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {245--258},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1543135.1542504},
 doi = {http://doi.acm.org/10.1145/1543135.1542504},
 acmid = {1542504},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflows, c, spatial memory safety},
} 

@inproceedings{Nagarakatte:2009:SHC:1542476.1542504,
 author = {Nagarakatte, Santosh and Zhao, Jianzhou and Martin, Milo M.K. and Zdancewic, Steve},
 title = {SoftBound: highly compatible and complete spatial memory safety for c},
 abstract = {The serious bugs and security vulnerabilities facilitated by C/C++'s lack of bounds checking are well known, yet C and C++ remain in widespread use. Unfortunately, C's arbitrary pointer arithmetic, conflation of pointers and arrays, and programmer-visible memory layout make retrofitting C/C++ with spatial safety guarantees extremely challenging. Existing approaches suffer from incompleteness, have high runtime overhead, or require non-trivial changes to the C source code. Thus far, these deficiencies have prevented widespread adoption of such techniques. This paper proposes SoftBound, a compile-time transformation for enforcing spatial safety of C. Inspired by HardBound, a previously proposed hardware-assisted approach, SoftBound similarly records base and bound information for every pointer as disjoint metadata. This decoupling enables SoftBound to provide spatial safety without requiring changes to C source code. Unlike HardBound, SoftBound is a software-only approach and performs metadata manipulation only when loading or storing pointer values. A formal proof shows that this is sufficient to provide spatial safety even in the presence of arbitrary casts. SoftBound's full checking mode provides complete spatial violation detection with 67\% runtime overhead on average. To further reduce overheads, SoftBound has a store-only checking mode that successfully detects all the security vulnerabilities in a test suite at the cost of only 22\% runtime overhead on average.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {245--258},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1542476.1542504},
 doi = {http://doi.acm.org/10.1145/1542476.1542504},
 acmid = {1542504},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflows, c, spatial memory safety},
} 

@inproceedings{Oiwa:2009:IMF:1542476.1542505,
 author = {Oiwa, Yutaka},
 title = {Implementation of the memory-safe full ANSI-C compiler},
 abstract = {This paper describes a completely memory-safe compiler for C language programs that is fully compatible with the ANSI C specification. Programs written in C often suffer from nasty errors due to dangling pointers and buffer overflow. Such errors in Internet server programs are often exploited by malicious attackers to crack an entire system. The origin of these errors is usually corruption of in-memory data structures caused by out-of-bound array accesses. Usual C compilers do not provide any protection against such out-of-bound access, although many other languages such as Java and ML do provide such protection. There have been several proposals for preventing such memory corruption from various aspects: runtime buffer overrun detectors, designs for new C-like languages, and compilers for (subsets of) the C language. However, as far as we know, none of them have achieved full memory protection and full compatibility with the C language specification at the same time. We propose the most powerful solution to this problem ever presented. We have developed Fail-Safe C</i>, a memory-safe implementation of the full ANSI C language. It detects and disallows all unsafe operations, yet conforms to the full ANSI C standard (including casts and unions). This paper introduces several techniques--both compile-time and runtime--to reduce the overhead of runtime checks, while still maintaining 100\% memory safety. This compiler lets programmers easily make their programs safe without heavy rewriting or porting of their code. It also supports many of the "dirty tricks" commonly used in many existing C programs, which do not strictly conform to the standard specification. In this paper, we demonstrate several real-world server programs that can be processed by our compiler and present technical details and benchmark results for it.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {259--269},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542505},
 doi = {http://doi.acm.org/10.1145/1542476.1542505},
 acmid = {1542505},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {c language, memory safety},
} 

@article{Oiwa:2009:IMF:1543135.1542505,
 author = {Oiwa, Yutaka},
 title = {Implementation of the memory-safe full ANSI-C compiler},
 abstract = {This paper describes a completely memory-safe compiler for C language programs that is fully compatible with the ANSI C specification. Programs written in C often suffer from nasty errors due to dangling pointers and buffer overflow. Such errors in Internet server programs are often exploited by malicious attackers to crack an entire system. The origin of these errors is usually corruption of in-memory data structures caused by out-of-bound array accesses. Usual C compilers do not provide any protection against such out-of-bound access, although many other languages such as Java and ML do provide such protection. There have been several proposals for preventing such memory corruption from various aspects: runtime buffer overrun detectors, designs for new C-like languages, and compilers for (subsets of) the C language. However, as far as we know, none of them have achieved full memory protection and full compatibility with the C language specification at the same time. We propose the most powerful solution to this problem ever presented. We have developed Fail-Safe C</i>, a memory-safe implementation of the full ANSI C language. It detects and disallows all unsafe operations, yet conforms to the full ANSI C standard (including casts and unions). This paper introduces several techniques--both compile-time and runtime--to reduce the overhead of runtime checks, while still maintaining 100\% memory safety. This compiler lets programmers easily make their programs safe without heavy rewriting or porting of their code. It also supports many of the "dirty tricks" commonly used in many existing C programs, which do not strictly conform to the standard specification. In this paper, we demonstrate several real-world server programs that can be processed by our compiler and present technical details and benchmark results for it.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {259--269},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542505},
 doi = {http://doi.acm.org/10.1145/1543135.1542505},
 acmid = {1542505},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {c language, memory safety},
} 

@inproceedings{Rubio-Gonzalez:2009:EPA:1542476.1542506,
 author = {Rubio-Gonz\'{a}lez, Cindy and Gunawi, Haryadi S. and Liblit, Ben and Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau, Andrea C.},
 title = {Error propagation analysis for file systems},
 abstract = {Unchecked errors are especially pernicious in operating system file management code. Transient or permanent hardware failures are inevitable, and error-management bugs at the file system layer can cause silent, unrecoverable data corruption. We propose an interprocedural static analysis that tracks errors as they propagate through file system code. Our implementation detects overwritten, out-of-scope, and unsaved unchecked errors. Analysis of four widely-used Linux file system implementations (CIFS, ext3, IBM JFS and ReiserFS), a relatively new file system implementation (ext4), and shared virtual file system (VFS) code uncovers 312 error propagation bugs. Our flow- and context-sensitive approach produces more precise results than related techniques while providing better diagnostic information, including possible execution paths that demonstrate each bug found.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542506},
 doi = {http://doi.acm.org/10.1145/1542476.1542506},
 acmid = {1542506},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, copy constant propagation, interprocedural dataflow analysis, static program analysis, weighted pushdown systems},
} 

@article{Rubio-Gonzalez:2009:EPA:1543135.1542506,
 author = {Rubio-Gonz\'{a}lez, Cindy and Gunawi, Haryadi S. and Liblit, Ben and Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau, Andrea C.},
 title = {Error propagation analysis for file systems},
 abstract = {Unchecked errors are especially pernicious in operating system file management code. Transient or permanent hardware failures are inevitable, and error-management bugs at the file system layer can cause silent, unrecoverable data corruption. We propose an interprocedural static analysis that tracks errors as they propagate through file system code. Our implementation detects overwritten, out-of-scope, and unsaved unchecked errors. Analysis of four widely-used Linux file system implementations (CIFS, ext3, IBM JFS and ReiserFS), a relatively new file system implementation (ext4), and shared virtual file system (VFS) code uncovers 312 error propagation bugs. Our flow- and context-sensitive approach produces more precise results than related techniques while providing better diagnostic information, including possible execution paths that demonstrate each bug found.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542506},
 doi = {http://doi.acm.org/10.1145/1543135.1542506},
 acmid = {1542506},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, copy constant propagation, interprocedural dataflow analysis, static program analysis, weighted pushdown systems},
} 

@article{Qi:2009:SCF:1543135.1542508,
 author = {Qi, Xin and Myers, Andrew C.},
 title = {Sharing classes between families},
 abstract = {Class sharing is a new language mechanism for building extensible software systems. Recent work has separately explored two different kinds of extensibility: first, family inheritance, in which an entire family of related classes can be inherited, and second, adaptation, in which existing objects are extended in place with new behavior and state. Class sharing integrates these two kinds of extensibility mechanisms. With little programmer effort, objects of one family can be used as members of another, while preserving relationships among objects. Therefore, a family of classes can be adapted in place with new functionality spanning multiple classes. Object graphs can evolve from one family to another, adding or removing functionality even at run time. Several new mechanisms support this flexibility while ensuring type safety. Class sharing has been implemented as an extension to Java, and its utility for evolving and extending software is demonstrated with realistic systems.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542508},
 doi = {http://doi.acm.org/10.1145/1543135.1542508},
 acmid = {1542508},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {family inheritance, masked types, views},
} 

@inproceedings{Qi:2009:SCF:1542476.1542508,
 author = {Qi, Xin and Myers, Andrew C.},
 title = {Sharing classes between families},
 abstract = {Class sharing is a new language mechanism for building extensible software systems. Recent work has separately explored two different kinds of extensibility: first, family inheritance, in which an entire family of related classes can be inherited, and second, adaptation, in which existing objects are extended in place with new behavior and state. Class sharing integrates these two kinds of extensibility mechanisms. With little programmer effort, objects of one family can be used as members of another, while preserving relationships among objects. Therefore, a family of classes can be adapted in place with new functionality spanning multiple classes. Object graphs can evolve from one family to another, adding or removing functionality even at run time. Several new mechanisms support this flexibility while ensuring type safety. Class sharing has been implemented as an extension to Java, and its utility for evolving and extending software is demonstrated with realistic systems.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542508},
 doi = {http://doi.acm.org/10.1145/1542476.1542508},
 acmid = {1542508},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {family inheritance, masked types, views},
} 

@inproceedings{Rendel:2009:TS:1542476.1542509,
 author = {Rendel, Tillmann and Ostermann, Klaus and Hofer, Christian},
 title = {Typed self-representation},
 abstract = {Self-representation -- the ability to represent programs in their own language -- has important applications in reflective languages and many other domains of programming language design. Although approaches to designing typed program representations for sublanguages of some base language have become quite popular recently, the question whether a fully metacircular typed self-representation is possible is still open. This paper makes a big step towards this aim by defining the F<sub>\&#969;</sub>* calculus, an extension of the higher-order polymorphic lambda calculus F<sub>\&#969;</sub> that allows typed self-representations. While the usability of these representations for metaprogramming is still limited, we believe that our approach makes a significant step towards a new generation of reflective languages that are both safe and efficient.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {293--303},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542509},
 doi = {http://doi.acm.org/10.1145/1542476.1542509},
 acmid = {1542509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lambda calculus, language design, reflection, self interpretation, types},
} 

@article{Rendel:2009:TS:1543135.1542509,
 author = {Rendel, Tillmann and Ostermann, Klaus and Hofer, Christian},
 title = {Typed self-representation},
 abstract = {Self-representation -- the ability to represent programs in their own language -- has important applications in reflective languages and many other domains of programming language design. Although approaches to designing typed program representations for sublanguages of some base language have become quite popular recently, the question whether a fully metacircular typed self-representation is possible is still open. This paper makes a big step towards this aim by defining the F<sub>\&#969;</sub>* calculus, an extension of the higher-order polymorphic lambda calculus F<sub>\&#969;</sub> that allows typed self-representations. While the usability of these representations for metaprogramming is still limited, we believe that our approach makes a significant step towards a new generation of reflective languages that are both safe and efficient.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {293--303},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542509},
 doi = {http://doi.acm.org/10.1145/1543135.1542509},
 acmid = {1542509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lambda calculus, language design, reflection, self interpretation, types},
} 

@inproceedings{Kawaguchi:2009:TDS:1542476.1542510,
 author = {Kawaguchi, Ming and Rondon, Patrick and Jhala, Ranjit},
 title = {Type-based data structure verification},
 abstract = {We present a refinement type-based approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel type-based mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in <sc>dsolve</sc>, which uses liquid types to verify <sc>ocaml</sc> programs. We present experiments that show that our type-based approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binary-search-ordering, and acyclicity by more than an order of magnitude.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {304--315},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542510},
 doi = {http://doi.acm.org/10.1145/1542476.1542510},
 acmid = {1542510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
} 

@article{Kawaguchi:2009:TDS:1543135.1542510,
 author = {Kawaguchi, Ming and Rondon, Patrick and Jhala, Ranjit},
 title = {Type-based data structure verification},
 abstract = {We present a refinement type-based approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel type-based mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in <sc>dsolve</sc>, which uses liquid types to verify <sc>ocaml</sc> programs. We present experiments that show that our type-based approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binary-search-ordering, and acyclicity by more than an order of magnitude.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {304--315},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542510},
 doi = {http://doi.acm.org/10.1145/1543135.1542510},
 acmid = {1542510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
} 

@inproceedings{Tristan:2009:VVL:1542476.1542512,
 author = {Tristan, Jean-Baptiste and Leroy, Xavier},
 title = {Verified validation of lazy code motion},
 abstract = {Translation validation establishes a posteriori</i> the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanically-checked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semantics-preserving and was integrated in the CompCert formally verified compiler.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {316--326},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542512},
 doi = {http://doi.acm.org/10.1145/1542476.1542512},
 acmid = {1542512},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lazy code motion, redundancy elimination, the coq proof assistant, translation validation, verified compilers},
} 

@article{Tristan:2009:VVL:1543135.1542512,
 author = {Tristan, Jean-Baptiste and Leroy, Xavier},
 title = {Verified validation of lazy code motion},
 abstract = {Translation validation establishes a posteriori</i> the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanically-checked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semantics-preserving and was integrated in the CompCert formally verified compiler.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {316--326},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542512},
 doi = {http://doi.acm.org/10.1145/1543135.1542512},
 acmid = {1542512},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lazy code motion, redundancy elimination, the coq proof assistant, translation validation, verified compilers},
} 

@inproceedings{Kundu:2009:POC:1542476.1542513,
 author = {Kundu, Sudipta and Tatlock, Zachary and Lerner, Sorin},
 title = {Proving optimizations correct using parameterized program equivalence},
 abstract = {Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {327--337},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542513},
 doi = {http://doi.acm.org/10.1145/1542476.1542513},
 acmid = {1542513},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler optimization, correctness, translation validation},
} 

@article{Kundu:2009:POC:1543135.1542513,
 author = {Kundu, Sudipta and Tatlock, Zachary and Lerner, Sorin},
 title = {Proving optimizations correct using parameterized program equivalence},
 abstract = {Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {327--337},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542513},
 doi = {http://doi.acm.org/10.1145/1543135.1542513},
 acmid = {1542513},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler optimization, correctness, translation validation},
} 

@article{Zee:2009:IPL:1543135.1542514,
 author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin C.},
 title = {An integrated proof language for imperative programs},
 abstract = {We present an integrated proof language for guiding the actions of multiple reasoning systems as they work together to prove complex correctness properties of imperative programs. The language operates in the context of a program verification system that uses multiple reasoning systems to discharge generated proof obligations. It is designed to 1) enable developers to resolve key choice points in complex program correctness proofs, thereby enabling automated reasoning systems to successfully prove the desired correctness properties; 2) allow developers to identify key lemmas for the reasoning systems to prove, thereby guiding the reasoning systems to find an effective proof decomposition; 3) enable multiple reasoning systems to work together productively to prove a single correctness property by providing a mechanism that developers can use to divide the property into lemmas, each of which is suitable for a different reasoning system; and 4) enable developers to identify specific lemmas that the reasoning systems should use when attempting to prove other lemmas or correctness properties, thereby appropriately confining the search space so that the reasoning systems can find a proof in an acceptable amount of time. The language includes a rich set of declarative proof constructs that enables developers to direct the reasoning systems as little or as much as they desire. Because the declarative proof statements are embedded into the program as specialized comments, they also serve as verified documentation and are a natural extension of the assertion mechanism found in most program verification systems. We have implemented our integrated proof language in the context of a program verification system for Java and used the resulting system to verify a collection of linked data structure implementations. Our experience indicates that our proof language makes it possible to successfully prove complex program correctness properties that are otherwise beyond the reach of automated reasoning systems.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {338--351},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1543135.1542514},
 doi = {http://doi.acm.org/10.1145/1543135.1542514},
 acmid = {1542514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {proof system, theorem prover, verification},
} 

@inproceedings{Zee:2009:IPL:1542476.1542514,
 author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin C.},
 title = {An integrated proof language for imperative programs},
 abstract = {We present an integrated proof language for guiding the actions of multiple reasoning systems as they work together to prove complex correctness properties of imperative programs. The language operates in the context of a program verification system that uses multiple reasoning systems to discharge generated proof obligations. It is designed to 1) enable developers to resolve key choice points in complex program correctness proofs, thereby enabling automated reasoning systems to successfully prove the desired correctness properties; 2) allow developers to identify key lemmas for the reasoning systems to prove, thereby guiding the reasoning systems to find an effective proof decomposition; 3) enable multiple reasoning systems to work together productively to prove a single correctness property by providing a mechanism that developers can use to divide the property into lemmas, each of which is suitable for a different reasoning system; and 4) enable developers to identify specific lemmas that the reasoning systems should use when attempting to prove other lemmas or correctness properties, thereby appropriately confining the search space so that the reasoning systems can find a proof in an acceptable amount of time. The language includes a rich set of declarative proof constructs that enables developers to direct the reasoning systems as little or as much as they desire. Because the declarative proof statements are embedded into the program as specialized comments, they also serve as verified documentation and are a natural extension of the assertion mechanism found in most program verification systems. We have implemented our integrated proof language in the context of a program verification system for Java and used the resulting system to verify a collection of linked data structure implementations. Our experience indicates that our proof language makes it possible to successfully prove complex program correctness properties that are otherwise beyond the reach of automated reasoning systems.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {338--351},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1542476.1542514},
 doi = {http://doi.acm.org/10.1145/1542476.1542514},
 acmid = {1542514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {proof system, theorem prover, verification},
} 

@inproceedings{Ravitch:2009:AGL:1542476.1542516,
 author = {Ravitch, Tristan and Jackson, Steve and Aderhold, Eric and Liblit, Ben},
 title = {Automatic generation of library bindings using static analysis},
 abstract = {High-level languages are growing in popularity. However, decades of C software development have produced large libraries of fast, time-tested, meritorious code that are impractical to recreate from scratch. Cross-language bindings can expose low-level C code to high-level languages. Unfortunately, writing bindings by hand is tedious and error-prone, while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect. We present an improved binding-generation strategy based on static analysis of unannotated library source code. We characterize three high-level idioms that are not uniquely expressible in C's low-level type system: array parameters, resource managers, and multiple return values. We describe a suite of interprocedural analyses that recover this high-level information, and we show how the results can be used in a binding generator for the Python programming language. In experiments with four large C libraries, we find that our approach avoids the mistakes characteristic of hand-written bindings while offering a level of Python integration unmatched by prior automated approaches. Among the thousands of functions in the public interfaces of these libraries, roughly 40\% exhibit the behaviors detected by our static analyses.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {352--362},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542516},
 doi = {http://doi.acm.org/10.1145/1542476.1542516},
 acmid = {1542516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bindings, dataflow analysis, ffi, foreign function interfaces, multi-language code reuse, static library analysis},
} 

@article{Ravitch:2009:AGL:1543135.1542516,
 author = {Ravitch, Tristan and Jackson, Steve and Aderhold, Eric and Liblit, Ben},
 title = {Automatic generation of library bindings using static analysis},
 abstract = {High-level languages are growing in popularity. However, decades of C software development have produced large libraries of fast, time-tested, meritorious code that are impractical to recreate from scratch. Cross-language bindings can expose low-level C code to high-level languages. Unfortunately, writing bindings by hand is tedious and error-prone, while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect. We present an improved binding-generation strategy based on static analysis of unannotated library source code. We characterize three high-level idioms that are not uniquely expressible in C's low-level type system: array parameters, resource managers, and multiple return values. We describe a suite of interprocedural analyses that recover this high-level information, and we show how the results can be used in a binding generator for the Python programming language. In experiments with four large C libraries, we find that our approach avoids the mistakes characteristic of hand-written bindings while offering a level of Python integration unmatched by prior automated approaches. Among the thousands of functions in the public interfaces of these libraries, roughly 40\% exhibit the behaviors detected by our static analyses.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {352--362},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542516},
 doi = {http://doi.acm.org/10.1145/1543135.1542516},
 acmid = {1542516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bindings, dataflow analysis, ffi, foreign function interfaces, multi-language code reuse, static library analysis},
} 

@article{Chandra:2009:SPA:1543135.1542517,
 author = {Chandra, Satish and Fink, Stephen J. and Sridharan, Manu},
 title = {Snugglebug: a powerful approach to weakest preconditions},
 abstract = {Symbolic analysis shows promise as a foundation for bug-finding, specification inference, verification, and test generation. This paper addresses demand-driven symbolic analysis for object-oriented programs and frameworks. Many such codes comprise large, partial programs with highly dynamic behaviors--polymorphism, reflection, and so on--posing significant scalability challenges for any static analysis. We present an approach based on interprocedural backwards propagation of weakest preconditions. We present several novel techniques to improve the efficiency of such analysis. First, we present directed call graph construction</i>, where call graph construction and symbolic analysis are interleaved. With this technique, call graph construction is guided by constraints discovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative call graph. Second, we describe generalization</i>, a technique that greatly increases the reusability of procedure summaries computed during interprocedural analysis. Instead of tabulating how a procedure transforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only the pertinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom logic simplifier with weakest precondition computation dramatically improves performance. We have implemented the analysis in a tool called <sc>Snugglebug</sc> and evaluated it as a bug-report feasibility checker. Our results show that the algorithmic techniques were critical for successfully analyzing large Java applications.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {363--374},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542517},
 doi = {http://doi.acm.org/10.1145/1543135.1542517},
 acmid = {1542517},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interprocedural analysis, symbolic analysis, weakest preconditions},
} 

@inproceedings{Chandra:2009:SPA:1542476.1542517,
 author = {Chandra, Satish and Fink, Stephen J. and Sridharan, Manu},
 title = {Snugglebug: a powerful approach to weakest preconditions},
 abstract = {Symbolic analysis shows promise as a foundation for bug-finding, specification inference, verification, and test generation. This paper addresses demand-driven symbolic analysis for object-oriented programs and frameworks. Many such codes comprise large, partial programs with highly dynamic behaviors--polymorphism, reflection, and so on--posing significant scalability challenges for any static analysis. We present an approach based on interprocedural backwards propagation of weakest preconditions. We present several novel techniques to improve the efficiency of such analysis. First, we present directed call graph construction</i>, where call graph construction and symbolic analysis are interleaved. With this technique, call graph construction is guided by constraints discovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative call graph. Second, we describe generalization</i>, a technique that greatly increases the reusability of procedure summaries computed during interprocedural analysis. Instead of tabulating how a procedure transforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only the pertinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom logic simplifier with weakest precondition computation dramatically improves performance. We have implemented the analysis in a tool called <sc>Snugglebug</sc> and evaluated it as a bug-report feasibility checker. Our results show that the algorithmic techniques were critical for successfully analyzing large Java applications.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {363--374},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542517},
 doi = {http://doi.acm.org/10.1145/1542476.1542517},
 acmid = {1542517},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interprocedural analysis, symbolic analysis, weakest preconditions},
} 

@article{Gulwani:2009:CRP:1543135.1542518,
 author = {Gulwani, Sumit and Jain, Sagar and Koskinen, Eric},
 title = {Control-flow refinement and progress invariants for bound analysis},
 abstract = {Symbolic complexity bounds help programmers understand the performance characteristics of their implementations. Existing work provides techniques for statically determining bounds of procedures with simple control-flow. However, procedures with nested loops or multiple paths through a single loop are challenging. In this paper we describe two techniques, control-flow refinement</i> and progress invariants</i>, that together enable estimation of precise bounds for procedures with nested and multi-path loops. Control-flow refinement transforms a multi-path loop into a semantically equivalent code fragment with simpler loops by making the structure of path interleaving explicit. We show that this enables non-disjunctive invariant generation tools to find a bound on many procedures for which previous techniques were unable to prove termination. Progress invariants characterize relationships between consecutive states that can arise at a program location. We further present an algorithm that uses progress invariants to compute precise bounds for nested loops. The utility of these two techniques goes beyond our application to symbolic bound analysis. In particular, we discuss applications of control-flow refinement to proving safety properties that otherwise require disjunctive invariants. We have applied our methodology to over 670,000 lines of code of a significant Microsoft product and were able to find symbolic bounds for 90\% of the loops. We are not aware of any other published results that report experiences running a bound analysis on a real code-base.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {375--385},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542518},
 doi = {http://doi.acm.org/10.1145/1543135.1542518},
 acmid = {1542518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bound analysis, control-flow refinement, formal verification, program verification, progress invariants, termination},
} 

@inproceedings{Gulwani:2009:CRP:1542476.1542518,
 author = {Gulwani, Sumit and Jain, Sagar and Koskinen, Eric},
 title = {Control-flow refinement and progress invariants for bound analysis},
 abstract = {Symbolic complexity bounds help programmers understand the performance characteristics of their implementations. Existing work provides techniques for statically determining bounds of procedures with simple control-flow. However, procedures with nested loops or multiple paths through a single loop are challenging. In this paper we describe two techniques, control-flow refinement</i> and progress invariants</i>, that together enable estimation of precise bounds for procedures with nested and multi-path loops. Control-flow refinement transforms a multi-path loop into a semantically equivalent code fragment with simpler loops by making the structure of path interleaving explicit. We show that this enables non-disjunctive invariant generation tools to find a bound on many procedures for which previous techniques were unable to prove termination. Progress invariants characterize relationships between consecutive states that can arise at a program location. We further present an algorithm that uses progress invariants to compute precise bounds for nested loops. The utility of these two techniques goes beyond our application to symbolic bound analysis. In particular, we discuss applications of control-flow refinement to proving safety properties that otherwise require disjunctive invariants. We have applied our methodology to over 670,000 lines of code of a significant Microsoft product and were able to find symbolic bounds for 90\% of the loops. We are not aware of any other published results that report experiences running a bound analysis on a real code-base.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {375--385},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542518},
 doi = {http://doi.acm.org/10.1145/1542476.1542518},
 acmid = {1542518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bound analysis, control-flow refinement, formal verification, program verification, progress invariants, termination},
} 

@inproceedings{Inoue:2009:SMM:1542476.1542520,
 author = {Inoue, Hiroshi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {A study of memory management for web-based applications on multicore processors},
 abstract = {More and more server workloads are becoming Web-based. In these Web-based workloads, most of the memory objects are used only during one transaction. We study the effect of the memory management approaches on the performance of such Web-based applications on two modern multicore processors. In particular, using six PHP applications, we compare a general-purpose allocator (the default allocator of the PHP runtime) and a region-based allocator, which can reduce the cost of memory management by not supporting per-object free. The region-based allocator achieves better performance for all workloads on one processor core due to its smaller memory management cost. However, when using eight cores, the region-based allocator suffers from hidden costs of increased bus traffics and the performance is reduced for many workloads by as much as 27.2\% compared to the default allocator. This is because the memory bandwidth tends to become a bottleneck in systems with multicore processors. We propose a new memory management approach, defrag-dodging</i>, to maximize the performance of the Web-based workloads on multicore processors. In our approach, we reduce the memory management cost by avoiding defragmentation overhead in the malloc and free functions during a transaction. We found that the transactions in Web-based applications are short enough to ignore heap fragmentation, and hence the costs of the defrag-mentation activities in existing general-purpose allocators outweigh their benefits. By comparing our approach against the region-based approach, we show that a per-object free capability can reduce bus traffic and achieve higher performance on multicore processors. We demonstrate that our defrag-dodging approach improves the performance of all the evaluated applications on both processors by up to 11.4\% and 51.5\% over the default allocator and the region-based allocator, respectively.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {386--396},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542520},
 doi = {http://doi.acm.org/10.1145/1542476.1542520},
 acmid = {1542520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic memory management, region-based memory management, scripting language, web-based applications},
} 

@article{Inoue:2009:SMM:1543135.1542520,
 author = {Inoue, Hiroshi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {A study of memory management for web-based applications on multicore processors},
 abstract = {More and more server workloads are becoming Web-based. In these Web-based workloads, most of the memory objects are used only during one transaction. We study the effect of the memory management approaches on the performance of such Web-based applications on two modern multicore processors. In particular, using six PHP applications, we compare a general-purpose allocator (the default allocator of the PHP runtime) and a region-based allocator, which can reduce the cost of memory management by not supporting per-object free. The region-based allocator achieves better performance for all workloads on one processor core due to its smaller memory management cost. However, when using eight cores, the region-based allocator suffers from hidden costs of increased bus traffics and the performance is reduced for many workloads by as much as 27.2\% compared to the default allocator. This is because the memory bandwidth tends to become a bottleneck in systems with multicore processors. We propose a new memory management approach, defrag-dodging</i>, to maximize the performance of the Web-based workloads on multicore processors. In our approach, we reduce the memory management cost by avoiding defragmentation overhead in the malloc and free functions during a transaction. We found that the transactions in Web-based applications are short enough to ignore heap fragmentation, and hence the costs of the defrag-mentation activities in existing general-purpose allocators outweigh their benefits. By comparing our approach against the region-based approach, we show that a per-object free capability can reduce bus traffic and achieve higher performance on multicore processors. We demonstrate that our defrag-dodging approach improves the performance of all the evaluated applications on both processors by up to 11.4\% and 51.5\% over the default allocator and the region-based allocator, respectively.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {386--396},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542520},
 doi = {http://doi.acm.org/10.1145/1543135.1542520},
 acmid = {1542520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic memory management, region-based memory management, scripting language, web-based applications},
} 

@inproceedings{Novark:2009:EPL:1542476.1542521,
 author = {Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Efficiently and precisely locating memory leaks and bloat},
 abstract = {Inefficient use of memory, including leaks and bloat, remain a significant challenge for C and C++ developers. Applications with these problems become slower over time as their working set grows and can become unresponsive. At the same time, memory leaks and bloat remain notoriously difficult to debug, and comprise a large number of reported bugs in mature applications. Previous tools for diagnosing memory inefficiencies-based on garbage collection, binary rewriting, or code sampling-impose high overheads (up to 100X) or generate many false alarms. This paper presents Hound, a runtime system that helps track down the sources of memory leaks and bloat in C and C++ applications. Hound employs data sampling</i>, a staleness-tracking approach based on a novel heap organization, to make it both precise and efficient. Hound has no false positives, and its runtime and space overhead are low enough that it can be used in deployed applications. We demonstrate Hound's efficacy across a suite of synthetic benchmarks and real applications.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {397--407},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542521},
 doi = {http://doi.acm.org/10.1145/1542476.1542521},
 acmid = {1542521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic memory allocation, heap profiling, hound, memory leak detection, virtual compaction},
} 

@article{Novark:2009:EPL:1543135.1542521,
 author = {Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Efficiently and precisely locating memory leaks and bloat},
 abstract = {Inefficient use of memory, including leaks and bloat, remain a significant challenge for C and C++ developers. Applications with these problems become slower over time as their working set grows and can become unresponsive. At the same time, memory leaks and bloat remain notoriously difficult to debug, and comprise a large number of reported bugs in mature applications. Previous tools for diagnosing memory inefficiencies-based on garbage collection, binary rewriting, or code sampling-impose high overheads (up to 100X) or generate many false alarms. This paper presents Hound, a runtime system that helps track down the sources of memory leaks and bloat in C and C++ applications. Hound employs data sampling</i>, a staleness-tracking approach based on a novel heap organization, to make it both precise and efficient. Hound has no false positives, and its runtime and space overhead are low enough that it can be used in deployed applications. We demonstrate Hound's efficacy across a suite of synthetic benchmarks and real applications.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {397--407},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542521},
 doi = {http://doi.acm.org/10.1145/1543135.1542521},
 acmid = {1542521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic memory allocation, heap profiling, hound, memory leak detection, virtual compaction},
} 

@inproceedings{Shacham:2009:CAS:1542476.1542522,
 author = {Shacham, Ohad and Vechev, Martin and Yahav, Eran},
 title = {Chameleon: adaptive selection of collections},
 abstract = {Languages such as Java and C#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a low-overhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-thefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON on top of a IBM's J9 production JVM, and evaluated it over a small set of benchmarks. We show that for some applications, using CHAMELEON leads to a significant improvement of the memory footprint of the application.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {408--418},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1542476.1542522},
 doi = {http://doi.acm.org/10.1145/1542476.1542522},
 acmid = {1542522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bloat, collections, java, semantic profiler},
} 

@article{Shacham:2009:CAS:1543135.1542522,
 author = {Shacham, Ohad and Vechev, Martin and Yahav, Eran},
 title = {Chameleon: adaptive selection of collections},
 abstract = {Languages such as Java and C#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a low-overhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-thefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON on top of a IBM's J9 production JVM, and evaluated it over a small set of benchmarks. We show that for some applications, using CHAMELEON leads to a significant improvement of the memory footprint of the application.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {408--418},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1543135.1542522},
 doi = {http://doi.acm.org/10.1145/1543135.1542522},
 acmid = {1542522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bloat, collections, java, semantic profiler},
} 

@inproceedings{Xu:2009:GFP:1542476.1542523,
 author = {Xu, Guoqing and Arnold, Matthew and Mitchell, Nick and Rountev, Atanas and Sevitsky, Gary},
 title = {Go with the flow: profiling copies to find runtime bloat},
 abstract = {Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods, and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent, performance experts analyze deployed applications and regularly find gains of 2x or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Bloat results from a pile-up of seemingly harmless decisions. Each adds temporary objects and method calls, and often copies values between those temporary objects. While data copies are not the entirety of bloat, we have observed that they are excellent indicators of regions of excessive activity. By optimizing copies, one is likely to remove the objects that carry copied values, and the method calls that allocate and populate them. We introduce copy profiling</i>, a technique that summarizes runtime activity in terms of chains of data copies. A flat copy profile counts copies by method. We show how flat profiles alone can be helpful. In many cases, diagnosing a problem requires data flow context. Tracking and making sense of raw copy chains does not scale, so we introduce a summarizing abstraction called the copy graph</i>. We implement three clients analyses that, using the copy graph, expose common patterns of bloat, such as finding hot copy chains and discovering temporary data structures. We demonstrate, with examples from a large-scale commercial application and several benchmarks, that copy profiling can be used by a programmer to quickly find opportunities for large performance gains.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {419--430},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542523},
 doi = {http://doi.acm.org/10.1145/1542476.1542523},
 acmid = {1542523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {copy graph, heap analysis, memory bloat, profiling},
} 

@article{Xu:2009:GFP:1543135.1542523,
 author = {Xu, Guoqing and Arnold, Matthew and Mitchell, Nick and Rountev, Atanas and Sevitsky, Gary},
 title = {Go with the flow: profiling copies to find runtime bloat},
 abstract = {Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods, and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent, performance experts analyze deployed applications and regularly find gains of 2x or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Bloat results from a pile-up of seemingly harmless decisions. Each adds temporary objects and method calls, and often copies values between those temporary objects. While data copies are not the entirety of bloat, we have observed that they are excellent indicators of regions of excessive activity. By optimizing copies, one is likely to remove the objects that carry copied values, and the method calls that allocate and populate them. We introduce copy profiling</i>, a technique that summarizes runtime activity in terms of chains of data copies. A flat copy profile counts copies by method. We show how flat profiles alone can be helpful. In many cases, diagnosing a problem requires data flow context. Tracking and making sense of raw copy chains does not scale, so we introduce a summarizing abstraction called the copy graph</i>. We implement three clients analyses that, using the copy graph, expose common patterns of bloat, such as finding hot copy chains and discovering temporary data structures. We demonstrate, with examples from a large-scale commercial application and several benchmarks, that copy profiling can be used by a programmer to quickly find opportunities for large performance gains.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {419--430},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542523},
 doi = {http://doi.acm.org/10.1145/1543135.1542523},
 acmid = {1542523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {copy graph, heap analysis, memory bloat, profiling},
} 

@article{Saha:2009:PMH:1543135.1542525,
 author = {Saha, Bratin and Zhou, Xiaocheng and Chen, Hu and Gao, Ying and Yan, Shoumeng and Rajagopalan, Mohan and Fang, Jesse and Zhang, Peinan and Ronen, Ronny and Mendelson, Avi},
 title = {Programming model for a heterogeneous x86 platform},
 abstract = {The client computing platform is moving towards a heterogeneous architecture consisting of a combination of cores focused on scalar performance, and a set of throughput-oriented cores. The throughput oriented cores (e.g. a GPU) may be connected over both coherent and non-coherent interconnects, and have different ISAs. This paper describes a programming model for such heterogeneous platforms. We discuss the language constructs, runtime implementation, and the memory model for such a programming environment. We implemented this programming environment in a x86 heterogeneous platform simulator. We ported a number of workloads to our programming environment, and present the performance of our programming environment on these workloads.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {431--440},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1543135.1542525},
 doi = {http://doi.acm.org/10.1145/1543135.1542525},
 acmid = {1542525},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heterogeneous platforms, programming model},
} 

@inproceedings{Saha:2009:PMH:1542476.1542525,
 author = {Saha, Bratin and Zhou, Xiaocheng and Chen, Hu and Gao, Ying and Yan, Shoumeng and Rajagopalan, Mohan and Fang, Jesse and Zhang, Peinan and Ronen, Ronny and Mendelson, Avi},
 title = {Programming model for a heterogeneous x86 platform},
 abstract = {The client computing platform is moving towards a heterogeneous architecture consisting of a combination of cores focused on scalar performance, and a set of throughput-oriented cores. The throughput oriented cores (e.g. a GPU) may be connected over both coherent and non-coherent interconnects, and have different ISAs. This paper describes a programming model for such heterogeneous platforms. We discuss the language constructs, runtime implementation, and the memory model for such a programming environment. We implemented this programming environment in a x86 heterogeneous platform simulator. We ported a number of workloads to our programming environment, and present the performance of our programming environment on these workloads.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {431--440},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1542476.1542525},
 doi = {http://doi.acm.org/10.1145/1542476.1542525},
 acmid = {1542525},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heterogeneous platforms, programming model},
} 

@article{Tallent:2009:BAM:1543135.1542526,
 author = {Tallent, Nathan R. and Mellor-Crummey, John M. and Fagan, Michael W.},
 title = {Binary analysis for measurement and attribution of program performance},
 abstract = {Modern programs frequently employ sophisticated modular designs. As a result, performance problems cannot be identified from costs attributed to routines in isolation; understanding code performance requires information about a routine's calling context. Existing performance tools fall short in this respect. Prior strategies for attributing context-sensitive performance at the source level either compromise measurement accuracy, remain too close to the binary, or require custom compilers. To understand the performance of fully optimized modular code, we developed two novel binary analysis techniques: 1) on-the-fly</i> analysis of optimized machine code to enable minimally intrusive and accurate attribution of costs to dynamic calling contexts; and 2) post-mortem analysis of optimized machine code and its debugging sections to recover its program structure and reconstruct a mapping back to its source code. By combining the recovered static program structure with dynamic calling context information, we can accurately attribute performance metrics to calling contexts, procedures, loops, and inlined instances of procedures. We demonstrate that the fusion of this information provides unique insight into the performance of complex modular codes. This work is implemented in the HPCToolkit performance tools (http://hpctoolkit.org).},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {441--452},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542526},
 doi = {http://doi.acm.org/10.1145/1543135.1542526},
 acmid = {1542526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary analysis, call path profiling, hpctoolkit, performance tools, static analysis},
} 

@inproceedings{Tallent:2009:BAM:1542476.1542526,
 author = {Tallent, Nathan R. and Mellor-Crummey, John M. and Fagan, Michael W.},
 title = {Binary analysis for measurement and attribution of program performance},
 abstract = {Modern programs frequently employ sophisticated modular designs. As a result, performance problems cannot be identified from costs attributed to routines in isolation; understanding code performance requires information about a routine's calling context. Existing performance tools fall short in this respect. Prior strategies for attributing context-sensitive performance at the source level either compromise measurement accuracy, remain too close to the binary, or require custom compilers. To understand the performance of fully optimized modular code, we developed two novel binary analysis techniques: 1) on-the-fly</i> analysis of optimized machine code to enable minimally intrusive and accurate attribution of costs to dynamic calling contexts; and 2) post-mortem analysis of optimized machine code and its debugging sections to recover its program structure and reconstruct a mapping back to its source code. By combining the recovered static program structure with dynamic calling context information, we can accurately attribute performance metrics to calling contexts, procedures, loops, and inlined instances of procedures. We demonstrate that the fusion of this information provides unique insight into the performance of complex modular codes. This work is implemented in the HPCToolkit performance tools (http://hpctoolkit.org).},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {441--452},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542526},
 doi = {http://doi.acm.org/10.1145/1542476.1542526},
 acmid = {1542526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary analysis, call path profiling, hpctoolkit, performance tools, static analysis},
} 

@inproceedings{Hoffman:2009:STA:1542476.1542527,
 author = {Hoffman, Kevin J. and Eugster, Patrick and Jagannathan, Suresh},
 title = {Semantics-aware trace analysis},
 abstract = {As computer systems continue to become more powerful and complex, so do programs. High-level abstractions introduced to deal with complexity in large programs, while simplifying human reasoning, can often obfuscate salient program properties gleaned from automated source-level analysis through subtle (often non-local) interactions. Consequently, understanding the effects of program changes and whether these changes violate intended protocols become difficult to infer. Refactorings, and feature additions, modifications, or removals can introduce hard-to-catch bugs that often go undetected until many revisions later. To address these issues, this paper presents a novel dynamic program analysis that builds a semantic view</i> of program executions. These views reflect program abstractions and aspects; however, views are not simply projections of execution traces, but are linked to each other to capture semantic interactions among abstractions at different levels of granularity in a scalable manner. We describe our approach in the context of Java and demonstrate its utility to improve regression analysis</i>. We first formalize a subset of Java and a grammar for traces generated at program execution. We then introduce several types of views used to analyze regression bugs along with a novel, scalable technique for semantic differencing of traces from different versions of the same program. Benchmark results on large open-source Java programs demonstrate that semantic-aware trace differencing can identify precise and useful details about the underlying cause for a regression, even in programs that use reflection, multithreading, or dynamic code generation, features that typically confound other analysis techniques.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {453--464},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1542476.1542527},
 doi = {http://doi.acm.org/10.1145/1542476.1542527},
 acmid = {1542527},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, debugging aids, semantic tracing, testing tools, trace views, tracing},
} 

@article{Hoffman:2009:STA:1543135.1542527,
 author = {Hoffman, Kevin J. and Eugster, Patrick and Jagannathan, Suresh},
 title = {Semantics-aware trace analysis},
 abstract = {As computer systems continue to become more powerful and complex, so do programs. High-level abstractions introduced to deal with complexity in large programs, while simplifying human reasoning, can often obfuscate salient program properties gleaned from automated source-level analysis through subtle (often non-local) interactions. Consequently, understanding the effects of program changes and whether these changes violate intended protocols become difficult to infer. Refactorings, and feature additions, modifications, or removals can introduce hard-to-catch bugs that often go undetected until many revisions later. To address these issues, this paper presents a novel dynamic program analysis that builds a semantic view</i> of program executions. These views reflect program abstractions and aspects; however, views are not simply projections of execution traces, but are linked to each other to capture semantic interactions among abstractions at different levels of granularity in a scalable manner. We describe our approach in the context of Java and demonstrate its utility to improve regression analysis</i>. We first formalize a subset of Java and a grammar for traces generated at program execution. We then introduce several types of views used to analyze regression bugs along with a novel, scalable technique for semantic differencing of traces from different versions of the same program. Benchmark results on large open-source Java programs demonstrate that semantic-aware trace differencing can identify precise and useful details about the underlying cause for a regression, even in programs that use reflection, multithreading, or dynamic code generation, features that typically confound other analysis techniques.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {453--464},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1543135.1542527},
 doi = {http://doi.acm.org/10.1145/1543135.1542527},
 acmid = {1542527},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, debugging aids, semantic tracing, testing tools, trace views, tracing},
} 

@inproceedings{Gal:2009:TJT:1542476.1542528,
 author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
 title = {Trace-based just-in-time type specialization for dynamic languages},
 abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '09},
 year = {2009},
 isbn = {978-1-60558-392-1},
 location = {Dublin, Ireland},
 pages = {465--478},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1542476.1542528},
 doi = {http://doi.acm.org/10.1145/1542476.1542528},
 acmid = {1542528},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamically typed languages, trace-based compilation},
} 

@article{Gal:2009:TJT:1543135.1542528,
 author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
 title = {Trace-based just-in-time type specialization for dynamic languages},
 abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {6},
 month = {June},
 year = {2009},
 issn = {0362-1340},
 pages = {465--478},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1543135.1542528},
 doi = {http://doi.acm.org/10.1145/1543135.1542528},
 acmid = {1542528},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamically typed languages, trace-based compilation},
} 

@inproceedings{Richards:2010:ADB:1806596.1806598,
 author = {Richards, Gregor and Lebresne, Sylvain and Burg, Brian and Vitek, Jan},
 title = {An analysis of the dynamic behavior of JavaScript programs},
 abstract = {The JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widely-used JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806598},
 doi = {http://doi.acm.org/10.1145/1806596.1806598},
 acmid = {1806598},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic behavior, dynamic metrics, execution tracing, javascript, program analysis},
} 

@article{Richards:2010:ADB:1809028.1806598,
 author = {Richards, Gregor and Lebresne, Sylvain and Burg, Brian and Vitek, Jan},
 title = {An analysis of the dynamic behavior of JavaScript programs},
 abstract = {The JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widely-used JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806598},
 doi = {http://doi.acm.org/10.1145/1809028.1806598},
 acmid = {1806598},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic behavior, dynamic metrics, execution tracing, javascript, program analysis},
} 

@article{Bond:2010:BEC:1809028.1806599,
 author = {Bond, Michael D. and Baker, Graham Z. and Guyer, Samuel Z.},
 title = {Breadcrumbs: efficient context sensitivity for dynamic bug detection analyses},
 abstract = {Calling context--the set of active methods on the stack--is critical for understanding the dynamic behavior of large programs. Dynamic program analysis tools, however, are almost exclusively context insensitive because of the prohibitive cost of representing calling contexts at run time. Deployable dynamic analyses, in particular, have been limited to reporting only static program locations. This paper presents Breadcrumbs, an efficient technique for recording and reporting dynamic calling contexts. It builds on an existing technique for computing a compact (one word) encoding of each calling context that client analyses can use in place of a program location. The key feature of our system is a search algorithm that can reconstruct a calling context from its encoding using only a static call graph and a small amount of dynamic information collected at cold (infrequently executed) callsites. Breadcrumbs requires no offline training or program modifications, and handles all language features, including dynamic class loading. We use Breadcrumbs to add context sensitivity to two dynamic analyses: a data-race detector and an analysis for diagnosing null pointer exceptions. On average, it adds 10\% to 20\% runtime overhead, depending on a tunable parameter that controls how much dynamic information is collected. Collecting less information lowers the overhead, but can result in a search space explosion. In some cases this causes reconstruction to fail, but in most cases Breadcrumbs \&gt;produces non-trivial calling contexts that have the potential to significantly improve both the precision of the analyses and the quality of the bug reports.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806599},
 doi = {http://doi.acm.org/10.1145/1809028.1806599},
 acmid = {1806599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug detection, context sensitivity, dynamic analysis},
} 

@inproceedings{Bond:2010:BEC:1806596.1806599,
 author = {Bond, Michael D. and Baker, Graham Z. and Guyer, Samuel Z.},
 title = {Breadcrumbs: efficient context sensitivity for dynamic bug detection analyses},
 abstract = {Calling context--the set of active methods on the stack--is critical for understanding the dynamic behavior of large programs. Dynamic program analysis tools, however, are almost exclusively context insensitive because of the prohibitive cost of representing calling contexts at run time. Deployable dynamic analyses, in particular, have been limited to reporting only static program locations. This paper presents Breadcrumbs, an efficient technique for recording and reporting dynamic calling contexts. It builds on an existing technique for computing a compact (one word) encoding of each calling context that client analyses can use in place of a program location. The key feature of our system is a search algorithm that can reconstruct a calling context from its encoding using only a static call graph and a small amount of dynamic information collected at cold (infrequently executed) callsites. Breadcrumbs requires no offline training or program modifications, and handles all language features, including dynamic class loading. We use Breadcrumbs to add context sensitivity to two dynamic analyses: a data-race detector and an analysis for diagnosing null pointer exceptions. On average, it adds 10\% to 20\% runtime overhead, depending on a tunable parameter that controls how much dynamic information is collected. Collecting less information lowers the overhead, but can result in a search space explosion. In some cases this causes reconstruction to fail, but in most cases Breadcrumbs \&gt;produces non-trivial calling contexts that have the potential to significantly improve both the precision of the analyses and the quality of the bug reports.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806599},
 doi = {http://doi.acm.org/10.1145/1806596.1806599},
 acmid = {1806599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug detection, context sensitivity, dynamic analysis},
} 

@inproceedings{Ruwase:2010:DLE:1806596.1806600,
 author = {Ruwase, Olatunji and Chen, Shimin and Gibbons, Phillip B. and Mowry, Todd C.},
 title = {Decoupled lifeguards: enabling path optimizations for dynamic correctness checking tools},
 abstract = {Dynamic correctness checking tools (a.k.a. lifeguards) can detect a wide array of correctness issues, such as memory, security, and concurrency misbehavior, in unmodified executables at run time. However, lifeguards that are implemented using dynamic binary instrumentation (DBI) often slow down the monitored application by 10-50X, while proposals that replace DBI with hardware still see 3-8X slowdowns. The remaining overhead is the cost of performing the lifeguard analysis itself. In this paper, we explore compiler optimization techniques to reduce this overhead. The lifeguard software is typically structured as a set of event-driven handlers, where the events are individual instructions in the monitored application's dynamic instruction stream. We propose to decouple</i> the lifeguard checking code from the application that it is monitoring so that the lifeguard analysis can be invoked at the granularity of hot paths</i> in the monitored application. In this way, we are able to find many more opportunities for eliminating redundant work in the lifeguard analysis, even starting with well-optimized applications and hand-tuned lifeguard handlers. Experimental results with two lifeguard frameworks - one DBI-based and one hardware-assisted - show significant reduction in monitoring overhead.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {25--35},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806600},
 doi = {http://doi.acm.org/10.1145/1806596.1806600},
 acmid = {1806600},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic code optimization, dynamic correctness checking, dynamic program analysis},
} 

@article{Ruwase:2010:DLE:1809028.1806600,
 author = {Ruwase, Olatunji and Chen, Shimin and Gibbons, Phillip B. and Mowry, Todd C.},
 title = {Decoupled lifeguards: enabling path optimizations for dynamic correctness checking tools},
 abstract = {Dynamic correctness checking tools (a.k.a. lifeguards) can detect a wide array of correctness issues, such as memory, security, and concurrency misbehavior, in unmodified executables at run time. However, lifeguards that are implemented using dynamic binary instrumentation (DBI) often slow down the monitored application by 10-50X, while proposals that replace DBI with hardware still see 3-8X slowdowns. The remaining overhead is the cost of performing the lifeguard analysis itself. In this paper, we explore compiler optimization techniques to reduce this overhead. The lifeguard software is typically structured as a set of event-driven handlers, where the events are individual instructions in the monitored application's dynamic instruction stream. We propose to decouple</i> the lifeguard checking code from the application that it is monitoring so that the lifeguard analysis can be invoked at the granularity of hot paths</i> in the monitored application. In this way, we are able to find many more opportunities for eliminating redundant work in the lifeguard analysis, even starting with well-optimized applications and hand-tuned lifeguard handlers. Experimental results with two lifeguard frameworks - one DBI-based and one hardware-assisted - show significant reduction in monitoring overhead.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {25--35},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806600},
 doi = {http://doi.acm.org/10.1145/1809028.1806600},
 acmid = {1806600},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic code optimization, dynamic correctness checking, dynamic program analysis},
} 

@article{Lee:2010:JSD:1809028.1806601,
 author = {Lee, Byeongcheol and Wiedermann, Ben and Hirzel, Martin and Grimm, Robert and McKinley, Kathryn S.},
 title = {Jinn: synthesizing dynamic bug detectors for foreign language interfaces},
 abstract = {Programming language specifications mandate static and dynamic analyses to preclude syntactic and semantic errors. Although individual languages are usually well-specified, composing languages is not, and this poor specification is a source of many errors in multilingual </i>programs. For example, virtually all Java programs compose Java and C using the Java Native Interface (JNI). Since JNI is informally specified, developers have difficulty using it correctly, and current Java compilers and virtual machines (VMs) inconsistently check only a subset of JNI constraints. This paper's most significant contribution is to show how to synthesize dynamic analyses from state machines to detect foreign function interface (FFI) violations. We identify three classes of FFI constraints encoded by eleven state machines that capture thousands of JNI and Python/C FFI rules. We use a mapping function to specify which state machines, transitions, and program entities (threads, objects, references) to check at each FFI call and return. From this function, we synthesize a context-specific dynamic analysis to find FFI bugs. We build bug detection tools for JNI and Python/C using this approach. For JNI, we dynamically and transparently interpose the analysis on Java and C language transitions through the JVM tools interface. The resulting tool, called Jinn, is compiler and virtual machine independent</i>. It detects and diagnoses a wide variety of FFI bugs that other tools miss. This approach greatly reduces the annotation burden by exploiting common FFI constraints: whereas the generated Jinn code is 22,000+ lines, we wrote only 1,400 lines of state machine and mapping code. Overall, this paper lays the foundation for a more principled approach to developing correct multilingual software and a more concise and automated approach to FFI specification.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {36--49},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806601},
 doi = {http://doi.acm.org/10.1145/1809028.1806601},
 acmid = {1806601},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, ffi bugs, foreign function interfaces (FFI), java native interface (jni), multilingual programs, python/C, specification, specification generation},
} 

@inproceedings{Lee:2010:JSD:1806596.1806601,
 author = {Lee, Byeongcheol and Wiedermann, Ben and Hirzel, Martin and Grimm, Robert and McKinley, Kathryn S.},
 title = {Jinn: synthesizing dynamic bug detectors for foreign language interfaces},
 abstract = {Programming language specifications mandate static and dynamic analyses to preclude syntactic and semantic errors. Although individual languages are usually well-specified, composing languages is not, and this poor specification is a source of many errors in multilingual </i>programs. For example, virtually all Java programs compose Java and C using the Java Native Interface (JNI). Since JNI is informally specified, developers have difficulty using it correctly, and current Java compilers and virtual machines (VMs) inconsistently check only a subset of JNI constraints. This paper's most significant contribution is to show how to synthesize dynamic analyses from state machines to detect foreign function interface (FFI) violations. We identify three classes of FFI constraints encoded by eleven state machines that capture thousands of JNI and Python/C FFI rules. We use a mapping function to specify which state machines, transitions, and program entities (threads, objects, references) to check at each FFI call and return. From this function, we synthesize a context-specific dynamic analysis to find FFI bugs. We build bug detection tools for JNI and Python/C using this approach. For JNI, we dynamically and transparently interpose the analysis on Java and C language transitions through the JVM tools interface. The resulting tool, called Jinn, is compiler and virtual machine independent</i>. It detects and diagnoses a wide variety of FFI bugs that other tools miss. This approach greatly reduces the annotation burden by exploiting common FFI constraints: whereas the generated Jinn code is 22,000+ lines, we wrote only 1,400 lines of state machine and mapping code. Overall, this paper lays the foundation for a more principled approach to developing correct multilingual software and a more concise and automated approach to FFI specification.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {36--49},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806601},
 doi = {http://doi.acm.org/10.1145/1806596.1806601},
 acmid = {1806601},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, ffi bugs, foreign function interfaces (FFI), java native interface (jni), multilingual programs, python/C, specification, specification generation},
} 

@inproceedings{Prabhu:2010:SPS:1806596.1806603,
 author = {Prabhu, Prakash and Ramalingam, Ganesan and Vaswani, Kapil},
 title = {Safe programmable speculative parallelism},
 abstract = {Execution order constraints imposed by dependences can serialize computation, preventing parallelization of code and algorithms. Speculating on the value(s) carried by dependences is one way to break such critical dependences. Value speculation has been used effectively at a low level, by compilers and hardware. In this paper, we focus on the use of speculation by programmers</i> as an algorithmic paradigm to parallelize seemingly sequential code. We propose two new language constructs, speculative composition</i> and speculative iteration</i>. These constructs enable programmers to declaratively express speculative parallelism in programs: to indicate when and how to speculate, increasing the parallelism in the program, without concerning themselves with mundane implementation details. We present a core language with speculation constructs and mutable state and present a formal operational semantics for the language. We use the semantics to define the notion of a correct speculative execution as one that is equivalent to a non-speculative execution. In general, speculation requires a runtime mechanism to undo the effects of speculative computation in the case of mis predictions. We describe a set of conditions under which such rollback can be avoided. We present a static analysis that checks if a given program satisfies these conditions. This allows us to implement speculation efficiently, without the overhead required for rollbacks. We have implemented the speculation constructs as a C# library, along with the static checker for safety. We present an empirical evaluation of the efficacy of this approach to parallelization.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {50--61},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806603},
 doi = {http://doi.acm.org/10.1145/1806596.1806603},
 acmid = {1806603},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {purity, rollback freedom, safety, speculative parallelism, value speculation},
} 

@article{Prabhu:2010:SPS:1809028.1806603,
 author = {Prabhu, Prakash and Ramalingam, Ganesan and Vaswani, Kapil},
 title = {Safe programmable speculative parallelism},
 abstract = {Execution order constraints imposed by dependences can serialize computation, preventing parallelization of code and algorithms. Speculating on the value(s) carried by dependences is one way to break such critical dependences. Value speculation has been used effectively at a low level, by compilers and hardware. In this paper, we focus on the use of speculation by programmers</i> as an algorithmic paradigm to parallelize seemingly sequential code. We propose two new language constructs, speculative composition</i> and speculative iteration</i>. These constructs enable programmers to declaratively express speculative parallelism in programs: to indicate when and how to speculate, increasing the parallelism in the program, without concerning themselves with mundane implementation details. We present a core language with speculation constructs and mutable state and present a formal operational semantics for the language. We use the semantics to define the notion of a correct speculative execution as one that is equivalent to a non-speculative execution. In general, speculation requires a runtime mechanism to undo the effects of speculative computation in the case of mis predictions. We describe a set of conditions under which such rollback can be avoided. We present a static analysis that checks if a given program satisfies these conditions. This allows us to implement speculation efficiently, without the overhead required for rollbacks. We have implemented the speculation constructs as a C# library, along with the static checker for safety. We present an empirical evaluation of the efficacy of this approach to parallelization.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {50--61},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806603},
 doi = {http://doi.acm.org/10.1145/1809028.1806603},
 acmid = {1806603},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {purity, rollback freedom, safety, speculative parallelism, value speculation},
} 

@inproceedings{Tian:2010:SSP:1806596.1806604,
 author = {Tian, Chen and Feng, Min and Gupta, Rajiv},
 title = {Supporting speculative parallelization in the presence of dynamic data structures},
 abstract = {The availability of multicore processors has led to significant interest in compiler techniques for speculative parallelization of sequential programs. Isolation of speculative state from non-speculative state forms the basis of such speculative techniques as this separation enables recovery from misspeculations. In our prior work on CorD [35,36] we showed that for array and scalar variable based programs copying of data between speculative and non-speculative memory can be highly optimized to support state separation that yields significant speedups on multicore machines available today. However, we observe that in context of heap-intensive programs that operate on linked dynamic data structures, state separation based speculative parallelization poses many challenges. The copying of data structures from non-speculative to speculative state (copy-in operation) can be very expensive due to the large sizes of dynamic data structures. The copying of updated data structures from speculative state to non-speculative state (copy-out operation) is made complex due to the changes in the shape and size of the dynamic data structure made by the speculative computation. In addition, we must contend with the need to translate pointers internal to dynamic data structures between their non-speculative and speculative memory addresses. In this paper we develop an augmented design for the representation of dynamic data structures such that all of the above operations can be performed efficiently. Our experiments demonstrate significant speedups on a real machine for a set of programs that make extensive use of heap based dynamic data structures.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {62--73},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806604},
 doi = {http://doi.acm.org/10.1145/1806596.1806604},
 acmid = {1806604},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore processors, speculative parallelization},
} 

@article{Tian:2010:SSP:1809028.1806604,
 author = {Tian, Chen and Feng, Min and Gupta, Rajiv},
 title = {Supporting speculative parallelization in the presence of dynamic data structures},
 abstract = {The availability of multicore processors has led to significant interest in compiler techniques for speculative parallelization of sequential programs. Isolation of speculative state from non-speculative state forms the basis of such speculative techniques as this separation enables recovery from misspeculations. In our prior work on CorD [35,36] we showed that for array and scalar variable based programs copying of data between speculative and non-speculative memory can be highly optimized to support state separation that yields significant speedups on multicore machines available today. However, we observe that in context of heap-intensive programs that operate on linked dynamic data structures, state separation based speculative parallelization poses many challenges. The copying of data structures from non-speculative to speculative state (copy-in operation) can be very expensive due to the large sizes of dynamic data structures. The copying of updated data structures from speculative state to non-speculative state (copy-out operation) is made complex due to the changes in the shape and size of the dynamic data structure made by the speculative computation. In addition, we must contend with the need to translate pointers internal to dynamic data structures between their non-speculative and speculative memory addresses. In this paper we develop an augmented design for the representation of dynamic data structures such that all of the above operations can be performed efficiently. Our experiments demonstrate significant speedups on a real machine for a set of programs that make extensive use of heap based dynamic data structures.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {62--73},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806604},
 doi = {http://doi.acm.org/10.1145/1809028.1806604},
 acmid = {1806604},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore processors, speculative parallelization},
} 

@article{Kandemir:2010:CTA:1809028.1806605,
 author = {Kandemir, Mahmut and Yemliha, Taylan and Muralidhara, SaiPrashanth and Srikantaiah, Shekhar and Irwin, Mary Jane and Zhnag, Yuanrui},
 title = {Cache topology aware computation mapping for multicores},
 abstract = {The main contribution of this paper is a compiler based, cache topology aware code optimization scheme for emerging multicore systems. This scheme distributes the iterations of a loop to be executed in parallel across the cores of a target multicore machine and schedules the iterations assigned to each core. Our goal is to improve the utilization of the on-chip multi-layer cache hierarchy and to maximize overall application performance. We evaluate our cache topology aware approach using a set of twelve applications and three different commercial multicore machines. In addition, to study some of our experimental parameters in detail and to explore future multicore machines (with higher core counts and deeper on-chip cache hierarchies), we also conduct a simulation based study. The results collected from our experiments with three Intel multicore machines show that the proposed compiler-based approach is very effective in enhancing performance. In addition, our simulation results indicate that optimizing for the on-chip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806605},
 doi = {http://doi.acm.org/10.1145/1809028.1806605},
 acmid = {1806605},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, compiler, multi-level, multicore, topology-aware},
} 

@inproceedings{Kandemir:2010:CTA:1806596.1806605,
 author = {Kandemir, Mahmut and Yemliha, Taylan and Muralidhara, SaiPrashanth and Srikantaiah, Shekhar and Irwin, Mary Jane and Zhnag, Yuanrui},
 title = {Cache topology aware computation mapping for multicores},
 abstract = {The main contribution of this paper is a compiler based, cache topology aware code optimization scheme for emerging multicore systems. This scheme distributes the iterations of a loop to be executed in parallel across the cores of a target multicore machine and schedules the iterations assigned to each core. Our goal is to improve the utilization of the on-chip multi-layer cache hierarchy and to maximize overall application performance. We evaluate our cache topology aware approach using a set of twelve applications and three different commercial multicore machines. In addition, to study some of our experimental parameters in detail and to explore future multicore machines (with higher core counts and deeper on-chip cache hierarchies), we also conduct a simulation based study. The results collected from our experiments with three Intel multicore machines show that the proposed compiler-based approach is very effective in enhancing performance. In addition, our simulation results indicate that optimizing for the on-chip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806605},
 doi = {http://doi.acm.org/10.1145/1806596.1806605},
 acmid = {1806605},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, compiler, multi-level, multicore, topology-aware},
} 

@inproceedings{Yang:2010:GCM:1806596.1806606,
 author = {Yang, Yi and Xiang, Ping and Kong, Jingfei and Zhou, Huiyang},
 title = {A GPGPU compiler for memory optimization and parallelism management},
 abstract = {This paper presents a novel optimizing compiler for general purpose computation on graphics processing units (GPGPU). It addresses two major challenges of developing high performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management of parallelism. The input to our compiler is a na\&#239;ve GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler analyzes the code, identifies its memory access patterns, and generates both the optimized kernel and the kernel invocation parameters. Our optimization process includes vectorization and memory coalescing for memory bandwidth enhancement, tiling and unrolling for data reuse and parallelism management, and thread block remapping or address-offset insertion for partition-camping elimination. The experiments on a set of scientific and media processing algorithms show that our optimized code achieves very high performance, either superior or very close to the highly fine-tuned library, NVIDIA CUBLAS 2.2, and up to 128 times speedups over the naive versions. Another distinguishing feature of our compiler is the understandability of the optimized code, which is useful for performance analysis and algorithm refinement.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806606},
 doi = {http://doi.acm.org/10.1145/1806596.1806606},
 acmid = {1806606},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, gpgpu},
} 

@article{Yang:2010:GCM:1809028.1806606,
 author = {Yang, Yi and Xiang, Ping and Kong, Jingfei and Zhou, Huiyang},
 title = {A GPGPU compiler for memory optimization and parallelism management},
 abstract = {This paper presents a novel optimizing compiler for general purpose computation on graphics processing units (GPGPU). It addresses two major challenges of developing high performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management of parallelism. The input to our compiler is a na\&#239;ve GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler analyzes the code, identifies its memory access patterns, and generates both the optimized kernel and the kernel invocation parameters. Our optimization process includes vectorization and memory coalescing for memory bandwidth enhancement, tiling and unrolling for data reuse and parallelism management, and thread block remapping or address-offset insertion for partition-camping elimination. The experiments on a set of scientific and media processing algorithms show that our optimized code achieves very high performance, either superior or very close to the highly fine-tuned library, NVIDIA CUBLAS 2.2, and up to 128 times speedups over the naive versions. Another distinguishing feature of our compiler is the understandability of the optimized code, which is useful for performance analysis and algorithm refinement.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806606},
 doi = {http://doi.acm.org/10.1145/1809028.1806606},
 acmid = {1806606},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, gpgpu},
} 

@article{Eggers:2010:AL:1809028.1806608,
 author = {Eggers, Susan},
 title = {2010 Athena lecture},
 abstract = {Susan Eggers, a Professor of Computer Science and Engineering at the University of Washington, joined her department in 1989. She received a B.A. in 1965 from Connecticut College and a Ph.D. in 1989 from the University of California, Berkeley. Her research interests are in computer architecture and back-end compiler optimization, with an emphasis on experimental performance analysis. With her colleague Hank Levy and their students, she developed the first commercially viable multithreaded architecture, Simultaneous Multithreading, adopted by Intel (as Hyperthreading), IBM, Sun and others. Her current research is in the areas of distributed dataflow machines, FPGAs and chip multiprocessors. In 1989 Professor Eggers was awarded an IBM Faculty Development Award, in 1990 an NSF Presidential Young Investigator Award, in 1994 the Microsoft Professorship in Computer Science and Engineering, and in 2009 the ACM-W Athena Lecturer. She is a Fellow of the ACM and IEEE, a Fellow of the AAAS, and a member of the National Academy of Engineering.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {98--98},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1809028.1806608},
 doi = {http://doi.acm.org/10.1145/1809028.1806608},
 acmid = {1806608},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {invited talk},
} 

@inproceedings{Eggers:2010:AL:1806596.1806608,
 author = {Eggers, Susan},
 title = {2010 Athena lecture},
 abstract = {Susan Eggers, a Professor of Computer Science and Engineering at the University of Washington, joined her department in 1989. She received a B.A. in 1965 from Connecticut College and a Ph.D. in 1989 from the University of California, Berkeley. Her research interests are in computer architecture and back-end compiler optimization, with an emphasis on experimental performance analysis. With her colleague Hank Levy and their students, she developed the first commercially viable multithreaded architecture, Simultaneous Multithreading, adopted by Intel (as Hyperthreading), IBM, Sun and others. Her current research is in the areas of distributed dataflow machines, FPGAs and chip multiprocessors. In 1989 Professor Eggers was awarded an IBM Faculty Development Award, in 1990 an NSF Presidential Young Investigator Award, in 1994 the Microsoft Professorship in Computer Science and Engineering, and in 2009 the ACM-W Athena Lecturer. She is a Fellow of the ACM and IEEE, a Fellow of the AAAS, and a member of the National Academy of Engineering.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {98--98},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1806596.1806608},
 doi = {http://doi.acm.org/10.1145/1806596.1806608},
 acmid = {1806608},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {invited talk},
} 

@article{Yang:2010:SLI:1809028.1806610,
 author = {Yang, Jean and Hawblitzel, Chris},
 title = {Safe to the last instruction: automated verification of a type-safe operating system},
 abstract = {Typed assembly language (TAL) and Hoare logic can verify the absence of many kinds of errors in low-level code. We use TAL and Hoare logic to achieve highly automated, static verification of the safety of a new operating system called Verve. Our techniques and tools mechanically verify the safety of every assembly language instruction in the operating system, run-time system, drivers, and applications (in fact, every part of the system software except the boot loader). Verve consists of a "Nucleus" that provides primitive access to hardware and memory, a kernel that builds services on top of the Nucleus, and applications that run on top of the kernel. The Nucleus, written in verified assembly language, implements allocation, garbage collection, multiple stacks, interrupt handling, and device access. The kernel, written in C# and compiled to TAL, builds higher-level services, such as preemptive threads, on top of the Nucleus. A TAL checker verifies the safety of the kernel and applications. A Hoare-style verifier with an automated theorem prover verifies both the safety and correctness of the Nucleus. Verve is, to the best of our knowledge, the first operating system mechanically verified to guarantee both type and memory safety. More generally, Verve's approach demonstrates a practical way to mix high-level typed code with low-level untyped code in a verifiably safe manner.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {99--110},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806610},
 doi = {http://doi.acm.org/10.1145/1809028.1806610},
 acmid = {1806610},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating system, run-time system, type safety, verification},
} 

@inproceedings{Yang:2010:SLI:1806596.1806610,
 author = {Yang, Jean and Hawblitzel, Chris},
 title = {Safe to the last instruction: automated verification of a type-safe operating system},
 abstract = {Typed assembly language (TAL) and Hoare logic can verify the absence of many kinds of errors in low-level code. We use TAL and Hoare logic to achieve highly automated, static verification of the safety of a new operating system called Verve. Our techniques and tools mechanically verify the safety of every assembly language instruction in the operating system, run-time system, drivers, and applications (in fact, every part of the system software except the boot loader). Verve consists of a "Nucleus" that provides primitive access to hardware and memory, a kernel that builds services on top of the Nucleus, and applications that run on top of the kernel. The Nucleus, written in verified assembly language, implements allocation, garbage collection, multiple stacks, interrupt handling, and device access. The kernel, written in C# and compiled to TAL, builds higher-level services, such as preemptive threads, on top of the Nucleus. A TAL checker verifies the safety of the kernel and applications. A Hoare-style verifier with an automated theorem prover verifies both the safety and correctness of the Nucleus. Verve is, to the best of our knowledge, the first operating system mechanically verified to guarantee both type and memory safety. More generally, Verve's approach demonstrates a practical way to mix high-level typed code with low-level untyped code in a verifiably safe manner.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {99--110},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806610},
 doi = {http://doi.acm.org/10.1145/1806596.1806610},
 acmid = {1806610},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating system, run-time system, type safety, verification},
} 

@inproceedings{Tatlock:2010:BEV:1806596.1806611,
 author = {Tatlock, Zachary and Lerner, Sorin},
 title = {Bringing extensibility to verified compilers},
 abstract = {Verified compilers, such as Leroy's CompCert, are accompanied by a fully checked correctness proof. Both the compiler and proof are often constructed with an interactive proof assistant. This technique provides a strong, end-to-end correctness guarantee on top of a small trusted computing base. Unfortunately, these compilers are also challenging to extend since each additional transformation must be proven correct in full formal detail. At the other end of the spectrum, techniques for compiler correctness based on a domain-specific language for writing optimizations, such as Lerner's Rhodium and Cobalt, make the compiler easy to extend: the correctness of additional transformations can be checked completely automatically. Unfortunately, these systems provide a weaker guarantee since their end-to-end correctness has not been proven fully formally. We present an approach for compiler correctness that provides the best of both worlds by bridging the gap between compiler verification and compiler extensibility. In particular, we have extended Leroy's CompCert compiler with an execution engine for optimizations written in a domain specific and proved that this execution engine preserves program semantics, using the Coq proof assistant. We present our CompCert extension, XCert, including the details of its execution engine and proof of correctness in Coq. Furthermore, we report on the important lessons learned for making the proof development manageable.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {111--121},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806611},
 doi = {http://doi.acm.org/10.1145/1806596.1806611},
 acmid = {1806611},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler optimization, correctness, extensibility},
} 

@article{Tatlock:2010:BEV:1809028.1806611,
 author = {Tatlock, Zachary and Lerner, Sorin},
 title = {Bringing extensibility to verified compilers},
 abstract = {Verified compilers, such as Leroy's CompCert, are accompanied by a fully checked correctness proof. Both the compiler and proof are often constructed with an interactive proof assistant. This technique provides a strong, end-to-end correctness guarantee on top of a small trusted computing base. Unfortunately, these compilers are also challenging to extend since each additional transformation must be proven correct in full formal detail. At the other end of the spectrum, techniques for compiler correctness based on a domain-specific language for writing optimizations, such as Lerner's Rhodium and Cobalt, make the compiler easy to extend: the correctness of additional transformations can be checked completely automatically. Unfortunately, these systems provide a weaker guarantee since their end-to-end correctness has not been proven fully formally. We present an approach for compiler correctness that provides the best of both worlds by bridging the gap between compiler verification and compiler extensibility. In particular, we have extended Leroy's CompCert compiler with an execution engine for optimizations written in a domain specific and proved that this execution engine preserves program semantics, using the Coq proof assistant. We present our CompCert extension, XCert, including the details of its execution engine and proof of correctness in Coq. Furthermore, we report on the important lessons learned for making the proof development manageable.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {111--121},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806611},
 doi = {http://doi.acm.org/10.1145/1809028.1806611},
 acmid = {1806611},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler optimization, correctness, extensibility},
} 

@inproceedings{Chlipala:2010:USM:1806596.1806612,
 author = {Chlipala, Adam},
 title = {Ur: statically-typed metaprogramming with type-level record computation},
 abstract = {Dependent types</i> provide a strong foundation for specifying and verifying rich properties of programs through type-checking. The earliest implementations combined dependency, which allows types to mention program variables; with type-level computation, which facilitates expressive specifications that compute with recursive functions over types. While many recent applications of dependent types omit the latter facility, we argue in this paper that it deserves more attention, even when implemented without dependency. In particular, the ability to use functional programs as specifications enables statically-typed metaprogramming</i>: programs write programs, and static type-checking guarantees that the generating process never produces invalid code. Since our focus is on generic validity properties rather than full correctness verification, it is possible to engineer type inference systems that are very effective in narrow domains. As a demonstration, we present Ur, a programming language designed to facilitate metaprogramming with first-class records and names. On top of Ur, we implement Ur/Web, a special standard library that enables the development of modern Web applications. Ad-hoc code generation is already in wide use in the popular Web application frameworks, and we show how that generation may be tamed using types, without forcing metaprogram authors to write proofs or forcing metaprogram users to write any fancy types.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806612},
 doi = {http://doi.acm.org/10.1145/1806596.1806612},
 acmid = {1806612},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, metaprogramming},
} 

@article{Chlipala:2010:USM:1809028.1806612,
 author = {Chlipala, Adam},
 title = {Ur: statically-typed metaprogramming with type-level record computation},
 abstract = {Dependent types</i> provide a strong foundation for specifying and verifying rich properties of programs through type-checking. The earliest implementations combined dependency, which allows types to mention program variables; with type-level computation, which facilitates expressive specifications that compute with recursive functions over types. While many recent applications of dependent types omit the latter facility, we argue in this paper that it deserves more attention, even when implemented without dependency. In particular, the ability to use functional programs as specifications enables statically-typed metaprogramming</i>: programs write programs, and static type-checking guarantees that the generating process never produces invalid code. Since our focus is on generic validity properties rather than full correctness verification, it is possible to engineer type inference systems that are very effective in narrow domains. As a demonstration, we present Ur, a programming language designed to facilitate metaprogramming with first-class records and names. On top of Ur, we implement Ur/Web, a special standard library that enables the development of modern Web applications. Ad-hoc code generation is already in wide use in the popular Web application frameworks, and we show how that generation may be tamed using types, without forcing metaprogram authors to write proofs or forcing metaprogram users to write any fancy types.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806612},
 doi = {http://doi.acm.org/10.1145/1809028.1806612},
 acmid = {1806612},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependent types, metaprogramming},
} 

@inproceedings{Emmi:2010:PVT:1806596.1806613,
 author = {Emmi, Michael and Majumdar, Rupak and Manevich, Roman},
 title = {Parameterized verification of transactional memories},
 abstract = {We describe an automatic verification method to check whether transactional memories ensure strict serializability a key property assumed of the transactional interface. Our main contribution is a technique for effectively verifying parameterized systems. The technique merges ideas from parameterized hardware and protocol verification--verification by invisible invariants and symmetry reduction--with ideas from software verification--template-based invariant generation and satisfiability checking for quantified formul\&#230; (modulo theories). The combination enables us to precisely model and analyze unbounded systems while taming state explosion. Our technique enables automated proofs that two-phase locking (TPL), dynamic software transactional memory (DSTM), and transactional locking II (TL2) systems ensure strict serializability. The verification is challenging since the systems are unbounded in several dimensions: the number and length of concurrently executing transactions, and the size of the shared memory they access, have no finite limit. In contrast, state-of-the-art software model checking tools such as BLAST and TVLA are unable to validate either system, due to inherent expressiveness limitations or state explosion.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {134--145},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806613},
 doi = {http://doi.acm.org/10.1145/1806596.1806613},
 acmid = {1806613},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parameterized verification, transactional memory},
} 

@article{Emmi:2010:PVT:1809028.1806613,
 author = {Emmi, Michael and Majumdar, Rupak and Manevich, Roman},
 title = {Parameterized verification of transactional memories},
 abstract = {We describe an automatic verification method to check whether transactional memories ensure strict serializability a key property assumed of the transactional interface. Our main contribution is a technique for effectively verifying parameterized systems. The technique merges ideas from parameterized hardware and protocol verification--verification by invisible invariants and symmetry reduction--with ideas from software verification--template-based invariant generation and satisfiability checking for quantified formul\&#230; (modulo theories). The combination enables us to precisely model and analyze unbounded systems while taming state explosion. Our technique enables automated proofs that two-phase locking (TPL), dynamic software transactional memory (DSTM), and transactional locking II (TL2) systems ensure strict serializability. The verification is challenging since the systems are unbounded in several dimensions: the number and length of concurrently executing transactions, and the size of the shared memory they access, have no finite limit. In contrast, state-of-the-art software model checking tools such as BLAST and TVLA are unable to validate either system, due to inherent expressiveness limitations or state explosion.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {134--145},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806613},
 doi = {http://doi.acm.org/10.1145/1809028.1806613},
 acmid = {1806613},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parameterized verification, transactional memory},
} 

@article{Pizlo:2010:SFR:1809028.1806615,
 author = {Pizlo, Filip and Ziarek, Lukasz and Maj, Petr and Hosking, Antony L. and Blanton, Ethan and Vitek, Jan},
 title = {Schism: fragmentation-tolerant real-time garbage collection},
 abstract = {Managed languages such as Java and C# are being considered for use in hard real-time systems. A hurdle to their widespread adoption is the lack of garbage collection algorithms that offer predictable space-and-time performance in the face of fragmentation. We introduce SCHISM/CMR, a new concurrent and real-time garbage collector that is fragmentation tolerant and guarantees time-and-space worst-case bounds while providing good throughput. SCHISM/CMR combines mark-region collection of fragmented objects and arrays (arraylets) with separate replication-copying collection of immutable arraylet spines, so as to cope with external fragmentation when running in small heaps. We present an implementation of SCHISM/CMR in the Fiji VM, a high-performance Java virtual machine for mission-critical systems, along with a thorough experimental evaluation on a wide variety of architectures, including server-class and embedded systems. The results show that SCHISM/CMR tolerates fragmentation better than previous schemes, with a much more acceptable throughput penalty.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {146--159},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806615},
 doi = {http://doi.acm.org/10.1145/1809028.1806615},
 acmid = {1806615},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fragmentation, mark-region, mark-sweep, real-time, replication-copying},
} 

@inproceedings{Pizlo:2010:SFR:1806596.1806615,
 author = {Pizlo, Filip and Ziarek, Lukasz and Maj, Petr and Hosking, Antony L. and Blanton, Ethan and Vitek, Jan},
 title = {Schism: fragmentation-tolerant real-time garbage collection},
 abstract = {Managed languages such as Java and C# are being considered for use in hard real-time systems. A hurdle to their widespread adoption is the lack of garbage collection algorithms that offer predictable space-and-time performance in the face of fragmentation. We introduce SCHISM/CMR, a new concurrent and real-time garbage collector that is fragmentation tolerant and guarantees time-and-space worst-case bounds while providing good throughput. SCHISM/CMR combines mark-region collection of fragmented objects and arrays (arraylets) with separate replication-copying collection of immutable arraylet spines, so as to cope with external fragmentation when running in small heaps. We present an implementation of SCHISM/CMR in the Fiji VM, a high-performance Java virtual machine for mission-critical systems, along with a thorough experimental evaluation on a wide variety of architectures, including server-class and embedded systems. The results show that SCHISM/CMR tolerates fragmentation better than previous schemes, with a much more acceptable throughput penalty.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {146--159},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806615},
 doi = {http://doi.acm.org/10.1145/1806596.1806615},
 acmid = {1806615},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fragmentation, mark-region, mark-sweep, real-time, replication-copying},
} 

@inproceedings{Xu:2010:DIC:1806596.1806616,
 author = {Xu, Guoqing and Rountev, Atanas},
 title = {Detecting inefficiently-used containers to avoid bloat},
 abstract = {Runtime bloat degrades significantly the performance and scalability of software systems. An important source of bloat is the inefficient use of containers. It is expensive to create inefficiently-used containers and to invoke their associated methods, as this may ultimately execute large volumes of code, with call stacks dozens deep, and allocate many temporary objects. This paper presents practical static and dynamic tools that can find inappropriate use of containers in Java programs. At the core of these tools is a base static analysis that identifies, for each container, the objects that are added to this container and the key statements (i.e., heap loads and stores) that achieve the semantics of common container operations such as ADD</i> and GET</i>. The static tool finds problematic uses of containers by considering the nesting relationships among the loops where these semantics-achieving statements</i> are located, while the dynamic tool can instrument these statements and find inefficiencies by profiling their execution frequencies. The high precision of the base analysis is achieved by taking advantage of a context-free language (CFL)-reachability formulation of points-to analysis and by accounting for container-specific properties. It is demand-driven and client-driven, facilitating refinement specific to each queried container object and increasing scalability. The tools built with the help of this analysis can be used both to avoid the creation of container-related performance problems early during development, and to help with diagnosis when problems are observed during tuning. Our experimental results show that the static tool has a low false positive rate and produces more relevant information than its dynamic counterpart. Further case studies suggest that significant optimization opportunities can be found by focusing on statically-identified containers for which high allocation frequency is observed at run time.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {160--173},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806616},
 doi = {http://doi.acm.org/10.1145/1806596.1806616},
 acmid = {1806616},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cfl reachability, container bloat, points-to analysis},
} 

@article{Xu:2010:DIC:1809028.1806616,
 author = {Xu, Guoqing and Rountev, Atanas},
 title = {Detecting inefficiently-used containers to avoid bloat},
 abstract = {Runtime bloat degrades significantly the performance and scalability of software systems. An important source of bloat is the inefficient use of containers. It is expensive to create inefficiently-used containers and to invoke their associated methods, as this may ultimately execute large volumes of code, with call stacks dozens deep, and allocate many temporary objects. This paper presents practical static and dynamic tools that can find inappropriate use of containers in Java programs. At the core of these tools is a base static analysis that identifies, for each container, the objects that are added to this container and the key statements (i.e., heap loads and stores) that achieve the semantics of common container operations such as ADD</i> and GET</i>. The static tool finds problematic uses of containers by considering the nesting relationships among the loops where these semantics-achieving statements</i> are located, while the dynamic tool can instrument these statements and find inefficiencies by profiling their execution frequencies. The high precision of the base analysis is achieved by taking advantage of a context-free language (CFL)-reachability formulation of points-to analysis and by accounting for container-specific properties. It is demand-driven and client-driven, facilitating refinement specific to each queried container object and increasing scalability. The tools built with the help of this analysis can be used both to avoid the creation of container-related performance problems early during development, and to help with diagnosis when problems are observed during tuning. Our experimental results show that the static tool has a low false positive rate and produces more relevant information than its dynamic counterpart. Further case studies suggest that significant optimization opportunities can be found by focusing on statically-identified containers for which high allocation frequency is observed at run time.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {160--173},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806616},
 doi = {http://doi.acm.org/10.1145/1809028.1806616},
 acmid = {1806616},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cfl reachability, container bloat, points-to analysis},
} 

@article{Xu:2010:FLD:1809028.1806617,
 author = {Xu, Guoqing and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Schonberg, Edith and Sevitsky, Gary},
 title = {Finding low-utility data structures},
 abstract = {Many opportunities for easy, big-win, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat. We introduce a run-time analysis to discover these low-utility</i> data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hop-to-hop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce abstract dynamic thin slicing</i>, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach. We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {174--186},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1809028.1806617},
 doi = {http://doi.acm.org/10.1145/1809028.1806617},
 acmid = {1806617},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract dynamic thin slicing, cost benefit analysis, memory bloat},
} 

@inproceedings{Xu:2010:FLD:1806596.1806617,
 author = {Xu, Guoqing and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Schonberg, Edith and Sevitsky, Gary},
 title = {Finding low-utility data structures},
 abstract = {Many opportunities for easy, big-win, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat. We introduce a run-time analysis to discover these low-utility</i> data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hop-to-hop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce abstract dynamic thin slicing</i>, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach. We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {174--186},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1806596.1806617},
 doi = {http://doi.acm.org/10.1145/1806596.1806617},
 acmid = {1806617},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract dynamic thin slicing, cost benefit analysis, memory bloat},
} 

@inproceedings{Mytkowicz:2010:EAJ:1806596.1806618,
 author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
 title = {Evaluating the accuracy of Java profilers},
 abstract = {Performance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonly-used Java profilers (xprof , hprof , jprofile, and yourkit</i>) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement. This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly. We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {187--197},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806618},
 doi = {http://doi.acm.org/10.1145/1806596.1806618},
 acmid = {1806618},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias, observer effect, profiling},
} 

@article{Mytkowicz:2010:EAJ:1809028.1806618,
 author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
 title = {Evaluating the accuracy of Java profilers},
 abstract = {Performance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonly-used Java profilers (xprof , hprof , jprofile, and yourkit</i>) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement. This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly. We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {187--197},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806618},
 doi = {http://doi.acm.org/10.1145/1809028.1806618},
 acmid = {1806618},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias, observer effect, profiling},
} 

@inproceedings{Baek:2010:GFS:1806596.1806620,
 author = {Baek, Woongki and Chilimbi, Trishul M.},
 title = {Green: a framework for supporting energy-conscious programming using controlled approximation},
 abstract = {Energy-efficient computing is important in several systems ranging from embedded devices to large scale data centers. Several application domains offer the opportunity to tradeoff quality of service/solution (QoS) for improvements in performance and reduction in energy consumption. Programmers sometimes take advantage of such opportunities, albeit in an ad-hoc manner and often without providing any QoS guarantees. We propose a system called Green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical QoS guarantees. Green enables programmers to approximate expensive functions and loops and operates in two phases. In the calibration phase, it builds a model of the QoS loss produced by the approximation. This model is used in the operational phase to make approximation decisions based on the QoS constraints specified by the programmer. The operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and QoS model to provide strong statistical QoS guarantees. To evaluate the effectiveness of Green, we implemented our system and language extensions using the Phoenix compiler framework. Our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in-production, real-world web search engine, indicate that Green can produce significant improvements in performance and energy consumption with small and controlled QoS degradation.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {198--209},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806620},
 doi = {http://doi.acm.org/10.1145/1806596.1806620},
 acmid = {1806620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {controlled approximation, energy-conscious programming},
} 

@article{Baek:2010:GFS:1809028.1806620,
 author = {Baek, Woongki and Chilimbi, Trishul M.},
 title = {Green: a framework for supporting energy-conscious programming using controlled approximation},
 abstract = {Energy-efficient computing is important in several systems ranging from embedded devices to large scale data centers. Several application domains offer the opportunity to tradeoff quality of service/solution (QoS) for improvements in performance and reduction in energy consumption. Programmers sometimes take advantage of such opportunities, albeit in an ad-hoc manner and often without providing any QoS guarantees. We propose a system called Green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical QoS guarantees. Green enables programmers to approximate expensive functions and loops and operates in two phases. In the calibration phase, it builds a model of the QoS loss produced by the approximation. This model is used in the operational phase to make approximation decisions based on the QoS constraints specified by the programmer. The operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and QoS model to provide strong statistical QoS guarantees. To evaluate the effectiveness of Green, we implemented our system and language extensions using the Phoenix compiler framework. Our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in-production, real-world web search engine, indicate that Green can produce significant improvements in performance and energy consumption with small and controlled QoS degradation.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {198--209},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806620},
 doi = {http://doi.acm.org/10.1145/1809028.1806620},
 acmid = {1806620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {controlled approximation, energy-conscious programming},
} 

@inproceedings{Rajan:2010:GPM:1806596.1806621,
 author = {Rajan, Kaushik and Rajamani, Sriram and Yaduvanshi, Shashank},
 title = {GUESSTIMATE: a programming model for collaborative distributed systems},
 abstract = {We present a new programming model GUEESSTIMATE for developing collaborative distributed systems. The model allows atomic, isolated operations that transform a system from consistent state to consistent state, and provides a shared transactional store for a collection of such operations executed by various machines in a distributed system. In addition to "committed state" which is identical in all machines in the distributed system, GUESSTIMATE allows each machine to have a replicated local copy of the state (called "guesstimated state") so that operations on shared state can be executed locally without any blocking, while also guaranteeing that eventually all machines agree on the sequences of operations executed. Thus, each operation is executed multiple times, once at the time of issue when it updates the guesstimated state of the issuing machine, once when the operation is committed (atomically) to the committed state of all machines, and several times in between as the guesstimated state converges toward the committed state. While we expect the results of these executions of the operation to be identical most of the time in the class of applications we study, it is possible for an operation to succeed the first time when it is executed on the guesstimated state, and fail when it is committed. GUESSTIMATE provides facilities that allow the programmer to deal with this potential discrepancy. This paper presents our programming model, its operational semantics, its realization as an API in C#, and our experience building collaborative distributed applications with this model.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806621},
 doi = {http://doi.acm.org/10.1145/1806596.1806621},
 acmid = {1806621},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative applications, concurrency, distributed systems, language extensions},
} 

@article{Rajan:2010:GPM:1809028.1806621,
 author = {Rajan, Kaushik and Rajamani, Sriram and Yaduvanshi, Shashank},
 title = {GUESSTIMATE: a programming model for collaborative distributed systems},
 abstract = {We present a new programming model GUEESSTIMATE for developing collaborative distributed systems. The model allows atomic, isolated operations that transform a system from consistent state to consistent state, and provides a shared transactional store for a collection of such operations executed by various machines in a distributed system. In addition to "committed state" which is identical in all machines in the distributed system, GUESSTIMATE allows each machine to have a replicated local copy of the state (called "guesstimated state") so that operations on shared state can be executed locally without any blocking, while also guaranteeing that eventually all machines agree on the sequences of operations executed. Thus, each operation is executed multiple times, once at the time of issue when it updates the guesstimated state of the issuing machine, once when the operation is committed (atomically) to the committed state of all machines, and several times in between as the guesstimated state converges toward the committed state. While we expect the results of these executions of the operation to be identical most of the time in the class of applications we study, it is possible for an operation to succeed the first time when it is executed on the guesstimated state, and fail when it is committed. GUESSTIMATE provides facilities that allow the programmer to deal with this potential discrepancy. This paper presents our programming model, its operational semantics, its realization as an API in C#, and our experience building collaborative distributed applications with this model.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806621},
 doi = {http://doi.acm.org/10.1145/1809028.1806621},
 acmid = {1806621},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative applications, concurrency, distributed systems, language extensions},
} 

@inproceedings{Xi:2010:CML:1806596.1806622,
 author = {Xi, Qian and Walker, David},
 title = {A context-free markup language for semi-structured text},
 abstract = {An ad hoc data format</i> is any nonstandard, semi-structured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis</i>, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {221--232},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806622},
 doi = {http://doi.acm.org/10.1145/1806596.1806622},
 acmid = {1806622},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ANNE, PADS, ad hoc data, domain-specific languages, tool generation},
} 

@article{Xi:2010:CML:1809028.1806622,
 author = {Xi, Qian and Walker, David},
 title = {A context-free markup language for semi-structured text},
 abstract = {An ad hoc data format</i> is any nonstandard, semi-structured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis</i>, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {221--232},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806622},
 doi = {http://doi.acm.org/10.1145/1809028.1806622},
 acmid = {1806622},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ANNE, PADS, ad hoc data, domain-specific languages, tool generation},
} 

@article{Loitsch:2010:PFN:1809028.1806623,
 author = {Loitsch, Florian},
 title = {Printing floating-point numbers quickly and accurately with integers},
 abstract = {We present algorithms for accurately converting floating-point numbers to decimal representation. They are fast (up to 4 times faster than commonly used algorithms that use high-precision integers) and correct: any printed number will evaluate to the same number, when read again. Our algorithms are fast, because they require only fixed-size integer arithmetic. The sole requirement for the integer type is that it has at least two more bits than the significand of the floating-point number. Hence, for IEEE 754 double-precision numbers (having a 53-bit significand) an integer type with 55 bits is sufficient. Moreover we show how to exploit additional bits to improve the generated output. We present three algorithms with different properties: the first algorithm is the most basic one, and does not take advantage of any extra bits. It simply shows how to perform the binary-to-decimal transformation with the minimal number of bits. Our second algorithm improves on the first one by using the additional bits to produce a shorter (often the shortest) result. Finally we propose a third version that can be used when the shortest output is a requirement. The last algorithm either produces optimal decimal representations (with respect to shortness and rounding) or rejects its input. For IEEE 754 double-precision numbers and 64-bit integers roughly 99.4\% of all numbers can be processed efficiently. The remaining 0.6\% are rejected and need to be printed by a slower complete algorithm.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806623},
 doi = {http://doi.acm.org/10.1145/1809028.1806623},
 acmid = {1806623},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dtoa, floating-point printing},
} 

@inproceedings{Loitsch:2010:PFN:1806596.1806623,
 author = {Loitsch, Florian},
 title = {Printing floating-point numbers quickly and accurately with integers},
 abstract = {We present algorithms for accurately converting floating-point numbers to decimal representation. They are fast (up to 4 times faster than commonly used algorithms that use high-precision integers) and correct: any printed number will evaluate to the same number, when read again. Our algorithms are fast, because they require only fixed-size integer arithmetic. The sole requirement for the integer type is that it has at least two more bits than the significand of the floating-point number. Hence, for IEEE 754 double-precision numbers (having a 53-bit significand) an integer type with 55 bits is sufficient. Moreover we show how to exploit additional bits to improve the generated output. We present three algorithms with different properties: the first algorithm is the most basic one, and does not take advantage of any extra bits. It simply shows how to perform the binary-to-decimal transformation with the minimal number of bits. Our second algorithm improves on the first one by using the additional bits to produce a shorter (often the shortest) result. Finally we propose a third version that can be used when the shortest output is a requirement. The last algorithm either produces optimal decimal representations (with respect to shortness and rounding) or rejects its input. For IEEE 754 double-precision numbers and 64-bit integers roughly 99.4\% of all numbers can be processed efficiently. The remaining 0.6\% are rejected and need to be printed by a slower complete algorithm.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806623},
 doi = {http://doi.acm.org/10.1145/1806596.1806623},
 acmid = {1806623},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dtoa, floating-point printing},
} 

@inproceedings{Flanagan:2010:AMD:1806596.1806625,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {Adversarial memory for detecting destructive races},
 abstract = {Multithreaded programs are notoriously prone to race conditions, a problem exacerbated by the widespread adoption of multi-core processors with complex memory models and cache coherence protocols. Much prior work has focused on static and dynamic analyses for race detection, but these algorithms typically are unable to distinguish destructive races that cause erroneous behavior from benign races that do not. Performing this classification manually is difficult, time consuming, and error prone. This paper presents a new dynamic analysis technique that uses adversarial memory</i> to classify race conditions as destructive or benign on systems with relaxed memory models. Unlike a typical language implementation, which may only infrequently exhibit non-sequentially consistent behavior, our adversarial memory implementation exploits the full freedom of the memory model to return older, unexpected, or stale values for memory reads whenever possible, in an attempt to crash the target program (that is, to force the program to behave erroneously). A crashing execution provides concrete evidence of a destructive bug, and this bug can be strongly correlated with a specific race condition in the target program. Experimental results with our Jumble prototype for Java demonstrate that adversarial memory is highly effective at identifying destructive race conditions, and in distinguishing them from race conditions that are real but benign. Adversarial memory can also reveal destructive races that would not be detected by traditional testing (even after thousands of runs) or by model checkers that assume sequential consistency.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806625},
 doi = {http://doi.acm.org/10.1145/1806596.1806625},
 acmid = {1806625},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race conditions, relaxed memory models},
} 

@article{Flanagan:2010:AMD:1809028.1806625,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {Adversarial memory for detecting destructive races},
 abstract = {Multithreaded programs are notoriously prone to race conditions, a problem exacerbated by the widespread adoption of multi-core processors with complex memory models and cache coherence protocols. Much prior work has focused on static and dynamic analyses for race detection, but these algorithms typically are unable to distinguish destructive races that cause erroneous behavior from benign races that do not. Performing this classification manually is difficult, time consuming, and error prone. This paper presents a new dynamic analysis technique that uses adversarial memory</i> to classify race conditions as destructive or benign on systems with relaxed memory models. Unlike a typical language implementation, which may only infrequently exhibit non-sequentially consistent behavior, our adversarial memory implementation exploits the full freedom of the memory model to return older, unexpected, or stale values for memory reads whenever possible, in an attempt to crash the target program (that is, to force the program to behave erroneously). A crashing execution provides concrete evidence of a destructive bug, and this bug can be strongly correlated with a specific race condition in the target program. Experimental results with our Jumble prototype for Java demonstrate that adversarial memory is highly effective at identifying destructive race conditions, and in distinguishing them from race conditions that are real but benign. Adversarial memory can also reveal destructive races that would not be detected by traditional testing (even after thousands of runs) or by model checkers that assume sequential consistency.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806625},
 doi = {http://doi.acm.org/10.1145/1809028.1806625},
 acmid = {1806625},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, dynamic analysis, race conditions, relaxed memory models},
} 

@article{Bond:2010:PPD:1809028.1806626,
 author = {Bond, Michael D. and Coons, Katherine E. and McKinley, Kathryn S.},
 title = {PACER: proportional detection of data races},
 abstract = {Data races indicate serious concurrency bugs such as order, atomicity, and sequential consistency violations. Races are difficult to find and fix, often manifesting only after deployment. The frequency and unpredictability of these bugs will only increase as software adds parallelism to exploit multicore hardware. Unfortunately, sound and precise race detectors slow programs by factors of eight or more and do not scale to large numbers of threads. This paper presents a precise, low-overhead sampling-based</i> data race detector called Pacer. PACER makes a proportionality</i> guarantee: it detects any race at a rate equal to the sampling rate, by finding races whose first access occurs during a global sampling period. During sampling, PACER tracks all accesses using the dynamically sound and precise FastTrack algorithm. In nonsampling periods, Pacer discards sampled access information that cannot be part of a reported race, and</i> Pacer simplifies tracking of the happens-before relationship, yielding near-constant, instead of linear, overheads. Experimental results confirm our theoretical guarantees. PACER reports races in proportion to the sampling rate. Its time and space overheads scale with the sampling rate, and sampling rates of 1-3\% yield overheads low enough to consider in production software. The resulting system provides a "get what you pay for" approach that is suitable for identifying real, hard-to-reproduce races in deployed systems.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {255--268},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806626},
 doi = {http://doi.acm.org/10.1145/1809028.1806626},
 acmid = {1806626},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bugs, concurrency, data races, sampling},
} 

@inproceedings{Bond:2010:PPD:1806596.1806626,
 author = {Bond, Michael D. and Coons, Katherine E. and McKinley, Kathryn S.},
 title = {PACER: proportional detection of data races},
 abstract = {Data races indicate serious concurrency bugs such as order, atomicity, and sequential consistency violations. Races are difficult to find and fix, often manifesting only after deployment. The frequency and unpredictability of these bugs will only increase as software adds parallelism to exploit multicore hardware. Unfortunately, sound and precise race detectors slow programs by factors of eight or more and do not scale to large numbers of threads. This paper presents a precise, low-overhead sampling-based</i> data race detector called Pacer. PACER makes a proportionality</i> guarantee: it detects any race at a rate equal to the sampling rate, by finding races whose first access occurs during a global sampling period. During sampling, PACER tracks all accesses using the dynamically sound and precise FastTrack algorithm. In nonsampling periods, Pacer discards sampled access information that cannot be part of a reported race, and</i> Pacer simplifies tracking of the happens-before relationship, yielding near-constant, instead of linear, overheads. Experimental results confirm our theoretical guarantees. PACER reports races in proportion to the sampling rate. Its time and space overheads scale with the sampling rate, and sampling rates of 1-3\% yield overheads low enough to consider in production software. The resulting system provides a "get what you pay for" approach that is suitable for identifying real, hard-to-reproduce races in deployed systems.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {255--268},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806626},
 doi = {http://doi.acm.org/10.1145/1806596.1806626},
 acmid = {1806626},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bugs, concurrency, data races, sampling},
} 

@article{Nakaike:2010:LER:1809028.1806627,
 author = {Nakaike, Takuya and Michael, Maged M.},
 title = {Lock elision for read-only critical sections in Java},
 abstract = {It is not uncommon in parallel workloads to encounter shared data structures with read-mostly access patterns, where operations that update data are infrequent and most operations are read-only. Typically, data consistency is guaranteed using mutual exclusion or read-write locks. The cost of atomic update of lock variables result in high overheads and high cache coherence traffic under active sharing, thus slowing down single thread performance and limiting scalability. In this paper, we present SOLERO (Software Optimistic Lock Elision for Read-Only critical sections)</i>, a new lock implementation called for optimizing read-only critical sections in Java based on sequential locks. SOLERO is compatible with the conventional lock implementation of Java. However, unlike the conventional implementation, only critical sections that may write data or have side effects need to update lock variables, while read-only critical sections need only read lock variables without writing them. Each writing critical section changes the lock value to a new value. Hence, a read-only critical section is guaranteed to be consistent if the lock is free and its value does not change from the beginning to the end of the read-only critical section. Using Java workloads including SPECjbb2005 and the HashMap and TreeMap Java classes, we evaluate the performance impact of applying SOLERO to read-mostly locks. Our experimental results show performance improvements across the board, often substantial, in both single thread speed and scalability over the conventional lock implementation (mutual exclusion) and read-write locks. SOLERO improves the performance of SPECjbb2005 by 3-5\% on single and multiple threads. The results using the HashMap and TreeMap benchmarks show that SOLERO outperforms the conventional lock implementation and read-write locks by substantial multiples on multi-threads.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1809028.1806627},
 doi = {http://doi.acm.org/10.1145/1809028.1806627},
 acmid = {1806627},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, just-in-time compiler, lock, lock elision, monitor, optimization, synchronization},
} 

@inproceedings{Nakaike:2010:LER:1806596.1806627,
 author = {Nakaike, Takuya and Michael, Maged M.},
 title = {Lock elision for read-only critical sections in Java},
 abstract = {It is not uncommon in parallel workloads to encounter shared data structures with read-mostly access patterns, where operations that update data are infrequent and most operations are read-only. Typically, data consistency is guaranteed using mutual exclusion or read-write locks. The cost of atomic update of lock variables result in high overheads and high cache coherence traffic under active sharing, thus slowing down single thread performance and limiting scalability. In this paper, we present SOLERO (Software Optimistic Lock Elision for Read-Only critical sections)</i>, a new lock implementation called for optimizing read-only critical sections in Java based on sequential locks. SOLERO is compatible with the conventional lock implementation of Java. However, unlike the conventional implementation, only critical sections that may write data or have side effects need to update lock variables, while read-only critical sections need only read lock variables without writing them. Each writing critical section changes the lock value to a new value. Hence, a read-only critical section is guaranteed to be consistent if the lock is free and its value does not change from the beginning to the end of the read-only critical section. Using Java workloads including SPECjbb2005 and the HashMap and TreeMap Java classes, we evaluate the performance impact of applying SOLERO to read-mostly locks. Our experimental results show performance improvements across the board, often substantial, in both single thread speed and scalability over the conventional lock implementation (mutual exclusion) and read-write locks. SOLERO improves the performance of SPECjbb2005 by 3-5\% on single and multiple threads. The results using the HashMap and TreeMap benchmarks show that SOLERO outperforms the conventional lock implementation and read-write locks by substantial multiples on multi-threads.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1806596.1806627},
 doi = {http://doi.acm.org/10.1145/1806596.1806627},
 acmid = {1806627},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, just-in-time compiler, lock, lock elision, monitor, optimization, synchronization},
} 

@inproceedings{Chaudhuri:2010:SI:1806596.1806629,
 author = {Chaudhuri, Swarat and Solar-Lezama, Armando},
 title = {Smooth interpretation},
 abstract = {We present smooth interpretation</i>, a method to systematically approximate numerical imperative programs by smooth mathematical functions. This approximation facilitates the use of numerical search techniques like gradient descent for program analysis and synthesis. The method extends to programs the notion of Gaussian smoothing</i>, a popular signal-processing technique that filters out noise and discontinuities from a signal by taking its convolution with a Gaussian function. In our setting, Gaussian smoothing executes a program according to a probabilistic semantics; the execution of program P</i> on an input x</i> after Gaussian smoothing can be summarized as follows: (1) Apply a Gaussian perturbation to x</i> -- the perturbed input is a random variable following a normal distribution with mean x</i>. (2) Compute and return the expected output</i> of P</i> on this perturbed input. Computing the expectation explicitly would require the execution of P</i> on all possible inputs, but smooth interpretation bypasses this requirement by using a form of symbolic execution to approximate the effect of Gaussian smoothing on P</i>. The result is an efficient but approximate implementation of Gaussian smoothing of programs. Smooth interpretation has the effect of attenuating features of a program that impede numerical searches of its input space -- for example, discontinuities resulting from conditional branches are replaced by continuous transitions. We apply smooth interpretation to the problem of synthesizing values of numerical control parameters in embedded control applications. This problem is naturally formulated as one of numerical optimization: the goal is to find parameter values that minimize the error between the resulting program and a programmer-provided behavioral specification. Solving this problem by directly applying numerical optimization techniques is often impractical due to the discontinuities in the error function. By eliminating these discontinuities, smooth interpretation makes it possible to search the parameter space efficiently by means of simple gradient descent. Our experiments demonstrate the value of this strategy in synthesizing parameters for several challenging programs, including models of an automated gear shift and a PID controller.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {279--291},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1806596.1806629},
 doi = {http://doi.acm.org/10.1145/1806596.1806629},
 acmid = {1806629},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuity, parameter synthesis, program smoothing},
} 

@article{Chaudhuri:2010:SI:1809028.1806629,
 author = {Chaudhuri, Swarat and Solar-Lezama, Armando},
 title = {Smooth interpretation},
 abstract = {We present smooth interpretation</i>, a method to systematically approximate numerical imperative programs by smooth mathematical functions. This approximation facilitates the use of numerical search techniques like gradient descent for program analysis and synthesis. The method extends to programs the notion of Gaussian smoothing</i>, a popular signal-processing technique that filters out noise and discontinuities from a signal by taking its convolution with a Gaussian function. In our setting, Gaussian smoothing executes a program according to a probabilistic semantics; the execution of program P</i> on an input x</i> after Gaussian smoothing can be summarized as follows: (1) Apply a Gaussian perturbation to x</i> -- the perturbed input is a random variable following a normal distribution with mean x</i>. (2) Compute and return the expected output</i> of P</i> on this perturbed input. Computing the expectation explicitly would require the execution of P</i> on all possible inputs, but smooth interpretation bypasses this requirement by using a form of symbolic execution to approximate the effect of Gaussian smoothing on P</i>. The result is an efficient but approximate implementation of Gaussian smoothing of programs. Smooth interpretation has the effect of attenuating features of a program that impede numerical searches of its input space -- for example, discontinuities resulting from conditional branches are replaced by continuous transitions. We apply smooth interpretation to the problem of synthesizing values of numerical control parameters in embedded control applications. This problem is naturally formulated as one of numerical optimization: the goal is to find parameter values that minimize the error between the resulting program and a programmer-provided behavioral specification. Solving this problem by directly applying numerical optimization techniques is often impractical due to the discontinuities in the error function. By eliminating these discontinuities, smooth interpretation makes it possible to search the parameter space efficiently by means of simple gradient descent. Our experiments demonstrate the value of this strategy in synthesizing parameters for several challenging programs, including models of an automated gear shift and a PID controller.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {279--291},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1809028.1806629},
 doi = {http://doi.acm.org/10.1145/1809028.1806629},
 acmid = {1806629},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuity, parameter synthesis, program smoothing},
} 

@inproceedings{Gulwani:2010:RP:1806596.1806630,
 author = {Gulwani, Sumit and Zuleger, Florian},
 title = {The reachability-bound problem},
 abstract = {We define the reachability-bound problem</i> to be the problem of finding a symbolic worst-case bound on the number of times a given control location inside a procedure is visited in terms of the inputs to that procedure. This has applications in bounding resources consumed by a program such as time, memory, network-traffic, power, as well as estimating quantitative properties (as opposed to boolean properties) of data in programs, such as information leakage or uncertainty propagation. Our approach to solving the reachability-bound problem brings together two different techniques for reasoning about loops in an effective manner. One of these techniques is an abstract-interpretation based iterative technique for computing precise disjunctive invariants (to summarize nested loops). The other technique is a non-iterative proof-rules based technique (for loop bound computation) that takes over the role of doing inductive reasoning, while deriving its power from the use of SMT solvers to reason about abstract loop-free fragments. Our solution to the reachability-bound problem allows us to compute precise symbolic complexity bounds for several loops in .Net base-class libraries for which earlier techniques fail. We also illustrate the precision of our algorithm for disjunctive invariant computation (which has a more general applicability beyond the reachability-bound problem) on a set of benchmark examples.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {292--304},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1806596.1806630},
 doi = {http://doi.acm.org/10.1145/1806596.1806630},
 acmid = {1806630},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disjunctive invariants, pattern matching, ranking functions, resource bound analysis, transitive closure},
} 

@article{Gulwani:2010:RP:1809028.1806630,
 author = {Gulwani, Sumit and Zuleger, Florian},
 title = {The reachability-bound problem},
 abstract = {We define the reachability-bound problem</i> to be the problem of finding a symbolic worst-case bound on the number of times a given control location inside a procedure is visited in terms of the inputs to that procedure. This has applications in bounding resources consumed by a program such as time, memory, network-traffic, power, as well as estimating quantitative properties (as opposed to boolean properties) of data in programs, such as information leakage or uncertainty propagation. Our approach to solving the reachability-bound problem brings together two different techniques for reasoning about loops in an effective manner. One of these techniques is an abstract-interpretation based iterative technique for computing precise disjunctive invariants (to summarize nested loops). The other technique is a non-iterative proof-rules based technique (for loop bound computation) that takes over the role of doing inductive reasoning, while deriving its power from the use of SMT solvers to reason about abstract loop-free fragments. Our solution to the reachability-bound problem allows us to compute precise symbolic complexity bounds for several loops in .Net base-class libraries for which earlier techniques fail. We also illustrate the precision of our algorithm for disjunctive invariant computation (which has a more general applicability beyond the reachability-bound problem) on a set of benchmark examples.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {292--304},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1809028.1806630},
 doi = {http://doi.acm.org/10.1145/1809028.1806630},
 acmid = {1806630},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disjunctive invariants, pattern matching, ranking functions, resource bound analysis, transitive closure},
} 

@inproceedings{Might:2010:REK:1806596.1806631,
 author = {Might, Matthew and Smaragdakis, Yannis and Van Horn, David},
 title = {Resolving and exploiting the <i>k</i>-CFA paradox: illuminating functional vs. object-oriented program analysis},
 abstract = {Low-level program analysis is a fundamental problem, taking the shape of "flow analysis" in functional languages and "points-to" analysis in imperative and object-oriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited cross-understanding. One of the few links is Shivers's k</i>-CFA work, which has advanced the concept of "context-sensitive analysis" and is widely known in both communities. Recent results indicate that the relationship between the functional and object-oriented incarnations of k</i>-CFA is not as well understood as thought. Van Horn and Mairson proved k</i>-CFA for k</i> \&#8805; 1 to be EXPTIME-complete; hence, no polynomial-time algorithm can exist. Yet, there are several polynomial-time formulations of context-sensitive points-to analyses in object-oriented languages. Thus, it seems that functional k</i>-CFA may actually be a profoundly different analysis from object-oriented k</i>-CFA. We resolve this paradox by showing that the exact same specification of k</i>-CFA is polynomial-time for object-oriented languages yet exponential-time for functional ones: objects and closures are subtly different, in a way that interacts crucially with context-sensitivity and complexity. This illumination leads to an immediate payoff: by projecting the object-oriented treatment of objects onto closures, we derive a polynomial-time hierarchy of context-sensitive CFAs for functional programs.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {305--315},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806631},
 doi = {http://doi.acm.org/10.1145/1806596.1806631},
 acmid = {1806631},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control-flow analysis, functional, k-cfa, m-cfa, object-oriented, pointer analysis, static analysis},
} 

@article{Might:2010:REK:1809028.1806631,
 author = {Might, Matthew and Smaragdakis, Yannis and Van Horn, David},
 title = {Resolving and exploiting the <i>k</i>-CFA paradox: illuminating functional vs. object-oriented program analysis},
 abstract = {Low-level program analysis is a fundamental problem, taking the shape of "flow analysis" in functional languages and "points-to" analysis in imperative and object-oriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited cross-understanding. One of the few links is Shivers's k</i>-CFA work, which has advanced the concept of "context-sensitive analysis" and is widely known in both communities. Recent results indicate that the relationship between the functional and object-oriented incarnations of k</i>-CFA is not as well understood as thought. Van Horn and Mairson proved k</i>-CFA for k</i> \&#8805; 1 to be EXPTIME-complete; hence, no polynomial-time algorithm can exist. Yet, there are several polynomial-time formulations of context-sensitive points-to analyses in object-oriented languages. Thus, it seems that functional k</i>-CFA may actually be a profoundly different analysis from object-oriented k</i>-CFA. We resolve this paradox by showing that the exact same specification of k</i>-CFA is polynomial-time for object-oriented languages yet exponential-time for functional ones: objects and closures are subtly different, in a way that interacts crucially with context-sensitivity and complexity. This illumination leads to an immediate payoff: by projecting the object-oriented treatment of objects onto closures, we derive a polynomial-time hierarchy of context-sensitive CFAs for functional programs.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {305--315},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806631},
 doi = {http://doi.acm.org/10.1145/1809028.1806631},
 acmid = {1806631},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control-flow analysis, functional, k-cfa, m-cfa, object-oriented, pointer analysis, static analysis},
} 

@inproceedings{Kuncak:2010:CFS:1806596.1806632,
 author = {Kuncak, Viktor and Mayer, Mika\"{e}l and Piskac, Ruzica and Suter, Philippe},
 title = {Complete functional synthesis},
 abstract = {Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way - they should succeed for a well-defined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by starting from decision procedures for linear arithmetic and data structures and transforming them into synthesis procedures. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {316--329},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806632},
 doi = {http://doi.acm.org/10.1145/1806596.1806632},
 acmid = {1806632},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bapa, decision procedure, presburger arithmetic, synthesis procedure},
} 

@article{Kuncak:2010:CFS:1809028.1806632,
 author = {Kuncak, Viktor and Mayer, Mika\"{e}l and Piskac, Ruzica and Suter, Philippe},
 title = {Complete functional synthesis},
 abstract = {Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way - they should succeed for a well-defined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by starting from decision procedures for linear arithmetic and data structures and transforming them into synthesis procedures. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {316--329},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806632},
 doi = {http://doi.acm.org/10.1145/1809028.1806632},
 acmid = {1806632},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bapa, decision procedure, presburger arithmetic, synthesis procedure},
} 

@inproceedings{Burckhardt:2010:LCA:1806596.1806634,
 author = {Burckhardt, Sebastian and Dern, Chris and Musuvathi, Madanlal and Tan, Roy},
 title = {Line-up: a complete and automatic linearizability checker},
 abstract = {Modular development of concurrent applications requires thread-safe components that behave correctly when called concurrently by multiple client threads. This paper focuses on linearizability, a specific formalization of thread safety, where all operations of a concurrent component appear to take effect instantaneously at some point between their call and return. The key insight of this paper is that if a component is intended to be deterministic, then it is possible to build an automatic linearizability checker by systematically enumerating the sequential behaviors of the component and then checking if each its concurrent behavior is equivalent to some sequential behavior. We develop this insight into a tool called Line-Up, the first complete and automatic checker for deterministic linearizability</i>. It is complete, because any reported violation proves that the implementation is not linearizable with respect to any</i> sequential deterministic specification. It is automatic, requiring no manual abstraction, no manual specification of semantics or commit points, no manually written test suites, no access to source code. We evaluate Line-Up by analyzing 13 classes with a total of 90 methods in two versions of the .NET Framework 4.0. The violations of deterministic linearizability reported by Line-Up exposed seven errors in the implementation that were fixed by the development team.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {330--340},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806634},
 doi = {http://doi.acm.org/10.1145/1806596.1806634},
 acmid = {1806634},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, linearizability, thread safety},
} 

@article{Burckhardt:2010:LCA:1809028.1806634,
 author = {Burckhardt, Sebastian and Dern, Chris and Musuvathi, Madanlal and Tan, Roy},
 title = {Line-up: a complete and automatic linearizability checker},
 abstract = {Modular development of concurrent applications requires thread-safe components that behave correctly when called concurrently by multiple client threads. This paper focuses on linearizability, a specific formalization of thread safety, where all operations of a concurrent component appear to take effect instantaneously at some point between their call and return. The key insight of this paper is that if a component is intended to be deterministic, then it is possible to build an automatic linearizability checker by systematically enumerating the sequential behaviors of the component and then checking if each its concurrent behavior is equivalent to some sequential behavior. We develop this insight into a tool called Line-Up, the first complete and automatic checker for deterministic linearizability</i>. It is complete, because any reported violation proves that the implementation is not linearizable with respect to any</i> sequential deterministic specification. It is automatic, requiring no manual abstraction, no manual specification of semantics or commit points, no manually written test suites, no access to source code. We evaluate Line-Up by analyzing 13 classes with a total of 90 methods in two versions of the .NET Framework 4.0. The violations of deterministic linearizability reported by Line-Up exposed seven errors in the implementation that were fixed by the development team.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {330--340},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806634},
 doi = {http://doi.acm.org/10.1145/1809028.1806634},
 acmid = {1806634},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, linearizability, thread safety},
} 

@article{Torlak:2010:MCA:1809028.1806635,
 author = {Torlak, Emina and Vaziri, Mandana and Dolby, Julian},
 title = {MemSAT: checking axiomatic specifications of memory models},
 abstract = {Memory models are hard to reason about due to their complexity, which stems from the need to strike a balance between ease-of-programming and allowing compiler and hardware optimizations. In this paper, we present an automated tool, MemSAT, that helps in debugging and reasoning about memory models. Given an axiomatic specification of a memory model and a multi-threaded test program containing assertions, MemSAT outputs a trace of the program in which both the assertions and the memory model axioms are satisfied, if one can be found. The tool is fully automatic and is based on a SAT solver. If it cannot find a trace, it outputs a minimal subset of the memory model and program constraints that are unsatisfiable. We used MemSAT to check several existing memory models against their published test cases, including the current Java Memory Model by Manson et al. and a revised version of it by Sevcik and Aspinall. We found subtle discrepancies between what was expected and the actual results of test programs.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {341--350},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1809028.1806635},
 doi = {http://doi.acm.org/10.1145/1809028.1806635},
 acmid = {1806635},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {axiomatic specifications, bounded model checking, memory models, sat},
} 

@inproceedings{Torlak:2010:MCA:1806596.1806635,
 author = {Torlak, Emina and Vaziri, Mandana and Dolby, Julian},
 title = {MemSAT: checking axiomatic specifications of memory models},
 abstract = {Memory models are hard to reason about due to their complexity, which stems from the need to strike a balance between ease-of-programming and allowing compiler and hardware optimizations. In this paper, we present an automated tool, MemSAT, that helps in debugging and reasoning about memory models. Given an axiomatic specification of a memory model and a multi-threaded test program containing assertions, MemSAT outputs a trace of the program in which both the assertions and the memory model axioms are satisfied, if one can be found. The tool is fully automatic and is based on a SAT solver. If it cannot find a trace, it outputs a minimal subset of the memory model and program constraints that are unsatisfiable. We used MemSAT to check several existing memory models against their published test cases, including the current Java Memory Model by Manson et al. and a revised version of it by Sevcik and Aspinall. We found subtle discrepancies between what was expected and the actual results of test programs.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {341--350},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1806596.1806635},
 doi = {http://doi.acm.org/10.1145/1806596.1806635},
 acmid = {1806635},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {axiomatic specifications, bounded model checking, memory models, sat},
} 

@inproceedings{Marino:2010:DSE:1806596.1806636,
 author = {Marino, Daniel and Singh, Abhayendra and Millstein, Todd and Musuvathi, Madanlal and Narayanasamy, Satish},
 title = {DRFX: a simple and efficient memory model for concurrent programming languages},
 abstract = {The most intuitive memory model for shared-memory multithreaded programming is sequential consistency</i>(SC), but it disallows the use of many compiler and hardware optimizations thereby impacting performance. Data-race-free (DRF) models, such as the proposed C++0x memory model, guarantee SC execution for datarace-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the DRFx memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception</i> which can be signaled to halt execution. If a program executes without throwing this exception, then DRFx guarantees that the execution is SC. If a program throws an MM exception during an execution, then DRFx guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compiler-designated program regions. We formalize our memory model, prove several properties about this model, describe a compiler and hardware design suitable for DRFx, and evaluate the performance overhead due to our compiler and hardware requirements.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {351--362},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806636},
 doi = {http://doi.acm.org/10.1145/1806596.1806636},
 acmid = {1806636},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data races, memory model exception, memory models, sequential consistency, soft fences},
} 

@article{Marino:2010:DSE:1809028.1806636,
 author = {Marino, Daniel and Singh, Abhayendra and Millstein, Todd and Musuvathi, Madanlal and Narayanasamy, Satish},
 title = {DRFX: a simple and efficient memory model for concurrent programming languages},
 abstract = {The most intuitive memory model for shared-memory multithreaded programming is sequential consistency</i>(SC), but it disallows the use of many compiler and hardware optimizations thereby impacting performance. Data-race-free (DRF) models, such as the proposed C++0x memory model, guarantee SC execution for datarace-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the DRFx memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception</i> which can be signaled to halt execution. If a program executes without throwing this exception, then DRFx guarantees that the execution is SC. If a program throws an MM exception during an execution, then DRFx guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compiler-designated program regions. We formalize our memory model, prove several properties about this model, describe a compiler and hardware design suitable for DRFx, and evaluate the performance overhead due to our compiler and hardware requirements.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {351--362},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806636},
 doi = {http://doi.acm.org/10.1145/1809028.1806636},
 acmid = {1806636},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data races, memory model exception, memory models, sequential consistency, soft fences},
} 

@inproceedings{Chambers:2010:FEE:1806596.1806638,
 author = {Chambers, Craig and Raniwala, Ashish and Perry, Frances and Adams, Stephen and Henry, Robert R. and Bradshaw, Robert and Weizenbaum, Nathan},
 title = {FlumeJava: easy, efficient data-parallel pipelines},
 abstract = {MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {363--375},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1806596.1806638},
 doi = {http://doi.acm.org/10.1145/1806596.1806638},
 acmid = {1806638},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-parallel programming, java, mapreduce},
} 

@article{Chambers:2010:FEE:1809028.1806638,
 author = {Chambers, Craig and Raniwala, Ashish and Perry, Frances and Adams, Stephen and Henry, Robert R. and Bradshaw, Robert and Weizenbaum, Nathan},
 title = {FlumeJava: easy, efficient data-parallel pipelines},
 abstract = {MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {363--375},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1809028.1806638},
 doi = {http://doi.acm.org/10.1145/1809028.1806638},
 acmid = {1806638},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-parallel programming, java, mapreduce},
} 

@article{Pan:2010:CPS:1809028.1806639,
 author = {Pan, Heidi and Hindman, Benjamin and Asanovi\'{c}, Krste},
 title = {Composing parallel software efficiently with lithe},
 abstract = {Applications composed of multiple parallel libraries perform poorly when those libraries interfere with one another by obliviously using the same physical cores, leading to destructive resource oversubscription. This paper presents the design and implementation of Lithe</i>, a low-level substrate that provides the basic primitives and a standard interface for composing parallel codes efficiently. Lithe can be inserted underneath the runtimes of legacy parallel libraries to provide bolt-on</i> composability without needing to change existing application code. Lithe can also serve as the foundation for building new parallel abstractions and libraries that automatically interoperate with one another. In this paper, we show versions of Threading Building Blocks (TBB) and OpenMP perform competitively with their original implementations when ported to Lithe. Furthermore, for two applications composed of multiple parallel libraries, we show that leveraging our substrate outperforms their original, even expertly tuned, implementations.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {376--387},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806639},
 doi = {http://doi.acm.org/10.1145/1809028.1806639},
 acmid = {1806639},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composability, cooperative scheduling, hierarchical scheduling, oversubscription, parallelism, resource management, user-level scheduling},
} 

@inproceedings{Pan:2010:CPS:1806596.1806639,
 author = {Pan, Heidi and Hindman, Benjamin and Asanovi\'{c}, Krste},
 title = {Composing parallel software efficiently with lithe},
 abstract = {Applications composed of multiple parallel libraries perform poorly when those libraries interfere with one another by obliviously using the same physical cores, leading to destructive resource oversubscription. This paper presents the design and implementation of Lithe</i>, a low-level substrate that provides the basic primitives and a standard interface for composing parallel codes efficiently. Lithe can be inserted underneath the runtimes of legacy parallel libraries to provide bolt-on</i> composability without needing to change existing application code. Lithe can also serve as the foundation for building new parallel abstractions and libraries that automatically interoperate with one another. In this paper, we show versions of Threading Building Blocks (TBB) and OpenMP perform competitively with their original implementations when ported to Lithe. Furthermore, for two applications composed of multiple parallel libraries, we show that leveraging our substrate outperforms their original, even expertly tuned, implementations.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {376--387},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806639},
 doi = {http://doi.acm.org/10.1145/1806596.1806639},
 acmid = {1806639},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composability, cooperative scheduling, hierarchical scheduling, oversubscription, parallelism, resource management, user-level scheduling},
} 

@article{Zhou:2010:BDO:1809028.1806640,
 author = {Zhou, Jin and Demsky, Brian},
 title = {Bamboo: a data-centric, object-oriented approach to many-core software},
 abstract = {Traditional data-oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low-level, thread-oriented concurrency primitives. This simplification comes at a cost --- traditional data-oriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data-oriented programming models to support mutation of arbitrary data structures. We have implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a K-means clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {388--399},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806640},
 doi = {http://doi.acm.org/10.1145/1809028.1806640},
 acmid = {1806640},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-centric languages, many-core programming},
} 

@inproceedings{Zhou:2010:BDO:1806596.1806640,
 author = {Zhou, Jin and Demsky, Brian},
 title = {Bamboo: a data-centric, object-oriented approach to many-core software},
 abstract = {Traditional data-oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low-level, thread-oriented concurrency primitives. This simplification comes at a cost --- traditional data-oriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data-oriented programming models to support mutation of arbitrary data structures. We have implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a K-means clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {388--399},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806640},
 doi = {http://doi.acm.org/10.1145/1806596.1806640},
 acmid = {1806640},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-centric languages, many-core programming},
} 

@article{Westbrook:2010:MJM:1809028.1806642,
 author = {Westbrook, Edwin and Ricken, Mathias and Inoue, Jun and Yao, Yilong and Abdelatif, Tamer and Taha, Walid},
 title = {Mint: Java multi-stage programming using weak separability},
 abstract = {Multi-stage programming (MSP) provides a disciplined approach to run-time code generation. In the purely functional setting, it has been shown how MSP can be used to reduce the overhead of abstractions, allowing clean, maintainable code without paying performance penalties. Unfortunately, MSP is difficult to combine with imperative features, which are prevalent in mainstream languages. The central difficulty is scope extrusion, wherein free variables can inadvertently be moved outside the scopes of their binders. This paper proposes a new approach to combining MSP with imperative features that occupies a "sweet spot" in the design space in terms of how well useful MSP applications can be expressed and how easy it is for programmers to understand. The key insight is that escapes (or "anti-quotes") must be weakly separable from the rest of the code, i.e. the computational effects occurring inside an escape that are visible outside the escape are guaranteed to not contain code. To demonstrate the feasibility of this approach, we formalize a type system based on Lightweight Java which we prove sound, and we also provide an implementation, called Mint, to validate both the expressivity of the type system and the effect of staging on the performance of Java programs.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {400--411},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806642},
 doi = {http://doi.acm.org/10.1145/1809028.1806642},
 acmid = {1806642},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, multi-stage programming, multi-staged languages, type systems},
} 

@inproceedings{Westbrook:2010:MJM:1806596.1806642,
 author = {Westbrook, Edwin and Ricken, Mathias and Inoue, Jun and Yao, Yilong and Abdelatif, Tamer and Taha, Walid},
 title = {Mint: Java multi-stage programming using weak separability},
 abstract = {Multi-stage programming (MSP) provides a disciplined approach to run-time code generation. In the purely functional setting, it has been shown how MSP can be used to reduce the overhead of abstractions, allowing clean, maintainable code without paying performance penalties. Unfortunately, MSP is difficult to combine with imperative features, which are prevalent in mainstream languages. The central difficulty is scope extrusion, wherein free variables can inadvertently be moved outside the scopes of their binders. This paper proposes a new approach to combining MSP with imperative features that occupies a "sweet spot" in the design space in terms of how well useful MSP applications can be expressed and how easy it is for programmers to understand. The key insight is that escapes (or "anti-quotes") must be weakly separable from the rest of the code, i.e. the computational effects occurring inside an escape that are visible outside the escape are guaranteed to not contain code. To demonstrate the feasibility of this approach, we formalize a type system based on Lightweight Java which we prove sound, and we also provide an implementation, called Mint, to validate both the expressivity of the type system and the effect of staging on the performance of Java programs.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {400--411},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806642},
 doi = {http://doi.acm.org/10.1145/1806596.1806642},
 acmid = {1806642},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, multi-stage programming, multi-staged languages, type systems},
} 

@inproceedings{Chen:2010:TCE:1806596.1806643,
 author = {Chen, Juan and Chugh, Ravi and Swamy, Nikhil},
 title = {Type-preserving compilation of end-to-end verification of security enforcement},
 abstract = {A number of programming languages use rich type systems to verify security properties of code. Some of these languages are meant for source programming, but programs written in these languages are compiled without explicit security proofs, limiting their utility in settings where proofs are necessary, e.g., proof-carrying authorization. Others languages do include explicit proofs, but these are generally lambda calculi not intended for source programming, that must be further compiled to an executable form. A language suitable for source programming backed by a compiler that enables end-to-end verification is missing. In this paper, we present a type-preserving compiler that translates programs written in FINE, a source-level functional language with dependent refinements and affine types, to DCIL, a new extension of the .NET Common Intermediate Language. FINE is type checked using an external SMT solver to reduce the proof burden on source programmers. We extract explicit LCF-style proof terms from the solver and carry these proof terms in the compilation to DCIL, thereby removing the solver from the trusted computing base. Explicit proofs enable DCIL to be used in a number of important scenarios, including the verification of mobile code, proof-carrying authorization, and evidence-based auditing. We report on our experience using FINE to build reference monitors for several applications, ranging from a plugin-based email client to a conference management server.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {412--423},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806643},
 doi = {http://doi.acm.org/10.1145/1806596.1806643},
 acmid = {1806643},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {authorization, bytecode languages, compilers, dependent types, functional programming, information flow, mobile code security, security type systems},
} 

@article{Chen:2010:TCE:1809028.1806643,
 author = {Chen, Juan and Chugh, Ravi and Swamy, Nikhil},
 title = {Type-preserving compilation of end-to-end verification of security enforcement},
 abstract = {A number of programming languages use rich type systems to verify security properties of code. Some of these languages are meant for source programming, but programs written in these languages are compiled without explicit security proofs, limiting their utility in settings where proofs are necessary, e.g., proof-carrying authorization. Others languages do include explicit proofs, but these are generally lambda calculi not intended for source programming, that must be further compiled to an executable form. A language suitable for source programming backed by a compiler that enables end-to-end verification is missing. In this paper, we present a type-preserving compiler that translates programs written in FINE, a source-level functional language with dependent refinements and affine types, to DCIL, a new extension of the .NET Common Intermediate Language. FINE is type checked using an external SMT solver to reduce the proof burden on source programmers. We extract explicit LCF-style proof terms from the solver and carry these proof terms in the compilation to DCIL, thereby removing the solver from the trusted computing base. Explicit proofs enable DCIL to be used in a number of important scenarios, including the verification of mobile code, proof-carrying authorization, and evidence-based auditing. We report on our experience using FINE to build reference monitors for several applications, ranging from a plugin-based email client to a conference management server.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {412--423},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806643},
 doi = {http://doi.acm.org/10.1145/1809028.1806643},
 acmid = {1806643},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {authorization, bytecode languages, compilers, dependent types, functional programming, information flow, mobile code security, security type systems},
} 

@article{Tate:2010:IOT:1809028.1806644,
 author = {Tate, Ross and Chen, Juan and Hawblitzel, Chris},
 title = {Inferable object-oriented typed assembly language},
 abstract = {A certifying compiler preserves type information through compilation to assembly language programs, producing typed assembly language (TAL) programs that can be verified for safety independently so that the compiler does not need to be trusted. There are two challenges for adopting certifying compilation in practice. First, requiring every compiler transformation and optimization to preserve types is a large burden on compilers, especially when adopting certifying compilation into existing optimizing non-certifying compilers. Second, type annotations significantly increase the size of assembly language programs. This paper proposes an alternative to traditional certifying compilers. It presents iTalX, the first inferable TAL type system that supports existential types, arrays, interfaces, and stacks. We have proved our inference algorithm is complete, meaning if an assembly language program is typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our algorithm is guaranteed to terminate even if the assembly language program is untypeable. We demonstrate that it is practical to infer such an expressive TAL by showing a prototype implementation of type inference for code compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type annotations for 98\% of functions in a suite of realistic C# benchmarks. The type-inference time is about 8\% of the compilation time. We needed to change only 2.5\% of the compiler code, mostly adding new code for defining types and for writing types to object files. Most transformations are untouched. Type-annotation size is only 17\% of the size of pure code and data, reducing type annotations in our previous certifying compiler [4] by 60\%. The compiler needs to preserve only essential type information such as method signatures, object-layout information, and types for static data and external labels. Even non-certifying compilers have most of this information available.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {424--435},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806644},
 doi = {http://doi.acm.org/10.1145/1809028.1806644},
 acmid = {1806644},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {certifying compiler, existential quantification, object-oriented compiler, type inference, typed assembly language (tal)},
} 

@inproceedings{Tate:2010:IOT:1806596.1806644,
 author = {Tate, Ross and Chen, Juan and Hawblitzel, Chris},
 title = {Inferable object-oriented typed assembly language},
 abstract = {A certifying compiler preserves type information through compilation to assembly language programs, producing typed assembly language (TAL) programs that can be verified for safety independently so that the compiler does not need to be trusted. There are two challenges for adopting certifying compilation in practice. First, requiring every compiler transformation and optimization to preserve types is a large burden on compilers, especially when adopting certifying compilation into existing optimizing non-certifying compilers. Second, type annotations significantly increase the size of assembly language programs. This paper proposes an alternative to traditional certifying compilers. It presents iTalX, the first inferable TAL type system that supports existential types, arrays, interfaces, and stacks. We have proved our inference algorithm is complete, meaning if an assembly language program is typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our algorithm is guaranteed to terminate even if the assembly language program is untypeable. We demonstrate that it is practical to infer such an expressive TAL by showing a prototype implementation of type inference for code compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type annotations for 98\% of functions in a suite of realistic C# benchmarks. The type-inference time is about 8\% of the compilation time. We needed to change only 2.5\% of the compiler code, mostly adding new code for defining types and for writing types to object files. Most transformations are untouched. Type-annotation size is only 17\% of the size of pure code and data, reducing type annotations in our previous certifying compiler [4] by 60\%. The compiler needs to preserve only essential type information such as method signatures, object-layout information, and types for static data and external labels. Even non-certifying compilers have most of this information available.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {424--435},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806644},
 doi = {http://doi.acm.org/10.1145/1806596.1806644},
 acmid = {1806644},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {certifying compiler, existential quantification, object-oriented compiler, type inference, typed assembly language (tal)},
} 

@inproceedings{Khoo:2010:MTC:1806596.1806645,
 author = {Khoo, Yit Phang and Chang, Bor-Yuh Evan and Foster, Jeffrey S.},
 title = {Mixing type checking and symbolic execution},
 abstract = {Static analysis designers must carefully balance precision and efficiency. In our experience, many static analysis tools are built around an elegant, core algorithm, but that algorithm is then extensively tweaked to add just enough precision for the coding idioms seen in practice, without sacrificing too much efficiency. There are several downsides to adding precision in this way: the tool's implementation becomes much more complicated; it can be hard for an end-user to interpret the tool's results; and as software systems vary tremendously in their coding styles, it may require significant algorithmic engineering to enhance a tool to perform well in a particular software domain. In this paper, we present Mix, a novel system that mixes type checking and symbolic execution. The key aspect of our approach is that these analyses are applied independently on disjoint parts of the program, in an off-the-shelf manner. At the boundaries between nested type checked and symbolically executed code regions, we use special mix rules to communicate information between the off-the-shelf systems. The resulting mixture is a provably sound analysis that is more precise than type checking alone and more efficient than exclusive symbolic execution. In addition, we also describe a prototype implementation, Mixy, for C. Mixy checks for potential null dereferences by mixing a null/non-null type qualifier inference system with a symbolic executor.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {436--447},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806645},
 doi = {http://doi.acm.org/10.1145/1806596.1806645},
 acmid = {1806645},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {false alarms, mix, mix rules, mixed off-the-shelf analysis, precision, symbolic execution, type checking},
} 

@article{Khoo:2010:MTC:1809028.1806645,
 author = {Khoo, Yit Phang and Chang, Bor-Yuh Evan and Foster, Jeffrey S.},
 title = {Mixing type checking and symbolic execution},
 abstract = {Static analysis designers must carefully balance precision and efficiency. In our experience, many static analysis tools are built around an elegant, core algorithm, but that algorithm is then extensively tweaked to add just enough precision for the coding idioms seen in practice, without sacrificing too much efficiency. There are several downsides to adding precision in this way: the tool's implementation becomes much more complicated; it can be hard for an end-user to interpret the tool's results; and as software systems vary tremendously in their coding styles, it may require significant algorithmic engineering to enhance a tool to perform well in a particular software domain. In this paper, we present Mix, a novel system that mixes type checking and symbolic execution. The key aspect of our approach is that these analyses are applied independently on disjoint parts of the program, in an off-the-shelf manner. At the boundaries between nested type checked and symbolically executed code regions, we use special mix rules to communicate information between the off-the-shelf systems. The resulting mixture is a provably sound analysis that is more precise than type checking alone and more efficient than exclusive symbolic execution. In addition, we also describe a prototype implementation, Mixy, for C. Mixy checks for potential null dereferences by mixing a null/non-null type qualifier inference system with a symbolic executor.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {436--447},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806645},
 doi = {http://doi.acm.org/10.1145/1809028.1806645},
 acmid = {1806645},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {false alarms, mix, mix rules, mixed off-the-shelf analysis, precision, symbolic execution, type checking},
} 

@article{Chen:2010:EIO:1809028.1806647,
 author = {Chen, Yang and Huang, Yuanjie and Eeckhout, Lieven and Fursin, Grigori and Peng, Liang and Temam, Olivier and Wu, Chengyong},
 title = {Evaluating iterative optimization across 1000 datasets},
 abstract = {While iterative optimization has become a popular compiler optimization approach, it is based on a premise which has never been truly evaluated: that it is possible to learn the best compiler optimizations across data sets. Up to now, most iterative optimization studies find the best optimizations through repeated runs on the same data set. Only a handful of studies have attempted to exercise iterative optimization on a few tens of data sets. In this paper, we truly put iterative compilation to the test for the first time by evaluating its effectiveness across a large number of data sets. We therefore compose KDataSets, a data set suite with 1000 data sets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization.We demonstrate that it is possible to derive a robust iterative optimization strategy across data sets: for all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves 86\% or more of the best possible speedup across all</i> data sets using Intel's ICC (83\% for GNU's GCC). This optimal combination is program-specific and yields speedups up to 1.71 on ICC and 2.23 on GCC over the highest optimization level (-fast and -O3, respectively). This finding makes the task of optimizing programs across data sets much easier than previously anticipated, and it paves the way for the practical and reliable usage of iterative optimization. Finally, we derive pre-shipping and post-shipping optimization strategies for software vendors.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {448--459},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806647},
 doi = {http://doi.acm.org/10.1145/1809028.1806647},
 acmid = {1806647},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, compiler optimization, iterative optimization},
} 

@inproceedings{Chen:2010:EIO:1806596.1806647,
 author = {Chen, Yang and Huang, Yuanjie and Eeckhout, Lieven and Fursin, Grigori and Peng, Liang and Temam, Olivier and Wu, Chengyong},
 title = {Evaluating iterative optimization across 1000 datasets},
 abstract = {While iterative optimization has become a popular compiler optimization approach, it is based on a premise which has never been truly evaluated: that it is possible to learn the best compiler optimizations across data sets. Up to now, most iterative optimization studies find the best optimizations through repeated runs on the same data set. Only a handful of studies have attempted to exercise iterative optimization on a few tens of data sets. In this paper, we truly put iterative compilation to the test for the first time by evaluating its effectiveness across a large number of data sets. We therefore compose KDataSets, a data set suite with 1000 data sets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization.We demonstrate that it is possible to derive a robust iterative optimization strategy across data sets: for all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves 86\% or more of the best possible speedup across all</i> data sets using Intel's ICC (83\% for GNU's GCC). This optimal combination is program-specific and yields speedups up to 1.71 on ICC and 2.23 on GCC over the highest optimization level (-fast and -O3, respectively). This finding makes the task of optimizing programs across data sets much easier than previously anticipated, and it paves the way for the practical and reliable usage of iterative optimization. Finally, we derive pre-shipping and post-shipping optimization strategies for software vendors.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {448--459},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806647},
 doi = {http://doi.acm.org/10.1145/1806596.1806647},
 acmid = {1806647},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, compiler optimization, iterative optimization},
} 

@article{Kamruzzaman:2010:SDS:1809028.1806648,
 author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
 title = {Software data spreading: leveraging distributed caches to improve single thread performance},
 abstract = {Single thread performance remains an important consideration even for multicore, multiprocessor systems. As a result, techniques for improving single thread performance using multiple cores have received considerable attention. This work describes a technique, software data spreading</i>, that leverages the cache capacity of extra cores and extra sockets rather than their computational resources. Software data spreading is a software-only technique that uses compiler-directed thread migration to aggregate cache capacity across cores and chips and improve performance. This paper describes an automated scheme that applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, NAS, and microbenchmark workloads show that data spreading can provide speedup of over 2, averaging 17\% for the SPEC and NAS applications on two systems. In addition, despite using more cores for the same computation, data spreading actually saves power since it reduces access to DRAM.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {460--470},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1809028.1806648},
 doi = {http://doi.acm.org/10.1145/1809028.1806648},
 acmid = {1806648},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, compilers, single-thread performance},
} 

@inproceedings{Kamruzzaman:2010:SDS:1806596.1806648,
 author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
 title = {Software data spreading: leveraging distributed caches to improve single thread performance},
 abstract = {Single thread performance remains an important consideration even for multicore, multiprocessor systems. As a result, techniques for improving single thread performance using multiple cores have received considerable attention. This work describes a technique, software data spreading</i>, that leverages the cache capacity of extra cores and extra sockets rather than their computational resources. Software data spreading is a software-only technique that uses compiler-directed thread migration to aggregate cache capacity across cores and chips and improve performance. This paper describes an automated scheme that applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, NAS, and microbenchmark workloads show that data spreading can provide speedup of over 2, averaging 17\% for the SPEC and NAS applications on two systems. In addition, despite using more cores for the same computation, data spreading actually saves power since it reduces access to DRAM.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {460--470},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1806596.1806648},
 doi = {http://doi.acm.org/10.1145/1806596.1806648},
 acmid = {1806648},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, compilers, single-thread performance},
} 

@inproceedings{Sartor:2010:ZDA:1806596.1806649,
 author = {Sartor, Jennifer B. and Blackburn, Stephen M. and Frampton, Daniel and Hirzel, Martin and McKinley, Kathryn S.},
 title = {Z-rays: divide arrays and conquer speed and flexibility},
 abstract = {Arrays are the ubiquitous organization for indexed data. Throughout programming language evolution, implementations have laid out arrays contiguously in memory. This layout is problematic in space and time. It causes heap fragmentation, garbage collection pauses in proportion to array size, and wasted memory for sparse and over-provisioned arrays. Because of array virtualization in managed languages, an array layout that consists of indirection pointers to fixed-size discontiguous memory blocks can mitigate these problems transparently. This design however incurs significant overhead, but is justified when real-time deadlines and space constraints trump performance. This paper proposes z-rays</i>, a discontiguous array design with flexibility and efficiency. A z-ray has a spine with indirection pointers to fixed-size memory blocks called arraylets</i>, and uses five optimizations: (1) inlining the first N array bytes into the spine, (2) lazy allocation, (3) zero compression, (4) fast array copy, and (5) arraylet copy-on-write. Whereas discontiguous arrays in prior work improve responsiveness and space efficiency, z-rays combine time efficiency and flexibility. On average, the best z-ray configuration performs within 12.7\% of an unmodified Java Virtual Machine on 19 benchmarks, whereas previous designs have two to three times</i> higher overheads. Furthermore, language implementers can configure z-ray optimizations for various design goals. This combination of performance and flexibility creates a better building block for past and future array optimization.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {471--482},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1806596.1806649},
 doi = {http://doi.acm.org/10.1145/1806596.1806649},
 acmid = {1806649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {arraylets, arrays, compression, heap, z-rays},
} 

@article{Sartor:2010:ZDA:1809028.1806649,
 author = {Sartor, Jennifer B. and Blackburn, Stephen M. and Frampton, Daniel and Hirzel, Martin and McKinley, Kathryn S.},
 title = {Z-rays: divide arrays and conquer speed and flexibility},
 abstract = {Arrays are the ubiquitous organization for indexed data. Throughout programming language evolution, implementations have laid out arrays contiguously in memory. This layout is problematic in space and time. It causes heap fragmentation, garbage collection pauses in proportion to array size, and wasted memory for sparse and over-provisioned arrays. Because of array virtualization in managed languages, an array layout that consists of indirection pointers to fixed-size discontiguous memory blocks can mitigate these problems transparently. This design however incurs significant overhead, but is justified when real-time deadlines and space constraints trump performance. This paper proposes z-rays</i>, a discontiguous array design with flexibility and efficiency. A z-ray has a spine with indirection pointers to fixed-size memory blocks called arraylets</i>, and uses five optimizations: (1) inlining the first N array bytes into the spine, (2) lazy allocation, (3) zero compression, (4) fast array copy, and (5) arraylet copy-on-write. Whereas discontiguous arrays in prior work improve responsiveness and space efficiency, z-rays combine time efficiency and flexibility. On average, the best z-ray configuration performs within 12.7\% of an unmodified Java Virtual Machine on 19 benchmarks, whereas previous designs have two to three times</i> higher overheads. Furthermore, language implementers can configure z-ray optimizations for various design goals. This combination of performance and flexibility creates a better building block for past and future array optimization.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {471--482},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1809028.1806649},
 doi = {http://doi.acm.org/10.1145/1809028.1806649},
 acmid = {1806649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {arraylets, arrays, compression, heap, z-rays},
} 

@inproceedings{Acar:2010:TDT:1806596.1806650,
 author = {Acar, Umut A. and Blelloch, Guy and Ley-Wild, Ruy and Tangwongsan, Kanat and Turkoglu, Duru},
 title = {Traceable data types for self-adjusting computation},
 abstract = {Self-adjusting computation provides an evaluation model where computations can respond automatically to modifications to their data by using a mechanism for propagating modifications through the computation. Current approaches to self-adjusting computation guarantee correctness by recording dependencies in a trace at the granularity of individual memory operations. Tracing at the granularity of memory operations, however, has some limitations: it can be asymptotically inefficient (\eg, compared to optimal solutions) because it cannot take advantage of problem-specific structure, it requires keeping a large computation trace (often proportional to the runtime of the program on the current input), and it introduces moderately large constant factors in practice. In this paper, we extend dependence-tracing to work at the granularity of the query and update operations of arbitrary (abstract) data types, instead of just reads and writes on memory cells. This can significantly reduce the number of dependencies that need to be kept in the trace and followed during an update. We define an interface for supporting a traceable version of a data type, which reports the earliest query that depends on (is changed by) revising operations back in time, and implement several such structures, including priority queues, queues, dictionaries, and counters. We develop a semantics for tracing, extend an existing self-adjusting language, \&#916;ML, and its implementation to support traceable data types, and present an experimental evaluation by considering a number of benchmarks. Our experiments show dramatic improvements on space and time, sometimes by as much as two orders of magnitude.},
 booktitle = {Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '10},
 year = {2010},
 isbn = {978-1-4503-0019-3},
 location = {Toronto, Ontario, Canada},
 pages = {483--496},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1806596.1806650},
 doi = {http://doi.acm.org/10.1145/1806596.1806650},
 acmid = {1806650},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {self-adjusting computation, traceable data types},
} 

@article{Acar:2010:TDT:1809028.1806650,
 author = {Acar, Umut A. and Blelloch, Guy and Ley-Wild, Ruy and Tangwongsan, Kanat and Turkoglu, Duru},
 title = {Traceable data types for self-adjusting computation},
 abstract = {Self-adjusting computation provides an evaluation model where computations can respond automatically to modifications to their data by using a mechanism for propagating modifications through the computation. Current approaches to self-adjusting computation guarantee correctness by recording dependencies in a trace at the granularity of individual memory operations. Tracing at the granularity of memory operations, however, has some limitations: it can be asymptotically inefficient (\eg, compared to optimal solutions) because it cannot take advantage of problem-specific structure, it requires keeping a large computation trace (often proportional to the runtime of the program on the current input), and it introduces moderately large constant factors in practice. In this paper, we extend dependence-tracing to work at the granularity of the query and update operations of arbitrary (abstract) data types, instead of just reads and writes on memory cells. This can significantly reduce the number of dependencies that need to be kept in the trace and followed during an update. We define an interface for supporting a traceable version of a data type, which reports the earliest query that depends on (is changed by) revising operations back in time, and implement several such structures, including priority queues, queues, dictionaries, and counters. We develop a semantics for tracing, extend an existing self-adjusting language, \&#916;ML, and its implementation to support traceable data types, and present an experimental evaluation by considering a number of benchmarks. Our experiments show dramatic improvements on space and time, sometimes by as much as two orders of magnitude.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {6},
 month = {June},
 year = {2010},
 issn = {0362-1340},
 pages = {483--496},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1809028.1806650},
 doi = {http://doi.acm.org/10.1145/1809028.1806650},
 acmid = {1806650},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {self-adjusting computation, traceable data types},
} 

@inproceedings{Novark:2007:EAC:1250734.1250736,
 author = {Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Exterminator: automatically correcting memory errors with high probability},
 abstract = {Programs written in C and C++ are susceptible to memory errors, including buffer overflows and dangling pointers. These errors, whichcan lead to crashes, erroneous execution, and security vulnerabilities, are notoriously costly to repair. Tracking down their location in the source code is difficult, even when the full memory state of the program is available. Once the errors are finally found, fixing them remains challenging: even for critical security-sensitive bugs, the average time between initial reports and the issuance of a patch is nearly one month. We present Exterminator, a system that automatically correct sheap-based memory errors without programmer intervention. Exterminator exploits randomization to pinpoint errors with high precision. From this information, Exterminator derives runtime patches</i> that fix these errors both in current and subsequent executions. In addition, Exterminator enables collaborative bug correction by merging patches generated by multiple users. We present analytical and empirical results that demonstrate Exterminator's effectiveness at detecting and correcting both injected and real faults.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250736},
 doi = {http://doi.acm.org/10.1145/1250734.1250736},
 acmid = {1250736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dieFast, dynamic memory allocation, error correction, exterminator, memory errors, probabilistic algorithms, randomized algorithms},
} 

@article{Novark:2007:EAC:1273442.1250736,
 author = {Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Exterminator: automatically correcting memory errors with high probability},
 abstract = {Programs written in C and C++ are susceptible to memory errors, including buffer overflows and dangling pointers. These errors, whichcan lead to crashes, erroneous execution, and security vulnerabilities, are notoriously costly to repair. Tracking down their location in the source code is difficult, even when the full memory state of the program is available. Once the errors are finally found, fixing them remains challenging: even for critical security-sensitive bugs, the average time between initial reports and the issuance of a patch is nearly one month. We present Exterminator, a system that automatically correct sheap-based memory errors without programmer intervention. Exterminator exploits randomization to pinpoint errors with high precision. From this information, Exterminator derives runtime patches</i> that fix these errors both in current and subsequent executions. In addition, Exterminator enables collaborative bug correction by merging patches generated by multiple users. We present analytical and empirical results that demonstrate Exterminator's effectiveness at detecting and correcting both injected and real faults.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250736},
 doi = {http://doi.acm.org/10.1145/1273442.1250736},
 acmid = {1250736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dieFast, dynamic memory allocation, error correction, exterminator, memory errors, probabilistic algorithms, randomized algorithms},
} 

@inproceedings{Burckhardt:2007:CCC:1250734.1250737,
 author = {Burckhardt, Sebastian and Alur, Rajeev and Martin, Milo M. K.},
 title = {CheckFence: checking consistency of concurrent data types on relaxed memory models},
 abstract = {Concurrency libraries can facilitate the development of multi-threaded programs by providing concurrent implementations of familiar data types such as queues or sets. There exist many optimized algorithms that can achieve superior performance on multiprocessors by allowing concurrent data accesses without using locks. Unfortunately, such algorithms can harbor subtle concurrency bugs. Moreover, they requirememory ordering fences to function correctly on relaxed memory models. To address these difficulties, we propose a verification approach that can exhaustively check all concurrent executions of a given test program on a relaxed memory model and can verify that they are observationally equivalent to a sequential execution. Our CheckFence</i> prototype automatically translates the C implementation code and the test program into a SAT formula, hands the latter to a standard SAT solver, and constructs counter example traces if there exist incorrect executions. Applying CheckFence</i> to five previously published algorithms, we were able to (1) find several bugs (some not previously known), and (2) determine how to place memory ordering fences for relaxed memory models.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250737},
 doi = {http://doi.acm.org/10.1145/1250734.1250737},
 acmid = {1250737},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent data structures, lock-free synchronization, memory models, multi-threading, sequential consistency, shared-memory multiprocessors, software model checking},
} 

@article{Burckhardt:2007:CCC:1273442.1250737,
 author = {Burckhardt, Sebastian and Alur, Rajeev and Martin, Milo M. K.},
 title = {CheckFence: checking consistency of concurrent data types on relaxed memory models},
 abstract = {Concurrency libraries can facilitate the development of multi-threaded programs by providing concurrent implementations of familiar data types such as queues or sets. There exist many optimized algorithms that can achieve superior performance on multiprocessors by allowing concurrent data accesses without using locks. Unfortunately, such algorithms can harbor subtle concurrency bugs. Moreover, they requirememory ordering fences to function correctly on relaxed memory models. To address these difficulties, we propose a verification approach that can exhaustively check all concurrent executions of a given test program on a relaxed memory model and can verify that they are observationally equivalent to a sequential execution. Our CheckFence</i> prototype automatically translates the C implementation code and the test program into a SAT formula, hands the latter to a standard SAT solver, and constructs counter example traces if there exist incorrect executions. Applying CheckFence</i> to five previously published algorithms, we were able to (1) find several bugs (some not previously known), and (2) determine how to place memory ordering fences for relaxed memory models.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250737},
 doi = {http://doi.acm.org/10.1145/1273442.1250737},
 acmid = {1250737},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent data structures, lock-free synchronization, memory models, multi-threading, sequential consistency, shared-memory multiprocessors, software model checking},
} 

@inproceedings{Narayanasamy:2007:ACB:1250734.1250738,
 author = {Narayanasamy, Satish and Wang, Zhenghao and Tigani, Jordan and Edwards, Andrew and Calder, Brad},
 title = {Automatically classifying benign and harmful data racesallusing replay analysis},
 abstract = {Many concurrency bugs in multi-threaded programs are due to dataraces. There have been many efforts to develop static and dynamic mechanisms to automatically find the data races. Most of the prior work has focused on finding the data races and eliminating the false positives. In this paper, we instead focus on a dynamic analysis technique to automatically classify the data races into two categories - the dataraces that are potentially benign and the data races that are potentially harmful. A harmful data race is a real bug that needs to be fixed. This classification is needed to focus the triaging effort on those data races that are potentially harmful. Without prioritizing the data races we have found that there are too many data races to triage. Our second focus is to automatically provide to the developer a reproducible scenario of the data race, which allows the developer to understand the different effects of a harmful data race on a program's execution. To achieve the above, we record a multi-threaded program's execution in a replay log. The replay log is used to replay the multi-threaded program, and during replay we find the data races using a happens-before based algorithm. To automatically classify if a data race that we find is potentially benign or potentially harmful, were play the execution twice for a given data race - one for each possible order between the conflicting memory operations. If the two replays for the two orders produce the same result, then we classify the data race to be potentially benign. We discuss our experiences in using our replay based dynamic data race checker on several Microsoft applications.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {22--31},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250738},
 doi = {http://doi.acm.org/10.1145/1250734.1250738},
 acmid = {1250738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benign data races, concurrency Bbugs, replay},
} 

@article{Narayanasamy:2007:ACB:1273442.1250738,
 author = {Narayanasamy, Satish and Wang, Zhenghao and Tigani, Jordan and Edwards, Andrew and Calder, Brad},
 title = {Automatically classifying benign and harmful data racesallusing replay analysis},
 abstract = {Many concurrency bugs in multi-threaded programs are due to dataraces. There have been many efforts to develop static and dynamic mechanisms to automatically find the data races. Most of the prior work has focused on finding the data races and eliminating the false positives. In this paper, we instead focus on a dynamic analysis technique to automatically classify the data races into two categories - the dataraces that are potentially benign and the data races that are potentially harmful. A harmful data race is a real bug that needs to be fixed. This classification is needed to focus the triaging effort on those data races that are potentially harmful. Without prioritizing the data races we have found that there are too many data races to triage. Our second focus is to automatically provide to the developer a reproducible scenario of the data race, which allows the developer to understand the different effects of a harmful data race on a program's execution. To achieve the above, we record a multi-threaded program's execution in a replay log. The replay log is used to replay the multi-threaded program, and during replay we find the data races using a happens-before based algorithm. To automatically classify if a data race that we find is potentially benign or potentially harmful, were play the execution twice for a given data race - one for each possible order between the conflicting memory operations. If the two replays for the two orders produce the same result, then we classify the data race to be potentially benign. We discuss our experiences in using our replay based dynamic data race checker on several Microsoft applications.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {22--31},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250738},
 doi = {http://doi.acm.org/10.1145/1273442.1250738},
 acmid = {1250738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benign data races, concurrency Bbugs, replay},
} 

@inproceedings{Wassermann:2007:SPA:1250734.1250739,
 author = {Wassermann, Gary and Su, Zhendong},
 title = {Sound and precise analysis of web applications for injection vulnerabilities},
 abstract = {Web applications are popular targets of security attacks. One common type of such attacks is SQL injection, where an attacker exploits faulty application code to execute maliciously crafted database queries. Bothstatic and dynamic approaches have been proposed to detect or prevent SQL injections; while dynamic approaches provide protection for deployed software, static approaches can detect potential vulnerabilities before software deployment. Previous static approaches are mostly based on tainted information flow tracking and have at least some of the following limitations: (1) they do not model the precise semantics of input sanitization routines; (2) they require manually written specifications, either for each query or for bug patterns; or (3) they are not fully automated and may require user intervention at various points in the analysis. In this paper, we address these limitations by proposing a precise, sound</i>, and fully automated</i> analysis technique for SQL injection. Our technique avoids the need for specifications by consideringas attacks those queries for which user input changes the intended syntactic structure of the generated query. It checks conformance to this policy byconservatively characterizing the values a string variable may assume with a context free grammar, tracking the nonterminals that represent user-modifiable data, and modeling string operations precisely as language transducers. We have implemented the proposed technique for PHP, the most widely-used web scripting language. Our tool successfully discovered previously unknown and sometimes subtle vulnerabilities in real-world programs, has a low false positive rate, and scales to large programs (with approx. 100K loc).},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {32--41},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250739},
 doi = {http://doi.acm.org/10.1145/1250734.1250739},
 acmid = {1250739},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {static analysis, string analysis, web applications},
} 

@article{Wassermann:2007:SPA:1273442.1250739,
 author = {Wassermann, Gary and Su, Zhendong},
 title = {Sound and precise analysis of web applications for injection vulnerabilities},
 abstract = {Web applications are popular targets of security attacks. One common type of such attacks is SQL injection, where an attacker exploits faulty application code to execute maliciously crafted database queries. Bothstatic and dynamic approaches have been proposed to detect or prevent SQL injections; while dynamic approaches provide protection for deployed software, static approaches can detect potential vulnerabilities before software deployment. Previous static approaches are mostly based on tainted information flow tracking and have at least some of the following limitations: (1) they do not model the precise semantics of input sanitization routines; (2) they require manually written specifications, either for each query or for bug patterns; or (3) they are not fully automated and may require user intervention at various points in the analysis. In this paper, we address these limitations by proposing a precise, sound</i>, and fully automated</i> analysis technique for SQL injection. Our technique avoids the need for specifications by consideringas attacks those queries for which user input changes the intended syntactic structure of the generated query. It checks conformance to this policy byconservatively characterizing the values a string variable may assume with a context free grammar, tracking the nonterminals that represent user-modifiable data, and modeling string operations precisely as language transducers. We have implemented the proposed technique for PHP, the most widely-used web scripting language. Our tool successfully discovered previously unknown and sometimes subtle vulnerabilities in real-world programs, has a low false positive rate, and scales to large programs (with approx. 100K loc).},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {32--41},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250739},
 doi = {http://doi.acm.org/10.1145/1273442.1250739},
 acmid = {1250739},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {static analysis, string analysis, web applications},
} 

@inproceedings{Perry:2007:FTA:1250734.1250741,
 author = {Perry, Frances and Mackey, Lester and Reis, George A. and Ligatti, Jay and August, David I. and Walker, David},
 title = {Fault-tolerant typed assembly language},
 abstract = {A transient hardware fault</i> occurs when an energetic particle strikes a transistor, causing it to change state. Although transient faults do not permanently damage the hardware, they may corrupt computations by altering stored values and signal transfers. In this paper, we propose a new scheme for provably safe and reliable computing in the presence of transient hardware faults. In our scheme, software computations are replicated to provide redundancy while special instructions compare the independently computed results to detect errors before writing critical data. In stark contrast to any previous efforts in this area, we have analyzed our fault tolerance scheme from a formal, theoretical perspective. To be specific, first, we provide an operational semantics for our assembly language, which includes a precise formal definition of our fault model. Second, we develop an assembly-level type system designed to detect reliability problems in compiled code. Third, we provide a formal specification for program fault tolerance under the given fault model and prove that all well-typed programs are indeed fault tolerant. In addition to the formal analysis, we evaluate our detection scheme and show that it only takes 34\% longer to execute than the unreliable version.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {42--53},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250741},
 doi = {http://doi.acm.org/10.1145/1250734.1250741},
 acmid = {1250741},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault tolerance, soft faults, transient hardware faults, typed assembly language},
} 

@article{Perry:2007:FTA:1273442.1250741,
 author = {Perry, Frances and Mackey, Lester and Reis, George A. and Ligatti, Jay and August, David I. and Walker, David},
 title = {Fault-tolerant typed assembly language},
 abstract = {A transient hardware fault</i> occurs when an energetic particle strikes a transistor, causing it to change state. Although transient faults do not permanently damage the hardware, they may corrupt computations by altering stored values and signal transfers. In this paper, we propose a new scheme for provably safe and reliable computing in the presence of transient hardware faults. In our scheme, software computations are replicated to provide redundancy while special instructions compare the independently computed results to detect errors before writing critical data. In stark contrast to any previous efforts in this area, we have analyzed our fault tolerance scheme from a formal, theoretical perspective. To be specific, first, we provide an operational semantics for our assembly language, which includes a precise formal definition of our fault model. Second, we develop an assembly-level type system designed to detect reliability problems in compiled code. Third, we provide a formal specification for program fault tolerance under the given fault model and prove that all well-typed programs are indeed fault tolerant. In addition to the formal analysis, we evaluate our detection scheme and show that it only takes 34\% longer to execute than the unreliable version.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {42--53},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250741},
 doi = {http://doi.acm.org/10.1145/1273442.1250741},
 acmid = {1250741},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault tolerance, soft faults, transient hardware faults, typed assembly language},
} 

@inproceedings{Chlipala:2007:CTC:1250734.1250742,
 author = {Chlipala, Adam},
 title = {A certified type-preserving compiler from lambda calculus to assembly language},
 abstract = {We present a certified compiler from the simply-typed lambda calculus to assembly language. The compiler is certified in the sense that it comes with a machine-checked proof of semantics preservation, performed with the Coq proof assistant. The compiler and the terms of its several intermediate languages are given dependent types that guarantee that only well-typed programs are representable. Thus, type preservation for each compiler pass follows without any significant "proofs" of the usual kind. Semantics preservation is proved based on denotational semantics assigned to the intermediate languages. We demonstrate how working with a type-preserving compiler enables type-directed proof search to discharge large parts of our proof obligations automatically.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {54--65},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250742},
 doi = {http://doi.acm.org/10.1145/1250734.1250742},
 acmid = {1250742},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler verification, denotational semantics, dependent types, interactive proof assistants},
} 

@article{Chlipala:2007:CTC:1273442.1250742,
 author = {Chlipala, Adam},
 title = {A certified type-preserving compiler from lambda calculus to assembly language},
 abstract = {We present a certified compiler from the simply-typed lambda calculus to assembly language. The compiler is certified in the sense that it comes with a machine-checked proof of semantics preservation, performed with the Coq proof assistant. The compiler and the terms of its several intermediate languages are given dependent types that guarantee that only well-typed programs are representable. Thus, type preservation for each compiler pass follows without any significant "proofs" of the usual kind. Semantics preservation is proved based on denotational semantics assigned to the intermediate languages. We demonstrate how working with a type-preserving compiler enables type-directed proof search to discharge large parts of our proof obligations automatically.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {54--65},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250742},
 doi = {http://doi.acm.org/10.1145/1273442.1250742},
 acmid = {1250742},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler verification, denotational semantics, dependent types, interactive proof assistants},
} 

@inproceedings{Cai:2007:CSC:1250734.1250743,
 author = {Cai, Hongxu and Shao, Zhong and Vaynberg, Alexander},
 title = {Certified self-modifying code},
 abstract = {Self-modifying code (SMC), in this paper, broadly refers to anyprogram that loads, generates, or mutates code at runtime. It is widely used in many of the world's critical software systems tosupport runtime code generation and optimization, dynamic loading and linking, OS boot loader, just-in-time compilation, binary translation,or dynamic code encryption and obfuscation. Unfortunately, SMC is alsoextremely difficult to reason about: existing formal verification techniques-including Hoare logic and type system-consistentlyassume that program code stored in memory is fixedand immutable; this severely limits their applicability and power. This paper presents a simple but novel Hoare-logic-like framework that supports modular verification of general von-Neumann machine code with runtime code manipulation. By dropping the assumption that code memory is fixed and immutable, we are forced to apply local reasoningand separation logic at the very beginning, and treat program code uniformly as regular data structure. We address the interaction between separation and code memory and show how to establish the frame rules for local reasoning even in the presence of SMC. Our frameworkis realistic, but designed to be highly generic, so that it can support assembly code under all modern CPUs (including both x86 andMIPS). Our system is expressive and fully mechanized. We prove itssoundness in the Coq proof assistant and demonstrate its power by certifying a series of realistic examples and applications-all of which can directly run on the SPIM simulator or any stock x86 hardware.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250743},
 doi = {http://doi.acm.org/10.1145/1250734.1250743},
 acmid = {1250743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assembly code verification, hoare logic, modular verification, runtime code manipulation, self-modifying code},
} 

@article{Cai:2007:CSC:1273442.1250743,
 author = {Cai, Hongxu and Shao, Zhong and Vaynberg, Alexander},
 title = {Certified self-modifying code},
 abstract = {Self-modifying code (SMC), in this paper, broadly refers to anyprogram that loads, generates, or mutates code at runtime. It is widely used in many of the world's critical software systems tosupport runtime code generation and optimization, dynamic loading and linking, OS boot loader, just-in-time compilation, binary translation,or dynamic code encryption and obfuscation. Unfortunately, SMC is alsoextremely difficult to reason about: existing formal verification techniques-including Hoare logic and type system-consistentlyassume that program code stored in memory is fixedand immutable; this severely limits their applicability and power. This paper presents a simple but novel Hoare-logic-like framework that supports modular verification of general von-Neumann machine code with runtime code manipulation. By dropping the assumption that code memory is fixed and immutable, we are forced to apply local reasoningand separation logic at the very beginning, and treat program code uniformly as regular data structure. We address the interaction between separation and code memory and show how to establish the frame rules for local reasoning even in the presence of SMC. Our frameworkis realistic, but designed to be highly generic, so that it can support assembly code under all modern CPUs (including both x86 andMIPS). Our system is expressive and fully mechanized. We prove itssoundness in the Coq proof assistant and demonstrate its power by certifying a series of realistic examples and applications-all of which can directly run on the SPIM simulator or any stock x86 hardware.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250743},
 doi = {http://doi.acm.org/10.1145/1273442.1250743},
 acmid = {1250743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assembly code verification, hoare logic, modular verification, runtime code manipulation, self-modifying code},
} 

@article{Shpeisman:2007:EIO:1273442.1250744,
 author = {Shpeisman, Tatiana and Menon, Vijay and Adl-Tabatabai, Ali-Reza and Balensiefer, Steven and Grossman, Dan and Hudson, Richard L. and Moore, Katherine F. and Saha, Bratin},
 title = {Enforcing isolation and ordering in STM},
 abstract = {Transactional memory provides a new concurrency control mechanism that avoids many of the pitfalls of lock-based synchronization. High-performance software transactional memory (STM) implementations thus far provide weak atomicity</i>: Accessing shared data both inside and outside a transaction can result in unexpected, implementation-dependent behavior. To guarantee isolation and consistent ordering in such a system, programmers are expected to enclose all shared-memory accesses inside transactions. A system that provides strong atomicity</i> guarantees isolation even in the presence of threads that access shared data outside transactions. A strongly-atomic system also orders transactions with conflicting non-transactional memory operations in a consistent manner. In this paper, we discuss some surprising pitfalls of weak atomicity, and we present an STM system that avoids these problems via strong atomicity. We demonstrate how to implement non-transactional data accesses via efficient read and write barriers, and we present compiler optimizations that further reduce the overheads of these barriers. We introduce a dynamic escape analysis</i> that differentiates private and public data at runtime to make barriers cheaper and a static not-accessed-in-transaction</i> analysis that removes many barriers completely. Our results on a set of Java programs show that strong atomicity can be implemented efficiently in a high-performance STM system.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {78--88},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250744},
 doi = {http://doi.acm.org/10.1145/1273442.1250744},
 acmid = {1250744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compiler optimizations, escape analysis, isolation, ordering, strong atomicity, transactional memory, virtual machines, weak atomicity},
} 

@inproceedings{Shpeisman:2007:EIO:1250734.1250744,
 author = {Shpeisman, Tatiana and Menon, Vijay and Adl-Tabatabai, Ali-Reza and Balensiefer, Steven and Grossman, Dan and Hudson, Richard L. and Moore, Katherine F. and Saha, Bratin},
 title = {Enforcing isolation and ordering in STM},
 abstract = {Transactional memory provides a new concurrency control mechanism that avoids many of the pitfalls of lock-based synchronization. High-performance software transactional memory (STM) implementations thus far provide weak atomicity</i>: Accessing shared data both inside and outside a transaction can result in unexpected, implementation-dependent behavior. To guarantee isolation and consistent ordering in such a system, programmers are expected to enclose all shared-memory accesses inside transactions. A system that provides strong atomicity</i> guarantees isolation even in the presence of threads that access shared data outside transactions. A strongly-atomic system also orders transactions with conflicting non-transactional memory operations in a consistent manner. In this paper, we discuss some surprising pitfalls of weak atomicity, and we present an STM system that avoids these problems via strong atomicity. We demonstrate how to implement non-transactional data accesses via efficient read and write barriers, and we present compiler optimizations that further reduce the overheads of these barriers. We introduce a dynamic escape analysis</i> that differentiates private and public data at runtime to make barriers cheaper and a static not-accessed-in-transaction</i> analysis that removes many barriers completely. Our results on a set of Java programs show that strong atomicity can be implemented efficiently in a high-performance STM system.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {78--88},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250744},
 doi = {http://doi.acm.org/10.1145/1250734.1250744},
 acmid = {1250744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compiler optimizations, escape analysis, isolation, ordering, strong atomicity, transactional memory, virtual machines, weak atomicity},
} 

@inproceedings{Nethercote:2007:VFH:1250734.1250746,
 author = {Nethercote, Nicholas and Seward, Julian},
 title = {Valgrind: a framework for heavyweight dynamic binary instrumentation},
 abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values</i>-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {89--100},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250746},
 doi = {http://doi.acm.org/10.1145/1250734.1250746},
 acmid = {1250746},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Memcheck, Valgrind, dynamic binary analysis, dynamic binary instrumentation, shadow values},
} 

@article{Nethercote:2007:VFH:1273442.1250746,
 author = {Nethercote, Nicholas and Seward, Julian},
 title = {Valgrind: a framework for heavyweight dynamic binary instrumentation},
 abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values</i>-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {89--100},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250746},
 doi = {http://doi.acm.org/10.1145/1273442.1250746},
 acmid = {1250746},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Memcheck, Valgrind, dynamic binary analysis, dynamic binary instrumentation, shadow values},
} 

@inproceedings{Ha:2007:IER:1250734.1250747,
 author = {Ha, Jungwoo and Rossbach, Christopher J. and Davis, Jason V. and Roy, Indrajit and Ramadan, Hany E. and Porter, Donald E. and Chen, David L. and Witchel, Emmett},
 title = {Improved error reporting for software that uses black-box components},
 abstract = {An error occurs when software cannot complete a requested action as a result of some problem with its input, configuration, or environment. A high-quality error report allows a user to understand and correct the problem. Unfortunately, the quality of error reports has been decreasing as software becomes more complex and layered. End-users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites. Developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from a black-box software component. We introduce Clarify, a system that improves error reporting by classifying application behavior. Clarify uses minimally invasive monitoring to generate a behavior profile</i>, which is a summary of the program's execution history. A machine learning classifier uses the behavior profile to classify the application's behavior, thereby enabling a more precise error report than the output of the application itself. We evaluate a prototype Clarify system on ambiguous error messages generated by large, modern applications like gcc, La-TeX, and the Linux kernel. For a performance cost of less than 1\% on user applications and 4.7\% on the Linux kernel, the proto type correctly disambiguates at least 85\% of application behaviors that result in ambiguous error reports. This accuracy does not degrade significantly with more behaviors: a Clarify classifier for 81 La-TeX error messages is at most 2.5\% less accurate than a classifier for 27 LaTeX error messages. Finally, we show that without any human effort to build a classifier, Clarify can provide nearest-neighbor software support, where users who experience a problem are told about 5 other users who might have had the same problem. On average 2.3 of the 5 users that Clarify identifies have experienced the same problem.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {101--111},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250747},
 doi = {http://doi.acm.org/10.1145/1250734.1250747},
 acmid = {1250747},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {classification, error report, machine learning, profiling, software support},
} 

@article{Ha:2007:IER:1273442.1250747,
 author = {Ha, Jungwoo and Rossbach, Christopher J. and Davis, Jason V. and Roy, Indrajit and Ramadan, Hany E. and Porter, Donald E. and Chen, David L. and Witchel, Emmett},
 title = {Improved error reporting for software that uses black-box components},
 abstract = {An error occurs when software cannot complete a requested action as a result of some problem with its input, configuration, or environment. A high-quality error report allows a user to understand and correct the problem. Unfortunately, the quality of error reports has been decreasing as software becomes more complex and layered. End-users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites. Developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from a black-box software component. We introduce Clarify, a system that improves error reporting by classifying application behavior. Clarify uses minimally invasive monitoring to generate a behavior profile</i>, which is a summary of the program's execution history. A machine learning classifier uses the behavior profile to classify the application's behavior, thereby enabling a more precise error report than the output of the application itself. We evaluate a prototype Clarify system on ambiguous error messages generated by large, modern applications like gcc, La-TeX, and the Linux kernel. For a performance cost of less than 1\% on user applications and 4.7\% on the Linux kernel, the proto type correctly disambiguates at least 85\% of application behaviors that result in ambiguous error reports. This accuracy does not degrade significantly with more behaviors: a Clarify classifier for 81 La-TeX error messages is at most 2.5\% less accurate than a classifier for 27 LaTeX error messages. Finally, we show that without any human effort to build a classifier, Clarify can provide nearest-neighbor software support, where users who experience a problem are told about 5 other users who might have had the same problem. On average 2.3 of the 5 users that Clarify identifies have experienced the same problem.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {101--111},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250747},
 doi = {http://doi.acm.org/10.1145/1273442.1250747},
 acmid = {1250747},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {classification, error report, machine learning, profiling, software support},
} 

@article{Sridharan:2007:TS:1273442.1250748,
 author = {Sridharan, Manu and Fink, Stephen J. and Bodik, Rastislav},
 title = {Thin slicing},
 abstract = {Program slicing systematically identifies parts of a program relevant to a seed statement. Unfortunately, slices of modern programs often grow too large for human consumption. We argue that unwieldy slices arise primarily from an overly broad definition of relevance, rather than from analysis imprecision. While a traditional slice includes all statements that may affect</i> a point of interest, not all such statements appear equally relevant to a human. As an improved method of finding relevant statements, we propose thin slicing</i>. A thin slice consists only of producer statements</i> for the seed, i.e.</i>, those statements that help compute and copy avalue to the seed. Statements that explain why producers affect the seed are excluded. For example, for a seed that reads a value from a container object, a thin slice includes statements that store the value into the container, but excludes statements that manipulate pointers to the container itself. Thin slices can also be hierarchically expanded to include statements explaining how producers affect the seed, yielding a traditional slice in the limit. We evaluated thin slicing for a set of debugging and program understanding tasks. The evaluation showed that thin slices usually included the desired statements for the tasks (e.g.</i>, the buggy statement for a debugging task). Furthermore, in simulated use of a slicing tool, thin slices revealed desired statements after inspecting 3.3 times fewer statements than traditional slicing for our debugging tasks and 9.4 times fewer statements for our program understanding tasks. Finally, our thin slicing algorithm scales well to relatively large Java benchmarks, suggesting that thin slicing represents an attractive option for practical tools.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {112--122},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250748},
 doi = {http://doi.acm.org/10.1145/1273442.1250748},
 acmid = {1250748},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, program understanding, slicing},
} 

@inproceedings{Sridharan:2007:TS:1250734.1250748,
 author = {Sridharan, Manu and Fink, Stephen J. and Bodik, Rastislav},
 title = {Thin slicing},
 abstract = {Program slicing systematically identifies parts of a program relevant to a seed statement. Unfortunately, slices of modern programs often grow too large for human consumption. We argue that unwieldy slices arise primarily from an overly broad definition of relevance, rather than from analysis imprecision. While a traditional slice includes all statements that may affect</i> a point of interest, not all such statements appear equally relevant to a human. As an improved method of finding relevant statements, we propose thin slicing</i>. A thin slice consists only of producer statements</i> for the seed, i.e.</i>, those statements that help compute and copy avalue to the seed. Statements that explain why producers affect the seed are excluded. For example, for a seed that reads a value from a container object, a thin slice includes statements that store the value into the container, but excludes statements that manipulate pointers to the container itself. Thin slices can also be hierarchically expanded to include statements explaining how producers affect the seed, yielding a traditional slice in the limit. We evaluated thin slicing for a set of debugging and program understanding tasks. The evaluation showed that thin slices usually included the desired statements for the tasks (e.g.</i>, the buggy statement for a debugging task). Furthermore, in simulated use of a slicing tool, thin slices revealed desired statements after inspecting 3.3 times fewer statements than traditional slicing for our debugging tasks and 9.4 times fewer statements for our program understanding tasks. Finally, our thin slicing algorithm scales well to relatively large Java benchmarks, suggesting that thin slicing represents an attractive option for practical tools.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {112--122},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250748},
 doi = {http://doi.acm.org/10.1145/1250734.1250748},
 acmid = {1250748},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, program understanding, slicing},
} 

@article{Ramanathan:2007:SSI:1273442.1250749,
 author = {Ramanathan, Murali Krishna and Grama, Ananth and Jagannathan, Suresh},
 title = {Static specification inference using predicate mining},
 abstract = {The reliability and correctness of complex software systems can be significantly enhanced through well-defined specifications that dictate the use of various units of abstraction (e.g., modules, or procedures). Often times, however, specifications are either missing, imprecise, or simply too complex to encode within a signature, necessitating specification inference. The process of inferring specifications from complex software systems forms the focus of this paper. We describe a static inference mechanism for identifying the preconditions that must hold whenever a procedure is called. These preconditions may reflect both data flow properties (e.g., whenever p</i> is called, variable x</i> must be non-null) as well as control-flow properties (e.g., every call to p</i> must bepreceded by a call to q</i>). We derive these preconditions using a ninter-procedural path-sensitive dataflow analysis that gathers predicates at each program point. We apply mining techniques to these predicates to make specification inference robust to errors. This technique also allows us to derive higher-level specifications that abstract structural similarities among predicates (e.g., procedure p</i> is called immediately after a conditional test that checks whether some variable v</i> is non-null.) We describe an implementation of these techniques, and validate the effectiveness of the approach on a number of large open-source benchmarks. Experimental results confirm that our mining algorithms are efficient, and that the specifications derived are both precise and useful-the implementation discovers several critical, yet previously, undocumented preconditions for well-tested libraries.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {123--134},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250749},
 doi = {http://doi.acm.org/10.1145/1273442.1250749},
 acmid = {1250749},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {preconditions, predicate mining, program analysis, specification inference},
} 

@inproceedings{Ramanathan:2007:SSI:1250734.1250749,
 author = {Ramanathan, Murali Krishna and Grama, Ananth and Jagannathan, Suresh},
 title = {Static specification inference using predicate mining},
 abstract = {The reliability and correctness of complex software systems can be significantly enhanced through well-defined specifications that dictate the use of various units of abstraction (e.g., modules, or procedures). Often times, however, specifications are either missing, imprecise, or simply too complex to encode within a signature, necessitating specification inference. The process of inferring specifications from complex software systems forms the focus of this paper. We describe a static inference mechanism for identifying the preconditions that must hold whenever a procedure is called. These preconditions may reflect both data flow properties (e.g., whenever p</i> is called, variable x</i> must be non-null) as well as control-flow properties (e.g., every call to p</i> must bepreceded by a call to q</i>). We derive these preconditions using a ninter-procedural path-sensitive dataflow analysis that gathers predicates at each program point. We apply mining techniques to these predicates to make specification inference robust to errors. This technique also allows us to derive higher-level specifications that abstract structural similarities among predicates (e.g., procedure p</i> is called immediately after a conditional test that checks whether some variable v</i> is non-null.) We describe an implementation of these techniques, and validate the effectiveness of the approach on a number of large open-source benchmarks. Experimental results confirm that our mining algorithms are efficient, and that the specifications derived are both precise and useful-the implementation discovers several critical, yet previously, undocumented preconditions for well-tested libraries.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {123--134},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250749},
 doi = {http://doi.acm.org/10.1145/1250734.1250749},
 acmid = {1250749},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {preconditions, predicate mining, program analysis, specification inference},
} 

@article{Scherpelz:2007:AIO:1273442.1250750,
 author = {Scherpelz, Erika Rice and Lerner, Sorin and Chambers, Craig},
 title = {Automatic inference of optimizer flow functions from semantic meanings},
 abstract = {Previous work presented a language called Rhodium for writing program analyses and transformations, in the form of declarative flow functions that propagate instances of user-defined dataflow fact schemas. Each dataflow fact schema specifies a semantic meaning, which allows the Rhodium system to automatically verify the correctness of the user's flow functions. In this work, we have reversed the roles of the flow functions and semantic meanings: rather than checking</i> the correctness of the user-written flow functions using the facts' semantic meanings, we automatically infer</i> correct flow functions solely from the meanings of the dataflow fact schemas. We have implemented our algorithm for inferring flow functions from fact schemas in the context of the Whirlwind compiler, and have used this implementation to infer flow functions for a variety of fact schemas. The automatically generated flow functions cover most of the situations covered by an earlier suite of handwritten rules.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250750},
 doi = {http://doi.acm.org/10.1145/1273442.1250750},
 acmid = {1250750},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Scherpelz:2007:AIO:1250734.1250750,
 author = {Scherpelz, Erika Rice and Lerner, Sorin and Chambers, Craig},
 title = {Automatic inference of optimizer flow functions from semantic meanings},
 abstract = {Previous work presented a language called Rhodium for writing program analyses and transformations, in the form of declarative flow functions that propagate instances of user-defined dataflow fact schemas. Each dataflow fact schema specifies a semantic meaning, which allows the Rhodium system to automatically verify the correctness of the user's flow functions. In this work, we have reversed the roles of the flow functions and semantic meanings: rather than checking</i> the correctness of the user-written flow functions using the facts' semantic meanings, we automatically infer</i> correct flow functions solely from the meanings of the dataflow fact schemas. We have implemented our algorithm for inferring flow functions from fact schemas in the context of the Whirlwind compiler, and have used this implementation to infer flow functions for a variety of fact schemas. The automatically generated flow functions cover most of the situations covered by an earlier suite of handwritten rules.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250750},
 doi = {http://doi.acm.org/10.1145/1250734.1250750},
 acmid = {1250750},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Morita:2007:AIG:1250734.1250752,
 author = {Morita, Kazutaka and Morihata, Akimasa and Matsuzaki, Kiminori and Hu, Zhenjiang and Takeichi, Masato},
 title = {Automatic inversion generates divide-and-conquer parallel programs},
 abstract = {Divide-and-conquer algorithms are suitable for modern parallel machines, tending to have large amounts of inherent parallelism and working well with caches and deep memory hierarchies. Among others, list homomorphisms are a class of recursive functions on lists, which match very well with the divide-and-conquer paradigm. However, direct programming with list homomorphisms is a challenge for many programmers. In this paper, we propose and implement a novel systemthat can automatically derive cost-optimal list homomorphisms from a pair of sequential programs, based on the third homomorphism theorem. Our idea is to reduce extraction of list homomorphisms to derivation of weak right inverses. We show that a weak right inverse</i> always exists and can be automatically generated from a wide class of sequential programs. We demonstrate our system with several nontrivial examples, including the maximum prefix sum problem, the prefix sum computation, the maximum segment sum problem, and the line-of-sight problem. The experimental results show practical efficiency of our automatic parallelization algorithm and good speedups of the generated parallel programs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250752},
 doi = {http://doi.acm.org/10.1145/1250734.1250752},
 acmid = {1250752},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {divide-and-conquer parallelism, inversion, program transformation, third homomorphism theorem},
} 

@article{Morita:2007:AIG:1273442.1250752,
 author = {Morita, Kazutaka and Morihata, Akimasa and Matsuzaki, Kiminori and Hu, Zhenjiang and Takeichi, Masato},
 title = {Automatic inversion generates divide-and-conquer parallel programs},
 abstract = {Divide-and-conquer algorithms are suitable for modern parallel machines, tending to have large amounts of inherent parallelism and working well with caches and deep memory hierarchies. Among others, list homomorphisms are a class of recursive functions on lists, which match very well with the divide-and-conquer paradigm. However, direct programming with list homomorphisms is a challenge for many programmers. In this paper, we propose and implement a novel systemthat can automatically derive cost-optimal list homomorphisms from a pair of sequential programs, based on the third homomorphism theorem. Our idea is to reduce extraction of list homomorphisms to derivation of weak right inverses. We show that a weak right inverse</i> always exists and can be automatically generated from a wide class of sequential programs. We demonstrate our system with several nontrivial examples, including the maximum prefix sum problem, the prefix sum computation, the maximum segment sum problem, and the line-of-sight problem. The experimental results show practical efficiency of our automatic parallelization algorithm and good speedups of the generated parallel programs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250752},
 doi = {http://doi.acm.org/10.1145/1273442.1250752},
 acmid = {1250752},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {divide-and-conquer parallelism, inversion, program transformation, third homomorphism theorem},
} 

@inproceedings{Wang:2007:EAP:1250734.1250753,
 author = {Wang, Perry H. and Collins, Jamison D. and Chinya, Gautham N. and Jiang, Hong and Tian, Xinmin and Girkar, Milind and Yang, Nick Y. and Lueh, Guei-Yuan and Wang, Hong},
 title = {EXOCHI: architecture and programming environment for a heterogeneous multi-core multithreaded system},
 abstract = {Future mainstream microprocessors will likely integrate specialized accelerators, such as GPUs, onto a single die to achieve better performance and power efficiency. However, it remains a keen challenge to program such a heterogeneous multicore platform, since these specialized accelerators feature ISAs and functionality that are significantly different from the general purpose CPU cores. In this paper, we present EXOCHI: (1) Exoskeleton Sequencer</i>(EXO), an architecture to represent heterogeneous acceleratorsas ISA-based MIMD architecture resources, and a shared virtual memory heterogeneous multithreaded program execution model that tightly couples specialized accelerator cores with generalpurpose CPU cores, and (2) C for Heterogeneous Integration</i>(CHI), an integrated C/C++ programming environment that supports accelerator-specific inline assembly and domain-specific languages. The CHI compiler extends the OpenMP pragma for heterogeneous multithreading programming, and produces a single fat binary with code sections corresponding to different instruction sets. The runtime can judiciously spread parallel computation across the heterogeneous cores to optimize performance and power. We have prototyped the EXO architecture on a physical heterogeneous platform consisting of an Intel\&#174; Core\&#8482; 2 Duo processor and an 8-core 32-thread Intel\&#174; Graphics Media Accelerator X3000. In addition, we have implemented the CHI integrated programming environment with the Intel\&#174; C++ Compiler, runtime toolset, and debugger. On the EXO prototype system, we have enhanced a suite of production-quality media kernels for video and image processing to utilize the accelerator through the CHI programming interface, achieving significant speedup (1.41X to10.97X) over execution on the IA32 CPU alone.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {156--166},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250753},
 doi = {http://doi.acm.org/10.1145/1250734.1250753},
 acmid = {1250753},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, heterogeneous multi-cores, openMP},
} 

@article{Wang:2007:EAP:1273442.1250753,
 author = {Wang, Perry H. and Collins, Jamison D. and Chinya, Gautham N. and Jiang, Hong and Tian, Xinmin and Girkar, Milind and Yang, Nick Y. and Lueh, Guei-Yuan and Wang, Hong},
 title = {EXOCHI: architecture and programming environment for a heterogeneous multi-core multithreaded system},
 abstract = {Future mainstream microprocessors will likely integrate specialized accelerators, such as GPUs, onto a single die to achieve better performance and power efficiency. However, it remains a keen challenge to program such a heterogeneous multicore platform, since these specialized accelerators feature ISAs and functionality that are significantly different from the general purpose CPU cores. In this paper, we present EXOCHI: (1) Exoskeleton Sequencer</i>(EXO), an architecture to represent heterogeneous acceleratorsas ISA-based MIMD architecture resources, and a shared virtual memory heterogeneous multithreaded program execution model that tightly couples specialized accelerator cores with generalpurpose CPU cores, and (2) C for Heterogeneous Integration</i>(CHI), an integrated C/C++ programming environment that supports accelerator-specific inline assembly and domain-specific languages. The CHI compiler extends the OpenMP pragma for heterogeneous multithreading programming, and produces a single fat binary with code sections corresponding to different instruction sets. The runtime can judiciously spread parallel computation across the heterogeneous cores to optimize performance and power. We have prototyped the EXO architecture on a physical heterogeneous platform consisting of an Intel\&#174; Core\&#8482; 2 Duo processor and an 8-core 32-thread Intel\&#174; Graphics Media Accelerator X3000. In addition, we have implemented the CHI integrated programming environment with the Intel\&#174; C++ Compiler, runtime toolset, and debugger. On the EXO prototype system, we have enhanced a suite of production-quality media kernels for video and image processing to utilize the accelerator through the CHI programming interface, achieving significant speedup (1.41X to10.97X) over execution on the IA32 CPU alone.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {156--166},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250753},
 doi = {http://doi.acm.org/10.1145/1273442.1250753},
 acmid = {1250753},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, heterogeneous multi-cores, openMP},
} 

@article{Solar-Lezama:2007:SS:1273442.1250754,
 author = {Solar-Lezama, Armando and Arnold, Gilad and Tancau, Liviu and Bodik, Rastislav and Saraswat, Vijay and Seshia, Sanjit},
 title = {Sketching stencils},
 abstract = {Performance of stencil computations can be significantly improved through smart implementations that improve memory locality, computation reuse, or parallelize the computation. Unfortunately, efficient implementations are hard to obtain because they often involve non-traditional transformations, which means that they cannot be produced by optimizing the reference stencil with a compiler. In fact, many stencils are produced by code generators that were tediously handcrafted. In this paper, we show how stencil implementations can be produced with sketching. Sketching is a software synthesis approach where the programmer develops a partial implementation--a sketch--and a separate specification of the desired functionality given by a reference (unoptimized) stencil. The synthesizer then completes the sketch to behave like the specification, filling in code fragments that are difficult to develop manually. Existing sketching systems work only for small finite programs, i.e.,</i>, programs that can be represented as small Boolean circuits. In this paper, we develop a sketching synthesizer that works for stencil computations, a large class of programs that, unlike circuits, have unbounded inputs and outputs, as well as an unbounded number of computations. The key contribution is a reduction algorithm that turns a stencil into a circuit, allowing us to synthesize stencils using an existing sketching synthesizer.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250754},
 doi = {http://doi.acm.org/10.1145/1273442.1250754},
 acmid = {1250754},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching, stencil},
} 

@inproceedings{Solar-Lezama:2007:SS:1250734.1250754,
 author = {Solar-Lezama, Armando and Arnold, Gilad and Tancau, Liviu and Bodik, Rastislav and Saraswat, Vijay and Seshia, Sanjit},
 title = {Sketching stencils},
 abstract = {Performance of stencil computations can be significantly improved through smart implementations that improve memory locality, computation reuse, or parallelize the computation. Unfortunately, efficient implementations are hard to obtain because they often involve non-traditional transformations, which means that they cannot be produced by optimizing the reference stencil with a compiler. In fact, many stencils are produced by code generators that were tediously handcrafted. In this paper, we show how stencil implementations can be produced with sketching. Sketching is a software synthesis approach where the programmer develops a partial implementation--a sketch--and a separate specification of the desired functionality given by a reference (unoptimized) stencil. The synthesizer then completes the sketch to behave like the specification, filling in code fragments that are difficult to develop manually. Existing sketching systems work only for small finite programs, i.e.,</i>, programs that can be represented as small Boolean circuits. In this paper, we develop a sketching synthesizer that works for stencil computations, a large class of programs that, unlike circuits, have unbounded inputs and outputs, as well as an unbounded number of computations. The key contribution is a reduction algorithm that turns a stencil into a circuit, allowing us to synthesize stencils using an existing sketching synthesizer.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250754},
 doi = {http://doi.acm.org/10.1145/1250734.1250754},
 acmid = {1250754},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching, stencil},
} 

@article{Killian:2007:MLS:1273442.1250755,
 author = {Killian, Charles Edwin and Anderson, James W. and Braud, Ryan and Jhala, Ranjit and Vahdat, Amin M.},
 title = {Mace: language support for building distributed systems},
 abstract = {Building distributed systems is particularly difficult because of the asynchronous, heterogeneous, and failure-prone environment where these systemsmust run. Tools for building distributed systems must strike a compromise between reducing programmer effort and increasing system efficiency. We present Mace</i>, a C++ language extension and source-to-source compiler that translates a concise but expressive distributed system specification into a C++ implementation. Mace overcomes the limitations of low-level languages by providing a unified framework for networking and event handling, and the limitations of high-level languages by allowing programmers to write program components in a controlled and structured manner in C++. By imposing structure and restrictions on how applications can be written, Mace supports debugging at a higher level, including support for efficient model checking and causal-path debugging. Because Mace programs compile to C++, programmers can use existing C++ tools, including optimizers, profilers, and debuggers to analyze their systems.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250755},
 doi = {http://doi.acm.org/10.1145/1273442.1250755},
 acmid = {1250755},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Mace, concurrency, debugging, distributed systems, domain specific languages, event driven programming, model checking},
} 

@inproceedings{Killian:2007:MLS:1250734.1250755,
 author = {Killian, Charles Edwin and Anderson, James W. and Braud, Ryan and Jhala, Ranjit and Vahdat, Amin M.},
 title = {Mace: language support for building distributed systems},
 abstract = {Building distributed systems is particularly difficult because of the asynchronous, heterogeneous, and failure-prone environment where these systemsmust run. Tools for building distributed systems must strike a compromise between reducing programmer effort and increasing system efficiency. We present Mace</i>, a C++ language extension and source-to-source compiler that translates a concise but expressive distributed system specification into a C++ implementation. Mace overcomes the limitations of low-level languages by providing a unified framework for networking and event handling, and the limitations of high-level languages by allowing programmers to write program components in a controlled and structured manner in C++. By imposing structure and restrictions on how applications can be written, Mace supports debugging at a higher level, including support for efficient model checking and causal-path debugging. Because Mace programs compile to C++, programmers can use existing C++ tools, including optimizers, profilers, and debuggers to analyze their systems.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250755},
 doi = {http://doi.acm.org/10.1145/1250734.1250755},
 acmid = {1250755},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Mace, concurrency, debugging, distributed systems, domain specific languages, event driven programming, model checking},
} 

@inproceedings{Li:2007:CET:1250734.1250756,
 author = {Li, Peng and Zdancewic, Steve},
 title = {Combining events and threads for scalable network services implementation and evaluation of monadic, application-level concurrency primitives},
 abstract = {This paper proposes to combine two seemingly opposed programming models for building massively concurrent network services: the event-driven model and the multithreaded model. The result is a hybrid design that offers the best of both worlds--the ease of use and expressiveness of threads and the flexibility and performance of events. This paper shows how the hybrid model can be implemented entirely at the application level using concurrency monads</i> in Haskell, which provides type-safe abstractions for both events and threads. This approach simplifies the development of massively concurrent software in a way that scales to real-world network services. The Haskell implementation supports exceptions, symmetrical multiprocessing, software transactional memory, asynchronous I/O mechanisms and application-level network protocol stacks. Experimental results demonstrate that this monad-based approach has good performance: the threads are extremely lightweight (scaling to ten million threads), and the I/O performance compares favorably to that of Linux NPTL. tens of thousands of simultaneous, mostly-idle client connections. Such massively-concurrent programs are difficult to implement, especially when other requirements, such as high performance and strong security, must also be met.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {189--199},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250756},
 doi = {http://doi.acm.org/10.1145/1250734.1250756},
 acmid = {1250756},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, event, haskell, implementation, monad, networking, programming, scalability, thread},
} 

@article{Li:2007:CET:1273442.1250756,
 author = {Li, Peng and Zdancewic, Steve},
 title = {Combining events and threads for scalable network services implementation and evaluation of monadic, application-level concurrency primitives},
 abstract = {This paper proposes to combine two seemingly opposed programming models for building massively concurrent network services: the event-driven model and the multithreaded model. The result is a hybrid design that offers the best of both worlds--the ease of use and expressiveness of threads and the flexibility and performance of events. This paper shows how the hybrid model can be implemented entirely at the application level using concurrency monads</i> in Haskell, which provides type-safe abstractions for both events and threads. This approach simplifies the development of massively concurrent software in a way that scales to real-world network services. The Haskell implementation supports exceptions, symmetrical multiprocessing, software transactional memory, asynchronous I/O mechanisms and application-level network protocol stacks. Experimental results demonstrate that this monad-based approach has good performance: the threads are extremely lightweight (scaling to ten million threads), and the I/O performance compares favorably to that of Linux NPTL. tens of thousands of simultaneous, mostly-idle client connections. Such massively-concurrent programs are difficult to implement, especially when other requirements, such as high performance and strong security, must also be met.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {189--199},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250756},
 doi = {http://doi.acm.org/10.1145/1273442.1250756},
 acmid = {1250756},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, event, haskell, implementation, monad, networking, programming, scalability, thread},
} 

@inproceedings{Kothari:2007:REP:1250734.1250757,
 author = {Kothari, Nupur and Gummadi, Ramakrishna and Millstein, Todd and Govindan, Ramesh},
 title = {Reliable and efficient programming abstractions for wireless sensor networks},
 abstract = {It is currently difficult to build practical and reliable programming systems out of distributed and resource-constrained sensor devices. The state of the art in today's sensornet programming is centered around a component-based language called nesC. nesC is a node-level</i> language-a program is written for an individual node in the network-and nesC programs use the services of an operating system called TinyOS. We are pursuing an approach to programming sensor networks that significantly raises the level of abstraction over this practice. The critical change is one of perspective: rather than writing programs from the point of view of an individual node, programmers implement a central</i> program that conceptually has access to the entire network. This approach pushes to the compiler the task of producing node-level programs that implement the desired behavio. We present the Pleiades programming language, its compiler, and its runtime. The Pleiades language extends the C language with constructs that allow programmers to name and access node-local state within the network and to specify simple forms of concurrent execution. The compiler and runtime system cooperate to implement Pleiades programs efficiently and reliably. First, the compiler employs a novel program analysis to translate Pleiades programs into message-efficient units of work implemented in nesC. The Pleiades runtime system orchestrates execution of these units, using TinyOS services, across a network of sensor nodes. Second, the compiler and runtime system employ novel locking, deadlock detection, and deadlock recovery algorithms that guarantee serializability in the face of concurrent execution. We illustrate the readability, reliability and efficiency benefits of the Pleiades language through detailed experiments, and demonstrate that the Pleiades implementation of a realistic application performs similar to a hand-coded nesC version that contains more than ten times as much code.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {200--210},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250757},
 doi = {http://doi.acm.org/10.1145/1250734.1250757},
 acmid = {1250757},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deadlocks, energy efficiency, macroprogramming, serializability, wireless sensor networks},
} 

@article{Kothari:2007:REP:1273442.1250757,
 author = {Kothari, Nupur and Gummadi, Ramakrishna and Millstein, Todd and Govindan, Ramesh},
 title = {Reliable and efficient programming abstractions for wireless sensor networks},
 abstract = {It is currently difficult to build practical and reliable programming systems out of distributed and resource-constrained sensor devices. The state of the art in today's sensornet programming is centered around a component-based language called nesC. nesC is a node-level</i> language-a program is written for an individual node in the network-and nesC programs use the services of an operating system called TinyOS. We are pursuing an approach to programming sensor networks that significantly raises the level of abstraction over this practice. The critical change is one of perspective: rather than writing programs from the point of view of an individual node, programmers implement a central</i> program that conceptually has access to the entire network. This approach pushes to the compiler the task of producing node-level programs that implement the desired behavio. We present the Pleiades programming language, its compiler, and its runtime. The Pleiades language extends the C language with constructs that allow programmers to name and access node-local state within the network and to specify simple forms of concurrent execution. The compiler and runtime system cooperate to implement Pleiades programs efficiently and reliably. First, the compiler employs a novel program analysis to translate Pleiades programs into message-efficient units of work implemented in nesC. The Pleiades runtime system orchestrates execution of these units, using TinyOS services, across a network of sensor nodes. Second, the compiler and runtime system employ novel locking, deadlock detection, and deadlock recovery algorithms that guarantee serializability in the face of concurrent execution. We illustrate the readability, reliability and efficiency benefits of the Pleiades language through detailed experiments, and demonstrate that the Pleiades implementation of a realistic application performs similar to a hand-coded nesC version that contains more than ten times as much code.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {200--210},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250757},
 doi = {http://doi.acm.org/10.1145/1273442.1250757},
 acmid = {1250757},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deadlocks, energy efficiency, macroprogramming, serializability, wireless sensor networks},
} 

@article{Kulkarni:2007:OPR:1273442.1250759,
 author = {Kulkarni, Milind and Pingali, Keshav and Walter, Bruce and Ramanarayanan, Ganesh and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism requires abstractions},
 abstract = {Irregular applications, which manipulate large, pointer-based data structures like graphs, are difficult to parallelize manually. Automatic tools and techniques such as restructuring compilers and run-time speculative execution have failed to uncover much parallelism in these applications, in spite of a lot of effort by the research community. These difficulties have even led some researchers to wonder if there is any coarse-grain parallelism worth exploiting in irregular applications. In this paper, we describe two real-world irregular applications: a Delaunay mesh refinement application and a graphics application thatperforms agglomerative clustering. By studying the algorithms and data structures used in theseapplications, we show that there is substantial coarse-grain, data parallelism in these applications, but that this parallelism is very dependent on the input data and therefore cannot be uncoveredby compiler analysis. In principle, optimistic techniques such asthread-level speculation can be used to uncover this parallelism, but we argue that current implementations cannot accomplish thisbecause they do not use the proper abstractions for the data structuresin these programs. These insights have informed our design of the Galois system</i>, an object-based optimistic parallelization system for irregular applications. There are three main aspects to Galois: (1) a small number of syntactic constructs for packaging optimistic parallelism as iteration over ordered and unordered sets, (2)assertions about methods in class libraries, and (3) a runtime scheme for detecting and recovering from potentially unsafe accesses to shared memory made by an optimistic computation. We show that Delaunay mesh generation and agglomerative clustering can be parallelized in a straight-forward way using the Galois approach, and we present experimental measurements to show that this approach is practical. These results suggest that Galois is a practical approach to exploiting data parallelismin irregular programs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250759},
 doi = {http://doi.acm.org/10.1145/1273442.1250759},
 acmid = {1250759},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstractions, irregular programs, optimistic parallelism},
} 

@inproceedings{Kulkarni:2007:OPR:1250734.1250759,
 author = {Kulkarni, Milind and Pingali, Keshav and Walter, Bruce and Ramanarayanan, Ganesh and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism requires abstractions},
 abstract = {Irregular applications, which manipulate large, pointer-based data structures like graphs, are difficult to parallelize manually. Automatic tools and techniques such as restructuring compilers and run-time speculative execution have failed to uncover much parallelism in these applications, in spite of a lot of effort by the research community. These difficulties have even led some researchers to wonder if there is any coarse-grain parallelism worth exploiting in irregular applications. In this paper, we describe two real-world irregular applications: a Delaunay mesh refinement application and a graphics application thatperforms agglomerative clustering. By studying the algorithms and data structures used in theseapplications, we show that there is substantial coarse-grain, data parallelism in these applications, but that this parallelism is very dependent on the input data and therefore cannot be uncoveredby compiler analysis. In principle, optimistic techniques such asthread-level speculation can be used to uncover this parallelism, but we argue that current implementations cannot accomplish thisbecause they do not use the proper abstractions for the data structuresin these programs. These insights have informed our design of the Galois system</i>, an object-based optimistic parallelization system for irregular applications. There are three main aspects to Galois: (1) a small number of syntactic constructs for packaging optimistic parallelism as iteration over ordered and unordered sets, (2)assertions about methods in class libraries, and (3) a runtime scheme for detecting and recovering from potentially unsafe accesses to shared memory made by an optimistic computation. We show that Delaunay mesh generation and agglomerative clustering can be parallelized in a straight-forward way using the Galois approach, and we present experimental measurements to show that this approach is practical. These results suggest that Galois is a practical approach to exploiting data parallelismin irregular programs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250759},
 doi = {http://doi.acm.org/10.1145/1250734.1250759},
 acmid = {1250759},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstractions, irregular programs, optimistic parallelism},
} 

@inproceedings{Ding:2007:SBO:1250734.1250760,
 author = {Ding, Chen and Shen, Xipeng and Kelsey, Kirk and Tice, Chris and Huang, Ruke and Zhang, Chengliang},
 title = {Software behavior oriented parallelization},
 abstract = {Many sequential applications are difficult to parallelize because of unpredictable control flow, indirect data access, and input-dependent parallelism. These difficulties led us to build a software system for behavior oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading just part of the source code, or a profiling tool examining merely one or few executions. The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculation, critical-path minimization, and value-based correctness checking. On a recently acquired multi-core, multi-processor PC, the BOP system reduced the end-to-end execution time by integer factors for a Lisp interpreter, a data compressor, a language parser, and a scientific library, with no change to the underlying hardware or operating system.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250760},
 doi = {http://doi.acm.org/10.1145/1250734.1250760},
 acmid = {1250760},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {program behavior, speculative parallelization},
} 

@article{Ding:2007:SBO:1273442.1250760,
 author = {Ding, Chen and Shen, Xipeng and Kelsey, Kirk and Tice, Chris and Huang, Ruke and Zhang, Chengliang},
 title = {Software behavior oriented parallelization},
 abstract = {Many sequential applications are difficult to parallelize because of unpredictable control flow, indirect data access, and input-dependent parallelism. These difficulties led us to build a software system for behavior oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading just part of the source code, or a profiling tool examining merely one or few executions. The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculation, critical-path minimization, and value-based correctness checking. On a recently acquired multi-core, multi-processor PC, the BOP system reduced the end-to-end execution time by integer factors for a Lisp interpreter, a data compressor, a language parser, and a scientific library, with no change to the underlying hardware or operating system.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250760},
 doi = {http://doi.acm.org/10.1145/1273442.1250760},
 acmid = {1250760},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {program behavior, speculative parallelization},
} 

@inproceedings{Krishnamoorthy:2007:EAP:1250734.1250761,
 author = {Krishnamoorthy, Sriram and Baskaran, Muthu and Bondhugula, Uday and Ramanujam, J. and Rountev, Atanas and Sadayappan, P},
 title = {Effective automatic parallelization of stencil computations},
 abstract = {Performance optimization of stencil computations has been widely studied in the literature, since they occur in many computationally intensive scientific and engineering applications. Compiler frameworks have also been developed that can transform sequential stencil codes for optimization of data locality and parallelism. However, loop skewing is typically required in order to tile stencil codes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. In this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly addresses the issue of load-balanced execution of tiles. Experimental results are provided that demonstrate the effectiveness of the approach.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250761},
 doi = {http://doi.acm.org/10.1145/1250734.1250761},
 acmid = {1250761},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, load, stencil computations, tiling},
} 

@article{Krishnamoorthy:2007:EAP:1273442.1250761,
 author = {Krishnamoorthy, Sriram and Baskaran, Muthu and Bondhugula, Uday and Ramanujam, J. and Rountev, Atanas and Sadayappan, P},
 title = {Effective automatic parallelization of stencil computations},
 abstract = {Performance optimization of stencil computations has been widely studied in the literature, since they occur in many computationally intensive scientific and engineering applications. Compiler frameworks have also been developed that can transform sequential stencil codes for optimization of data locality and parallelism. However, loop skewing is typically required in order to tile stencil codes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. In this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly addresses the issue of load-balanced execution of tiles. Experimental results are provided that demonstrate the effectiveness of the approach.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250761},
 doi = {http://doi.acm.org/10.1145/1273442.1250761},
 acmid = {1250761},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, load, stencil computations, tiling},
} 

@article{Elmas:2007:GRT:1273442.1250762,
 author = {Elmas, Tayfun and Qadeer, Shaz and Tasiran, Serdar},
 title = {Goldilocks: a race and transaction-aware java runtime},
 abstract = {Data races often result in unexpected and erroneous behavior. In addition to causing data corruption and leading programs to crash, the presence of data races complicates the semantics of an execution which might no longer be sequentially consistent. Motivated by these observations, we have designed and implemented a Java runtime system that monitors program executions and throws a DataRaceException when a data race is about to occur. Analogous to other runtime exceptions, the DataRaceException provides two key benefits. First, accesses causing race conditions are interruptedand handled before they cause errors that may be difficult to diagnose later. Second, if no DataRaceException is thrown in an execution, it is guaranteed to be sequentially consistent. This strong guarantee helps to rule out many concurrency-related possibilities as the cause of erroneous behavior. When a DataRaceException is caught, the operation, thread, or program causing it can be terminated gracefully. Alternatively, the DataRaceException can serve as a conflict-detection mechanism inoptimistic uses of concurrency. We start with the definition of data-race-free executions in the Java memory model. We generalize this definition to executions that use transactions in addition to locks and volatile variables for synchronization. We present a precise and efficient algorithm for dynamically verifying that an execution is free of data races. This algorithm generalizes the Goldilocks algorithm for data-race detectionby handling transactions and providing the ability to distinguish between read and write accesses. We have implemented our algorithm and the DataRaceException in the Kaffe Java Virtual Machine. We have evaluated our system on a variety of publicly available Java benchmarks and a few microbenchmarks that combine lock-based and transaction-based synchronization. Our experiments indicate that our implementation has reasonable overhead. Therefore, we believe that inaddition to being a debugging tool, the DataRaceException may be a viable mechanism to enforce the safety of executions of multithreaded Java programs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {245--255},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250762},
 doi = {http://doi.acm.org/10.1145/1273442.1250762},
 acmid = {1250762},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java runtime, data-race detection, runtime monitoring, software transactions},
} 

@inproceedings{Elmas:2007:GRT:1250734.1250762,
 author = {Elmas, Tayfun and Qadeer, Shaz and Tasiran, Serdar},
 title = {Goldilocks: a race and transaction-aware java runtime},
 abstract = {Data races often result in unexpected and erroneous behavior. In addition to causing data corruption and leading programs to crash, the presence of data races complicates the semantics of an execution which might no longer be sequentially consistent. Motivated by these observations, we have designed and implemented a Java runtime system that monitors program executions and throws a DataRaceException when a data race is about to occur. Analogous to other runtime exceptions, the DataRaceException provides two key benefits. First, accesses causing race conditions are interruptedand handled before they cause errors that may be difficult to diagnose later. Second, if no DataRaceException is thrown in an execution, it is guaranteed to be sequentially consistent. This strong guarantee helps to rule out many concurrency-related possibilities as the cause of erroneous behavior. When a DataRaceException is caught, the operation, thread, or program causing it can be terminated gracefully. Alternatively, the DataRaceException can serve as a conflict-detection mechanism inoptimistic uses of concurrency. We start with the definition of data-race-free executions in the Java memory model. We generalize this definition to executions that use transactions in addition to locks and volatile variables for synchronization. We present a precise and efficient algorithm for dynamically verifying that an execution is free of data races. This algorithm generalizes the Goldilocks algorithm for data-race detectionby handling transactions and providing the ability to distinguish between read and write accesses. We have implemented our algorithm and the DataRaceException in the Kaffe Java Virtual Machine. We have evaluated our system on a variety of publicly available Java benchmarks and a few microbenchmarks that combine lock-based and transaction-based synchronization. Our experiments indicate that our implementation has reasonable overhead. Therefore, we believe that inaddition to being a debugging tool, the DataRaceException may be a viable mechanism to enforce the safety of executions of multithreaded Java programs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {245--255},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250762},
 doi = {http://doi.acm.org/10.1145/1250734.1250762},
 acmid = {1250762},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java runtime, data-race detection, runtime monitoring, software transactions},
} 

@article{Guo:2007:SAI:1273442.1250764,
 author = {Guo, Bolei and Vachharajani, Neil and August, David I.},
 title = {Shape analysis with inductive recursion synthesis},
 abstract = {Separation logic with recursively defined predicates allows for concise yet precise description of the shapes of data structures. However, most uses of separation logic for program analysis rely on pre-defined recursive predicates, limiting the class of programs analyzable to those that manipulate only a priori data structures. This paper describes a general algorithm based on inductive program synthesis</i> that automatically infers recursive shape invariants, yielding a shape analysis based on separation logic that can be applied to any program. A key strength of separation logic is that it facilitates, via explicit expression of structural separation, local reasoning about heap where the effects of altering one part of a data structure are analyzed in isolation from the rest. The interaction between local reasoning and the global invariants given by recursive predicates is a difficult area, especially in the presence of complex internal sharing in the data structures. Existing approaches, using logic rules specifically designed for the list predicate to unfold and fold linked-lists, again require a priori knowledge about the shapes of the data structures and do not easily generalize to more complex data structures. We introduce a notion of "truncation points" in a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary data structures.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {256--265},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250764},
 doi = {http://doi.acm.org/10.1145/1273442.1250764},
 acmid = {1250764},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial intelligence, inductive recursion synthesis, loop invariant inference, separation logic, shape analysis},
} 

@inproceedings{Guo:2007:SAI:1250734.1250764,
 author = {Guo, Bolei and Vachharajani, Neil and August, David I.},
 title = {Shape analysis with inductive recursion synthesis},
 abstract = {Separation logic with recursively defined predicates allows for concise yet precise description of the shapes of data structures. However, most uses of separation logic for program analysis rely on pre-defined recursive predicates, limiting the class of programs analyzable to those that manipulate only a priori data structures. This paper describes a general algorithm based on inductive program synthesis</i> that automatically infers recursive shape invariants, yielding a shape analysis based on separation logic that can be applied to any program. A key strength of separation logic is that it facilitates, via explicit expression of structural separation, local reasoning about heap where the effects of altering one part of a data structure are analyzed in isolation from the rest. The interaction between local reasoning and the global invariants given by recursive predicates is a difficult area, especially in the presence of complex internal sharing in the data structures. Existing approaches, using logic rules specifically designed for the list predicate to unfold and fold linked-lists, again require a priori knowledge about the shapes of the data structures and do not easily generalize to more complex data structures. We introduce a notion of "truncation points" in a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary data structures.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {256--265},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250764},
 doi = {http://doi.acm.org/10.1145/1250734.1250764},
 acmid = {1250764},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial intelligence, inductive recursion synthesis, loop invariant inference, separation logic, shape analysis},
} 

@inproceedings{Gotsman:2007:TSA:1250734.1250765,
 author = {Gotsman, Alexey and Berdine, Josh and Cook, Byron and Sagiv, Mooly},
 title = {Thread-modular shape analysis},
 abstract = {We present the first shape analysis for multithreaded programs that avoids the explicit enumeration of execution-interleavings. Our approach is to automatically infer a resource invariant associated with each lock that describes the part of the heap protected by the lock. This allows us to use a sequential shape analysis on each thread. We show that resource invariants of a certain class can be characterized as least fixed points and computed via repeated applications of shape analysis only on each individual thread. Based on this approach, we have implemented a thread-modular shape analysis tool and applied it to concurrent heap-manipulating code from Windows device drivers.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {266--277},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250765},
 doi = {http://doi.acm.org/10.1145/1250734.1250765},
 acmid = {1250765},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, concurrent programming, shape analysis, static analysis},
} 

@article{Gotsman:2007:TSA:1273442.1250765,
 author = {Gotsman, Alexey and Berdine, Josh and Cook, Byron and Sagiv, Mooly},
 title = {Thread-modular shape analysis},
 abstract = {We present the first shape analysis for multithreaded programs that avoids the explicit enumeration of execution-interleavings. Our approach is to automatically infer a resource invariant associated with each lock that describes the part of the heap protected by the lock. This allows us to use a sequential shape analysis on each thread. We show that resource invariants of a certain class can be characterized as least fixed points and computed via repeated applications of shape analysis only on each individual thread. Based on this approach, we have implemented a thread-modular shape analysis tool and applied it to concurrent heap-manipulating code from Windows device drivers.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {266--277},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250765},
 doi = {http://doi.acm.org/10.1145/1273442.1250765},
 acmid = {1250765},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, concurrent programming, shape analysis, static analysis},
} 

@inproceedings{Lattner:2007:MCP:1250734.1250766,
 author = {Lattner, Chris and Lenharth, Andrew and Adve, Vikram},
 title = {Making context-sensitive points-to analysis with heap cloning practical for the real world},
 abstract = {Context-sensitive pointer analysis algorithms with full "heapcloning" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a context-sensitive, field-sensitive algorithm with fullheap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C code in 1-3 seconds,takes less than 5\% of the time it takes for GCC to compile the code (which includes no whole-program analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K linesof code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flow-insensitive and unification-basedanalysis, which are essential to avoid exponential behavior in practice;(b) sacrificing context-sensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(N</i><sup>2</sup>) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other context-sensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 10x-15xin our larger programs, and have found that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250766},
 doi = {http://doi.acm.org/10.1145/1250734.1250766},
 acmid = {1250766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive, field-sensitive, interprocedural, pointer analysis, recursive data structure, static analysis},
} 

@article{Lattner:2007:MCP:1273442.1250766,
 author = {Lattner, Chris and Lenharth, Andrew and Adve, Vikram},
 title = {Making context-sensitive points-to analysis with heap cloning practical for the real world},
 abstract = {Context-sensitive pointer analysis algorithms with full "heapcloning" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a context-sensitive, field-sensitive algorithm with fullheap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C code in 1-3 seconds,takes less than 5\% of the time it takes for GCC to compile the code (which includes no whole-program analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K linesof code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flow-insensitive and unification-basedanalysis, which are essential to avoid exponential behavior in practice;(b) sacrificing context-sensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(N</i><sup>2</sup>) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other context-sensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 10x-15xin our larger programs, and have found that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250766},
 doi = {http://doi.acm.org/10.1145/1273442.1250766},
 acmid = {1250766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive, field-sensitive, interprocedural, pointer analysis, recursive data structure, static analysis},
} 

@article{Hardekopf:2007:AGF:1273442.1250767,
 author = {Hardekopf, Ben and Lin, Calvin},
 title = {The ant and the grasshopper: fast and accurate pointer analysis for millions of lines of code},
 abstract = {Pointer information is a prerequisite for most program analyses, and the quality of this information can greatly affect their precision and performance. Inclusion-based (i.e. Andersen-style) pointer analysis is an important point in the space of pointer analyses, offering a potential sweet-spot in the trade-off between precision and performance. However, current techniques for inclusion-based pointer analysis can have difficulties delivering on this potential. We introduce and evaluate two novel techniques for inclusion-based pointer analysis---one lazy, one eager<sup>1</sup>---that significantly improve upon the current state-of-the-art without impacting precision. These techniques focus on the problem of online cycle detection, a critical optimization for scaling such analyses. Using a suite of six open-source C programs, which range in size from 169K to 2.17M LOC, we compare our techniques against the three best inclusion-based analyses--described by Heintze and Tardieu [11], by Pearce et al. [21], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is on average 3.2 xfaster than Heintze and Tardieu's algorithm, 6.4 xfaster than Pearce et al.'s algorithm, and 20.6 faster than Berndl et al.'s algorithm. We also investigate the use of different data structures to represent points-to sets, examining the impact on both performance and memory consumption. We compare a sparse-bitmap implementation used in the GCC compiler with a BDD-based implementation, and we find that the BDD implementation is on average 2x slower than using sparse bitmaps but uses 5.5x less memory.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {290--299},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250767},
 doi = {http://doi.acm.org/10.1145/1273442.1250767},
 acmid = {1250767},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pointer analysis},
} 

@inproceedings{Hardekopf:2007:AGF:1250734.1250767,
 author = {Hardekopf, Ben and Lin, Calvin},
 title = {The ant and the grasshopper: fast and accurate pointer analysis for millions of lines of code},
 abstract = {Pointer information is a prerequisite for most program analyses, and the quality of this information can greatly affect their precision and performance. Inclusion-based (i.e. Andersen-style) pointer analysis is an important point in the space of pointer analyses, offering a potential sweet-spot in the trade-off between precision and performance. However, current techniques for inclusion-based pointer analysis can have difficulties delivering on this potential. We introduce and evaluate two novel techniques for inclusion-based pointer analysis---one lazy, one eager<sup>1</sup>---that significantly improve upon the current state-of-the-art without impacting precision. These techniques focus on the problem of online cycle detection, a critical optimization for scaling such analyses. Using a suite of six open-source C programs, which range in size from 169K to 2.17M LOC, we compare our techniques against the three best inclusion-based analyses--described by Heintze and Tardieu [11], by Pearce et al. [21], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is on average 3.2 xfaster than Heintze and Tardieu's algorithm, 6.4 xfaster than Pearce et al.'s algorithm, and 20.6 faster than Berndl et al.'s algorithm. We also investigate the use of different data structures to represent points-to sets, examining the impact on both performance and memory consumption. We compare a sparse-bitmap implementation used in the GCC compiler with a BDD-based implementation, and we find that the BDD implementation is on average 2x slower than using sparse bitmaps but uses 5.5x less memory.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {290--299},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250767},
 doi = {http://doi.acm.org/10.1145/1250734.1250767},
 acmid = {1250767},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pointer analysis},
} 

@inproceedings{Beyer:2007:PI:1250734.1250769,
 author = {Beyer, Dirk and Henzinger, Thomas A. and Majumdar, Rupak and Rybalchenko, Andrey},
 title = {Path invariants},
 abstract = {The success of software verification depends on the ability to find a suitable abstraction of a program automatically. We propose a method for automated abstraction refinement which overcomes some limitations of current predicate discovery schemes. In current schemes, the cause of a false alarm is identified as an infeasible error path, and the abstraction is refined in order to remove that path. By contrast, we view the cause of a false alarm -the spurious counterexample</i>- as a full-fledged program, namely, a fragment of the original program whose control-flow graph may contain loops and represent unbounded computations. There are two advantages to using such path programs</i> as counterexamples for abstraction refinement. First, we can bring the whole machinery of program analysis to bear on path programs, which are typically small compared to the original program. Specifically, we use constraint-based invariant generation to automatically infer invariants of path programs-so-called path invariants</i>. Second, we use path invariants for abstraction refinement in order to remove not one infeasibility at a time, but at once all (possibly infinitely many) infeasible error computations that are represented by a path program. Unlike previous predicate discovery schemes, our method handles loops without unrolling them; it infers abstractions that involve universal quantification and naturally incorporates disjunctive reasoning.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {300--309},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250769},
 doi = {http://doi.acm.org/10.1145/1250734.1250769},
 acmid = {1250769},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstraction refinement, formal verification, invariant synthesis, predicate abstraction, software model checking},
} 

@article{Beyer:2007:PI:1273442.1250769,
 author = {Beyer, Dirk and Henzinger, Thomas A. and Majumdar, Rupak and Rybalchenko, Andrey},
 title = {Path invariants},
 abstract = {The success of software verification depends on the ability to find a suitable abstraction of a program automatically. We propose a method for automated abstraction refinement which overcomes some limitations of current predicate discovery schemes. In current schemes, the cause of a false alarm is identified as an infeasible error path, and the abstraction is refined in order to remove that path. By contrast, we view the cause of a false alarm -the spurious counterexample</i>- as a full-fledged program, namely, a fragment of the original program whose control-flow graph may contain loops and represent unbounded computations. There are two advantages to using such path programs</i> as counterexamples for abstraction refinement. First, we can bring the whole machinery of program analysis to bear on path programs, which are typically small compared to the original program. Specifically, we use constraint-based invariant generation to automatically infer invariants of path programs-so-called path invariants</i>. Second, we use path invariants for abstraction refinement in order to remove not one infeasibility at a time, but at once all (possibly infinitely many) infeasible error computations that are represented by a path program. Unlike previous predicate discovery schemes, our method handles loops without unrolling them; it infers abstractions that involve universal quantification and naturally incorporates disjunctive reasoning.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {300--309},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250769},
 doi = {http://doi.acm.org/10.1145/1273442.1250769},
 acmid = {1250769},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstraction refinement, formal verification, invariant synthesis, predicate abstraction, software model checking},
} 

@article{Shankar:2007:DAI:1273442.1250770,
 author = {Shankar, Ajeet and Bod\'{\i}k, Rastislav},
 title = {DITTO: automatic incrementalization of data structure invariant checks (in Java)},
 abstract = {We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing linearly with data structure size.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250770},
 doi = {http://doi.acm.org/10.1145/1273442.1250770},
 acmid = {1250770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic, data structure invariants, dynamic optimization, incrementalization, optimistic memoization, program analysis},
} 

@inproceedings{Shankar:2007:DAI:1250734.1250770,
 author = {Shankar, Ajeet and Bod\'{\i}k, Rastislav},
 title = {DITTO: automatic incrementalization of data structure invariant checks (in Java)},
 abstract = {We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing linearly with data structure size.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250770},
 doi = {http://doi.acm.org/10.1145/1250734.1250770},
 acmid = {1250770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic, data structure invariants, dynamic optimization, incrementalization, optimistic memoization, program analysis},
} 

@inproceedings{Cook:2007:PTT:1250734.1250771,
 author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
 title = {Proving thread termination},
 abstract = {Concurrent programs are often designed such that certain functions executing within critical threads must terminate. Examples of such cases can be found in operating systems, web servers, e-mail clients, etc. Unfortunately, no known automatic program termination prover supports a practical method of proving the termination of threads. In this paper we describe such a procedure. The procedure's scalability is achieved through the use of environment models that abstract away the surrounding threads. The procedure's accuracy is due to a novel method of incrementally constructing environment abstractions. Our method finds the conditions that a thread requires of its environment in order to establish termination by looking at the conditions necessary to prove that certain paths through the thread represent well-founded relations if executed in isolation of the other threads</i>. The paper gives a description of experimental results using an implementation of our procedureon Windows device drivers and adescription of a previously unknown bug found withthe tool.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {320--330},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250771},
 doi = {http://doi.acm.org/10.1145/1250734.1250771},
 acmid = {1250771},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, formal verification, model checking, program verification, termination, threads},
} 

@article{Cook:2007:PTT:1273442.1250771,
 author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
 title = {Proving thread termination},
 abstract = {Concurrent programs are often designed such that certain functions executing within critical threads must terminate. Examples of such cases can be found in operating systems, web servers, e-mail clients, etc. Unfortunately, no known automatic program termination prover supports a practical method of proving the termination of threads. In this paper we describe such a procedure. The procedure's scalability is achieved through the use of environment models that abstract away the surrounding threads. The procedure's accuracy is due to a novel method of incrementally constructing environment abstractions. Our method finds the conditions that a thread requires of its environment in order to establish termination by looking at the conditions necessary to prove that certain paths through the thread represent well-founded relations if executed in isolation of the other threads</i>. The paper gives a description of experimental results using an implementation of our procedureon Windows device drivers and adescription of a previously unknown bug found withthe tool.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {320--330},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250771},
 doi = {http://doi.acm.org/10.1145/1273442.1250771},
 acmid = {1250771},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, formal verification, model checking, program verification, termination, threads},
} 

@inproceedings{Kodumal:2007:RAS:1250734.1250772,
 author = {Kodumal, John and Aiken, Alex},
 title = {Regularly annotated set constraints},
 abstract = {A general class of program analyses area combination of context-free and regular language reachability. We define regularly annotated set constraints</i>, a constraint formalism that captures this class. Our results extend the class of reachability problems expressible naturally in a single constraint formalism, including such diverse applications as interprocedural dataflow analysis, precise type-based flow analysis, and pushdown model checking.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {331--341},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250772},
 doi = {http://doi.acm.org/10.1145/1250734.1250772},
 acmid = {1250772},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {annotated inclusion constraints, context-free language reachability, flow analysis, pushdown model checking, set constraints},
} 

@article{Kodumal:2007:RAS:1273442.1250772,
 author = {Kodumal, John and Aiken, Alex},
 title = {Regularly annotated set constraints},
 abstract = {A general class of program analyses area combination of context-free and regular language reachability. We define regularly annotated set constraints</i>, a constraint formalism that captures this class. Our results extend the class of reachability problems expressible naturally in a single constraint formalism, including such diverse applications as interprocedural dataflow analysis, precise type-based flow analysis, and pushdown model checking.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {331--341},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250772},
 doi = {http://doi.acm.org/10.1145/1273442.1250772},
 acmid = {1250772},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {annotated inclusion constraints, context-free language reachability, flow analysis, pushdown model checking, set constraints},
} 

@inproceedings{Geneves:2007:ESA:1250734.1250773,
 author = {Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil and Schmitt, Alan},
 title = {Efficient static analysis of XML paths and types},
 abstract = {We present an algorithm to solve XPath decision problems under regular tree type constraints and show its use to statically type-check XPath queries. To this end, we prove the decidability of a logic with converse for finite ordered trees whose time complexity is a simple exponential of the size of a formula. The logic corresponds to the alternation free modal \&#956;-calculus without greatest fixpoint, restricted to finite trees, and where formulas are cycle-free. Our proof method is based on two auxiliary results. First, XML regular tree types and XPath expressions have a linear translation to cycle-free formulas. Second, the least and greatest fixpoints are equivalent for finite trees, hence the logic is closed under negation. Building on these results, we describe a practical, effective system for solving the satisfiability of a formula. The system has been experimented with some decision problems such as XPath emptiness, containment, overlap, and coverage, with or without type constraints. The benefit of the approach is that our system can be effectively used in static analyzers for programming languages manipulating both XPath expressions and XML type annotations (as input and output types).},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {342--351},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250773},
 doi = {http://doi.acm.org/10.1145/1250734.1250773},
 acmid = {1250773},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {XPath, modal logic, satisfiability, type checking},
} 

@article{Geneves:2007:ESA:1273442.1250773,
 author = {Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil and Schmitt, Alan},
 title = {Efficient static analysis of XML paths and types},
 abstract = {We present an algorithm to solve XPath decision problems under regular tree type constraints and show its use to statically type-check XPath queries. To this end, we prove the decidability of a logic with converse for finite ordered trees whose time complexity is a simple exponential of the size of a formula. The logic corresponds to the alternation free modal \&#956;-calculus without greatest fixpoint, restricted to finite trees, and where formulas are cycle-free. Our proof method is based on two auxiliary results. First, XML regular tree types and XPath expressions have a linear translation to cycle-free formulas. Second, the least and greatest fixpoints are equivalent for finite trees, hence the logic is closed under negation. Building on these results, we describe a practical, effective system for solving the satisfiability of a formula. The system has been experimented with some decision problems such as XPath emptiness, containment, overlap, and coverage, with or without type constraints. The benefit of the approach is that our system can be effectively used in static analyzers for programming languages manipulating both XPath expressions and XML type annotations (as input and output types).},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {342--351},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250773},
 doi = {http://doi.acm.org/10.1145/1273442.1250773},
 acmid = {1250773},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {XPath, modal logic, satisfiability, type checking},
} 

@inproceedings{Titzer:2007:ESA:1250734.1250775,
 author = {Titzer, Ben L. and Auerbach, Joshua and Bacon, David F. and Palsberg, Jens},
 title = {The ExoVM system for automatic VM and application reduction},
 abstract = {Embedded systems pose unique challenges to Java application developers and virtual machine designers. Chief among these challenges is the memory footprint of both the virtual machine and the applications that run within it. With the rapidly increasing set of features provided by the Java language, virtual machine designers are often forced to build custom implementations that make various tradeoffs between the footprint of the virtual machine and the subset of the Java language and class libraries that are supported. In this paper, we present the ExoVM, a system in which an application is initialized in a fully featured virtual machine, and then the code, data, and virtual machine features necessary to execute it are packaged into a binary image. Key to this process is feature analysis</i>, a technique for computing the reachable code and data of a Java program and its implementation inside the VM simultaneously. The ExoVM reduces the need to develop customized embedded virtual machines by reusing a single VM infrastructure and automatically eliding the implementation of unused Java features on a per-program basis. We present a constraint-based instantiation of the analysis technique, an implementation in IBM's J9 Java VM, experiments evaluating our technique for the EEMBC benchmark suite, and some discussion of the individual costs of some of Java's features. Our evaluation shows that our system can reduce the non-heap memory allocation of the virtual machine by as much as 75\%. We discuss VM and language design decisions that our work shows are important in targeting embedded systems, supporting the long-term goal of a common VM infrastructure spanning from motes to large servers.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {352--362},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250775},
 doi = {http://doi.acm.org/10.1145/1250734.1250775},
 acmid = {1250775},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VM design, VM modularity, dead code elimination, embedded systems, feature analysis, persistence, pre-initialization, static analysis, static compilation},
} 

@article{Titzer:2007:ESA:1273442.1250775,
 author = {Titzer, Ben L. and Auerbach, Joshua and Bacon, David F. and Palsberg, Jens},
 title = {The ExoVM system for automatic VM and application reduction},
 abstract = {Embedded systems pose unique challenges to Java application developers and virtual machine designers. Chief among these challenges is the memory footprint of both the virtual machine and the applications that run within it. With the rapidly increasing set of features provided by the Java language, virtual machine designers are often forced to build custom implementations that make various tradeoffs between the footprint of the virtual machine and the subset of the Java language and class libraries that are supported. In this paper, we present the ExoVM, a system in which an application is initialized in a fully featured virtual machine, and then the code, data, and virtual machine features necessary to execute it are packaged into a binary image. Key to this process is feature analysis</i>, a technique for computing the reachable code and data of a Java program and its implementation inside the VM simultaneously. The ExoVM reduces the need to develop customized embedded virtual machines by reusing a single VM infrastructure and automatically eliding the implementation of unused Java features on a per-program basis. We present a constraint-based instantiation of the analysis technique, an implementation in IBM's J9 Java VM, experiments evaluating our technique for the EEMBC benchmark suite, and some discussion of the individual costs of some of Java's features. Our evaluation shows that our system can reduce the non-heap memory allocation of the virtual machine by as much as 75\%. We discuss VM and language design decisions that our work shows are important in targeting embedded systems, supporting the long-term goal of a common VM infrastructure spanning from motes to large servers.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {352--362},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250775},
 doi = {http://doi.acm.org/10.1145/1273442.1250775},
 acmid = {1250775},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VM design, VM modularity, dead code elimination, embedded systems, feature analysis, persistence, pre-initialization, static analysis, static compilation},
} 

@article{Cooprider:2007:OCO:1273442.1250776,
 author = {Cooprider, Nathan Dean and Regehr, John David},
 title = {Offline compression for on-chip ram},
 abstract = {We present offline RAM compression</i>, an automated source-to-source transformation that reduces a program's data size. Statically allocated scalars, pointers, structures, and arrays are encoded and packed based on the results of a whole-program analysis in the value set and pointer set domains. We target embedded software written in C that relies heavily on static memory allocation and runs on Harvard-architecture microcontrollers supporting just a few KB of on-chip RAM. On a collection of embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average of 12\%, in addition to a 10\% reduction through a dead-data elimination pass that is also driven by our whole-program analysis, for a total RAM savings of 22\%. We also developeda technique for giving developers access to a flexible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU efficiency. This technique is based on a model for estimating the cost/benefit ratio of compressing each variable and then selectively compressing only those variables that present a good value proposition in terms of the desired tradeoffs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {363--372},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250776},
 doi = {http://doi.acm.org/10.1145/1273442.1250776},
 acmid = {1250776},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TinyOS, data compression, embedded software, memory optimization, sensor networks, static analysis},
} 

@inproceedings{Cooprider:2007:OCO:1250734.1250776,
 author = {Cooprider, Nathan Dean and Regehr, John David},
 title = {Offline compression for on-chip ram},
 abstract = {We present offline RAM compression</i>, an automated source-to-source transformation that reduces a program's data size. Statically allocated scalars, pointers, structures, and arrays are encoded and packed based on the results of a whole-program analysis in the value set and pointer set domains. We target embedded software written in C that relies heavily on static memory allocation and runs on Harvard-architecture microcontrollers supporting just a few KB of on-chip RAM. On a collection of embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average of 12\%, in addition to a 10\% reduction through a dead-data elimination pass that is also driven by our whole-program analysis, for a total RAM savings of 22\%. We also developeda technique for giving developers access to a flexible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU efficiency. This technique is based on a model for estimating the cost/benefit ratio of compressing each variable and then selectively compressing only those variables that present a good value proposition in terms of the desired tradeoffs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {363--372},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250776},
 doi = {http://doi.acm.org/10.1145/1250734.1250776},
 acmid = {1250776},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TinyOS, data compression, embedded software, memory optimization, sensor networks, static analysis},
} 

@article{Schneider:2007:OOD:1273442.1250777,
 author = {Schneider, Florian T. and Payer, Mathias and Gross, Thomas R.},
 title = {Online optimizations driven by hardware performance monitoring},
 abstract = {Hardware performance monitors provide detailed direct feedback about application behavior and are an additional source of infor-mation that a compiler may use for optimization. A JIT compiler is in a good position to make use of such information because it is running on the same platform as the user applications. As hardware platforms become more and more complex, it becomes more and more difficult to model their behavior. Profile information that captures general program properties (like execution frequency of methods or basic blocks) may be useful, but does not capture sufficient information about the execution platform. Machine-level performance data obtained from a hardware performance monitor can not only direct the compiler to those parts of the program that deserve its attention but also determine if an optimization step actually improved the performance of the application. This paper presents an infrastructure based on a dynamic compiler+runtime environment for Java that incorporates machine-level information as an additional kind of feedback for the compiler and runtime environment. The low-overhead monitoring system provides fine-grained performance data that can be tracked back to individual Java bytecode instructions. As an example, the paper presents results for object co-allocation in a generational garbage collector that optimizes spatial locality of objects on-line using measurements about cache misses. In the best case, the execution time is reduced by 14\% and L1 cache misses by 28\%.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {373--382},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250777},
 doi = {http://doi.acm.org/10.1145/1273442.1250777},
 acmid = {1250777},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, dynamic optimization, hardware performance monitors, just-in-time compilation},
} 

@inproceedings{Schneider:2007:OOD:1250734.1250777,
 author = {Schneider, Florian T. and Payer, Mathias and Gross, Thomas R.},
 title = {Online optimizations driven by hardware performance monitoring},
 abstract = {Hardware performance monitors provide detailed direct feedback about application behavior and are an additional source of infor-mation that a compiler may use for optimization. A JIT compiler is in a good position to make use of such information because it is running on the same platform as the user applications. As hardware platforms become more and more complex, it becomes more and more difficult to model their behavior. Profile information that captures general program properties (like execution frequency of methods or basic blocks) may be useful, but does not capture sufficient information about the execution platform. Machine-level performance data obtained from a hardware performance monitor can not only direct the compiler to those parts of the program that deserve its attention but also determine if an optimization step actually improved the performance of the application. This paper presents an infrastructure based on a dynamic compiler+runtime environment for Java that incorporates machine-level information as an additional kind of feedback for the compiler and runtime environment. The low-overhead monitoring system provides fine-grained performance data that can be tracked back to individual Java bytecode instructions. As an example, the paper presents results for object co-allocation in a generational garbage collector that optimizes spatial locality of objects on-line using measurements about cache misses. In the best case, the execution time is reduced by 14\% and L1 cache misses by 28\%.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {373--382},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250777},
 doi = {http://doi.acm.org/10.1145/1250734.1250777},
 acmid = {1250777},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, dynamic optimization, hardware performance monitors, just-in-time compilation},
} 

@article{Li:2007:UUC:1273442.1250778,
 author = {Li, Weijia and Zhang, Youtao and Yang, Jun and Zheng, Jiang},
 title = {UCC: update-conscious compilation for energy efficiency in wireless sensor networks},
 abstract = {Wireless sensor networks (WSN), composed of a large number of low-cost, battery-powered sensors, have recently emerged as promising computing platforms for many non-traditional applications. The preloaded code on remote sensors often needs to be updated after deployment in order for the WSN to adapt to the changing demands from the users. Post-deployment code dissemination is challenging as the data are transmitted via battery-powered wireless communication. Recent studies show that the energy for sending a single bit is about the same as executing 1000 instructions in aWSN. Therefore it is important to achieve energy efficiency in code dissemination. In this paper, we propose novel update-conscious compilation(UCC)</i> techniques for energy-efficient code dissemination in WSNs. An update-conscious compiler, when compiling the modified code, includes the compilation decisions that were made when generating the old binary. The compiler employs a detailed energy model and strives to match the old decisions for a more energy-efficient result. In most cases, matching the previous decisions improves the binary code similarity, reduces the amount of data to be transmitted to remote sensors, and thus, consumes less energy. In this paper, we develop update-conscious register allocation and data layout algorithms. Our experimental results show that they can achieve great improvements over the traditional, update-oblivious approaches.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {383--393},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250778},
 doi = {http://doi.acm.org/10.1145/1273442.1250778},
 acmid = {1250778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code dissemination, register allocation, sensor networks},
} 

@inproceedings{Li:2007:UUC:1250734.1250778,
 author = {Li, Weijia and Zhang, Youtao and Yang, Jun and Zheng, Jiang},
 title = {UCC: update-conscious compilation for energy efficiency in wireless sensor networks},
 abstract = {Wireless sensor networks (WSN), composed of a large number of low-cost, battery-powered sensors, have recently emerged as promising computing platforms for many non-traditional applications. The preloaded code on remote sensors often needs to be updated after deployment in order for the WSN to adapt to the changing demands from the users. Post-deployment code dissemination is challenging as the data are transmitted via battery-powered wireless communication. Recent studies show that the energy for sending a single bit is about the same as executing 1000 instructions in aWSN. Therefore it is important to achieve energy efficiency in code dissemination. In this paper, we propose novel update-conscious compilation(UCC)</i> techniques for energy-efficient code dissemination in WSNs. An update-conscious compiler, when compiling the modified code, includes the compilation decisions that were made when generating the old binary. The compiler employs a detailed energy model and strives to match the old decisions for a more energy-efficient result. In most cases, matching the previous decisions improves the binary code similarity, reduces the amount of data to be transmitted to remote sensors, and thus, consumes less energy. In this paper, we develop update-conscious register allocation and data layout algorithms. Our experimental results show that they can achieve great improvements over the traditional, update-oblivious approaches.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {383--393},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250778},
 doi = {http://doi.acm.org/10.1145/1250734.1250778},
 acmid = {1250778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code dissemination, register allocation, sensor networks},
} 

@inproceedings{Li:2007:PER:1250734.1250779,
 author = {Li, Feihui and Chen, Guangyu and Kandemir, Mahmut and Kolcu, Ibrahim},
 title = {Profile-driven energy reduction in network-on-chips},
 abstract = {Reducing energy consumption of a Network-on-Chip (NoC) is a critical design goal, especially for power-constrained embedded systems.In response, prior research has proposed several circuit/architectural level mechanisms to reduce NoC power consumption. This paper considers the problem from a different perspective and demonstrates that compiler analysis can be very helpful for enhancing the effectiveness of a hardware-based link power management mechanism by increasing the duration of communication links' idle periods. The proposed profile-based approach achieves its goal by maximizing the communication link reuse through compiler-directed, static message re-routing. That is, it clusters the required data communications into a small set of communication links at any given time, which increases the idle periods for the remaining communication links in the network. This helps hardware shut down more communication links and their corresponding buffers to reduce leakage power. The current experimental evaluation, with twelve data-intensive embedded applications, shows that the proposed profile-driven compiler approach reduces leakage energy by more than 35\% (on average) as compared to a pure hardware-based link power management scheme.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {394--404},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250779},
 doi = {http://doi.acm.org/10.1145/1250734.1250779},
 acmid = {1250779},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Network-on-Chip, compiler, power, routing},
} 

@article{Li:2007:PER:1273442.1250779,
 author = {Li, Feihui and Chen, Guangyu and Kandemir, Mahmut and Kolcu, Ibrahim},
 title = {Profile-driven energy reduction in network-on-chips},
 abstract = {Reducing energy consumption of a Network-on-Chip (NoC) is a critical design goal, especially for power-constrained embedded systems.In response, prior research has proposed several circuit/architectural level mechanisms to reduce NoC power consumption. This paper considers the problem from a different perspective and demonstrates that compiler analysis can be very helpful for enhancing the effectiveness of a hardware-based link power management mechanism by increasing the duration of communication links' idle periods. The proposed profile-based approach achieves its goal by maximizing the communication link reuse through compiler-directed, static message re-routing. That is, it clusters the required data communications into a small set of communication links at any given time, which increases the idle periods for the remaining communication links in the network. This helps hardware shut down more communication links and their corresponding buffers to reduce leakage power. The current experimental evaluation, with twelve data-intensive embedded applications, shows that the proposed profile-driven compiler approach reduces leakage energy by more than 35\% (on average) as compared to a pure hardware-based link power management scheme.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {394--404},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250779},
 doi = {http://doi.acm.org/10.1145/1273442.1250779},
 acmid = {1250779},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Network-on-Chip, compiler, power, routing},
} 

@article{Renganarayanan:2007:PTL:1273442.1250780,
 author = {Renganarayanan, Lakshminarayanan and Kim, DaeGon and Rajopadhye, Sanjay and Strout, Michelle Mills},
 title = {Parameterized tiled loops for free},
 abstract = {Parameterized tiled loops-where the tile sizes are not fixed at compile time, but remain symbolic parameters until later--are quite useful for iterative compilers and "auto-tuners" that produce highly optimized libraries and codes. Tile size parameterization could also enable optimizations such as register tiling to become dynamic optimizations. Although it is easy to generate such loops for (hyper) rectangular iteration spaces tiled with (hyper) rectangular tiles, many important computations do not fall into this restricted domain. Parameterized tile code generation for the general case of convex iteration spaces being tiled by (hyper) rectangular tiles has in the past been solved with bounding box approaches or symbolic Fourier Motzkin approaches. However, both approaches have less than ideal code generation efficiency and resulting code quality. We present the theoretical foundations, implementation, and experimental validation of a simple, unified technique for generating parameterized tiled code. Our code generation efficiency is comparable to all existing code generation techniques including those for fixed tile sizes, and the resulting code is as efficient as, if not more than, all previous techniques. Thus the technique provides parameterized tiled loops for free! Our "one-size-fits-all" solution, which is available as open source software can be adapted for use in production compilers.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {405--414},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250780},
 doi = {http://doi.acm.org/10.1145/1273442.1250780},
 acmid = {1250780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Fourier-Motzkin elimination, bounding box, code generation, parameterized tiling},
} 

@inproceedings{Renganarayanan:2007:PTL:1250734.1250780,
 author = {Renganarayanan, Lakshminarayanan and Kim, DaeGon and Rajopadhye, Sanjay and Strout, Michelle Mills},
 title = {Parameterized tiled loops for free},
 abstract = {Parameterized tiled loops-where the tile sizes are not fixed at compile time, but remain symbolic parameters until later--are quite useful for iterative compilers and "auto-tuners" that produce highly optimized libraries and codes. Tile size parameterization could also enable optimizations such as register tiling to become dynamic optimizations. Although it is easy to generate such loops for (hyper) rectangular iteration spaces tiled with (hyper) rectangular tiles, many important computations do not fall into this restricted domain. Parameterized tile code generation for the general case of convex iteration spaces being tiled by (hyper) rectangular tiles has in the past been solved with bounding box approaches or symbolic Fourier Motzkin approaches. However, both approaches have less than ideal code generation efficiency and resulting code quality. We present the theoretical foundations, implementation, and experimental validation of a simple, unified technique for generating parameterized tiled code. Our code generation efficiency is comparable to all existing code generation techniques including those for fixed tile sizes, and the resulting code is as efficient as, if not more than, all previous techniques. Thus the technique provides parameterized tiled loops for free! Our "one-size-fits-all" solution, which is available as open source software can be adapted for use in production compilers.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {405--414},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250780},
 doi = {http://doi.acm.org/10.1145/1250734.1250780},
 acmid = {1250780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Fourier-Motzkin elimination, bounding box, code generation, parameterized tiling},
} 

@inproceedings{Zhang:2007:TLE:1250734.1250782,
 author = {Zhang, Xiangyu and Tallam, Sriraman and Gupta, Neelam and Gupta, Rajiv},
 title = {Towards locating execution omission errors},
 abstract = {Execution omission errors are known to be difficult to locate using dynamic analysis. These errors lead to a failure at runtime because of the omission of execution of some statements that would have been executed if the program had no errors. Since dynamic analysis is typically designed to focus on dynamic information arising from executed statements, and statements whose execution is omitted do not produce dynamic information, detection of execution omission errors becomes a challenging task. For example, while dynamic slices are very effective in capturing faulty code for other types of errors, they fail to capture faulty code in presence of execution omission errors. To address this issue relevant slices have been defined to consider certain static dependences (called potential dependences) in addition to dynamic dependences. However, due to the conservative nature of static analysis, overly large slices are produced. In this paper, we propose a fully dynamic</i> solution to locating execution omission errors using dynamic slices. We introduce the notion of implicit dependences</i> which are dependences that are normally invisible to dynamic slicing due to the omission of execution of some statements. We design a dynamic method that forces the execution of the omitted code by switching outcomes of relevant predicates such that those implicit dependences are exposed and become available for dynamic slicing. Dynamic slices can be computed and effectively pruned to produce fault candidate sets containing the execution omission errors. We solve two main problems: verifying the existence of a single implicit dependence through predicate switching, and recovering the implicit dependences in a demand driven manner such that a small number of verifications are required before the root cause is captured. Our experiments show that the proposed technique is highly effective in capturing execution omission errors.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {415--424},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250782},
 doi = {http://doi.acm.org/10.1145/1250734.1250782},
 acmid = {1250782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, execution omission, implicit dependence, potential dependence, predicate switching, relevant slicing},
} 

@article{Zhang:2007:TLE:1273442.1250782,
 author = {Zhang, Xiangyu and Tallam, Sriraman and Gupta, Neelam and Gupta, Rajiv},
 title = {Towards locating execution omission errors},
 abstract = {Execution omission errors are known to be difficult to locate using dynamic analysis. These errors lead to a failure at runtime because of the omission of execution of some statements that would have been executed if the program had no errors. Since dynamic analysis is typically designed to focus on dynamic information arising from executed statements, and statements whose execution is omitted do not produce dynamic information, detection of execution omission errors becomes a challenging task. For example, while dynamic slices are very effective in capturing faulty code for other types of errors, they fail to capture faulty code in presence of execution omission errors. To address this issue relevant slices have been defined to consider certain static dependences (called potential dependences) in addition to dynamic dependences. However, due to the conservative nature of static analysis, overly large slices are produced. In this paper, we propose a fully dynamic</i> solution to locating execution omission errors using dynamic slices. We introduce the notion of implicit dependences</i> which are dependences that are normally invisible to dynamic slicing due to the omission of execution of some statements. We design a dynamic method that forces the execution of the omitted code by switching outcomes of relevant predicates such that those implicit dependences are exposed and become available for dynamic slicing. Dynamic slices can be computed and effectively pruned to produce fault candidate sets containing the execution omission errors. We solve two main problems: verifying the existence of a single implicit dependence through predicate switching, and recovering the implicit dependences in a demand driven manner such that a small number of verifications are required before the root cause is captured. Our experiments show that the proposed technique is highly effective in capturing execution omission errors.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {415--424},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250782},
 doi = {http://doi.acm.org/10.1145/1273442.1250782},
 acmid = {1250782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, execution omission, implicit dependence, potential dependence, predicate switching, relevant slicing},
} 

@article{Lerner:2007:STM:1273442.1250783,
 author = {Lerner, Benjamin S. and Flower, Matthew and Grossman, Dan and Chambers, Craig},
 title = {Searching for type-error messages},
 abstract = {Advanced type systems often need some form of type inference to reduce the burden of explicit typing, but type inference often leads to poor error messages for ill-typed programs. This work pursues a new approach to constructing compilers and presenting type-error messages in which the type-checker itself does not produce the messages. Instead, it is an oracle for a search procedure that finds similar programs that do type-check. Our two-fold goal is to improve error messages while simplifying compiler construction. Our primary implementation and evaluation is for Caml, a language with full type inference. We also present a prototype for C++ template functions, where type instantiation is implicit. A key extension is making our approach robust even when the program has multiple independent type errors.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {425--434},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250783},
 doi = {http://doi.acm.org/10.1145/1273442.1250783},
 acmid = {1250783},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error messages, objective Caml, seminal, type-checking, type-inference},
} 

@inproceedings{Lerner:2007:STM:1250734.1250783,
 author = {Lerner, Benjamin S. and Flower, Matthew and Grossman, Dan and Chambers, Craig},
 title = {Searching for type-error messages},
 abstract = {Advanced type systems often need some form of type inference to reduce the burden of explicit typing, but type inference often leads to poor error messages for ill-typed programs. This work pursues a new approach to constructing compilers and presenting type-error messages in which the type-checker itself does not produce the messages. Instead, it is an oracle for a search procedure that finds similar programs that do type-check. Our two-fold goal is to improve error messages while simplifying compiler construction. Our primary implementation and evaluation is for Caml, a language with full type inference. We also present a prototype for C++ template functions, where type instantiation is implicit. A key extension is making our approach robust even when the program has multiple independent type errors.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {425--434},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250783},
 doi = {http://doi.acm.org/10.1145/1250734.1250783},
 acmid = {1250783},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error messages, objective Caml, seminal, type-checking, type-inference},
} 

@article{Dillig:2007:SED:1273442.1250784,
 author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex},
 title = {Static error detection using semantic inconsistency inference},
 abstract = {Inconsistency checking is a method for detecting software errors that relies only on examining multiple uses of a value. We propose that inconsistency inference is best understood as a variant of the older and better understood problem of type inference. Using this insight, we describe a precise and formal framework for discovering inconsistency errors. Unlike previous approaches to the problem, our technique for finding inconsistency errors is purely semantic and can deal with complex aliasing and path-sensitive conditions. We have built a nullde reference analysis of C programs based on semantic inconsistency inference and have used it to find hundreds of previously unknown null dereference errors in widely used C programs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {435--445},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250784},
 doi = {http://doi.acm.org/10.1145/1273442.1250784},
 acmid = {1250784},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, inconsistency, satisfiability, static analysis},
} 

@inproceedings{Dillig:2007:SED:1250734.1250784,
 author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex},
 title = {Static error detection using semantic inconsistency inference},
 abstract = {Inconsistency checking is a method for detecting software errors that relies only on examining multiple uses of a value. We propose that inconsistency inference is best understood as a variant of the older and better understood problem of type inference. Using this insight, we describe a precise and formal framework for discovering inconsistency errors. Unlike previous approaches to the problem, our technique for finding inconsistency errors is purely semantic and can deal with complex aliasing and path-sensitive conditions. We have built a nullde reference analysis of C programs based on semantic inconsistency inference and have used it to find hundreds of previously unknown null dereference errors in widely used C programs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {435--445},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1250734.1250784},
 doi = {http://doi.acm.org/10.1145/1250734.1250784},
 acmid = {1250784},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, inconsistency, satisfiability, static analysis},
} 

@inproceedings{Musuvathi:2007:ICB:1250734.1250785,
 author = {Musuvathi, Madanlal and Qadeer, Shaz},
 title = {Iterative context bounding for systematic testing of multithreaded programs},
 abstract = {Multithreaded programs are difficult to get right because of unexpected interaction between concurrently executing threads. Traditional testing methods are inadequate for catching subtle concurrency errors which manifest themselves late in the development cycle and post-deployment. Model checking or systematic exploration of program behavior is a promising alternative to traditional testing methods. However, it is difficult to perform systematic search on large programs as the number of possible program behaviors grows exponentially with the program size. Confronted with this state-explosion problem, traditional model checkers perform iterative depth-bounded search. Although effective for message-passing software, iterative depth-bounding is inadequate for multithreaded software. This paper proposes iterative context-bounding, a new search algorithm that systematically explores the executions of a multithreaded program in an order that prioritizes executions with fewer context switches</i>. We distinguish between preempting and nonpreempting context switches, and show that bounding the number of preempting context switches to a small number significantly alleviates the state explosion, without limiting the depth of explored executions. We show both theoretically and empirically that context-bounded search is an effective method for exploring the behaviors of multithreaded programs. We have implemented our algorithmin two model checkers and applied it to a number of real-world multithreaded programs. Our implementation uncovered 9 previously unknown bugs in our benchmarks, each of which was exposed by an execution with at most 2 preempting context switches. Our initial experience with the technique is encouraging and demonstrates that iterative context-bounding is a significant improvement over existing techniques for testing multithreaded programs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {446--455},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1250734.1250785},
 doi = {http://doi.acm.org/10.1145/1250734.1250785},
 acmid = {1250785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, context-bounding, model checking, multithreading, partial-order reduction, shared-memory programs, software testing},
} 

@article{Musuvathi:2007:ICB:1273442.1250785,
 author = {Musuvathi, Madanlal and Qadeer, Shaz},
 title = {Iterative context bounding for systematic testing of multithreaded programs},
 abstract = {Multithreaded programs are difficult to get right because of unexpected interaction between concurrently executing threads. Traditional testing methods are inadequate for catching subtle concurrency errors which manifest themselves late in the development cycle and post-deployment. Model checking or systematic exploration of program behavior is a promising alternative to traditional testing methods. However, it is difficult to perform systematic search on large programs as the number of possible program behaviors grows exponentially with the program size. Confronted with this state-explosion problem, traditional model checkers perform iterative depth-bounded search. Although effective for message-passing software, iterative depth-bounding is inadequate for multithreaded software. This paper proposes iterative context-bounding, a new search algorithm that systematically explores the executions of a multithreaded program in an order that prioritizes executions with fewer context switches</i>. We distinguish between preempting and nonpreempting context switches, and show that bounding the number of preempting context switches to a small number significantly alleviates the state explosion, without limiting the depth of explored executions. We show both theoretically and empirically that context-bounded search is an effective method for exploring the behaviors of multithreaded programs. We have implemented our algorithmin two model checkers and applied it to a number of real-world multithreaded programs. Our implementation uncovered 9 previously unknown bugs in our benchmarks, each of which was exposed by an execution with at most 2 preempting context switches. Our initial experience with the technique is encouraging and demonstrates that iterative context-bounding is a significant improvement over existing techniques for testing multithreaded programs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {446--455},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273442.1250785},
 doi = {http://doi.acm.org/10.1145/1273442.1250785},
 acmid = {1250785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, context-bounding, model checking, multithreading, partial-order reduction, shared-memory programs, software testing},
} 

@inproceedings{Vechev:2007:CSS:1250734.1250787,
 author = {Vechev, Martin T. and Yahav, Eran and Bacon, David F. and Rinetzky, Noam},
 title = {CGCExplorer: a semi-automated search procedure for provably correct concurrent collectors},
 abstract = {Concurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent mark-and-sweep collectors. In our framework, the designer specifies a set of "building blocks" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space. We capture the intuition behind some common mark-and-sweep algorithms using a set of building blocks. We utilize our framework to automatically explore a space of more than 1,600,000 algorithms built from these blocks, and derive over 100 correct fine-grained algorithms with various space, synchronization, and precision tradeoffs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {456--467},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250787},
 doi = {http://doi.acm.org/10.1145/1250734.1250787},
 acmid = {1250787},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent algorithms, concurrent garbage collection, synthesis, verification},
} 

@article{Vechev:2007:CSS:1273442.1250787,
 author = {Vechev, Martin T. and Yahav, Eran and Bacon, David F. and Rinetzky, Noam},
 title = {CGCExplorer: a semi-automated search procedure for provably correct concurrent collectors},
 abstract = {Concurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent mark-and-sweep collectors. In our framework, the designer specifies a set of "building blocks" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space. We capture the intuition behind some common mark-and-sweep algorithms using a set of building blocks. We utilize our framework to automatically explore a space of more than 1,600,000 algorithms built from these blocks, and derive over 100 correct fine-grained algorithms with various space, synchronization, and precision tradeoffs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {456--467},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250787},
 doi = {http://doi.acm.org/10.1145/1273442.1250787},
 acmid = {1250787},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent algorithms, concurrent garbage collection, synthesis, verification},
} 

@article{McCreight:2007:GFC:1273442.1250788,
 author = {McCreight, Andrew and Shao, Zhong and Lin, Chunxiao and Li, Long},
 title = {A general framework for certifying garbage collectors and their mutators},
 abstract = {Garbage-collected languages such as Java and C# are becoming more and more widely used in both high-end software and real-time embedded applications. The correctness of the GC implementation is essential to the reliability and security of a large portion of the world's mission-critical software. Unfortunately, garbage collectors--especially incremental and concurrent ones--are extremely hard to implement correctly. In this paper, we present a new uniform approach to verifying the safety of both a mutator and its garbage collector in Hoare-style logic. We define a formal garbage collector interface general enough to reason about a variety of algorithms while allowing the mutator to ignore implementation-specific details of the collector. Our approach supports collectors that require read and write barriers. We have used our approach to mechanically verify assembly implementations of mark-sweep, copying and incremental copying GCs in Coq, as well as sample mutator programs that can be linked with any of the GCs to produce a fully-verified garbage-collected program. Our work provides a foundation for reasoning about complex mutator-collector interaction and makes an important advance toward building fully certified production-quality GCs.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {468--479},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250788},
 doi = {http://doi.acm.org/10.1145/1273442.1250788},
 acmid = {1250788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract data type, assembly code verification, garbage collection, proof-carrying code, separation logic},
} 

@inproceedings{McCreight:2007:GFC:1250734.1250788,
 author = {McCreight, Andrew and Shao, Zhong and Lin, Chunxiao and Li, Long},
 title = {A general framework for certifying garbage collectors and their mutators},
 abstract = {Garbage-collected languages such as Java and C# are becoming more and more widely used in both high-end software and real-time embedded applications. The correctness of the GC implementation is essential to the reliability and security of a large portion of the world's mission-critical software. Unfortunately, garbage collectors--especially incremental and concurrent ones--are extremely hard to implement correctly. In this paper, we present a new uniform approach to verifying the safety of both a mutator and its garbage collector in Hoare-style logic. We define a formal garbage collector interface general enough to reason about a variety of algorithms while allowing the mutator to ignore implementation-specific details of the collector. Our approach supports collectors that require read and write barriers. We have used our approach to mechanically verify assembly implementations of mark-sweep, copying and incremental copying GCs in Coq, as well as sample mutator programs that can be linked with any of the GCs to produce a fully-verified garbage-collected program. Our work provides a foundation for reasoning about complex mutator-collector interaction and makes an important advance toward building fully certified production-quality GCs.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {468--479},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250788},
 doi = {http://doi.acm.org/10.1145/1250734.1250788},
 acmid = {1250788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract data type, assembly code verification, garbage collection, proof-carrying code, separation logic},
} 

@inproceedings{Cherem:2007:PML:1250734.1250789,
 author = {Cherem, Sigmund and Princehouse, Lonnie and Rugina, Radu},
 title = {Practical memory leak detection using guarded value-flow analysis},
 abstract = {This paper presents a practical inter-procedural analysis algorithm for detecting memory leaks in C programs. Our algorithm tracks the flow of values from allocation points to deallocation points using a sparse representation of the program consisting of a value flow graph that captures def-use relations and value flows via program assignments. Edges in the graph are annotated with guards that describe branch conditions in the program. The memory leak analysis is reduced to a reachability problem over the guarded value flowgraph. Our implemented tool has been effective at detecting more than 60 memory leaks in the SPEC2000 benchmarks and in two open-source applications, bash and sshd, while keeping the false positive rate below 20\%. The sparse program representation makes the tool efficient in practice, and allows it to report concise error messages.},
 booktitle = {Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '07},
 year = {2007},
 isbn = {978-1-59593-633-2},
 location = {San Diego, California, USA},
 pages = {480--491},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1250734.1250789},
 doi = {http://doi.acm.org/10.1145/1250734.1250789},
 acmid = {1250789},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory leaks, memory management, static error detection, value-flow analysis},
} 

@article{Cherem:2007:PML:1273442.1250789,
 author = {Cherem, Sigmund and Princehouse, Lonnie and Rugina, Radu},
 title = {Practical memory leak detection using guarded value-flow analysis},
 abstract = {This paper presents a practical inter-procedural analysis algorithm for detecting memory leaks in C programs. Our algorithm tracks the flow of values from allocation points to deallocation points using a sparse representation of the program consisting of a value flow graph that captures def-use relations and value flows via program assignments. Edges in the graph are annotated with guards that describe branch conditions in the program. The memory leak analysis is reduced to a reachability problem over the guarded value flowgraph. Our implemented tool has been effective at detecting more than 60 memory leaks in the SPEC2000 benchmarks and in two open-source applications, bash and sshd, while keeping the false positive rate below 20\%. The sparse program representation makes the tool efficient in practice, and allows it to report concise error messages.},
 journal = {SIGPLAN Not.},
 volume = {42},
 issue = {6},
 month = {June},
 year = {2007},
 issn = {0362-1340},
 pages = {480--491},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273442.1250789},
 doi = {http://doi.acm.org/10.1145/1273442.1250789},
 acmid = {1250789},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory leaks, memory management, static error detection, value-flow analysis},
} 

@article{Henzinger:2004:RCC:996893.996844,
 author = {Henzinger, Thomas A. and Jhala, Ranjit and Majumdar, Rupak},
 title = {Race checking by context inference},
 abstract = {Software model checking has been successful for sequential</i> programs, where predicate abstraction offers suitable models, and counterexample-guided abstraction refinement permits the automatic inference of models. When checking concurrent</i> programs, we need to abstract threads as well as the contexts in which they execute. Stateless context models, such as predicates on global variables, prove insufficient for showing the absence of race conditions in many examples. We therefore use richer context models, which combine (1) predicates for abstracting data state, (2) control flow quotients for abstracting control state, and (3) counters for abstracting an unbounded number of threads. We infer suitable context models automatically by a combination of counterexample-guided abstraction refinement, bisimulation minimization, circular assume-guarantee reasoning, and parametric reasoning about an unbounded number of threads. This algorithm, called CIRC, has been implemented in BLAST and succeeds in checking many examples of NESC code for data races. In particular, BLAST proves the absence of races in several cases where previous race checkers give false positives.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996893.996844},
 doi = {http://doi.acm.org/10.1145/996893.996844},
 acmid = {996844},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {race conditions, software model checking},
} 

@inproceedings{Henzinger:2004:RCC:996841.996844,
 author = {Henzinger, Thomas A. and Jhala, Ranjit and Majumdar, Rupak},
 title = {Race checking by context inference},
 abstract = {Software model checking has been successful for sequential</i> programs, where predicate abstraction offers suitable models, and counterexample-guided abstraction refinement permits the automatic inference of models. When checking concurrent</i> programs, we need to abstract threads as well as the contexts in which they execute. Stateless context models, such as predicates on global variables, prove insufficient for showing the absence of race conditions in many examples. We therefore use richer context models, which combine (1) predicates for abstracting data state, (2) control flow quotients for abstracting control state, and (3) counters for abstracting an unbounded number of threads. We infer suitable context models automatically by a combination of counterexample-guided abstraction refinement, bisimulation minimization, circular assume-guarantee reasoning, and parametric reasoning about an unbounded number of threads. This algorithm, called CIRC, has been implemented in BLAST and succeeds in checking many examples of NESC code for data races. In particular, BLAST proves the absence of races in several cases where previous race checkers give false positives.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996841.996844},
 doi = {http://doi.acm.org/10.1145/996841.996844},
 acmid = {996844},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {race conditions, software model checking},
} 

@article{Qadeer:2004:KKS:996893.996845,
 author = {Qadeer, Shaz and Wu, Dinghao},
 title = {KISS: keep it simple and sequential},
 abstract = {The design of concurrent programs is error-prone due to the interaction between concurrently executing threads. Traditional automated techniques for finding errors in concurrent programs, such as model checking, explore all possible thread interleavings. Since the number of thread interleavings increases exponentially with the number of threads, such analyses have high computational complexity. In this paper, we present a novel analysis technique for concurrent programs that avoids this exponential complexity. Our analysis transforms a concurrent program into a sequential program that simulates the execution of a large subset of the behaviors of the concurrent program. The sequential program is then analyzed by a tool that only needs to understand the semantics of sequential execution. Our technique never</i> reports false errors but may miss errors. We have implemented the technique in KISS, an automated checker for multithreaded C programs, and obtained promising initial results by using KISS to detect race conditions in Windows device drivers.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/996893.996845},
 doi = {http://doi.acm.org/10.1145/996893.996845},
 acmid = {996845},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertion checking, concurrent software, model checking, program analysis, race detection},
} 

@inproceedings{Qadeer:2004:KKS:996841.996845,
 author = {Qadeer, Shaz and Wu, Dinghao},
 title = {KISS: keep it simple and sequential},
 abstract = {The design of concurrent programs is error-prone due to the interaction between concurrently executing threads. Traditional automated techniques for finding errors in concurrent programs, such as model checking, explore all possible thread interleavings. Since the number of thread interleavings increases exponentially with the number of threads, such analyses have high computational complexity. In this paper, we present a novel analysis technique for concurrent programs that avoids this exponential complexity. Our analysis transforms a concurrent program into a sequential program that simulates the execution of a large subset of the behaviors of the concurrent program. The sequential program is then analyzed by a tool that only needs to understand the semantics of sequential execution. Our technique never</i> reports false errors but may miss errors. We have implemented the technique in KISS, an automated checker for multithreaded C programs, and obtained promising initial results by using KISS to detect race conditions in Windows device drivers.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/996841.996845},
 doi = {http://doi.acm.org/10.1145/996841.996845},
 acmid = {996845},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertion checking, concurrent software, model checking, program analysis, race detection},
} 

@article{Yahav:2004:VSP:996893.996846,
 author = {Yahav, Eran and Ramalingam, G.},
 title = {Verifying safety properties using separation and heterogeneous abstractions},
 abstract = {In this paper, we show how separation</i> (decomposing a verification problem into a collection of verification subproblems) can be used to improve the efficiency and precision of verification of safety properties. We present a simple language for specifying separation strategies</i> for decomposing a single verification problem into a set of subproblems. (The strategy specification is distinct from the safety property specification and is specified separately.) We present a general framework of heterogeneous abstraction</i> that allows different parts of the heap to be abstracted using different degrees of precision at different points during the analysis. We show how the goals of separation (i.e., more efficient verification) can be realized by first using a separation strategy to transform (instrument) a verification problem instance (consisting of a safety property specification and an input program), and by then utilizing heterogeneous abstraction during the verification of the transformed verification problem.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {25--34},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/996893.996846},
 doi = {http://doi.acm.org/10.1145/996893.996846},
 acmid = {996846},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, program analysis, safety properties, typestate verification, verification},
} 

@inproceedings{Yahav:2004:VSP:996841.996846,
 author = {Yahav, Eran and Ramalingam, G.},
 title = {Verifying safety properties using separation and heterogeneous abstractions},
 abstract = {In this paper, we show how separation</i> (decomposing a verification problem into a collection of verification subproblems) can be used to improve the efficiency and precision of verification of safety properties. We present a simple language for specifying separation strategies</i> for decomposing a single verification problem into a set of subproblems. (The strategy specification is distinct from the safety property specification and is specified separately.) We present a general framework of heterogeneous abstraction</i> that allows different parts of the heap to be abstracted using different degrees of precision at different points during the analysis. We show how the goals of separation (i.e., more efficient verification) can be realized by first using a separation strategy to transform (instrument) a verification problem instance (consisting of a safety property specification and an input program), and by then utilizing heterogeneous abstraction during the verification of the transformed verification problem.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {25--34},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/996841.996846},
 doi = {http://doi.acm.org/10.1145/996841.996846},
 acmid = {996846},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, program analysis, safety properties, typestate verification, verification},
} 

@article{Michael:2004:SLD:996893.996848,
 author = {Michael, Maged M.},
 title = {Scalable lock-free dynamic memory allocation},
 abstract = {Dynamic memory allocators (malloc/free) rely on mutual exclusion locks for protecting the consistency of their shared data structures under multithreading. The use of locking has many disadvantages with respect to performance, availability, robustness, and programming flexibility. A lock-free memory allocator guarantees progress regardless of whether some threads are delayed or even killed and regardless of scheduling policies. This paper presents a completely lock-free memory allocator. It uses only widely-available operating system support and hardware atomic instructions. It offers guaranteed availability even under arbitrary thread termination and crash-failure, and it is immune to deadlock regardless of scheduling policies, and hence it can be used even in interrupt handlers and real-time applications without requiring special scheduler support. Also, by leveraging some high-level structures from Hoard, our allocator is highly scalable, limits space blowup to a constant factor, and is capable of avoiding false sharing. In addition, our allocator allows finer concurrency and much lower latency than Hoard. We use PowerPC shared memory multiprocessor systems to compare the performance of our allocator with the default AIX 5.1 libc malloc, and two widely-used multithread allocators, Hoard and Ptmalloc. Our allocator outperforms the other allocators in virtually all cases and often by substantial margins, under various levels of parallelism and allocation patterns. Furthermore, our allocator also offers the lowest contention-free latency among the allocators by significant margins.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996848},
 doi = {http://doi.acm.org/10.1145/996893.996848},
 acmid = {996848},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {async-signal-safe, availability, lock-free, malloc},
} 

@inproceedings{Michael:2004:SLD:996841.996848,
 author = {Michael, Maged M.},
 title = {Scalable lock-free dynamic memory allocation},
 abstract = {Dynamic memory allocators (malloc/free) rely on mutual exclusion locks for protecting the consistency of their shared data structures under multithreading. The use of locking has many disadvantages with respect to performance, availability, robustness, and programming flexibility. A lock-free memory allocator guarantees progress regardless of whether some threads are delayed or even killed and regardless of scheduling policies. This paper presents a completely lock-free memory allocator. It uses only widely-available operating system support and hardware atomic instructions. It offers guaranteed availability even under arbitrary thread termination and crash-failure, and it is immune to deadlock regardless of scheduling policies, and hence it can be used even in interrupt handlers and real-time applications without requiring special scheduler support. Also, by leveraging some high-level structures from Hoard, our allocator is highly scalable, limits space blowup to a constant factor, and is capable of avoiding false sharing. In addition, our allocator allows finer concurrency and much lower latency than Hoard. We use PowerPC shared memory multiprocessor systems to compare the performance of our allocator with the default AIX 5.1 libc malloc, and two widely-used multithread allocators, Hoard and Ptmalloc. Our allocator outperforms the other allocators in virtually all cases and often by substantial margins, under various levels of parallelism and allocation patterns. Furthermore, our allocator also offers the lowest contention-free latency among the allocators by significant margins.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996848},
 doi = {http://doi.acm.org/10.1145/996841.996848},
 acmid = {996848},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {async-signal-safe, availability, lock-free, malloc},
} 

@inproceedings{Flatt:2004:KSA:996841.996849,
 author = {Flatt, Matthew and Findler, Robert Bruce},
 title = {Kill-safe synchronization abstractions},
 abstract = {When an individual task can be forcefully terminated at any time, cooperating tasks must communicate carefully. For example, if two tasks share an object, and if one task is terminated while it manipulates the object, the object may remain in an inconsistent or frozen state that incapacitates the other task. To support communication among terminable tasks, language run-time systems (and operating systems) provide kill-safe abstractions for inter-task communication. No kill-safe guarantee is available, however, for abstractions that are implemented outside the run-time system.In this paper, we show how a run-time system can support new kill-safe abstractions without requiring modification to the run-time system, and without requiring the run-time system to trust any new code. Our design frees the run-time implementor to provide only a modest set of synchronization primitives in the trusted computing base, while still allowing tasks to communicate using sophisticated abstractions.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996849},
 doi = {http://doi.acm.org/10.1145/996841.996849},
 acmid = {996849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flatt:2004:KSA:996893.996849,
 author = {Flatt, Matthew and Findler, Robert Bruce},
 title = {Kill-safe synchronization abstractions},
 abstract = {When an individual task can be forcefully terminated at any time, cooperating tasks must communicate carefully. For example, if two tasks share an object, and if one task is terminated while it manipulates the object, the object may remain in an inconsistent or frozen state that incapacitates the other task. To support communication among terminable tasks, language run-time systems (and operating systems) provide kill-safe abstractions for inter-task communication. No kill-safe guarantee is available, however, for abstractions that are implemented outside the run-time system.In this paper, we show how a run-time system can support new kill-safe abstractions without requiring modification to the run-time system, and without requiring the run-time system to trust any new code. Our design frees the run-time implementor to provide only a modest set of synchronization primitives in the trusted computing base, while still allowing tasks to communicate using sophisticated abstractions.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996849},
 doi = {http://doi.acm.org/10.1145/996893.996849},
 acmid = {996849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:2004:MPD:996893.996851,
 author = {Johnson, Troy A. and Eigenmann, Rudolf and Vijaykumar, T. N.},
 title = {Min-cut program decomposition for thread-level speculation},
 abstract = {With billion-transistor chips on the horizon, single-chip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a min-cut-based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graph-theoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74\% for floating-point programs and 23\% for integer programs on a four-processor chip, improving on the 52\% and 13\% achieved by the previous heuristics.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996851},
 doi = {http://doi.acm.org/10.1145/996893.996851},
 acmid = {996851},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, min-cut, partitioning, program decomposition, thread-level speculation},
} 

@inproceedings{Johnson:2004:MPD:996841.996851,
 author = {Johnson, Troy A. and Eigenmann, Rudolf and Vijaykumar, T. N.},
 title = {Min-cut program decomposition for thread-level speculation},
 abstract = {With billion-transistor chips on the horizon, single-chip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a min-cut-based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graph-theoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74\% for floating-point programs and 23\% for integer programs on a four-processor chip, improving on the 52\% and 13\% achieved by the previous heuristics.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996851},
 doi = {http://doi.acm.org/10.1145/996841.996851},
 acmid = {996851},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, min-cut, partitioning, program decomposition, thread-level speculation},
} 

@article{Du:2004:CCF:996893.996852,
 author = {Du, Zhao-Hui and Lim, Chu-Cheow and Li, Xiao-Feng and Yang, Chen and Zhao, Qingyu and Ngai, Tin-Fook},
 title = {A cost-driven compilation framework for speculative parallelization of sequential programs},
 abstract = {The emerging hardware support for thread-level speculation opens new opportunities to parallelize sequential programs beyond the traditional limits. By speculating that many data dependences are unlikely during runtime, consecutive iterations of a sequential loop can be executed speculatively in parallel. Runtime parallelism is obtained when the speculation is correct. To take full advantage of this new execution model, a program needs to be programmed or compiled in such a way that it exhibits high degree of speculative thread-level parallelism. We propose a comprehensive cost-driven compilation framework to perform speculative parallelization. Based on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative parallel loops and selects only those loops whose speculative parallel execution is likely to improve program performance. The framework also supports and uses enabling techniques such as loop unrolling, software value prediction and dependence profiling to expose more speculative parallelism. The proposed framework was implemented on the ORC compiler. Our evaluation showed that the cost-driven speculative parallelization was effective. Our compiler was able to generate good speculative parallel loops in ten Spec2000Int benchmarks, which currently achieve an average 8\% speedup. We anticipate an average 15.6\% speedup when all enabling techniques are in place.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/996893.996852},
 doi = {http://doi.acm.org/10.1145/996893.996852},
 acmid = {996852},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cost-driven compilation, loop transformation, speculative multithreading, speculative parallel threading, speculative parallelization, thread-level speculation},
} 

@inproceedings{Du:2004:CCF:996841.996852,
 author = {Du, Zhao-Hui and Lim, Chu-Cheow and Li, Xiao-Feng and Yang, Chen and Zhao, Qingyu and Ngai, Tin-Fook},
 title = {A cost-driven compilation framework for speculative parallelization of sequential programs},
 abstract = {The emerging hardware support for thread-level speculation opens new opportunities to parallelize sequential programs beyond the traditional limits. By speculating that many data dependences are unlikely during runtime, consecutive iterations of a sequential loop can be executed speculatively in parallel. Runtime parallelism is obtained when the speculation is correct. To take full advantage of this new execution model, a program needs to be programmed or compiled in such a way that it exhibits high degree of speculative thread-level parallelism. We propose a comprehensive cost-driven compilation framework to perform speculative parallelization. Based on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative parallel loops and selects only those loops whose speculative parallel execution is likely to improve program performance. The framework also supports and uses enabling techniques such as loop unrolling, software value prediction and dependence profiling to expose more speculative parallelism. The proposed framework was implemented on the ORC compiler. Our evaluation showed that the cost-driven speculative parallelization was effective. Our compiler was able to generate good speculative parallel loops in ten Spec2000Int benchmarks, which currently achieve an average 8\% speedup. We anticipate an average 15.6\% speedup when all enabling techniques are in place.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/996841.996852},
 doi = {http://doi.acm.org/10.1145/996841.996852},
 acmid = {996852},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cost-driven compilation, loop transformation, speculative multithreading, speculative parallel threading, speculative parallelization, thread-level speculation},
} 

@inproceedings{Eichenberger:2004:VSA:996841.996853,
 author = {Eichenberger, Alexandre E. and Wu, Peng and O'Brien, Kevin},
 title = {Vectorization for SIMD architectures with alignment constraints},
 abstract = {When vectorizing for SIMD architectures that are commonly employed by today's multimedia extensions, one of the new challenges that arise is the handling of memory alignment. Prior research has focused primarily on vectorizing loops where all memory references are properly aligned. An important aspect of this problem, namely, how to vectorize misaligned memory references, still remains unaddressed.This paper presents a compilation scheme that systematically vectorizes loops in the presence of misaligned memory references. The core of our technique is to automatically reorganize data in registers to satisfy the alignment requirement imposed by the hardware. To reduce the data reorganization overhead, we propose several techniques to minimize the number of data reorganization operations generated. During the code generation, our algorithm also exploits temporal reuse when aligning references that access contiguous memory across loop iterations. Our code generation scheme guarantees to never load the same data associated with a single static access twice. Experimental results indicate near peak speedup factors, e.g., 3.71 for 4 data per vector and 6.06 for 8 data per vector, respectively, for a set of loops where 75\% or more of the static memory references are misaligned.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {82--93},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996853},
 doi = {http://doi.acm.org/10.1145/996841.996853},
 acmid = {996853},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, alignment, compiler, multimedia extensions, simdization, vectorization},
} 

@article{Eichenberger:2004:VSA:996893.996853,
 author = {Eichenberger, Alexandre E. and Wu, Peng and O'Brien, Kevin},
 title = {Vectorization for SIMD architectures with alignment constraints},
 abstract = {When vectorizing for SIMD architectures that are commonly employed by today's multimedia extensions, one of the new challenges that arise is the handling of memory alignment. Prior research has focused primarily on vectorizing loops where all memory references are properly aligned. An important aspect of this problem, namely, how to vectorize misaligned memory references, still remains unaddressed.This paper presents a compilation scheme that systematically vectorizes loops in the presence of misaligned memory references. The core of our technique is to automatically reorganize data in registers to satisfy the alignment requirement imposed by the hardware. To reduce the data reorganization overhead, we propose several techniques to minimize the number of data reorganization operations generated. During the code generation, our algorithm also exploits temporal reuse when aligning references that access contiguous memory across loop iterations. Our code generation scheme guarantees to never load the same data associated with a single static access twice. Experimental results indicate near peak speedup factors, e.g., 3.71 for 4 data per vector and 6.06 for 8 data per vector, respectively, for a set of loops where 75\% or more of the static memory references are misaligned.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {82--93},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996853},
 doi = {http://doi.acm.org/10.1145/996893.996853},
 acmid = {996853},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, alignment, compiler, multimedia extensions, simdization, vectorization},
} 

@article{Zhang:2004:CED:996893.996855,
 author = {Zhang, Xiangyu and Gupta, Rajiv},
 title = {Cost effective dynamic program slicing},
 abstract = {Although dynamic program slicing was first introduced to aid in user level debugging, applications aimed at improving software quality, reliability, security, and performance have since been identified as candidates for using dynamic slicing. However, the dynamic dependence graph constructed to compute dynamic slices can easily cause slicing algorithms to run out of memory for realistic program runs. In this paper we present the design and evaluation of a cost effective dynamic program slicing algorithm. This algorithm is based upon a dynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph can be held in memory and dynamic slices can be quickly computed. A compact representation is derived by recognizing that all dynamic dependences (data and control) need not be individually represented. We identify sets of dynamic dependence edges between a pair of statements that can share</i> a single representative edge. We further show that the dependence graph can be transformed in a manner that increases sharing and sharing can be performed even in the presence of aliasing. Experiments show that transformed dynamic dependence graphs explicitly represent only 6\% of the dependence edges present in the full dynamic dependence graph. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our compacted graphs range from 20 to 210 Megabytes in size. Average slicing times for our algorithm range from 1.74 to 36.25 seconds across several benchmarks from SPECInt2000/95.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {94--106},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996893.996855},
 doi = {http://doi.acm.org/10.1145/996893.996855},
 acmid = {996855},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dynamic dependence graph, testing},
} 

@inproceedings{Zhang:2004:CED:996841.996855,
 author = {Zhang, Xiangyu and Gupta, Rajiv},
 title = {Cost effective dynamic program slicing},
 abstract = {Although dynamic program slicing was first introduced to aid in user level debugging, applications aimed at improving software quality, reliability, security, and performance have since been identified as candidates for using dynamic slicing. However, the dynamic dependence graph constructed to compute dynamic slices can easily cause slicing algorithms to run out of memory for realistic program runs. In this paper we present the design and evaluation of a cost effective dynamic program slicing algorithm. This algorithm is based upon a dynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph can be held in memory and dynamic slices can be quickly computed. A compact representation is derived by recognizing that all dynamic dependences (data and control) need not be individually represented. We identify sets of dynamic dependence edges between a pair of statements that can share</i> a single representative edge. We further show that the dependence graph can be transformed in a manner that increases sharing and sharing can be performed even in the presence of aliasing. Experiments show that transformed dynamic dependence graphs explicitly represent only 6\% of the dependence edges present in the full dynamic dependence graph. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our compacted graphs range from 20 to 210 Megabytes in size. Average slicing times for our algorithm range from 1.74 to 36.25 seconds across several benchmarks from SPECInt2000/95.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {94--106},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996841.996855},
 doi = {http://doi.acm.org/10.1145/996841.996855},
 acmid = {996855},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dynamic dependence graph, testing},
} 

@article{Collberg:2004:DPS:996893.996856,
 author = {Collberg, C. and Carter, E. and Debray, S. and Huntwork, A. and Kececioglu, J. and Linn, C. and Stepp, M.},
 title = {Dynamic path-based software watermarking},
 abstract = {Software watermarking is a tool used to combat software piracy by embedding identifying information into a program. Most existing proposals for software watermarking have the shortcoming that the mark can be destroyed via fairly straightforward semantics-preserving code transformations. This paper introduces path-based watermarking, a new approach to software watermarking based on the dynamic branching behavior of programs. The advantage of this technique is that error-correcting and tamper-proofing techniques can be used to make path-based watermarks resilient against a wide variety of attacks. Experimental results, using both Java bytecode and IA-32 native code, indicate that even relatively large watermarks can be embedded into programs at modest cost.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {107--118},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996856},
 doi = {http://doi.acm.org/10.1145/996893.996856},
 acmid = {996856},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software piracy, software protection, watermarking},
} 

@inproceedings{Collberg:2004:DPS:996841.996856,
 author = {Collberg, C. and Carter, E. and Debray, S. and Huntwork, A. and Kececioglu, J. and Linn, C. and Stepp, M.},
 title = {Dynamic path-based software watermarking},
 abstract = {Software watermarking is a tool used to combat software piracy by embedding identifying information into a program. Most existing proposals for software watermarking have the shortcoming that the mark can be destroyed via fairly straightforward semantics-preserving code transformations. This paper introduces path-based watermarking, a new approach to software watermarking based on the dynamic branching behavior of programs. The advantage of this technique is that error-correcting and tamper-proofing techniques can be used to make path-based watermarks resilient against a wide variety of attacks. Experimental results, using both Java bytecode and IA-32 native code, indicate that even relatively large watermarks can be embedded into programs at modest cost.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {107--118},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996856},
 doi = {http://doi.acm.org/10.1145/996841.996856},
 acmid = {996856},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software piracy, software protection, watermarking},
} 

@article{Wang:2004:PAA:996893.996857,
 author = {Wang, Cheng and Li, Zhiyuan},
 title = {Parametric analysis for adaptive computation offloading},
 abstract = {Many programs can be invoked under different execution options, input parameters and data files. Such different execution contexts may lead to strikingly different execution instances. The optimal code generation may be sensitive to the execution instances. In this paper, we show how to use parametric program analysis to deal with this issue for the optimization problem of computation offloading.Computation offloading has been shown to be an effective way to improve performance and energy saving on mobile devices. Optimal program partitioning for computation offloading depends on the tradeoff between the computation workload and the communication cost. The computation workload and communication requirement may change with different execution instances. Optimal decisions on program partitioning must be made at run time when sufficient information about workload and communication requirement becomes available.Our cost analysis obtains program computation workload and communication cost expressed as functions of run-time parameters, and our parametric partitioning algorithm finds the optimal program partitioning corresponding to different ranges of run-time parameters. At run time, the transformed program self-schedules its tasks on either the mobile device or the server, based on the optimal program partitioning that corresponds to the current values of run-time parameters. Experimental results on an HP IPAQ handheld device show that different run-time parameters can lead to quite different program partitioning decisions.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {119--130},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996857},
 doi = {http://doi.acm.org/10.1145/996893.996857},
 acmid = {996857},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive optimization, computation offloading, distributed system, handheld devices, program analysis, program partitioning, program profiling, program transformation},
} 

@inproceedings{Wang:2004:PAA:996841.996857,
 author = {Wang, Cheng and Li, Zhiyuan},
 title = {Parametric analysis for adaptive computation offloading},
 abstract = {Many programs can be invoked under different execution options, input parameters and data files. Such different execution contexts may lead to strikingly different execution instances. The optimal code generation may be sensitive to the execution instances. In this paper, we show how to use parametric program analysis to deal with this issue for the optimization problem of computation offloading.Computation offloading has been shown to be an effective way to improve performance and energy saving on mobile devices. Optimal program partitioning for computation offloading depends on the tradeoff between the computation workload and the communication cost. The computation workload and communication requirement may change with different execution instances. Optimal decisions on program partitioning must be made at run time when sufficient information about workload and communication requirement becomes available.Our cost analysis obtains program computation workload and communication cost expressed as functions of run-time parameters, and our parametric partitioning algorithm finds the optimal program partitioning corresponding to different ranges of run-time parameters. At run time, the transformed program self-schedules its tasks on either the mobile device or the server, based on the optimal program partitioning that corresponds to the current values of run-time parameters. Experimental results on an HP IPAQ handheld device show that different run-time parameters can lead to quite different program partitioning decisions.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {119--130},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996857},
 doi = {http://doi.acm.org/10.1145/996841.996857},
 acmid = {996857},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive optimization, computation offloading, distributed system, handheld devices, program analysis, program partitioning, program profiling, program transformation},
} 

@article{Whaley:2004:CCP:996893.996859,
 author = {Whaley, John and Lam, Monica S.},
 title = {Cloning-based context-sensitive pointer alias analysis using binary decision diagrams},
 abstract = {This paper presents the first scalable context-sensitive, inclusion-based pointer alias analysis for Java programs. Our approach to context sensitivity is to create a clone of a method for every context of interest, and run a context-insensitive</i> algorithm over the expanded call graph to get context-sensitive</i> results. For precision, we generate a clone for every acyclic path through a program's call graph, treating methods in a strongly connected component as a single node. Normally, this formulation is hopelessly intractable as a call graph often has 10 14 acyclic paths or more. We show that these exponential relations can be computed efficiently using binary decision diagrams (BDDs). Key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. We applied our algorithm to the most popular applications available on Sourceforge, and found that the largest programs, with hundreds of thousands of Java bytecodes, can be analyzed in under 20 minutes.This paper shows that pointer analysis, and many other queries and algorithms, can be described succinctly and declaratively using Datalog, a logic programming language. We have developed a system called bddbddb that automatically translates Datalog programs into highly efficient BDD implementations. We used this approach to develop a variety of context-sensitive algorithms including side effect analysis, type analysis, and escape analysis.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {131--144},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/996893.996859},
 doi = {http://doi.acm.org/10.1145/996893.996859},
 acmid = {996859},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Datalog, Java, binary decision diagrams, cloning, context-sensitive, inclusion-based, logic programming, pointer analysis, program analysis, scalable},
} 

@inproceedings{Whaley:2004:CCP:996841.996859,
 author = {Whaley, John and Lam, Monica S.},
 title = {Cloning-based context-sensitive pointer alias analysis using binary decision diagrams},
 abstract = {This paper presents the first scalable context-sensitive, inclusion-based pointer alias analysis for Java programs. Our approach to context sensitivity is to create a clone of a method for every context of interest, and run a context-insensitive</i> algorithm over the expanded call graph to get context-sensitive</i> results. For precision, we generate a clone for every acyclic path through a program's call graph, treating methods in a strongly connected component as a single node. Normally, this formulation is hopelessly intractable as a call graph often has 10 14 acyclic paths or more. We show that these exponential relations can be computed efficiently using binary decision diagrams (BDDs). Key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. We applied our algorithm to the most popular applications available on Sourceforge, and found that the largest programs, with hundreds of thousands of Java bytecodes, can be analyzed in under 20 minutes.This paper shows that pointer analysis, and many other queries and algorithms, can be described succinctly and declaratively using Datalog, a logic programming language. We have developed a system called bddbddb that automatically translates Datalog programs into highly efficient BDD implementations. We used this approach to develop a variety of context-sensitive algorithms including side effect analysis, type analysis, and escape analysis.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {131--144},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/996841.996859},
 doi = {http://doi.acm.org/10.1145/996841.996859},
 acmid = {996859},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Datalog, Java, binary decision diagrams, cloning, context-sensitive, inclusion-based, logic programming, pointer analysis, program analysis, scalable},
} 

@article{Zhu:2004:SPA:996893.996860,
 author = {Zhu, Jianwen and Calman, Silvian},
 title = {Symbolic pointer analysis revisited},
 abstract = {Pointer analysis is a critical problem in optimizing compiler, parallelizing compiler, software engineering and most recently, hardware synthesis. While recent efforts have suggested symbolic method, which uses Bryant's Binary Decision Diagram as an alternative to capture the point-to relation, no speed advantage has been demonstrated for context-insensitive analysis, and results for context-sensitive analysis are only preliminary.In this paper, we refine the concept of symbolic transfer function proposed earlier and establish a common framework for both context-insensitive and context-sensitive pointer analysis. With this framework, our transfer function of a procedure can abstract away the impact of its callers and callees, and represent its point-to information completely, compactly and canonically. In addition, we propose a symbolic representation of the invocation graph, which can otherwise be exponentially large. In contrast to the classical frameworks where context-sensitive point-to information of a procedure has to be obtained by the application of its transfer function exponentially many times, our method can obtain point-to information of all contexts in a single application. Our experimental evaluation on a wide range of C benchmarks indicates that our context-sensitive pointer analysis can be made almost as fast as its context-insensitive counterpart.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {145--157},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996893.996860},
 doi = {http://doi.acm.org/10.1145/996893.996860},
 acmid = {996860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, call graph construction, pointer analysis},
} 

@inproceedings{Zhu:2004:SPA:996841.996860,
 author = {Zhu, Jianwen and Calman, Silvian},
 title = {Symbolic pointer analysis revisited},
 abstract = {Pointer analysis is a critical problem in optimizing compiler, parallelizing compiler, software engineering and most recently, hardware synthesis. While recent efforts have suggested symbolic method, which uses Bryant's Binary Decision Diagram as an alternative to capture the point-to relation, no speed advantage has been demonstrated for context-insensitive analysis, and results for context-sensitive analysis are only preliminary.In this paper, we refine the concept of symbolic transfer function proposed earlier and establish a common framework for both context-insensitive and context-sensitive pointer analysis. With this framework, our transfer function of a procedure can abstract away the impact of its callers and callees, and represent its point-to information completely, compactly and canonically. In addition, we propose a symbolic representation of the invocation graph, which can otherwise be exponentially large. In contrast to the classical frameworks where context-sensitive point-to information of a procedure has to be obtained by the application of its transfer function exponentially many times, our method can obtain point-to information of all contexts in a single application. Our experimental evaluation on a wide range of C benchmarks indicates that our context-sensitive pointer analysis can be made almost as fast as its context-insensitive counterpart.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {145--157},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/996841.996860},
 doi = {http://doi.acm.org/10.1145/996841.996860},
 acmid = {996860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, call graph construction, pointer analysis},
} 

@article{Lhotak:2004:JBR:996893.996861,
 author = {Lhot\'{a}k, Ond\v{r}ej and Hendren, Laurie},
 title = {Jedd: a BDD-based relational extension of Java},
 abstract = {In this paper we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly.The paper provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations.The motivation for designing Jedd was to support the development of whole program analyses based on BDDs, and we have used Jedd to express five key interrelated whole program analyses in our Soot compiler framework. We provide some examples of this application and discuss our experiences using Jedd.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {158--169},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996861},
 doi = {http://doi.acm.org/10.1145/996893.996861},
 acmid = {996861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, binary decision diagrams, boolean formula satisfiability, language design, program analysis, relations},
} 

@inproceedings{Lhotak:2004:JBR:996841.996861,
 author = {Lhot\'{a}k, Ond\v{r}ej and Hendren, Laurie},
 title = {Jedd: a BDD-based relational extension of Java},
 abstract = {In this paper we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly.The paper provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations.The motivation for designing Jedd was to support the development of whole program analyses based on BDDs, and we have used Jedd to express five key interrelated whole program analyses in our Soot compiler framework. We provide some examples of this application and discuss our experiences using Jedd.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {158--169},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996861},
 doi = {http://doi.acm.org/10.1145/996841.996861},
 acmid = {996861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, binary decision diagrams, boolean formula satisfiability, language design, program analysis, relations},
} 

@inproceedings{Appel:2004:SPP:996841.996842,
 author = {Appel, Andrew W.},
 title = {Social processes and proofs of theorems and programs, revisited},
 abstract = {Language-based security is a protection mechanism that allows software components to interact in a shared address space, such that each component is guaranteed to respect its interfaces and not steal or corrupt internal data of other components. This protection mechanism is complicated to implement correctly, so we might want a formal verification of it.But we know by a famous result of DeMillo, Lipton, and Perlis (POPL 1978) that formal verification (1) is not what mathematicians do, (2) can never be practical, and (3) cannot tell us anything truly useful. Is this still true 25 years later?The question is, then, how can we carefully skirt the legitimate objections of DeMillo et al. and successfully use formal verification in a context where it can do some good. I'll talk about Foundational Proof-Carrying Code, a machine-checked soundness proof for a protection mechanism usable in Java-like virtual machines.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {170--170},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/996841.996842},
 doi = {http://doi.acm.org/10.1145/996841.996842},
 acmid = {996842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kulkarni:2004:FSE:996893.996863,
 author = {Kulkarni, Prasad and Hines, Stephen and Hiser, Jason and Whalley, David and Davidson, Jack and Jones, Douglas},
 title = {Fast searches for effective optimization phase sequences},
 abstract = {It has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where longer compilation times may be tolerated in the final stage of development. In this paper we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 65\% on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show that the average number of required generations decreased by 68\%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996863},
 doi = {http://doi.acm.org/10.1145/996893.996863},
 acmid = {996863},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {genetic algorithms, interactive compilation, phase ordering},
} 

@inproceedings{Kulkarni:2004:FSE:996841.996863,
 author = {Kulkarni, Prasad and Hines, Stephen and Hiser, Jason and Whalley, David and Davidson, Jack and Jones, Douglas},
 title = {Fast searches for effective optimization phase sequences},
 abstract = {It has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where longer compilation times may be tolerated in the final stage of development. In this paper we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 65\% on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show that the average number of required generations decreased by 68\%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996863},
 doi = {http://doi.acm.org/10.1145/996841.996863},
 acmid = {996863},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {genetic algorithms, interactive compilation, phase ordering},
} 

@article{Cavazos:2004:IHD:996893.996864,
 author = {Cavazos, John and Moss, J. Eliot B.},
 title = {Inducing heuristics to decide whether to schedule},
 abstract = {Instruction scheduling is a compiler optimization that can improve program speed, sometimes by 10\% or more, but it can also be expensive. Furthermore, time spent optimizing is more important in a Java just-in-time (JIT) compiler than in a traditional one because a JIT compiles code at run time, adding to the running time of the program. We found that, on any given block of code, instruction scheduling often does not produce significant benefit and sometimes degrades speed. Thus, we hoped that we could focus scheduling effort on those blocks that benefit from it.Using supervised learning we induced heuristics to predict which blocks benefit from scheduling. The induced function chooses, for each block, between list scheduling and not scheduling the block at all. Using the induced function we obtained over 90\% of the improvement of scheduling every block but with less than 25\% of the scheduling effort. When used in combination with profile-based adaptive optimization, the induced function remains effective but gives a smaller reduction in scheduling effort. Deciding when to optimize, and which optimization(s) to apply, is an important open problem area in compiler research. We show that supervised learning solves one of these problems well.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996864},
 doi = {http://doi.acm.org/10.1145/996893.996864},
 acmid = {996864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, Jikes RVM, compiler optimization, instruction scheduling, machine learning, supervised learning},
} 

@inproceedings{Cavazos:2004:IHD:996841.996864,
 author = {Cavazos, John and Moss, J. Eliot B.},
 title = {Inducing heuristics to decide whether to schedule},
 abstract = {Instruction scheduling is a compiler optimization that can improve program speed, sometimes by 10\% or more, but it can also be expensive. Furthermore, time spent optimizing is more important in a Java just-in-time (JIT) compiler than in a traditional one because a JIT compiles code at run time, adding to the running time of the program. We found that, on any given block of code, instruction scheduling often does not produce significant benefit and sometimes degrades speed. Thus, we hoped that we could focus scheduling effort on those blocks that benefit from it.Using supervised learning we induced heuristics to predict which blocks benefit from scheduling. The induced function chooses, for each block, between list scheduling and not scheduling the block at all. Using the induced function we obtained over 90\% of the improvement of scheduling every block but with less than 25\% of the scheduling effort. When used in combination with profile-based adaptive optimization, the induced function remains effective but gives a smaller reduction in scheduling effort. Deciding when to optimize, and which optimization(s) to apply, is an important open problem area in compiler research. We show that supervised learning solves one of these problems well.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996864},
 doi = {http://doi.acm.org/10.1145/996841.996864},
 acmid = {996864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, Jikes RVM, compiler optimization, instruction scheduling, machine learning, supervised learning},
} 

@article{Vachharajani:2004:LSS:996893.996865,
 author = {Vachharajani, Manish and Vachharajani, Neil and August, David I.},
 title = {The liberty structural specification language: a high-level modeling language for component reuse},
 abstract = {Rapid exploration of the design space with simulation models is essential for quality hardware systems research and development. Despite striking commonalities across hardware systems, designers routinely fail to achieve high levels of reuse across models constructed in existing general-purpose and domain-specific languages. This lack of reuse adversely impacts hardware system design by slowing the rate at which ideas are evaluated. This paper presents an examination of existing languages to reveal their fundamental limitations regarding reuse in hardware modeling. With this understanding, a solution is described in the context of the design and implementation of the Liberty Structural Specification Language (LSS), the input language for a publicly available high-level digital-hardware modeling tool called the Liberty Simulation Environment. LSS is the first language to enable low-overhead reuse by simultaneously supporting static inference based on hardware structure and</i> flexibility via parameterizable structure. Through LSS, this paper also introduces a new type inference algorithm and a new programming language technique, called use-based specialization</i>, which, in a manner analogous to type inference, customizes reusable components by statically inferring structural properties that otherwise would have had to have been specified manually.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996865},
 doi = {http://doi.acm.org/10.1145/996893.996865},
 acmid = {996865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Liberty Simulation Environment (LSE), Liberty Structural Specification (LSS), component reuse, simulator construction, structural modeling, type inference, use-based specialization},
} 

@inproceedings{Vachharajani:2004:LSS:996841.996865,
 author = {Vachharajani, Manish and Vachharajani, Neil and August, David I.},
 title = {The liberty structural specification language: a high-level modeling language for component reuse},
 abstract = {Rapid exploration of the design space with simulation models is essential for quality hardware systems research and development. Despite striking commonalities across hardware systems, designers routinely fail to achieve high levels of reuse across models constructed in existing general-purpose and domain-specific languages. This lack of reuse adversely impacts hardware system design by slowing the rate at which ideas are evaluated. This paper presents an examination of existing languages to reveal their fundamental limitations regarding reuse in hardware modeling. With this understanding, a solution is described in the context of the design and implementation of the Liberty Structural Specification Language (LSS), the input language for a publicly available high-level digital-hardware modeling tool called the Liberty Simulation Environment. LSS is the first language to enable low-overhead reuse by simultaneously supporting static inference based on hardware structure and</i> flexibility via parameterizable structure. Through LSS, this paper also introduces a new type inference algorithm and a new programming language technique, called use-based specialization</i>, which, in a manner analogous to type inference, customizes reusable components by statically inferring structural properties that otherwise would have had to have been specified manually.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996865},
 doi = {http://doi.acm.org/10.1145/996841.996865},
 acmid = {996865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Liberty Simulation Environment (LSE), Liberty Structural Specification (LSS), component reuse, simulator construction, structural modeling, type inference, use-based specialization},
} 

@article{Kodumal:2004:SCR:996893.996867,
 author = {Kodumal, John and Aiken, Alex},
 title = {The set constraint/CFL reachability connection in practice},
 abstract = {Many program analyses can be reduced to graph reachability problems involving a limited form of context-free language reachability called Dyck-CFL reachability. We show a new reduction from Dyck-CFL reachability to set constraints that can be used in practice to solve these problems. Our reduction is much simpler than the general reduction from context-free language reachability to set constraints. We have implemented our reduction on top of a set constraints toolkit and tested its performance on a substantial polymorphic flow analysis application.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996867},
 doi = {http://doi.acm.org/10.1145/996893.996867},
 acmid = {996867},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-free language reachability, flow analysis, set constraints, type qualifiers},
} 

@inproceedings{Kodumal:2004:SCR:996841.996867,
 author = {Kodumal, John and Aiken, Alex},
 title = {The set constraint/CFL reachability connection in practice},
 abstract = {Many program analyses can be reduced to graph reachability problems involving a limited form of context-free language reachability called Dyck-CFL reachability. We show a new reduction from Dyck-CFL reachability to set constraints that can be used in practice to solve these problems. Our reduction is much simpler than the general reduction from context-free language reachability to set constraints. We have implemented our reduction on top of a set constraints toolkit and tested its performance on a substantial polymorphic flow analysis application.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996867},
 doi = {http://doi.acm.org/10.1145/996841.996867},
 acmid = {996867},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-free language reachability, flow analysis, set constraints, type qualifiers},
} 

@article{Liu:2004:PRP:996893.996868,
 author = {Liu, Yanhong A. and Rothamel, Tom and Yu, Fuxiang and Stoller, Scott D. and Hu, Nanjun},
 title = {Parametric regular path queries},
 abstract = {Regular path queries are a way of declaratively expressing queries on graphs as regular-expression-like patterns that are matched against paths in the graph. There are two kinds of queries: existential queries, which specify properties about individual paths, and universal queries, which specify properties about all paths. They provide a simple and convenient framework for expressing program analyses as queries on graph representations of programs, for expressing verification (model-checking) problems as queries on transition systems, for querying semi-structured data, etc. Parametric regular path queries extend the patterns with variables, called parameters, which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relate.This paper shows how a variety of program analysis and model-checking problems can be expressed easily and succinctly using parametric regular path queries. The paper describes the specification, design, analysis, and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries. Major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries, detailed complexity analysis of the algorithms, detailed analytical and experimental performance comparison of variations of the algorithms and data structures, and investigation of efficiency tradeoffs between different formulations of queries.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996868},
 doi = {http://doi.acm.org/10.1145/996893.996868},
 acmid = {996868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, data tructures, graph query languages, memoization, model checking, optimization, precomputation, program analysis, regular expressions, regular path queries},
} 

@inproceedings{Liu:2004:PRP:996841.996868,
 author = {Liu, Yanhong A. and Rothamel, Tom and Yu, Fuxiang and Stoller, Scott D. and Hu, Nanjun},
 title = {Parametric regular path queries},
 abstract = {Regular path queries are a way of declaratively expressing queries on graphs as regular-expression-like patterns that are matched against paths in the graph. There are two kinds of queries: existential queries, which specify properties about individual paths, and universal queries, which specify properties about all paths. They provide a simple and convenient framework for expressing program analyses as queries on graph representations of programs, for expressing verification (model-checking) problems as queries on transition systems, for querying semi-structured data, etc. Parametric regular path queries extend the patterns with variables, called parameters, which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relate.This paper shows how a variety of program analysis and model-checking problems can be expressed easily and succinctly using parametric regular path queries. The paper describes the specification, design, analysis, and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries. Major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries, detailed complexity analysis of the algorithms, detailed analytical and experimental performance comparison of variations of the algorithms and data structures, and investigation of efficiency tradeoffs between different formulations of queries.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996868},
 doi = {http://doi.acm.org/10.1145/996841.996868},
 acmid = {996868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, data tructures, graph query languages, memoization, model checking, optimization, precomputation, program analysis, regular expressions, regular path queries},
} 

@article{Venet:2004:PES:996893.996869,
 author = {Venet, Arnaud and Brat, Guillaume},
 title = {Precise and efficient static array bound checking for large embedded C programs},
 abstract = {In this paper we describe the design and implementation of a static array-bound checker for a family of embedded programs: the flight control software of recent Mars missions. These codes are large (up to 280 KLOC), pointer intensive, heavily multithreaded and written in an object-oriented style, which makes their analysis very challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code in a couple of hours with a precision of 80\%. The scalability and precision of the analyzer are achieved by using an incremental framework in which a pointer analysis and a numerical analysis of array indices mutually refine each other. CGS has been designed so that it can distribute the analysis over several processors in a cluster of machines. To the best of our knowledge this is the first distributed implementation of static analysis algorithms. Throughout the paper we will discuss the scalability setbacks that we encountered during the construction of the tool and their impact on the initial design decisions.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996869},
 doi = {http://doi.acm.org/10.1145/996893.996869},
 acmid = {996869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, array-bound checking, difference-bound matrices, pointer analysis, program verification},
} 

@inproceedings{Venet:2004:PES:996841.996869,
 author = {Venet, Arnaud and Brat, Guillaume},
 title = {Precise and efficient static array bound checking for large embedded C programs},
 abstract = {In this paper we describe the design and implementation of a static array-bound checker for a family of embedded programs: the flight control software of recent Mars missions. These codes are large (up to 280 KLOC), pointer intensive, heavily multithreaded and written in an object-oriented style, which makes their analysis very challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code in a couple of hours with a precision of 80\%. The scalability and precision of the analyzer are achieved by using an incremental framework in which a pointer analysis and a numerical analysis of array indices mutually refine each other. CGS has been designed so that it can distribute the analysis over several processors in a cluster of machines. To the best of our knowledge this is the first distributed implementation of static analysis algorithms. Throughout the paper we will discuss the scalability setbacks that we encountered during the construction of the tool and their impact on the initial design decisions.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996869},
 doi = {http://doi.acm.org/10.1145/996841.996869},
 acmid = {996869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, array-bound checking, difference-bound matrices, pointer analysis, program verification},
} 

@article{Chin:2004:RIO:996893.996871,
 author = {Chin, Wei-Ngan and Craciun, Florin and Qin, Shengchao and Rinard, Martin},
 title = {Region inference for an object-oriented language},
 abstract = {Region-based memory management offers several important potential advantages over garbage collection, including real-time performance, better data locality, and more efficient use of limited memory. Researchers have advocated the use of regions for functional, imperative, and object-oriented languages. Lexically scoped regions are now a core feature of the Real-Time Specification for Java (RTSJ)[5].Recent research in region-based programming for Java has focused on region checking, which requires manual effort to augment the program with region annotations. In this paper, we propose an automatic region inference system for a core subset of Java. To provide an inference method that is both precise and practical, we support classes and methods that are region-polymorphic, with region-polymorphic recursion for methods. One challenging aspect is to ensure region safety in the presence of features such as class subtyping, method overriding, and downcast operations. Our region inference rules can handle these object-oriented features safely without creating dangling references.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {243--254},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996871},
 doi = {http://doi.acm.org/10.1145/996893.996871},
 acmid = {996871},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {downcasts, memory management, method overriding, object-oriented languages, region inference, type systems},
} 

@inproceedings{Chin:2004:RIO:996841.996871,
 author = {Chin, Wei-Ngan and Craciun, Florin and Qin, Shengchao and Rinard, Martin},
 title = {Region inference for an object-oriented language},
 abstract = {Region-based memory management offers several important potential advantages over garbage collection, including real-time performance, better data locality, and more efficient use of limited memory. Researchers have advocated the use of regions for functional, imperative, and object-oriented languages. Lexically scoped regions are now a core feature of the Real-Time Specification for Java (RTSJ)[5].Recent research in region-based programming for Java has focused on region checking, which requires manual effort to augment the program with region annotations. In this paper, we propose an automatic region inference system for a core subset of Java. To provide an inference method that is both precise and practical, we support classes and methods that are region-polymorphic, with region-polymorphic recursion for methods. One challenging aspect is to ensure region safety in the presence of features such as class subtyping, method overriding, and downcast operations. Our region inference rules can handle these object-oriented features safely without creating dangling references.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {243--254},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996871},
 doi = {http://doi.acm.org/10.1145/996841.996871},
 acmid = {996871},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {downcasts, memory management, method overriding, object-oriented languages, region inference, type systems},
} 

@inproceedings{Zhong:2004:ARS:996841.996872,
 author = {Zhong, Yutao and Orlovich, Maksim and Shen, Xipeng and Ding, Chen},
 title = {Array regrouping and structure splitting using whole-program reference affinity},
 abstract = {While the memory of most machines is organized as a hierarchy, program data are laid out in a uniform address space. This paper defines a model of reference affinity</i>, which measures how close a group of data are accessed together in a reference trace. It proves that the model gives a hierarchical partition of program data. At the top is the set of all data with the weakest affinity. At the bottom is each data element with the strongest affinity. Based on the theoretical model, the paper presents k-distance analysis</i>, a practical test for the hierarchical affinity of source-level data. When used for array regrouping and structure splitting, k</i>-distance analysis consistently outperforms data organizations given by the programmer, compiler analysis, frequency profiling, statistical clustering, and all other methods we have tried.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {255--266},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996872},
 doi = {http://doi.acm.org/10.1145/996841.996872},
 acmid = {996872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {array regrouping, program locality, program transformation, reference affinity, reuse signature, structure splitting, volume distance},
} 

@article{Zhong:2004:ARS:996893.996872,
 author = {Zhong, Yutao and Orlovich, Maksim and Shen, Xipeng and Ding, Chen},
 title = {Array regrouping and structure splitting using whole-program reference affinity},
 abstract = {While the memory of most machines is organized as a hierarchy, program data are laid out in a uniform address space. This paper defines a model of reference affinity</i>, which measures how close a group of data are accessed together in a reference trace. It proves that the model gives a hierarchical partition of program data. At the top is the set of all data with the weakest affinity. At the bottom is each data element with the strongest affinity. Based on the theoretical model, the paper presents k-distance analysis</i>, a practical test for the hierarchical affinity of source-level data. When used for array regrouping and structure splitting, k</i>-distance analysis consistently outperforms data organizations given by the programmer, compiler analysis, frequency profiling, statistical clustering, and all other methods we have tried.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {255--266},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996872},
 doi = {http://doi.acm.org/10.1145/996893.996872},
 acmid = {996872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {array regrouping, program locality, program transformation, reference affinity, reuse signature, structure splitting, volume distance},
} 

@inproceedings{Adl-Tabatabai:2004:PIB:996841.996873,
 author = {Adl-Tabatabai, Ali-Reza and Hudson, Richard L. and Serrano, Mauricio J. and Subramoney, Sreenivas},
 title = {Prefetch injection based on hardware monitoring and object metadata},
 abstract = {Cache miss stalls hurt performance because of the large gap between memory and processor speeds - for example, the popular server benchmark SPEC JBB2000 spends 45\% of its cycles stalled waiting for memory requests on the Itanium\&#174; 2 processor. Traversing linked data structures causes a large portion of these stalls. Prefetching for linked data structures remains a major challenge because serial data dependencies between elements in a linked data structure preclude the timely materialization of prefetch addresses. This paper presents Mississippi Delta</i> (MS Delta), a novel technique for prefetching linked data structures that closely integrates the hardware performance monitor (HPM), the garbage collector's global view of heap and object layout, the type-level metadata inherent in type-safe programs, and JIT compiler analysis. The garbage collector uses the HPM's data cache miss information to identify cache miss intensive traversal paths through linked data structures, and then discovers regular distances (deltas</i>) between these linked objects. JIT compiler analysis injects prefetch instructions using deltas to materialize prefetch addresses.We have implemented MS Delta in a fully dynamic profile-guided optimization system: the StarJIT dynamic compiler [1] and the ORP Java virtual machine [9]. We demonstrate a 28-29\% reduction in stall cycles attributable to the high-latency cache misses targeted by MS Delta and a speedup of 11-14\% on the cache miss intensive SPEC JBB2000 benchmark.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {267--276},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/996841.996873},
 doi = {http://doi.acm.org/10.1145/996841.996873},
 acmid = {996873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache misses, compiler optimization, garbage collection, prefetching, profile-guided optimization, virtual machines},
} 

@article{Adl-Tabatabai:2004:PIB:996893.996873,
 author = {Adl-Tabatabai, Ali-Reza and Hudson, Richard L. and Serrano, Mauricio J. and Subramoney, Sreenivas},
 title = {Prefetch injection based on hardware monitoring and object metadata},
 abstract = {Cache miss stalls hurt performance because of the large gap between memory and processor speeds - for example, the popular server benchmark SPEC JBB2000 spends 45\% of its cycles stalled waiting for memory requests on the Itanium\&#174; 2 processor. Traversing linked data structures causes a large portion of these stalls. Prefetching for linked data structures remains a major challenge because serial data dependencies between elements in a linked data structure preclude the timely materialization of prefetch addresses. This paper presents Mississippi Delta</i> (MS Delta), a novel technique for prefetching linked data structures that closely integrates the hardware performance monitor (HPM), the garbage collector's global view of heap and object layout, the type-level metadata inherent in type-safe programs, and JIT compiler analysis. The garbage collector uses the HPM's data cache miss information to identify cache miss intensive traversal paths through linked data structures, and then discovers regular distances (deltas</i>) between these linked objects. JIT compiler analysis injects prefetch instructions using deltas to materialize prefetch addresses.We have implemented MS Delta in a fully dynamic profile-guided optimization system: the StarJIT dynamic compiler [1] and the ORP Java virtual machine [9]. We demonstrate a 28-29\% reduction in stall cycles attributable to the high-latency cache misses targeted by MS Delta and a speedup of 11-14\% on the cache miss intensive SPEC JBB2000 benchmark.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {267--276},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/996893.996873},
 doi = {http://doi.acm.org/10.1145/996893.996873},
 acmid = {996873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache misses, compiler optimization, garbage collection, prefetching, profile-guided optimization, virtual machines},
} 

@inproceedings{Smith:2004:GAG:996841.996875,
 author = {Smith, Michael D. and Ramsey, Norman and Holloway, Glenn},
 title = {A generalized algorithm for graph-coloring register allocation},
 abstract = {Graph-coloring register allocation is an elegant and extremely popular optimization for modern machines. But as currently formulated, it does not handle two characteristics commonly found in commercial architectures. First, a single register name may appear in multiple register classes, where a class is a set of register names that are interchangeable in a particular role. Second, multiple register names may be aliases for a single hardware register. We present a generalization of graph-coloring register allocation that handles these problematic characteristics while preserving the elegance and practicality of traditional graph coloring. Our generalization adapts easily to a new target machine, requiring only the sets of names in the register classes and a map of the register aliases. It also drops easily into a well-known graph-coloring allocator, is efficient at compile time, and produces high-quality code.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996875},
 doi = {http://doi.acm.org/10.1145/996841.996875},
 acmid = {996875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation},
} 

@article{Smith:2004:GAG:996893.996875,
 author = {Smith, Michael D. and Ramsey, Norman and Holloway, Glenn},
 title = {A generalized algorithm for graph-coloring register allocation},
 abstract = {Graph-coloring register allocation is an elegant and extremely popular optimization for modern machines. But as currently formulated, it does not handle two characteristics commonly found in commercial architectures. First, a single register name may appear in multiple register classes, where a class is a set of register names that are interchangeable in a particular role. Second, multiple register names may be aliases for a single hardware register. We present a generalization of graph-coloring register allocation that handles these problematic characteristics while preserving the elegance and practicality of traditional graph coloring. Our generalization adapts easily to a new target machine, requiring only the sets of names in the register classes and a map of the register aliases. It also drops easily into a well-known graph-coloring allocator, is efficient at compile time, and produces high-quality code.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996875},
 doi = {http://doi.acm.org/10.1145/996893.996875},
 acmid = {996875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation},
} 

@inproceedings{Zhuang:2004:BRA:996841.996876,
 author = {Zhuang, Xiaotong and Pande, Santosh},
 title = {Balancing register allocation across threads for a multithreaded network processor},
 abstract = {Modern network processors employ multi-threading to allow concurrency amongst multiple packet processing tasks. We studied the properties of applications running on the network processors and observed that their imbalanced register requirements across different threads at different program points could lead to poor performance. Many times application needs demand some threads to be more performance critical than others and thus by controlling the register allocation across threads one could impact the performance of the threads and get the desired performance properties for concurrent threads. This prompts our work.Our register allocator aims to distribute available registers to different threads according to their needs. The compiler analyzes the register needs of each thread both at the point of a context switch as well as internally. Compiler then designates some registers as shared and some as private to each thread. Shared registers are allocated across all threads explicitly by the compiler. Values that are live across a context switch can not be kept in shared registers due to safety reasons; thus, only those live ranges that are internal to the context switch can be safely allocated to shared registers. Spill can cause a context switch. and thus, the problems of context switch and allocation are closely coupled and we propose a solution to this problem. The proposed interference graphs (GIG,BIG,IIG) distinguish variables that must use a thread's private registers from those that can use shared registers. We first estimate the register requirement bounds, then reduce from the upper bound gradually to achieve a good register balance among threads. To reduce the register needs, move insertions are inserted at program points that split the live ranges or the nodes on the interference graph. We show that the lower bound is reachable via live range splitting and is adequate for our benchmark programs for simultaneously assigning them on different threads. As our objective, the number of move instructions is minimized.Empirical results show that the compiler is able to effectively control the register allocation across threads by maximizing the number of shared registers. Speed-up for performance critical threads ranges from 18 to 24\% whereas degradation for performance of non-critical threads ranges only from 1 to 4\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation},
 series = {PLDI '04},
 year = {2004},
 isbn = {1-58113-807-5},
 location = {Washington DC, USA},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996841.996876},
 doi = {http://doi.acm.org/10.1145/996841.996876},
 acmid = {996876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multithreaded processor, network processor, register allocation},
} 

@article{Zhuang:2004:BRA:996893.996876,
 author = {Zhuang, Xiaotong and Pande, Santosh},
 title = {Balancing register allocation across threads for a multithreaded network processor},
 abstract = {Modern network processors employ multi-threading to allow concurrency amongst multiple packet processing tasks. We studied the properties of applications running on the network processors and observed that their imbalanced register requirements across different threads at different program points could lead to poor performance. Many times application needs demand some threads to be more performance critical than others and thus by controlling the register allocation across threads one could impact the performance of the threads and get the desired performance properties for concurrent threads. This prompts our work.Our register allocator aims to distribute available registers to different threads according to their needs. The compiler analyzes the register needs of each thread both at the point of a context switch as well as internally. Compiler then designates some registers as shared and some as private to each thread. Shared registers are allocated across all threads explicitly by the compiler. Values that are live across a context switch can not be kept in shared registers due to safety reasons; thus, only those live ranges that are internal to the context switch can be safely allocated to shared registers. Spill can cause a context switch. and thus, the problems of context switch and allocation are closely coupled and we propose a solution to this problem. The proposed interference graphs (GIG,BIG,IIG) distinguish variables that must use a thread's private registers from those that can use shared registers. We first estimate the register requirement bounds, then reduce from the upper bound gradually to achieve a good register balance among threads. To reduce the register needs, move insertions are inserted at program points that split the live ranges or the nodes on the interference graph. We show that the lower bound is reachable via live range splitting and is adequate for our benchmark programs for simultaneously assigning them on different threads. As our objective, the number of move instructions is minimized.Empirical results show that the compiler is able to effectively control the register allocation across threads by maximizing the number of shared registers. Speed-up for performance critical threads ranges from 18 to 24\% whereas degradation for performance of non-critical threads ranges only from 1 to 4\%.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {6},
 month = {June},
 year = {2004},
 issn = {0362-1340},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/996893.996876},
 doi = {http://doi.acm.org/10.1145/996893.996876},
 acmid = {996876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multithreaded processor, network processor, register allocation},
} 

@article{Carlstrom:2006:ATP:1133255.1133983,
 author = {Carlstrom, Brian D. and McDonald, Austen and Chafi, Hassan and Chung, JaeWoong and Minh, Chi Cao and Kozyrakis, Christos and Olukotun, Kunle},
 title = {The Atomos transactional programming language},
 abstract = {Atomos is the first programming language with implicit transactions, strong atomicity, and a scalable multiprocessor implementation. Atomos is derived from Java, but replaces its synchronization and conditional waiting constructs with simpler transactional alternatives.The Atomos watch statement allows programmers to specify fine-grained watch sets used with the Atomos retry conditional waiting statement for efficient transactional conflict-driven wakeup even in transactional memory systems with a limited number of transactional contexts. Atomos supports open-nested transactions</i>, which are necessary for building both scalable application programs and virtual machine implementations.The implementation of the Atomos scheduler demonstrates the use of open nesting within the virtual machine and introduces the concept of transactional memory violation handlers that allow programs to recover from data dependency violations without rolling back.Atomos programming examples are given to demonstrate the usefulness of transactional programming primitives. Atomos and Java are compared through the use of several benchmarks. The results demonstrate both the improvements in parallel programming ease and parallel program performance provided by Atomos.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133255.1133983},
 doi = {http://doi.acm.org/10.1145/1133255.1133983},
 acmid = {1133983},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conditional synchronization, java, multiprocessor architecture, transactional memory},
} 

@inproceedings{Carlstrom:2006:ATP:1133981.1133983,
 author = {Carlstrom, Brian D. and McDonald, Austen and Chafi, Hassan and Chung, JaeWoong and Minh, Chi Cao and Kozyrakis, Christos and Olukotun, Kunle},
 title = {The Atomos transactional programming language},
 abstract = {Atomos is the first programming language with implicit transactions, strong atomicity, and a scalable multiprocessor implementation. Atomos is derived from Java, but replaces its synchronization and conditional waiting constructs with simpler transactional alternatives.The Atomos watch statement allows programmers to specify fine-grained watch sets used with the Atomos retry conditional waiting statement for efficient transactional conflict-driven wakeup even in transactional memory systems with a limited number of transactional contexts. Atomos supports open-nested transactions</i>, which are necessary for building both scalable application programs and virtual machine implementations.The implementation of the Atomos scheduler demonstrates the use of open nesting within the virtual machine and introduces the concept of transactional memory violation handlers that allow programs to recover from data dependency violations without rolling back.Atomos programming examples are given to demonstrate the usefulness of transactional programming primitives. Atomos and Java are compared through the use of several benchmarks. The results demonstrate both the improvements in parallel programming ease and parallel program performance provided by Atomos.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133981.1133983},
 doi = {http://doi.acm.org/10.1145/1133981.1133983},
 acmid = {1133983},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conditional synchronization, java, multiprocessor architecture, transactional memory},
} 

@article{Harris:2006:OMT:1133255.1133984,
 author = {Harris, Tim and Plesko, Mark and Shinnar, Avraham and Tarditi, David},
 title = {Optimizing memory transactions},
 abstract = {Atomic blocks allow programmers to delimit sections of code as 'atomic', leaving the language's implementation to enforce atomicity. Existing work has shown how to implement atomic blocks over word-based transactional memory</i> that provides scalable multi-processor performance without requiring changes to the basic structure of objects in the heap. However, these implementations perform poorly because they interpose on all accesses to shared memory in the atomic block, redirecting updates to a thread-private log which must be searched by reads in the block and later reconciled with the heap when leaving the block.This paper takes a four-pronged approach to improving performance: (1) we introduce a new 'direct access' implementation that avoids searching thread-private logs, (2) we develop compiler optimizations to reduce the amount of logging (e.g. when a thread accesses the same data repeatedly in an atomic block), (3) we use runtime filtering to detect duplicate log entries that are missed statically, and (4) we present a series of GC-time techniques to compact the logs generated by long-running atomic blocks.Our implementation supports short-running scalable concurrent benchmarks with less than 50\\% overhead over a non-thread-safe baseline. We support long atomic blocks containing millions of shared memory accesses with a 2.5-4.5x slowdown.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133984},
 doi = {http://doi.acm.org/10.1145/1133255.1133984},
 acmid = {1133984},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, critical regions, transactional memory},
} 

@inproceedings{Harris:2006:OMT:1133981.1133984,
 author = {Harris, Tim and Plesko, Mark and Shinnar, Avraham and Tarditi, David},
 title = {Optimizing memory transactions},
 abstract = {Atomic blocks allow programmers to delimit sections of code as 'atomic', leaving the language's implementation to enforce atomicity. Existing work has shown how to implement atomic blocks over word-based transactional memory</i> that provides scalable multi-processor performance without requiring changes to the basic structure of objects in the heap. However, these implementations perform poorly because they interpose on all accesses to shared memory in the atomic block, redirecting updates to a thread-private log which must be searched by reads in the block and later reconciled with the heap when leaving the block.This paper takes a four-pronged approach to improving performance: (1) we introduce a new 'direct access' implementation that avoids searching thread-private logs, (2) we develop compiler optimizations to reduce the amount of logging (e.g. when a thread accesses the same data repeatedly in an atomic block), (3) we use runtime filtering to detect duplicate log entries that are missed statically, and (4) we present a series of GC-time techniques to compact the logs generated by long-running atomic blocks.Our implementation supports short-running scalable concurrent benchmarks with less than 50\\% overhead over a non-thread-safe baseline. We support long atomic blocks containing millions of shared memory accesses with a 2.5-4.5x slowdown.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133984},
 doi = {http://doi.acm.org/10.1145/1133981.1133984},
 acmid = {1133984},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, critical regions, transactional memory},
} 

@article{Adl-Tabatabai:2006:CRS:1133255.1133985,
 author = {Adl-Tabatabai, Ali-Reza and Lewis, Brian T. and Menon, Vijay and Murphy, Brian R. and Saha, Bratin and Shpeisman, Tatiana},
 title = {Compiler and runtime support for efficient software transactional memory},
 abstract = {Programmers have traditionally used locks to synchronize concurrent access to shared data. Lock-based synchronization, however, has well-known pitfalls: using locks for fine-grain synchronization and composing code that already uses locks are both difficult and prone to deadlock. Transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming. Transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications, opening the door for new compiler optimizations that target the overheads of transactional memory.This paper presents compiler and runtime optimizations for transactional memory language constructs. We present a high-performance software transactional memory system (STM) integrated into a managed runtime environment. Our system efficiently implements nested transactions that support both composition of transactions and partial roll back. Our JIT compiler is the first to optimize the overheads of STM, and we show novel techniques for enabling JIT optimizations on STM operations. We measure the performance of our optimizations on a 16-way SMP running multi-threaded transactional workloads. Our results show that these techniques enable transactional memory's performance to compete with that of well-tuned synchronization.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133985},
 doi = {http://doi.acm.org/10.1145/1133255.1133985},
 acmid = {1133985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compiler optimizations, locking, synchronization, transactional memory, virtual machines},
} 

@inproceedings{Adl-Tabatabai:2006:CRS:1133981.1133985,
 author = {Adl-Tabatabai, Ali-Reza and Lewis, Brian T. and Menon, Vijay and Murphy, Brian R. and Saha, Bratin and Shpeisman, Tatiana},
 title = {Compiler and runtime support for efficient software transactional memory},
 abstract = {Programmers have traditionally used locks to synchronize concurrent access to shared data. Lock-based synchronization, however, has well-known pitfalls: using locks for fine-grain synchronization and composing code that already uses locks are both difficult and prone to deadlock. Transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming. Transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications, opening the door for new compiler optimizations that target the overheads of transactional memory.This paper presents compiler and runtime optimizations for transactional memory language constructs. We present a high-performance software transactional memory system (STM) integrated into a managed runtime environment. Our system efficiently implements nested transactions that support both composition of transactions and partial roll back. Our JIT compiler is the first to optimize the overheads of STM, and we show novel techniques for enabling JIT optimizations on STM operations. We measure the performance of our optimizations on a 16-way SMP running multi-threaded transactional workloads. Our results show that these techniques enable transactional memory's performance to compete with that of well-tuned synchronization.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133985},
 doi = {http://doi.acm.org/10.1145/1133981.1133985},
 acmid = {1133985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compiler optimizations, locking, synchronization, transactional memory, virtual machines},
} 

@inproceedings{Grimm:2006:BET:1133981.1133987,
 author = {Grimm, Robert},
 title = {Better extensibility through modular syntax},
 abstract = {We explore how to make the benefits of modularity available for syntactic specifications and present Rats!</i>, a parser generator for Java that supports easily extensible syntax. Our parser generator builds on recent research on parsing expression grammars (PEGs), which, by being closed under composition, prioritizing choices, supporting unlimited lookahead, and integrating lexing and parsing, offer an attractive alternative to context-free grammars. PEGs are implemented by so-called packrat parsers, which are recursive descent parsers that memoize all intermediate results (hence their name). Memoization ensures linear-time performance in the presence of unlimited lookahead, but also results in an essentially lazy, functional parsing technique. In this paper, we explore how to leverage PEGs and packrat parsers as the foundation for extensible syntax. In particular, we show how make packrat parsing more widely applicable by implementing this lazy, functional technique in a strict, imperative language, while also generating better performing parsers through aggressive optimizations. Next, we develop a module system for organizing, modifying, and composing large-scale syntactic specifications. Finally, we describe a new technique for managing (global) parsing state in functional parsers. Our experimental evaluation demonstrates that the resulting parser generator succeeds at providing extensible syntax. In particular, Rats!</i> enables other grammar writers to realize real-world language extensions in little time and code, and it generates parsers that consistently out-perform parsers created by two GLR parser generators.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {38--51},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133981.1133987},
 doi = {http://doi.acm.org/10.1145/1133981.1133987},
 acmid = {1133987},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extensible syntax, module system, packrat parsing, parser generator, parsing expression grammar},
} 

@article{Grimm:2006:BET:1133255.1133987,
 author = {Grimm, Robert},
 title = {Better extensibility through modular syntax},
 abstract = {We explore how to make the benefits of modularity available for syntactic specifications and present Rats!</i>, a parser generator for Java that supports easily extensible syntax. Our parser generator builds on recent research on parsing expression grammars (PEGs), which, by being closed under composition, prioritizing choices, supporting unlimited lookahead, and integrating lexing and parsing, offer an attractive alternative to context-free grammars. PEGs are implemented by so-called packrat parsers, which are recursive descent parsers that memoize all intermediate results (hence their name). Memoization ensures linear-time performance in the presence of unlimited lookahead, but also results in an essentially lazy, functional parsing technique. In this paper, we explore how to leverage PEGs and packrat parsers as the foundation for extensible syntax. In particular, we show how make packrat parsing more widely applicable by implementing this lazy, functional technique in a strict, imperative language, while also generating better performing parsers through aggressive optimizations. Next, we develop a module system for organizing, modifying, and composing large-scale syntactic specifications. Finally, we describe a new technique for managing (global) parsing state in functional parsers. Our experimental evaluation demonstrates that the resulting parser generator succeeds at providing extensible syntax. In particular, Rats!</i> enables other grammar writers to realize real-world language extensions in little time and code, and it generates parsers that consistently out-perform parsers created by two GLR parser generators.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {38--51},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133255.1133987},
 doi = {http://doi.acm.org/10.1145/1133255.1133987},
 acmid = {1133987},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extensible syntax, module system, packrat parsing, parser generator, parsing expression grammar},
} 

@article{Ertl:2006:FFI:1133255.1133988,
 author = {Ertl, M. Anton and Casey, Kevin and Gregg, David},
 title = {Fast and flexible instruction selection with on-demand tree-parsing automata},
 abstract = {Tree parsing as supported by code generator generators like BEG, burg, iburg, lburg and ml-burg is a popular instruction selection method. There are two existing approaches for implementing tree parsing: dynamic programming, and tree-parsing automata; each approach has its advantages and disadvantages. We propose a new implementation approach that combines the advantages of both existing approaches: we start out with dynamic programming at compile time, but at every step we generate a state for a tree-parsing automaton, which is used the next time a tree matching the state is found, turning the instruction selector into a fast tree-parsing automaton. We have implemented this approach in the Gforth code generator. The implementation required little effort and reduced the startup time of Gforth by up to a factor of 2.5.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {52--60},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133255.1133988},
 doi = {http://doi.acm.org/10.1145/1133255.1133988},
 acmid = {1133988},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automaton, dynamic programming, instruction selection, lazy, tree parsing},
} 

@inproceedings{Ertl:2006:FFI:1133981.1133988,
 author = {Ertl, M. Anton and Casey, Kevin and Gregg, David},
 title = {Fast and flexible instruction selection with on-demand tree-parsing automata},
 abstract = {Tree parsing as supported by code generator generators like BEG, burg, iburg, lburg and ml-burg is a popular instruction selection method. There are two existing approaches for implementing tree parsing: dynamic programming, and tree-parsing automata; each approach has its advantages and disadvantages. We propose a new implementation approach that combines the advantages of both existing approaches: we start out with dynamic programming at compile time, but at every step we generate a state for a tree-parsing automaton, which is used the next time a tree matching the state is found, turning the instruction selector into a fast tree-parsing automaton. We have implemented this approach in the Gforth code generator. The implementation required little effort and reduced the startup time of Gforth by up to a factor of 2.5.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {52--60},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133981.1133988},
 doi = {http://doi.acm.org/10.1145/1133981.1133988},
 acmid = {1133988},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automaton, dynamic programming, instruction selection, lazy, tree parsing},
} 

@inproceedings{Triantafyllis:2006:FUW:1133981.1133989,
 author = {Triantafyllis, Spyridon and Bridges, Matthew J. and Raman, Easwaran and Ottoni, Guilherme and August, David I.},
 title = {A framework for unrestricted whole-program optimization},
 abstract = {Procedures have long been the basic units of compilation in conventional optimization frameworks. However, procedures are typically formed to serve software engineering rather than optimization goals, arbitrarily constraining code transformations. Techniques, such as aggressive inlining and interprocedural optimization, have been developed to alleviate this problem, but, due to code growth and compile time issues, these can be applied only sparingly.This paper introduces the Procedure Boundary Elimination (PBE) compilation framework, which allows unrestricted whole-program optimization. PBE allows all intra-procedural optimizations and analyses to operate on arbitrary subgraphs of the program, regardless of the original procedure boundaries and without resorting to inlining. In order to control compilation time, PBE also introduces novel extensions of region formation</i> and encapsulation</i>. PBE enables targeted code specialization</i>, which recovers the specialization benefits of inlining while keeping code growth in check. This paper shows that PBE attains better performance than inlining with half the code growth.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {61--71},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1133989},
 doi = {http://doi.acm.org/10.1145/1133981.1133989},
 acmid = {1133989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inlining, interprocedural analysis, interprocedural optimization, path-sensitive analysis, procedure unification, region encapsulation, region formation, region-based compilation, specialization, superblock, whole-program analysis, whole-program optimization},
} 

@article{Triantafyllis:2006:FUW:1133255.1133989,
 author = {Triantafyllis, Spyridon and Bridges, Matthew J. and Raman, Easwaran and Ottoni, Guilherme and August, David I.},
 title = {A framework for unrestricted whole-program optimization},
 abstract = {Procedures have long been the basic units of compilation in conventional optimization frameworks. However, procedures are typically formed to serve software engineering rather than optimization goals, arbitrarily constraining code transformations. Techniques, such as aggressive inlining and interprocedural optimization, have been developed to alleviate this problem, but, due to code growth and compile time issues, these can be applied only sparingly.This paper introduces the Procedure Boundary Elimination (PBE) compilation framework, which allows unrestricted whole-program optimization. PBE allows all intra-procedural optimizations and analyses to operate on arbitrary subgraphs of the program, regardless of the original procedure boundaries and without resorting to inlining. In order to control compilation time, PBE also introduces novel extensions of region formation</i> and encapsulation</i>. PBE enables targeted code specialization</i>, which recovers the specialization benefits of inlining while keeping code growth in check. This paper shows that PBE attains better performance than inlining with half the code growth.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {61--71},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1133989},
 doi = {http://doi.acm.org/10.1145/1133255.1133989},
 acmid = {1133989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inlining, interprocedural analysis, interprocedural optimization, path-sensitive analysis, procedure unification, region encapsulation, region formation, region-based compilation, specialization, superblock, whole-program analysis, whole-program optimization},
} 

@inproceedings{Neamtiu:2006:PDS:1133981.1133991,
 author = {Neamtiu, Iulian and Hicks, Michael and Stoyle, Gareth and Oriol, Manuel},
 title = {Practical dynamic software updating for C},
 abstract = {Software updates typically require stopping and restarting an application, but many systems cannot afford to halt service, or would prefer not to. Dynamic software updating</i> (DSU) addresses this difficulty by permitting programs to be updated while they run. DSU is appealing compared to other approaches for on-line upgrades because it is quite general and requires no redundant hardware. The challenge is in making DSU practical</i>: it should be flexible, and yet safe, efficient, and easy to use.In this paper, we present Ginseng, a DSU implementation for C that aims to meet this challenge. We compile programs specially so that they can be dynamically patched, and generate most of a dynamic patch automatically. Ginseng performs a series of analyses that when combined with some simple runtime support ensure that an update will not violate type-safety while guaranteeing that data is kept up-to-date. We have used Ginseng to construct and dynamically apply patches to three substantial open-source server programs---Very Secure FTP daemon</i>, OpenSSH sshd daemon</i>, and GNU Zebra</i>. In total, we dynamically patched each program with three years' worth of releases. Though the programs changed substantially, the majority of updates were easy to generate. Performance experiments show that all patches could be applied in less than 5 ms</i>, and that the overhead on application throughput due to updating support ranged from 0 to at most 32\%.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {72--83},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133991},
 doi = {http://doi.acm.org/10.1145/1133981.1133991},
 acmid = {1133991},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, function indirection, loop extraction, type wrapping},
} 

@article{Neamtiu:2006:PDS:1133255.1133991,
 author = {Neamtiu, Iulian and Hicks, Michael and Stoyle, Gareth and Oriol, Manuel},
 title = {Practical dynamic software updating for C},
 abstract = {Software updates typically require stopping and restarting an application, but many systems cannot afford to halt service, or would prefer not to. Dynamic software updating</i> (DSU) addresses this difficulty by permitting programs to be updated while they run. DSU is appealing compared to other approaches for on-line upgrades because it is quite general and requires no redundant hardware. The challenge is in making DSU practical</i>: it should be flexible, and yet safe, efficient, and easy to use.In this paper, we present Ginseng, a DSU implementation for C that aims to meet this challenge. We compile programs specially so that they can be dynamically patched, and generate most of a dynamic patch automatically. Ginseng performs a series of analyses that when combined with some simple runtime support ensure that an update will not violate type-safety while guaranteeing that data is kept up-to-date. We have used Ginseng to construct and dynamically apply patches to three substantial open-source server programs---Very Secure FTP daemon</i>, OpenSSH sshd daemon</i>, and GNU Zebra</i>. In total, we dynamically patched each program with three years' worth of releases. Though the programs changed substantially, the majority of updates were easy to generate. Performance experiments show that all patches could be applied in less than 5 ms</i>, and that the overhead on application throughput due to updating support ranged from 0 to at most 32\%.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {72--83},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133991},
 doi = {http://doi.acm.org/10.1145/1133255.1133991},
 acmid = {1133991},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic software updating, function indirection, loop extraction, type wrapping},
} 

@article{Fei:2006:APR:1133255.1133992,
 author = {Fei, Long and Midkiff, Samuel P.},
 title = {Artemis: practical runtime monitoring of applications for execution anomalies},
 abstract = {A number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. Because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. In this paper we propose the Artemis<sup>1</sup> is the Greek goddess of the hunt and wild animals. Our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. The Artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. Artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. Our experiments show that Artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that Artemis can effectively guide a monitoring tool to the buggy regions of a program. Our experimental results show that Artemis applied to a hardware-based PC-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {84--95},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133992},
 doi = {http://doi.acm.org/10.1145/1133255.1133992},
 acmid = {1133992},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context invariant, debugging, dynamic context, pointer-type table, runtime monitoring, selective monitoring, value invariant},
} 

@inproceedings{Fei:2006:APR:1133981.1133992,
 author = {Fei, Long and Midkiff, Samuel P.},
 title = {Artemis: practical runtime monitoring of applications for execution anomalies},
 abstract = {A number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. Because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. In this paper we propose the Artemis<sup>1</sup> is the Greek goddess of the hunt and wild animals. Our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. The Artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. Artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. Our experiments show that Artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that Artemis can effectively guide a monitoring tool to the buggy regions of a program. Our experimental results show that Artemis applied to a hardware-based PC-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {84--95},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133992},
 doi = {http://doi.acm.org/10.1145/1133981.1133992},
 acmid = {1133992},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context invariant, debugging, dynamic context, pointer-type table, runtime monitoring, selective monitoring, value invariant},
} 

@inproceedings{Acar:2006:EAS:1133981.1133993,
 author = {Acar, Umut A. and Blelloch, Guy E. and Blu Matthias and Tangwongsan, Kanat},
 title = {An experimental analysis of self-adjusting computation},
 abstract = {Dependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133993},
 doi = {http://doi.acm.org/10.1145/1133981.1133993},
 acmid = {1133993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational geometry, dynamic algorithms, dynamic dependence graphs, memorization, performance, self-adjusting computation},
} 

@article{Acar:2006:EAS:1133255.1133993,
 author = {Acar, Umut A. and Blelloch, Guy E. and Blu Matthias and Tangwongsan, Kanat},
 title = {An experimental analysis of self-adjusting computation},
 abstract = {Dependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133993},
 doi = {http://doi.acm.org/10.1145/1133255.1133993},
 acmid = {1133993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational geometry, dynamic algorithms, dynamic dependence graphs, memorization, performance, self-adjusting computation},
} 

@inproceedings{Barton:2006:SMP:1133981.1133995,
 author = {Barton, Christopher and Cas\c{c}aval, C\'{C}lin and Alm\'{a}si, George and Zheng, Yili and Farreras, Montse and Chatterje, Siddhartha and Amaral, Jos\'{e} Nelson},
 title = {Shared memory programming for large scale machines},
 abstract = {This paper describes the design and implementation of a scalable run-time system and an optimizing compiler for Unified Parallel C (UPC). An experimental evaluation on BlueGene/L\&#174;, a distributed-memory machine, demonstrates that the combination of the compiler with the runtime system produces programs with performance comparable to that of efficient MPI programs and good performance scalability up to hundreds of thousands of processors.Our runtime system design solves the problem of maintaining shared object consistency efficiently in a distributed memory machine. Our compiler infrastructure simplifies the code generated for parallel loops in UPC through the elimination of affinity tests, eliminates several levels of indirection for accesses to segments of shared arrays that the compiler can prove to be local, and implements remote update operations through a lower-cost asynchronous message. The performance evaluation uses three well-known benchmarks --- HPC RandomAccess, HPC STREAM and NAS CG --- to obtain scaling and absolute performance numbers for these benchmarks on up to 131072 processors, the full BlueGene/L machine. These results were used to win the HPC Challenge Competition at SC05 in Seattle WA, demonstrating that PGAS languages support both productivity and performance.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1133981.1133995},
 doi = {http://doi.acm.org/10.1145/1133981.1133995},
 acmid = {1133995},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BlueGene, PGAS programming model, UPC},
} 

@article{Barton:2006:SMP:1133255.1133995,
 author = {Barton, Christopher and Cas\c{c}aval, C\'{C}lin and Alm\'{a}si, George and Zheng, Yili and Farreras, Montse and Chatterje, Siddhartha and Amaral, Jos\'{e} Nelson},
 title = {Shared memory programming for large scale machines},
 abstract = {This paper describes the design and implementation of a scalable run-time system and an optimizing compiler for Unified Parallel C (UPC). An experimental evaluation on BlueGene/L\&#174;, a distributed-memory machine, demonstrates that the combination of the compiler with the runtime system produces programs with performance comparable to that of efficient MPI programs and good performance scalability up to hundreds of thousands of processors.Our runtime system design solves the problem of maintaining shared object consistency efficiently in a distributed memory machine. Our compiler infrastructure simplifies the code generated for parallel loops in UPC through the elimination of affinity tests, eliminates several levels of indirection for accesses to segments of shared arrays that the compiler can prove to be local, and implements remote update operations through a lower-cost asynchronous message. The performance evaluation uses three well-known benchmarks --- HPC RandomAccess, HPC STREAM and NAS CG --- to obtain scaling and absolute performance numbers for these benchmarks on up to 131072 processors, the full BlueGene/L machine. These results were used to win the HPC Challenge Competition at SC05 in Seattle WA, demonstrating that PGAS languages support both productivity and performance.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1133255.1133995},
 doi = {http://doi.acm.org/10.1145/1133255.1133995},
 acmid = {1133995},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BlueGene, PGAS programming model, UPC},
} 

@inproceedings{Ren:2006:ODP:1133981.1133996,
 author = {Ren, Gang and Wu, Peng and Padua, David},
 title = {Optimizing data permutations for SIMD devices},
 abstract = {The widespread presence of SIMD devices in today's microprocessors has made compiler techniques for these devices tremendously important. One of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for non-contiguous and misaligned memory references. These instructions are expensive and, therefore, it is of crucial importance to minimize their number to improve performance and, in many cases, enable speedups over scalar code.Although it is often difficult to optimize an isolated data reorganization operation, a collection of related data permutations can often be manipulated to reduce the number of operations. This paper presents a strategy to optimize all forms of data permutations. The strategy is organized into three steps. First, all data permutations in the source program are converted into a generic representation. These permutations can originate from vector accesses to non-contiguous and misaligned memory locations or result from compiler transformations. Second, an optimization algorithm is applied to reduce the number of data permutations in a basic block. By propagating permutations across statements and merging consecutive permutations whenever possible, the algorithm can significantly reduce the number of data permutations. Finally, a code generation algorithm translates generic permutation operations into native permutation instructions for the target platform. Experiments were conducted on various kinds of applications. The results show that up to 77\% of the permutation instructions are eliminated and, as a result, the average performance improvement is 48\% on VMX and 68\% on SSE2. For several applications, near perfect speedups have been achieved on both platforms.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {118--131},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133981.1133996},
 doi = {http://doi.acm.org/10.1145/1133981.1133996},
 acmid = {1133996},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD compilation, data permutation, optimization},
} 

@article{Ren:2006:ODP:1133255.1133996,
 author = {Ren, Gang and Wu, Peng and Padua, David},
 title = {Optimizing data permutations for SIMD devices},
 abstract = {The widespread presence of SIMD devices in today's microprocessors has made compiler techniques for these devices tremendously important. One of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for non-contiguous and misaligned memory references. These instructions are expensive and, therefore, it is of crucial importance to minimize their number to improve performance and, in many cases, enable speedups over scalar code.Although it is often difficult to optimize an isolated data reorganization operation, a collection of related data permutations can often be manipulated to reduce the number of operations. This paper presents a strategy to optimize all forms of data permutations. The strategy is organized into three steps. First, all data permutations in the source program are converted into a generic representation. These permutations can originate from vector accesses to non-contiguous and misaligned memory locations or result from compiler transformations. Second, an optimization algorithm is applied to reduce the number of data permutations in a basic block. By propagating permutations across statements and merging consecutive permutations whenever possible, the algorithm can significantly reduce the number of data permutations. Finally, a code generation algorithm translates generic permutation operations into native permutation instructions for the target platform. Experiments were conducted on various kinds of applications. The results show that up to 77\% of the permutation instructions are eliminated and, as a result, the average performance improvement is 48\% on VMX and 68\% on SSE2. For several applications, near perfect speedups have been achieved on both platforms.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {118--131},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133255.1133996},
 doi = {http://doi.acm.org/10.1145/1133255.1133996},
 acmid = {1133996},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD compilation, data permutation, optimization},
} 

@article{Nuzman:2006:AID:1133255.1133997,
 author = {Nuzman, Dorit and Rosen, Ira and Zaks, Ayal},
 title = {Auto-vectorization of interleaved data for SIMD},
 abstract = {Most implementations of the Single Instruction Multiple Data (SIMD) model available today require that data elements be packed in vector registers. Operations on disjoint vector elements are not supported directly and require explicit data reorganization manipulations. Computations on non-contiguous and especially interleaved data appear in important applications, which can greatly benefit from SIMD instructions once the data is reorganized properly. Vectorizing such computations efficiently is therefore an ambitious challenge for both programmers and vectorizing compilers. We demonstrate an automatic compilation scheme that supports effective vectorization in the presence of interleaved data with constant strides that are powers of 2, facilitating data reorganization. We demonstrate how our vectorization scheme applies to dominant SIMD architectures, and present experimental results on a wide range of key kernels, showing speedups in execution time up to 3.7 for interleaving levels (stride) as high as 8.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {132--143},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1133997},
 doi = {http://doi.acm.org/10.1145/1133255.1133997},
 acmid = {1133997},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, Viterbi, data reuse, subword parallelism, vectorization},
} 

@inproceedings{Nuzman:2006:AID:1133981.1133997,
 author = {Nuzman, Dorit and Rosen, Ira and Zaks, Ayal},
 title = {Auto-vectorization of interleaved data for SIMD},
 abstract = {Most implementations of the Single Instruction Multiple Data (SIMD) model available today require that data elements be packed in vector registers. Operations on disjoint vector elements are not supported directly and require explicit data reorganization manipulations. Computations on non-contiguous and especially interleaved data appear in important applications, which can greatly benefit from SIMD instructions once the data is reorganized properly. Vectorizing such computations efficiently is therefore an ambitious challenge for both programmers and vectorizing compilers. We demonstrate an automatic compilation scheme that supports effective vectorization in the presence of interleaved data with constant strides that are powers of 2, facilitating data reorganization. We demonstrate how our vectorization scheme applies to dominant SIMD architectures, and present experimental results on a wide range of key kernels, showing speedups in execution time up to 3.7 for interleaving levels (stride) as high as 8.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {132--143},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1133997},
 doi = {http://doi.acm.org/10.1145/1133981.1133997},
 acmid = {1133997},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, Viterbi, data reuse, subword parallelism, vectorization},
} 

@article{Dhurjati:2006:SEA:1133255.1133999,
 author = {Dhurjati, Dinakar and Kowshik, Sumant and Adve, Vikram},
 title = {SAFECode: enforcing alias analysis for weakly typed languages},
 abstract = {Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insight behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore, we show that the sound analysis information enables static checking techniques that eliminate many run-time checks. Our approach requires no source code changes, allows memory to be managedexplicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmark s and system codes, we show experimentally that the run-time overheads are low (less than 10\% in nearly all cases and 30\% in the worst case we have seen).We also show the effectiveness of static analyses in eliminating run-time checks.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {144--157},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133255.1133999},
 doi = {http://doi.acm.org/10.1145/1133255.1133999},
 acmid = {1133999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, automatic pool allocation, compilers, programming languages, region management},
} 

@inproceedings{Dhurjati:2006:SEA:1133981.1133999,
 author = {Dhurjati, Dinakar and Kowshik, Sumant and Adve, Vikram},
 title = {SAFECode: enforcing alias analysis for weakly typed languages},
 abstract = {Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insight behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore, we show that the sound analysis information enables static checking techniques that eliminate many run-time checks. Our approach requires no source code changes, allows memory to be managedexplicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmark s and system codes, we show experimentally that the run-time overheads are low (less than 10\% in nearly all cases and 30\% in the worst case we have seen).We also show the effectiveness of static analyses in eliminating run-time checks.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {144--157},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133981.1133999},
 doi = {http://doi.acm.org/10.1145/1133981.1133999},
 acmid = {1133999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, automatic pool allocation, compilers, programming languages, region management},
} 

@inproceedings{Berger:2006:DPM:1133981.1134000,
 author = {Berger, Emery D. and Zorn, Benjamin G.},
 title = {DieHard: probabilistic memory safety for unsafe languages},
 abstract = {Applications written in unsafe languages like C and C++ are vulnerable to memory errors such as buffer overflows, dangling pointers, and reads of uninitialized data. Such errors can lead to program crashes, security vulnerabilities, and unpredictable behavior. We present DieHard, a runtime system that tolerates these errors while probabilistically maintaining soundness. DieHard uses randomization and replication to achieve probabilistic memory safety by approximating an infinite-sized heap. DieHard's memory manager randomizes the location of objects in a heap that is at least twice as large as required. This algorithm prevents heap corruption and provides a probabilistic guarantee of avoiding memory errors. For additional safety, DieHard can operate in a replicated mode where multiple replicas of the same application are run simultaneously. By initializing each replica with a different random seed and requiring agreement on output, the replicated version of Die-Hard increases the likelihood of correct execution because errors are unlikely to have the same effect across all replicas. We present analytical and experimental results that show DieHard's resilience to a wide range of memory errors, including a heap-based buffer overflow in an actual application.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134000},
 doi = {http://doi.acm.org/10.1145/1133981.1134000},
 acmid = {1134000},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DieHard, dynamic memory allocation, probabilistic memory safety, randomization, replication},
} 

@article{Berger:2006:DPM:1133255.1134000,
 author = {Berger, Emery D. and Zorn, Benjamin G.},
 title = {DieHard: probabilistic memory safety for unsafe languages},
 abstract = {Applications written in unsafe languages like C and C++ are vulnerable to memory errors such as buffer overflows, dangling pointers, and reads of uninitialized data. Such errors can lead to program crashes, security vulnerabilities, and unpredictable behavior. We present DieHard, a runtime system that tolerates these errors while probabilistically maintaining soundness. DieHard uses randomization and replication to achieve probabilistic memory safety by approximating an infinite-sized heap. DieHard's memory manager randomizes the location of objects in a heap that is at least twice as large as required. This algorithm prevents heap corruption and provides a probabilistic guarantee of avoiding memory errors. For additional safety, DieHard can operate in a replicated mode where multiple replicas of the same application are run simultaneously. By initializing each replica with a different random seed and requiring agreement on output, the replicated version of Die-Hard increases the likelihood of correct execution because errors are unlikely to have the same effect across all replicas. We present analytical and experimental results that show DieHard's resilience to a wide range of memory errors, including a heap-based buffer overflow in an actual application.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134000},
 doi = {http://doi.acm.org/10.1145/1133255.1134000},
 acmid = {1134000},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DieHard, dynamic memory allocation, probabilistic memory safety, randomization, replication},
} 

@article{Zhang:2006:PDS:1133255.1134002,
 author = {Zhang, Xiangyu and Gupta, Neelam and Gupta, Rajiv},
 title = {Pruning dynamic slices with confidence},
 abstract = {Given an incorrect value produced during a failed program run (e.g., a wrong output value or a value that causes the program to crash), the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value. Although the dynamic slice often contains only a small percentage of the statements executed during the failed program run, the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty code.In this paper we develop a strategy for pruning the dynamic slice to identify a subset of statements in the dynamic slice that are likely responsible for producing the incorrect value. We observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values (e.g., a value produced by a statement in the dynamic slice of the incorrect value may also have been used in computing a correct output value prior to the incorrect value). For each such executed statement in the dynamic slice, using the value profiles of the executed statements, we compute a confidence value ranging from 0 to 1 - a higher confidence value corresponds to greater likelihood that the execution of the statement produced a correct value. Given a failed run involving execution of a single error, we demonstrate that the pruning of a dynamic slice by excluding only the statements with the confidence value of 1 is highly effective in reducing the size of the dynamic slice while retaining the faulty code in the slice. Our experiments show that the number of distinct statements in a pruned dynamic slice are 1.79 to 190.57 times less than the full dynamic slice. Confidence values also prioritize the statements in the dynamic slice according to the likelihood of them being faulty. We show that examining the statements in the order of increasing confidence values is an effective strategy for reducing the effort of fault location.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134002},
 doi = {http://doi.acm.org/10.1145/1133255.1134002},
 acmid = {1134002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data and control dependences, debugging, dynamic slicing},
} 

@inproceedings{Zhang:2006:PDS:1133981.1134002,
 author = {Zhang, Xiangyu and Gupta, Neelam and Gupta, Rajiv},
 title = {Pruning dynamic slices with confidence},
 abstract = {Given an incorrect value produced during a failed program run (e.g., a wrong output value or a value that causes the program to crash), the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value. Although the dynamic slice often contains only a small percentage of the statements executed during the failed program run, the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty code.In this paper we develop a strategy for pruning the dynamic slice to identify a subset of statements in the dynamic slice that are likely responsible for producing the incorrect value. We observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values (e.g., a value produced by a statement in the dynamic slice of the incorrect value may also have been used in computing a correct output value prior to the incorrect value). For each such executed statement in the dynamic slice, using the value profiles of the executed statements, we compute a confidence value ranging from 0 to 1 - a higher confidence value corresponds to greater likelihood that the execution of the statement produced a correct value. Given a failed run involving execution of a single error, we demonstrate that the pruning of a dynamic slice by excluding only the statements with the confidence value of 1 is highly effective in reducing the size of the dynamic slice while retaining the faulty code in the slice. Our experiments show that the number of distinct statements in a pruned dynamic slice are 1.79 to 190.57 times less than the full dynamic slice. Confidence values also prioritize the statements in the dynamic slice according to the likelihood of them being faulty. We show that examining the statements in the order of increasing confidence values is an effective strategy for reducing the effort of fault location.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134002},
 doi = {http://doi.acm.org/10.1145/1133981.1134002},
 acmid = {1134002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data and control dependences, debugging, dynamic slicing},
} 

@inproceedings{Johnson:2006:CDA:1133981.1134003,
 author = {Johnson, Troy A. and Eigenmann, Rudolf},
 title = {Context-sensitive domain-independent algorithm composition and selection},
 abstract = {Progressing beyond the productivity of present-day languages appears to require using domain-specific knowledge. Domain-specific languages and libraries (DSLs) proliferate, but most optimizations and language features have limited portability because each language's semantics are related closely to its domain. We explain how any DSL compiler can use a domain-independent</i> AI planner to implement algorithm composition as a language feature</i>. Our notion of composition addresses a common DSL problem: good library designers tend to minimize redundancy by including only fundamental procedures that users must chain together into call sequences. Novice users are confounded by not knowing an appropriate sequence to achieve their goal. Composition allows the programmer to define and call an abstract algorithm (AA) like a procedure. The compiler replaces an AA call with a sequence of library calls, while considering the calling context. Because AI planners compute a sequence of operations to reach a goal state, the compiler can implement composition by analyzing the calling context to provide the planner's initial state. Nevertheless, mapping composition onto planning is not straightforward because applying planning to software requires extensions to classical planning, and procedure specifications may be incomplete when expressed in a planning language. Compositions may not be provably correct, so our approach mitigates semantic incompleteness with unobtrusive programmer-compiler interaction. This tradeoff is key to making composition a practical and natural feature of otherwise imperative languages, whose users eschew complex logical specifications. Compositions satisfying an AA may not be equal in performance, memory usage, or precision and require selection of a preferred solution. We examine language design and implementation issues, and we perform a case study on the BioPerl bioinformatics library.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134003},
 doi = {http://doi.acm.org/10.1145/1133981.1134003},
 acmid = {1134003},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithm composition, algorithm selection, automated planning, bioinformatics, domain-specific languages},
} 

@article{Johnson:2006:CDA:1133255.1134003,
 author = {Johnson, Troy A. and Eigenmann, Rudolf},
 title = {Context-sensitive domain-independent algorithm composition and selection},
 abstract = {Progressing beyond the productivity of present-day languages appears to require using domain-specific knowledge. Domain-specific languages and libraries (DSLs) proliferate, but most optimizations and language features have limited portability because each language's semantics are related closely to its domain. We explain how any DSL compiler can use a domain-independent</i> AI planner to implement algorithm composition as a language feature</i>. Our notion of composition addresses a common DSL problem: good library designers tend to minimize redundancy by including only fundamental procedures that users must chain together into call sequences. Novice users are confounded by not knowing an appropriate sequence to achieve their goal. Composition allows the programmer to define and call an abstract algorithm (AA) like a procedure. The compiler replaces an AA call with a sequence of library calls, while considering the calling context. Because AI planners compute a sequence of operations to reach a goal state, the compiler can implement composition by analyzing the calling context to provide the planner's initial state. Nevertheless, mapping composition onto planning is not straightforward because applying planning to software requires extensions to classical planning, and procedure specifications may be incomplete when expressed in a planning language. Compositions may not be provably correct, so our approach mitigates semantic incompleteness with unobtrusive programmer-compiler interaction. This tradeoff is key to making composition a practical and natural feature of otherwise imperative languages, whose users eschew complex logical specifications. Compositions satisfying an AA may not be equal in performance, memory usage, or precision and require selection of a preferred solution. We examine language design and implementation issues, and we perform a case study on the BioPerl bioinformatics library.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134003},
 doi = {http://doi.acm.org/10.1145/1133255.1134003},
 acmid = {1134003},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithm composition, algorithm selection, automated planning, bioinformatics, domain-specific languages},
} 

@article{Chen:2006:RNE:1133255.1134004,
 author = {Chen, Guangyu and Li, Feihui and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Reducing NoC energy consumption through compiler-directed channel voltage scaling},
 abstract = {While scalable NoC (Network-on-Chip) based communication architectures have clear advantages over long point-to-point communication channels, their power consumption can be very high. In contrast to most of the existing hardware-based efforts on NoC power optimization, this paper proposes a compiler-directed approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better - from both performance and power perspectives - than a hardwarebased scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {193--203},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134004},
 doi = {http://doi.acm.org/10.1145/1133255.1134004},
 acmid = {1134004},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, energy, network-on-chip},
} 

@inproceedings{Chen:2006:RNE:1133981.1134004,
 author = {Chen, Guangyu and Li, Feihui and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Reducing NoC energy consumption through compiler-directed channel voltage scaling},
 abstract = {While scalable NoC (Network-on-Chip) based communication architectures have clear advantages over long point-to-point communication channels, their power consumption can be very high. In contrast to most of the existing hardware-based efforts on NoC power optimization, this paper proposes a compiler-directed approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better - from both performance and power perspectives - than a hardwarebased scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {193--203},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134004},
 doi = {http://doi.acm.org/10.1145/1133981.1134004},
 acmid = {1134004},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, energy, network-on-chip},
} 

@article{Koes:2006:GPR:1133255.1134006,
 author = {Koes, David Ryan and Goldstein, Seth Copen},
 title = {A global progressive register allocator},
 abstract = {This paper describes a global progressive register allocator, a register allocator that uses an expressive model of the register allocation problem to quickly find a good allocation and then progressively find better allocations until a provably optimal solution is found or a preset time limit is reached. The key contributions of this paper are an expressive model of global register allocation based on multicommodity network flows that explicitly represents spill code optimization, register preferences, copy insertion, and constant rematerialization; two fast, but effective, heuristic allocators based on this model; and a more elaborate progressive allocator that uses Lagrangian relaxation to compute the optimality of its allocations. Our progressive allocator demonstrates code size improvements as large as 16.75\% compared to a traditional graph allocator. On average, we observe an initial improvement of 3.47\%, which increases progressively to 6.84\% as more time is permitted for compilation.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {204--215},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134006},
 doi = {http://doi.acm.org/10.1145/1133255.1134006},
 acmid = {1134006},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {progressive solver, register alocation},
} 

@inproceedings{Koes:2006:GPR:1133981.1134006,
 author = {Koes, David Ryan and Goldstein, Seth Copen},
 title = {A global progressive register allocator},
 abstract = {This paper describes a global progressive register allocator, a register allocator that uses an expressive model of the register allocation problem to quickly find a good allocation and then progressively find better allocations until a provably optimal solution is found or a preset time limit is reached. The key contributions of this paper are an expressive model of global register allocation based on multicommodity network flows that explicitly represents spill code optimization, register preferences, copy insertion, and constant rematerialization; two fast, but effective, heuristic allocators based on this model; and a more elaborate progressive allocator that uses Lagrangian relaxation to compute the optimality of its allocations. Our progressive allocator demonstrates code size improvements as large as 16.75\% compared to a traditional graph allocator. On average, we observe an initial improvement of 3.47\%, which increases progressively to 6.84\% as more time is permitted for compilation.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {204--215},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134006},
 doi = {http://doi.acm.org/10.1145/1133981.1134006},
 acmid = {1134006},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {progressive solver, register alocation},
} 

@article{Nakaike:2006:PGL:1133255.1134007,
 author = {Nakaike, Takuya and Inagaki, Tatsushi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Profile-based global live-range splitting},
 abstract = {Live-range splitting is a technique to split the live range of a given variable into multiple subranges, each of which can be assigned to a different register or spilled out to memory in order to improve results of coloring register allocation. Previous techniques, such as aggressive live-range splitting, tend to produce extra spill code in the frequently executed (called hot) regions of the code, since they don't distinguish hot regions from others. We propose a new live-range splitting algorithm, which can reduce the amount of spill code in hot regions by coalescing the live ranges based on profile information after splitting the live ranges at every join and fork point in the control-flow graph. Our experimental results have shown that our new algorithm improved the performance of SPECjvm98 by up to 33\% over aggressive live-range splitting and 7\% over the base coloring algorithm without any live-range splitting.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {216--227},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134007},
 doi = {http://doi.acm.org/10.1145/1133255.1134007},
 acmid = {1134007},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, live-range splitting, register allocation},
} 

@inproceedings{Nakaike:2006:PGL:1133981.1134007,
 author = {Nakaike, Takuya and Inagaki, Tatsushi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Profile-based global live-range splitting},
 abstract = {Live-range splitting is a technique to split the live range of a given variable into multiple subranges, each of which can be assigned to a different register or spilled out to memory in order to improve results of coloring register allocation. Previous techniques, such as aggressive live-range splitting, tend to produce extra spill code in the frequently executed (called hot) regions of the code, since they don't distinguish hot regions from others. We propose a new live-range splitting algorithm, which can reduce the amount of spill code in hot regions by coalescing the live ranges based on profile information after splitting the live ranges at every join and fork point in the control-flow graph. Our experimental results have shown that our new algorithm improved the performance of SPECjvm98 by up to 33\% over aggressive live-range splitting and 7\% over the base coloring algorithm without any live-range splitting.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {216--227},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134007},
 doi = {http://doi.acm.org/10.1145/1133981.1134007},
 acmid = {1134007},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, live-range splitting, register allocation},
} 

@article{Bridges:2006:AIS:1133255.1134008,
 author = {Bridges, Matthew J. and Vachharajani, Neil and Ottoni, Guilherme and August, David I.},
 title = {Automatic instruction scheduler retargeting by reverse-engineering},
 abstract = {In order to generate high-quality code for modern processors, a compiler must aggressively schedule instructions, maximizing resource utilization for execution efficiency. For a compiler to produce such code, it must avoid structural hazards by being aware of the processor's available resources and of how these resources are utilized by each instruction. Unfortunately, the most prevalent approach to constructing such a scheduler, manually discovering and specifying this information, is both tedious and error-prone. This paper presents a new approach which, when given a processor or processor model, automatically determines this information. After establishing that the problem of perfectly determining a processor's structural hazards through probing is not solvable, this paper proposes a heuristic algorithm that discovers most of this information in practice. This can be used either to alleviate the problems associated with manual creation or to verify an existing specification. Scheduling with these automatically derived structural hazards yields almost all of the performance gain achieved using perfect hazard information.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {228--238},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134008},
 doi = {http://doi.acm.org/10.1145/1133255.1134008},
 acmid = {1134008},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic retargeting, compilers, instruction scheduling, reverse-engineering, structural hazard},
} 

@inproceedings{Bridges:2006:AIS:1133981.1134008,
 author = {Bridges, Matthew J. and Vachharajani, Neil and Ottoni, Guilherme and August, David I.},
 title = {Automatic instruction scheduler retargeting by reverse-engineering},
 abstract = {In order to generate high-quality code for modern processors, a compiler must aggressively schedule instructions, maximizing resource utilization for execution efficiency. For a compiler to produce such code, it must avoid structural hazards by being aware of the processor's available resources and of how these resources are utilized by each instruction. Unfortunately, the most prevalent approach to constructing such a scheduler, manually discovering and specifying this information, is both tedious and error-prone. This paper presents a new approach which, when given a processor or processor model, automatically determines this information. After establishing that the problem of perfectly determining a processor's structural hazards through probing is not solvable, this paper proposes a heuristic algorithm that discovers most of this information in practice. This can be used either to alleviate the problems associated with manual creation or to verify an existing specification. Scheduling with these automatically derived structural hazards yields almost all of the performance gain achieved using perfect hazard information.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {228--238},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134008},
 doi = {http://doi.acm.org/10.1145/1133981.1134008},
 acmid = {1134008},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic retargeting, compilers, instruction scheduling, reverse-engineering, structural hazard},
} 

@article{Lau:2006:OPA:1133255.1134010,
 author = {Lau, Jeremy and Arnold, Matthew and Hind, Michael and Calder, Brad},
 title = {Online performance auditing: using hot optimizations without getting burned},
 abstract = {As hardware complexity increases and virtualization is added at more layers of the execution stack, predicting the performance impact of optimizations becomes increasingly difficult. Production compilers and virtual machines invest substantial development effort in performance tuning to achieve good performance for a range of benchmarks. Although optimizations typically perform well on average, they often have unpredictable impact on running time, sometimes degrading performance significantly. Today's VMs perform sophisticated feedback-directed optimizations, but these techniques do not address performance degradations, and they actually make the situation worse by making the system more unpredictable.This paper presents an online framework for evaluating the effectiveness of optimizations, enabling an online system to automatically identify and correct performance anomalies that occur at runtime. This work opens the door for a fundamental shift in the way optimizations are developed and tuned for online systems, and may allow the body of work in offline empirical optimization search to be applied automatically at runtime. We present our implementation and evaluation of this system in a product Java VM.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {239--251},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133255.1134010},
 doi = {http://doi.acm.org/10.1145/1133255.1134010},
 acmid = {1134010},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, feedback-directed optmizations, virtual machines},
} 

@inproceedings{Lau:2006:OPA:1133981.1134010,
 author = {Lau, Jeremy and Arnold, Matthew and Hind, Michael and Calder, Brad},
 title = {Online performance auditing: using hot optimizations without getting burned},
 abstract = {As hardware complexity increases and virtualization is added at more layers of the execution stack, predicting the performance impact of optimizations becomes increasingly difficult. Production compilers and virtual machines invest substantial development effort in performance tuning to achieve good performance for a range of benchmarks. Although optimizations typically perform well on average, they often have unpredictable impact on running time, sometimes degrading performance significantly. Today's VMs perform sophisticated feedback-directed optimizations, but these techniques do not address performance degradations, and they actually make the situation worse by making the system more unpredictable.This paper presents an online framework for evaluating the effectiveness of optimizations, enabling an online system to automatically identify and correct performance anomalies that occur at runtime. This work opens the door for a fundamental shift in the way optimizations are developed and tuned for online systems, and may allow the body of work in offline empirical optimization search to be applied automatically at runtime. We present our implementation and evaluation of this system in a product Java VM.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {239--251},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133981.1134010},
 doi = {http://doi.acm.org/10.1145/1133981.1134010},
 acmid = {1134010},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, feedback-directed optmizations, virtual machines},
} 

@article{Chilimbi:2006:CCH:1133255.1134011,
 author = {Chilimbi, Trishul M. and Shaham, Ran},
 title = {Cache-conscious coallocation of hot data streams},
 abstract = {The memory system performance of many programs can be improved by coallocating contemporaneously accessed heap objects in the same cache block. We present a novel profile-based analysis for producing such a layout. The analysis achieves cacheconscious coallocation of a hot data stream H</i> (i.e., a regular data access pattern that frequently repeats) by isolating and combining allocation sites of object instances that appear in H</i> such that intervening allocations coming from other sites are separated. The coallocation solution produced by the analysis is enforced by an automatic tool, cminstr</i>, that redirects a program's heap allocations to a run-time coallocation library comalloc</i>. We also extend the analysis to coallocation at object field granularity. The resulting field coallocation solution generalizes common data restructuring techniques, such as field reordering, object splitting, and object merging, and allows their combination. Furthermore, it provides insight into object restructuring by breaking down the coallocation benefit on a per-technique basis, which provides the opportunity to pick the "sweet spot" for each program. Experimental results using a set of memory-performance-limited benchmarks, including a few SPECInt2000 programs, and Microsoft VisualFoxPro, indicate that programs possess significant coallocation opportunities. Automatic object coallocation improves execution time by 13\% on average in the presence of hardware prefetching. Hand-implemented field coallocation solutions for two of the benchmarks produced additional improvements (12\% and 22\%) but the effort involved suggests implementing an automated version for type-safe languages, such as Java and C#.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134011},
 doi = {http://doi.acm.org/10.1145/1133255.1134011},
 acmid = {1134011},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache optimization, data locality, data profiling, dynamic allocation, hot data streams, memory layout},
} 

@inproceedings{Chilimbi:2006:CCH:1133981.1134011,
 author = {Chilimbi, Trishul M. and Shaham, Ran},
 title = {Cache-conscious coallocation of hot data streams},
 abstract = {The memory system performance of many programs can be improved by coallocating contemporaneously accessed heap objects in the same cache block. We present a novel profile-based analysis for producing such a layout. The analysis achieves cacheconscious coallocation of a hot data stream H</i> (i.e., a regular data access pattern that frequently repeats) by isolating and combining allocation sites of object instances that appear in H</i> such that intervening allocations coming from other sites are separated. The coallocation solution produced by the analysis is enforced by an automatic tool, cminstr</i>, that redirects a program's heap allocations to a run-time coallocation library comalloc</i>. We also extend the analysis to coallocation at object field granularity. The resulting field coallocation solution generalizes common data restructuring techniques, such as field reordering, object splitting, and object merging, and allows their combination. Furthermore, it provides insight into object restructuring by breaking down the coallocation benefit on a per-technique basis, which provides the opportunity to pick the "sweet spot" for each program. Experimental results using a set of memory-performance-limited benchmarks, including a few SPECInt2000 programs, and Microsoft VisualFoxPro, indicate that programs possess significant coallocation opportunities. Automatic object coallocation improves execution time by 13\% on average in the presence of hardware prefetching. Hand-implemented field coallocation solutions for two of the benchmarks produced additional improvements (12\% and 22\%) but the effort involved suggests implementing an automated version for type-safe languages, such as Java and C#.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134011},
 doi = {http://doi.acm.org/10.1145/1133981.1134011},
 acmid = {1134011},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache optimization, data locality, data profiling, dynamic allocation, hot data streams, memory layout},
} 

@article{Zhuang:2006:AEA:1133255.1134012,
 author = {Zhuang, Xiaotong and Serrano, Mauricio J. and Cain, Harold W. and Choi, Jong-Deok},
 title = {Accurate, efficient, and adaptive calling context profiling},
 abstract = {Calling context profiles are used in many inter-procedural code optimizations and in overall program understanding. Unfortunately, the collection of profile information is highly intrusive due to the high frequency of method calls in most applications. Previously proposed calling-context profiling mechanisms consequently suffer from either low accuracy, high overhead, or both. We have developed a new approach for building the calling context tree at runtime, called adaptive bursting</i>. By selectively inhibiting redundant profiling, this approach dramatically reduces overhead while preserving profile accuracy. We first demonstrate the drawbacks of previously proposed calling context profiling mechanisms. We show that a low-overhead solution using sampled stack-walking alone is less than 50\% accurate, based on degree of overlap with a complete calling-context tree. We also show that a static bursting approach collects a highly accurate profile, but causes an unacceptable application slowdown. Our adaptive solution achieves 85\% degree of overlap and provides an 88\% hot-edge coverage when using a 0.1 hot-edge threshold, while dramatically reducing overhead compared to the static bursting approach.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {263--271},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133255.1134012},
 doi = {http://doi.acm.org/10.1145/1133255.1134012},
 acmid = {1134012},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, call graph, calling context, calling context tree, java virtual machine, profiling},
} 

@inproceedings{Zhuang:2006:AEA:1133981.1134012,
 author = {Zhuang, Xiaotong and Serrano, Mauricio J. and Cain, Harold W. and Choi, Jong-Deok},
 title = {Accurate, efficient, and adaptive calling context profiling},
 abstract = {Calling context profiles are used in many inter-procedural code optimizations and in overall program understanding. Unfortunately, the collection of profile information is highly intrusive due to the high frequency of method calls in most applications. Previously proposed calling-context profiling mechanisms consequently suffer from either low accuracy, high overhead, or both. We have developed a new approach for building the calling context tree at runtime, called adaptive bursting</i>. By selectively inhibiting redundant profiling, this approach dramatically reduces overhead while preserving profile accuracy. We first demonstrate the drawbacks of previously proposed calling context profiling mechanisms. We show that a low-overhead solution using sampled stack-walking alone is less than 50\% accurate, based on degree of overlap with a complete calling-context tree. We also show that a static bursting approach collects a highly accurate profile, but causes an unacceptable application slowdown. Our adaptive solution achieves 85\% degree of overlap and provides an 88\% hot-edge coverage when using a 0.1 hot-edge threshold, while dramatically reducing overhead compared to the static bursting approach.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {263--271},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133981.1134012},
 doi = {http://doi.acm.org/10.1145/1133981.1134012},
 acmid = {1134012},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, call graph, calling context, calling context tree, java virtual machine, profiling},
} 

@article{Jarvi:2006:ASG:1133255.1134014,
 author = {J\"{a}rvi, Jaakko and Gregor, Douglas and Willcock, Jeremiah and Lumsdaine, Andrew and Siek, Jeremy},
 title = {Algorithm specialization in generic programming: challenges of constrained generics in C++},
 abstract = {Generic programming has recently emerged as a paradigm for developing highly reusable software libraries, most notably in C++. We have designed and implemented a constrained generics extension for C++ to support modular type checking of generic algorithms and to address other issues associated with unconstrained generics. To be as broadly applicable as possible, generic algorithms are defined with minimal requirements on their inputs. At the same time, to achieve a high degree of efficiency, generic algorithms may have multiple implementations that exploit features of specific classes of inputs. This process of algorithm specialization relies on non-local type information and conflicts directly with the local nature of modular type checking. In this paper, we review the design and implementation of our extensions for generic programming in C++, describe the issues of algorithm specialization and modular type checking in detail, and discuss the important design tradeoffs in trying to accomplish both.We present the particular design that we chose for our implementation, with the goal of hitting the sweet spot in this interesting design space.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134014},
 doi = {http://doi.acm.org/10.1145/1133255.1134014},
 acmid = {1134014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concepts, constrained generics, generic programming, parametric polymorphism, specialization},
} 

@inproceedings{Jarvi:2006:ASG:1133981.1134014,
 author = {J\"{a}rvi, Jaakko and Gregor, Douglas and Willcock, Jeremiah and Lumsdaine, Andrew and Siek, Jeremy},
 title = {Algorithm specialization in generic programming: challenges of constrained generics in C++},
 abstract = {Generic programming has recently emerged as a paradigm for developing highly reusable software libraries, most notably in C++. We have designed and implemented a constrained generics extension for C++ to support modular type checking of generic algorithms and to address other issues associated with unconstrained generics. To be as broadly applicable as possible, generic algorithms are defined with minimal requirements on their inputs. At the same time, to achieve a high degree of efficiency, generic algorithms may have multiple implementations that exploit features of specific classes of inputs. This process of algorithm specialization relies on non-local type information and conflicts directly with the local nature of modular type checking. In this paper, we review the design and implementation of our extensions for generic programming in C++, describe the issues of algorithm specialization and modular type checking in detail, and discuss the important design tradeoffs in trying to accomplish both.We present the particular design that we chose for our implementation, with the goal of hitting the sweet spot in this interesting design space.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134014},
 doi = {http://doi.acm.org/10.1145/1133981.1134014},
 acmid = {1134014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concepts, constrained generics, generic programming, parametric polymorphism, specialization},
} 

@article{Spoonhower:2006:ESP:1133255.1134015,
 author = {Spoonhower, Daniel and Auerbach, Joshua and Bacon, David F. and Cheng, Perry and Grove, David},
 title = {Eventrons: a safe programming construct for high-frequency hard real-time applications},
 abstract = {While real-time garbage collection has achieved worst-case latencies on the order of a millisecond, this technology is approaching its practical limits. For tasks requiring extremely low latency, and especially periodic tasks with frequencies above 1 KHz, Java programmers must currently resort to the NoHeapRealtimeThread construct of the Real-Time Specification for Java. This technique requires expensive run-time checks, can result in unpredictable low-level exceptions, and inhibits communication with the rest of the garbage-collected application. We present Eventrons</i>, a programming construct that can arbitrarily preempt the garbage collector, yet guarantees safety and</i> allows its data to be visible to the garbage-collected heap. Eventrons are a strict subset of Java, and require no run-time memory access checks. Safety is enforced using a data-sensitive</i> analysis and simple run-time support with extremely low overhead. We have implemented Eventrons in IBM's J9 Java virtual machine, and present experimental results in which we ran Eventrons at frequencies up to 22 KHz (a 45 \&#956;</i>s period). Across 10 million periods, 99.997\% of the executions ran within 10 \&#956;</i>ss of their deadline, compared to 99.999\% of the executions of the equivalent program written in C.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134015},
 doi = {http://doi.acm.org/10.1145/1133255.1134015},
 acmid = {1134015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocation, real-time, scheduling},
} 

@inproceedings{Spoonhower:2006:ESP:1133981.1134015,
 author = {Spoonhower, Daniel and Auerbach, Joshua and Bacon, David F. and Cheng, Perry and Grove, David},
 title = {Eventrons: a safe programming construct for high-frequency hard real-time applications},
 abstract = {While real-time garbage collection has achieved worst-case latencies on the order of a millisecond, this technology is approaching its practical limits. For tasks requiring extremely low latency, and especially periodic tasks with frequencies above 1 KHz, Java programmers must currently resort to the NoHeapRealtimeThread construct of the Real-Time Specification for Java. This technique requires expensive run-time checks, can result in unpredictable low-level exceptions, and inhibits communication with the rest of the garbage-collected application. We present Eventrons</i>, a programming construct that can arbitrarily preempt the garbage collector, yet guarantees safety and</i> allows its data to be visible to the garbage-collected heap. Eventrons are a strict subset of Java, and require no run-time memory access checks. Safety is enforced using a data-sensitive</i> analysis and simple run-time support with extremely low overhead. We have implemented Eventrons in IBM's J9 Java virtual machine, and present experimental results in which we ran Eventrons at frequencies up to 22 KHz (a 45 \&#956;</i>s period). Across 10 million periods, 99.997\% of the executions ran within 10 \&#956;</i>ss of their deadline, compared to 99.999\% of the executions of the equivalent program written in C.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134015},
 doi = {http://doi.acm.org/10.1145/1133981.1134015},
 acmid = {1134015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocation, real-time, scheduling},
} 

@inproceedings{Shivers:2006:CTC:1133981.1134016,
 author = {Shivers, Olin and Might, Matthew},
 title = {Continuations and transducer composition},
 abstract = {On-line transducers are an important class of computational agent; we construct and compose together many software systems using them, such as stream processors, layered network protocols, DSP networks and graphics pipelines. We show an interesting use of continuations, that, when taken in a CPS setting, exposes the control flow of these systems. This enables a CPS-based compiler to optimise systems composed of these transducers, using only standard, known analyses and optimisations. Critically, the analysis permits optimisation across the composition of these transducers, allowing efficient construction of systems in a hierarchical way.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {295--307},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133981.1134016},
 doi = {http://doi.acm.org/10.1145/1133981.1134016},
 acmid = {1134016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {calculus, continuation-passing style (CPS), continuations, coroutines, flow analysis, functional languages, fusion, lambda, language design, program analysis, stream processing},
} 

@article{Shivers:2006:CTC:1133255.1134016,
 author = {Shivers, Olin and Might, Matthew},
 title = {Continuations and transducer composition},
 abstract = {On-line transducers are an important class of computational agent; we construct and compose together many software systems using them, such as stream processors, layered network protocols, DSP networks and graphics pipelines. We show an interesting use of continuations, that, when taken in a CPS setting, exposes the control flow of these systems. This enables a CPS-based compiler to optimise systems composed of these transducers, using only standard, known analyses and optimisations. Critically, the analysis permits optimisation across the composition of these transducers, allowing efficient construction of systems in a hierarchical way.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {295--307},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133255.1134016},
 doi = {http://doi.acm.org/10.1145/1133255.1134016},
 acmid = {1134016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {calculus, continuation-passing style (CPS), continuations, coroutines, flow analysis, functional languages, fusion, lambda, language design, program analysis, stream processing},
} 

@inproceedings{Naik:2006:ESR:1133981.1134018,
 author = {Naik, Mayur and Aiken, Alex and Whaley, John},
 title = {Effective static race detection for Java},
 abstract = {We present a novel technique for static race detection in Java programs, comprised of a series of stages that employ a combination of static analyses to successively reduce the pairs of memory accesses potentially involved in a race. We have implemented our technique and applied it to a suite of multi-threaded Java programs. Our experiments show that it is precise, scalable, and useful, reporting tens to hundreds of serious and previously unknown concurrency bugs in large, widely-used programs with few false alarms.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {308--319},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134018},
 doi = {http://doi.acm.org/10.1145/1133981.1134018},
 acmid = {1134018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, concurrency, multi-threading, static race detection, synchronization},
} 

@article{Naik:2006:ESR:1133255.1134018,
 author = {Naik, Mayur and Aiken, Alex and Whaley, John},
 title = {Effective static race detection for Java},
 abstract = {We present a novel technique for static race detection in Java programs, comprised of a series of stages that employ a combination of static analyses to successively reduce the pairs of memory accesses potentially involved in a race. We have implemented our technique and applied it to a suite of multi-threaded Java programs. Our experiments show that it is precise, scalable, and useful, reporting tens to hundreds of serious and previously unknown concurrency bugs in large, widely-used programs with few false alarms.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {308--319},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134018},
 doi = {http://doi.acm.org/10.1145/1133255.1134018},
 acmid = {1134018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, concurrency, multi-threading, static race detection, synchronization},
} 

@article{Pratikakis:2006:LCC:1133255.1134019,
 author = {Pratikakis, Polyvios and Foster, Jeffrey S. and Hicks, Michael},
 title = {LOCKSMITH: context-sensitive correlation analysis for race detection},
 abstract = {One common technique for preventing data races in multi-threaded programs is to ensure that all accesses to shared locations are consistently protected by a lock. We present a tool called LOCKSMITH for detecting data races in C programs by looking for violations of this pattern. We call the relationship between locks and the locations they protect consistent correlation</i>, and the core of our technique is a novel constraint-based analysis that infers consistent correlation context-sensitively, using the results to check that locations are properly guarded by locks. We present the core of our algorithm for a simple formal language \&#955;<sub>\&gt;</sub> which we have proven sound, and discuss how we scale it up to an algorithm that aims to be sound for all of C. We develop several techniques to improve the precision and performance of the analysis, including a sharing analysis for inferring thread locality; existential quantification for modeling locks in data structures; and heuristics for modeling unsafe features of C such as type casts. When applied to several benchmarks, including multi-threaded servers and Linux device drivers, LOCKSMITH found several races while producing a modest number of false alarm.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {320--331},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134019},
 doi = {http://doi.acm.org/10.1145/1133255.1134019},
 acmid = {1134019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitivity, correlation, locksmith, multi-threaded programming, race detection, type inference},
} 

@inproceedings{Pratikakis:2006:LCC:1133981.1134019,
 author = {Pratikakis, Polyvios and Foster, Jeffrey S. and Hicks, Michael},
 title = {LOCKSMITH: context-sensitive correlation analysis for race detection},
 abstract = {One common technique for preventing data races in multi-threaded programs is to ensure that all accesses to shared locations are consistently protected by a lock. We present a tool called LOCKSMITH for detecting data races in C programs by looking for violations of this pattern. We call the relationship between locks and the locations they protect consistent correlation</i>, and the core of our technique is a novel constraint-based analysis that infers consistent correlation context-sensitively, using the results to check that locations are properly guarded by locks. We present the core of our algorithm for a simple formal language \&#955;<sub>\&gt;</sub> which we have proven sound, and discuss how we scale it up to an algorithm that aims to be sound for all of C. We develop several techniques to improve the precision and performance of the analysis, including a sharing analysis for inferring thread locality; existential quantification for modeling locks in data structures; and heuristics for modeling unsafe features of C such as type casts. When applied to several benchmarks, including multi-threaded servers and Linux device drivers, LOCKSMITH found several races while producing a modest number of false alarm.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {320--331},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134019},
 doi = {http://doi.acm.org/10.1145/1133981.1134019},
 acmid = {1134019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitivity, correlation, locksmith, multi-threaded programming, race detection, type inference},
} 

@inproceedings{Chen:2006:PPG:1133981.1134021,
 author = {Chen, Wen-ke and Bhansali, Sanjay and Chilimbi, Trishul and Gao, Xiaofeng and Chuang, Weihaw},
 title = {Profile-guided proactive garbage collection for locality optimization},
 abstract = {Many applications written in garbage collected languages have large dynamic working sets and poor data locality. We present a new system for continuously improving program data locality at run time with low overhead. Our system proactively reorganizes the heap by leveraging the garbage collector and uses profile information collected through a low-overhead mechanism to guide the reorganization at run time. The key contributions include making a case that garbage collection should be viewed as a proactive technique for improving data locality by triggering garbage collection for locality optimization independently of normal garbage collection for space, combining page and cache locality optimization in the same system, and demonstrating that sampling provides sufficiently detailed data access information to guide both page and cache locality optimization with low runtime overhead. We present experimental results obtained by modifying a commercial, state-of-the-art garbage collector to support our claims. Independently triggering garbage collection for locality optimization significantly improved optimizations benefits. Combining page and cache locality optimizations in the same system provided larger average execution time improvements (17\%) than either alone (page 8\%, cache 7\%). Finally, using sampling limited profiling overhead to less than 3\%, on average.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {332--340},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133981.1134021},
 doi = {http://doi.acm.org/10.1145/1133981.1134021},
 acmid = {1134021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache optimization, data locality, garbage collectors, memory optimization, page optimization},
} 

@article{Chen:2006:PPG:1133255.1134021,
 author = {Chen, Wen-ke and Bhansali, Sanjay and Chilimbi, Trishul and Gao, Xiaofeng and Chuang, Weihaw},
 title = {Profile-guided proactive garbage collection for locality optimization},
 abstract = {Many applications written in garbage collected languages have large dynamic working sets and poor data locality. We present a new system for continuously improving program data locality at run time with low overhead. Our system proactively reorganizes the heap by leveraging the garbage collector and uses profile information collected through a low-overhead mechanism to guide the reorganization at run time. The key contributions include making a case that garbage collection should be viewed as a proactive technique for improving data locality by triggering garbage collection for locality optimization independently of normal garbage collection for space, combining page and cache locality optimization in the same system, and demonstrating that sampling provides sufficiently detailed data access information to guide both page and cache locality optimization with low runtime overhead. We present experimental results obtained by modifying a commercial, state-of-the-art garbage collector to support our claims. Independently triggering garbage collection for locality optimization significantly improved optimizations benefits. Combining page and cache locality optimizations in the same system provided larger average execution time improvements (17\%) than either alone (page 8\%, cache 7\%). Finally, using sampling limited profiling overhead to less than 3\%, on average.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {332--340},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1133255.1134021},
 doi = {http://doi.acm.org/10.1145/1133255.1134021},
 acmid = {1134021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache optimization, data locality, garbage collectors, memory optimization, page optimization},
} 

@inproceedings{Vechev:2006:CDC:1133981.1134022,
 author = {Vechev, Martin T. and Yahav, Eran and Bacon, David F.},
 title = {Correctness-preserving derivation of concurrent garbage collection algorithms},
 abstract = {Constructing correct concurrent garbage collection algorithms is notoriously hard. Numerous such algorithms have been proposed, implemented, and deployed - and yet the relationship among them in terms of speed and precision is poorly understood, and the validation of one algorithm does not carry over to others.As programs with low latency requirements written in garbagecollected languages become part of society's mission-critical infrastructure, it is imperative that we raise the level of confidence in the correctness of the underlying system, and that we understand the trade-offs inherent in our algorithmic choice.In this paper we present correctness-preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler, more precise, and easier to prove correct than algorithms used in practice--but also more expensive and with less concurrency. We then show how both pre-existing and new algorithms can be synthesized from the abstract algorithm by a series of our transformations. We relate the algorithms formally using a new definition of precision, and informally with respect to overhead and concurrency.This provides many insights about the nature of concurrent collection, allows the direct synthesis of new and useful algorithms, reduces the burden of proof to a single simple algorithm, and lays the groundwork for the automated synthesis of correct concurrent collectors.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {341--353},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133981.1134022},
 doi = {http://doi.acm.org/10.1145/1133981.1134022},
 acmid = {1134022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent algorithms, concurrent garbage collection, synthesis, verification},
} 

@article{Vechev:2006:CDC:1133255.1134022,
 author = {Vechev, Martin T. and Yahav, Eran and Bacon, David F.},
 title = {Correctness-preserving derivation of concurrent garbage collection algorithms},
 abstract = {Constructing correct concurrent garbage collection algorithms is notoriously hard. Numerous such algorithms have been proposed, implemented, and deployed - and yet the relationship among them in terms of speed and precision is poorly understood, and the validation of one algorithm does not carry over to others.As programs with low latency requirements written in garbagecollected languages become part of society's mission-critical infrastructure, it is imperative that we raise the level of confidence in the correctness of the underlying system, and that we understand the trade-offs inherent in our algorithmic choice.In this paper we present correctness-preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler, more precise, and easier to prove correct than algorithms used in practice--but also more expensive and with less concurrency. We then show how both pre-existing and new algorithms can be synthesized from the abstract algorithm by a series of our transformations. We relate the algorithms formally using a new definition of precision, and informally with respect to overhead and concurrency.This provides many insights about the nature of concurrent collection, allows the direct synthesis of new and useful algorithms, reduces the burden of proof to a single simple algorithm, and lays the groundwork for the automated synthesis of correct concurrent collectors.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {341--353},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1133255.1134022},
 doi = {http://doi.acm.org/10.1145/1133255.1134022},
 acmid = {1134022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent algorithms, concurrent garbage collection, synthesis, verification},
} 

@article{Kermany:2006:CCI:1133255.1134023,
 author = {Kermany, Haim and Petrank, Erez},
 title = {The Compressor: concurrent, incremental, and parallel compaction},
 abstract = {The widely used Mark-and-Sweep garbage collector has a drawback in that it does not move objects during collection. As a result, large long-running realistic applications, such as Web application servers, frequently face the fragmentation problem. To eliminate fragmentation, a heap compaction is run periodically. However, compaction typically imposes very long undesirable pauses in the application. While efficient concurrent collectors are ubiquitous in production runtime systems (such as JVMs), an efficient non-intrusive compactor is still missing.In this paper we present the Compressor, a novel compaction algorithm that is concurrent, parallel, and incremental. The Compressor compacts the entire heap to a single condensed area, while preserving the objects' order, but reduces pause times significantly, thereby allowing acceptable runs on large heaps. Furthermore, the Compressor is the first compactor that requires only a single heap pass. As such, it is the most efficient compactors known today, even when run in a parallel Stop-the-World manner (i.e., when the program threads are halted). Thus, to the best of our knowledge, the Compressor is the most efficient compactor known today. The Compressor was implemented on a Jikes Research RVM and we provide measurements demonstrating its qualities.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {354--363},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1133255.1134023},
 doi = {http://doi.acm.org/10.1145/1133255.1134023},
 acmid = {1134023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent garbage collection, garbage collection, memory management, runtime systems},
} 

@inproceedings{Kermany:2006:CCI:1133981.1134023,
 author = {Kermany, Haim and Petrank, Erez},
 title = {The Compressor: concurrent, incremental, and parallel compaction},
 abstract = {The widely used Mark-and-Sweep garbage collector has a drawback in that it does not move objects during collection. As a result, large long-running realistic applications, such as Web application servers, frequently face the fragmentation problem. To eliminate fragmentation, a heap compaction is run periodically. However, compaction typically imposes very long undesirable pauses in the application. While efficient concurrent collectors are ubiquitous in production runtime systems (such as JVMs), an efficient non-intrusive compactor is still missing.In this paper we present the Compressor, a novel compaction algorithm that is concurrent, parallel, and incremental. The Compressor compacts the entire heap to a single condensed area, while preserving the objects' order, but reduces pause times significantly, thereby allowing acceptable runs on large heaps. Furthermore, the Compressor is the first compactor that requires only a single heap pass. As such, it is the most efficient compactors known today, even when run in a parallel Stop-the-World manner (i.e., when the program threads are halted). Thus, to the best of our knowledge, the Compressor is the most efficient compactor known today. The Compressor was implemented on a Jikes Research RVM and we provide measurements demonstrating its qualities.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {354--363},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1133981.1134023},
 doi = {http://doi.acm.org/10.1145/1133981.1134023},
 acmid = {1134023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent garbage collection, garbage collection, memory management, runtime systems},
} 

@inproceedings{Guyer:2006:FSA:1133981.1134024,
 author = {Guyer, Samuel Z. and McKinley, Kathryn S. and Frampton, Daniel},
 title = {Free-Me: a static analysis for automatic individual object reclamation},
 abstract = {Garbage collection has proven benefits, including fewer memory related errors and reduced programmer effort. Garbage collection, however, trades space for time. It reclaims memory only when it is invoked: invoking it more frequently reclaims memory quickly, but incurs a significant cost; invoking it less frequently fills memory with dead objects. In contrast, explicit memory management provides prompt low cost reclamation, but at the expense of programmer effort.This work comes closer to the best of both worlds by adding novel compiler and runtime support for compiler inserted frees to a garbage-collected system. The compiler's free-me analysis identifies when objects become unreachable and inserts calls to free. It combines a lightweight pointer analysis with liveness information that detects when short-lived objects die. Our approach differs from stack and region allocation in two crucial ways. First, it frees objects incrementally exactly when they become unreachable, instead of based on program scope. Second, our system does not require allocation-site lifetime homogeneity, and thus frees objects on some paths and not on others. It also handles common patterns: it can free objects in loops and objects created by factory methods.We evaluate free() variations for free-list and bump-pointer allocators. Explicit freeing improves performance by promptly reclaiming objects and reducing collection load. Compared to marksweep alone, free-me cuts total time by 22\% on average, collector time by 50\% to 70\%, and allows programs to run in 17\% less memory. This combination retains the software engineering benefits of garbage collection while increasing space efficiency and improving performance, and thus is especially appealing for real-time and space constrained systems.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {364--375},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134024},
 doi = {http://doi.acm.org/10.1145/1133981.1134024},
 acmid = {1134024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, compiler-assisted, copying, generational, liveness, locality, mark-sweep, pointer analysis},
} 

@article{Guyer:2006:FSA:1133255.1134024,
 author = {Guyer, Samuel Z. and McKinley, Kathryn S. and Frampton, Daniel},
 title = {Free-Me: a static analysis for automatic individual object reclamation},
 abstract = {Garbage collection has proven benefits, including fewer memory related errors and reduced programmer effort. Garbage collection, however, trades space for time. It reclaims memory only when it is invoked: invoking it more frequently reclaims memory quickly, but incurs a significant cost; invoking it less frequently fills memory with dead objects. In contrast, explicit memory management provides prompt low cost reclamation, but at the expense of programmer effort.This work comes closer to the best of both worlds by adding novel compiler and runtime support for compiler inserted frees to a garbage-collected system. The compiler's free-me analysis identifies when objects become unreachable and inserts calls to free. It combines a lightweight pointer analysis with liveness information that detects when short-lived objects die. Our approach differs from stack and region allocation in two crucial ways. First, it frees objects incrementally exactly when they become unreachable, instead of based on program scope. Second, our system does not require allocation-site lifetime homogeneity, and thus frees objects on some paths and not on others. It also handles common patterns: it can free objects in loops and objects created by factory methods.We evaluate free() variations for free-list and bump-pointer allocators. Explicit freeing improves performance by promptly reclaiming objects and reducing collection load. Compared to marksweep alone, free-me cuts total time by 22\% on average, collector time by 50\% to 70\%, and allows programs to run in 17\% less memory. This combination retains the software engineering benefits of garbage collection while increasing space efficiency and improving performance, and thus is especially appealing for real-time and space constrained systems.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {364--375},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134024},
 doi = {http://doi.acm.org/10.1145/1133255.1134024},
 acmid = {1134024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, compiler-assisted, copying, generational, liveness, locality, mark-sweep, pointer analysis},
} 

@article{Gulwani:2006:CAI:1133255.1134026,
 author = {Gulwani, Sumit and Tiwari, Ashish},
 title = {Combining abstract interpreters},
 abstract = {We present a methodology for automatically combining abstract interpreters over given lattices to construct an abstract interpreter for the combination of those lattices. This lends modularity to the process of design and implementation of abstract interpreters.We define the notion of logical product of lattices. This kind of combination is more precise than the reduced product combination. We give algorithms to obtain the join operator and the existential quantification operator for the combined lattice from the corresponding operators of the individual lattices. We also give a bound on the number of steps required to reach a fixed point across loops during analysis over the combined lattice in terms of the corresponding bounds for the individual lattices. We prove that our combination methodology yields the most precise abstract interpretation operators over the logical product of lattices when the individual lattices are over theories that are convex, stably infinite, and disjoint.We also present an interesting application of logical product wherein some lattices can be reduced to combination of other (unrelated) lattices with known abstract interpreters.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {376--386},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134026},
 doi = {http://doi.acm.org/10.1145/1133255.1134026},
 acmid = {1134026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Nelson-Oppen combination, abstract interpreter, logical product, reduced product},
} 

@inproceedings{Gulwani:2006:CAI:1133981.1134026,
 author = {Gulwani, Sumit and Tiwari, Ashish},
 title = {Combining abstract interpreters},
 abstract = {We present a methodology for automatically combining abstract interpreters over given lattices to construct an abstract interpreter for the combination of those lattices. This lends modularity to the process of design and implementation of abstract interpreters.We define the notion of logical product of lattices. This kind of combination is more precise than the reduced product combination. We give algorithms to obtain the join operator and the existential quantification operator for the combined lattice from the corresponding operators of the individual lattices. We also give a bound on the number of steps required to reach a fixed point across loops during analysis over the combined lattice in terms of the corresponding bounds for the individual lattices. We prove that our combination methodology yields the most precise abstract interpretation operators over the logical product of lattices when the individual lattices are over theories that are convex, stably infinite, and disjoint.We also present an interesting application of logical product wherein some lattices can be reduced to combination of other (unrelated) lattices with known abstract interpreters.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {376--386},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133981.1134026},
 doi = {http://doi.acm.org/10.1145/1133981.1134026},
 acmid = {1134026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Nelson-Oppen combination, abstract interpreter, logical product, reduced product},
} 

@article{Sridharan:2006:RCP:1133255.1134027,
 author = {Sridharan, Manu and Bod\'{\i}k, Rastislav},
 title = {Refinement-based context-sensitive points-to analysis for Java},
 abstract = {We present a scalable and precise context-sensitive points-to analysis with three key properties: (1) filtering out of unrealizable paths, (2) a context-sensitive heap abstraction, and (3) a context-sensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinement-based analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and client-driven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61\% more casts than one of the most precise existing analyses across a suite of large benchmarks. The analysis checked the casts in under 13 minutes per benchmark (taking less than 1 second per query) and required only 35MB of memory, far less than previous approaches.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {387--400},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133255.1134027},
 doi = {http://doi.acm.org/10.1145/1133255.1134027},
 acmid = {1134027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive analysis, demand-driven analysis, points-to analysis, refinement},
} 

@inproceedings{Sridharan:2006:RCP:1133981.1134027,
 author = {Sridharan, Manu and Bod\'{\i}k, Rastislav},
 title = {Refinement-based context-sensitive points-to analysis for Java},
 abstract = {We present a scalable and precise context-sensitive points-to analysis with three key properties: (1) filtering out of unrealizable paths, (2) a context-sensitive heap abstraction, and (3) a context-sensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinement-based analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and client-driven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61\% more casts than one of the most precise existing analyses across a suite of large benchmarks. The analysis checked the casts in under 13 minutes per benchmark (taking less than 1 second per query) and required only 35MB of memory, far less than previous approaches.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {387--400},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133981.1134027},
 doi = {http://doi.acm.org/10.1145/1133981.1134027},
 acmid = {1134027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive analysis, demand-driven analysis, points-to analysis, refinement},
} 

@article{Feng:2006:MVA:1133255.1134028,
 author = {Feng, Xinyu and Shao, Zhong and Vaynberg, Alexander and Xiang, Sen and Ni, Zhaozhong},
 title = {Modular verification of assembly code with stack-based control abstractions},
 abstract = {Runtime stacks are critical components of any modern software--they are used to implement powerful control structures such as function call/return, stack cutting and unwinding, coroutines, and thread context switch. Stack operations, however, are very hard to reason about: there are no known formal specifications for certifying C-style setjmp/longjmp, stack cutting and unwinding, or weak continuations (in C--). In many proof-carrying code (PCC) systems, return code pointers and exception handlers are treated as general first-class functions (as in continuation-passing style) even though both should have more limited scopes.In this paper we show that stack-based control abstractions follow a much simpler pattern than general first-class code pointers. We present a simple but flexible Hoare-style framework for modular verification of assembly code with all kinds of stackbased control abstractions, including function call/return, tail call, setjmp/longjmp, weak continuation, stack cutting, stack unwinding, multi-return function call, coroutines, and thread context switch. Instead of presenting a specific logic for each control structure, we develop all reasoning systems as instances of a generic framework. This allows program modules and their proofs developed in different PCC systems to be linked together. Our system is fully mechanized. We give the complete soundness proof and a full verification of several examples in the Coq proof assistant.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {401--414},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133255.1134028},
 doi = {http://doi.acm.org/10.1145/1133255.1134028},
 acmid = {1134028},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assembly code verification, control abstractions, modularity, proof-carrying code, stack-based},
} 

@inproceedings{Feng:2006:MVA:1133981.1134028,
 author = {Feng, Xinyu and Shao, Zhong and Vaynberg, Alexander and Xiang, Sen and Ni, Zhaozhong},
 title = {Modular verification of assembly code with stack-based control abstractions},
 abstract = {Runtime stacks are critical components of any modern software--they are used to implement powerful control structures such as function call/return, stack cutting and unwinding, coroutines, and thread context switch. Stack operations, however, are very hard to reason about: there are no known formal specifications for certifying C-style setjmp/longjmp, stack cutting and unwinding, or weak continuations (in C--). In many proof-carrying code (PCC) systems, return code pointers and exception handlers are treated as general first-class functions (as in continuation-passing style) even though both should have more limited scopes.In this paper we show that stack-based control abstractions follow a much simpler pattern than general first-class code pointers. We present a simple but flexible Hoare-style framework for modular verification of assembly code with all kinds of stackbased control abstractions, including function call/return, tail call, setjmp/longjmp, weak continuation, stack cutting, stack unwinding, multi-return function call, coroutines, and thread context switch. Instead of presenting a specific logic for each control structure, we develop all reasoning systems as instances of a generic framework. This allows program modules and their proofs developed in different PCC systems to be linked together. Our system is fully mechanized. We give the complete soundness proof and a full verification of several examples in the Coq proof assistant.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {401--414},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1133981.1134028},
 doi = {http://doi.acm.org/10.1145/1133981.1134028},
 acmid = {1134028},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assembly code verification, control abstractions, modularity, proof-carrying code, stack-based},
} 

@article{Cook:2006:TPS:1133255.1134029,
 author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
 title = {Termination proofs for systems code},
 abstract = {Program termination is central to the process of ensuring that systems code can always react. We describe a new program termination prover that performs a path-sensitive and context-sensitive program analysis and provides capacity for large program fragments (i.e. more than 20,000 lines of code) together with support for programming language features such as arbitrarily nested loops, pointers, function-pointers, side-effects, etc.We also present experimental results on device driver dispatch routines from theWindows operating system. The most distinguishing aspect of our tool is how it shifts the balance between the two tasks of constructing and respectively checking the termination argument. Checking becomes the hard step. In this paper we show how we solve the corresponding challenge of checking with binary reachability analysis.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {6},
 month = {June},
 year = {2006},
 issn = {0362-1340},
 pages = {415--426},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133255.1134029},
 doi = {http://doi.acm.org/10.1145/1133255.1134029},
 acmid = {1134029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal verification, model checking, program termination, program verification},
} 

@inproceedings{Cook:2006:TPS:1133981.1134029,
 author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
 title = {Termination proofs for systems code},
 abstract = {Program termination is central to the process of ensuring that systems code can always react. We describe a new program termination prover that performs a path-sensitive and context-sensitive program analysis and provides capacity for large program fragments (i.e. more than 20,000 lines of code) together with support for programming language features such as arbitrarily nested loops, pointers, function-pointers, side-effects, etc.We also present experimental results on device driver dispatch routines from theWindows operating system. The most distinguishing aspect of our tool is how it shifts the balance between the two tasks of constructing and respectively checking the termination argument. Checking becomes the hard step. In this paper we show how we solve the corresponding challenge of checking with binary reachability analysis.},
 booktitle = {Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '06},
 year = {2006},
 isbn = {1-59593-320-4},
 location = {Ottawa, Ontario, Canada},
 pages = {415--426},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1133981.1134029},
 doi = {http://doi.acm.org/10.1145/1133981.1134029},
 acmid = {1134029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal verification, model checking, program termination, program verification},
} 

@inproceedings{Xu:2005:SVD:1065010.1065013,
 author = {Xu, Min and Bod\'{\i}k, Rastislav and Hill, Mark D.},
 title = {A serializability violation detector for shared-memory server programs},
 abstract = {We aim to improve reliability of multithreaded programs by proposing a dynamic detector that detects potentially erroneous program executions and their causes. We design and evaluate a Serializability Violation Detector</i> (SVD) that has two unique goals: (I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply alerting users that a software error may have occurred; and (II) helping debug programs by revealing causes of error symptoms.Two properties of SVD help in achieving these goals. First, to detect only erroneous executions, SVD checks serializability of atomic regions, which are code regions that need to be executed atomically. Second, to improve usability, SVD does not require a priori</i> annotations of atomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widely-used multithreaded server programs show that SVD finds real bugs and reports modest false positives. The goal of this paper is to develop a detector suitable for (I) BER-based avoidance of erroneous program executions; and (II) alerting users as software errors occur. We argue that such a detector should have the following two properties.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1065010.1065013},
 doi = {http://doi.acm.org/10.1145/1065010.1065013},
 acmid = {1065013},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multithreading, race conditions, serializability},
} 

@article{Xu:2005:SVD:1064978.1065013,
 author = {Xu, Min and Bod\'{\i}k, Rastislav and Hill, Mark D.},
 title = {A serializability violation detector for shared-memory server programs},
 abstract = {We aim to improve reliability of multithreaded programs by proposing a dynamic detector that detects potentially erroneous program executions and their causes. We design and evaluate a Serializability Violation Detector</i> (SVD) that has two unique goals: (I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply alerting users that a software error may have occurred; and (II) helping debug programs by revealing causes of error symptoms.Two properties of SVD help in achieving these goals. First, to detect only erroneous executions, SVD checks serializability of atomic regions, which are code regions that need to be executed atomically. Second, to improve usability, SVD does not require a priori</i> annotations of atomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widely-used multithreaded server programs show that SVD finds real bugs and reports modest false positives. The goal of this paper is to develop a detector suitable for (I) BER-based avoidance of erroneous program executions; and (II) alerting users as software errors occur. We argue that such a detector should have the following two properties.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1064978.1065013},
 doi = {http://doi.acm.org/10.1145/1064978.1065013},
 acmid = {1065013},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multithreading, race conditions, serializability},
} 

@article{Liblit:2005:SSB:1064978.1065014,
 author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
 title = {Scalable statistical bug isolation},
 abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065014},
 doi = {http://doi.acm.org/10.1145/1064978.1065014},
 acmid = {1065014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug isolation, feature selection, invariants, random sampling, statistical debugging},
} 

@inproceedings{Liblit:2005:SSB:1065010.1065014,
 author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
 title = {Scalable statistical bug isolation},
 abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065014},
 doi = {http://doi.acm.org/10.1145/1065010.1065014},
 acmid = {1065014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug isolation, feature selection, invariants, random sampling, statistical debugging},
} 

@inproceedings{Elmas:2005:VVC:1065010.1065015,
 author = {Elmas, Tayfun and Tasiran, Serdar and Qadeer, Shaz},
 title = {VYRD: verifYing concurrent programs by runtime refinement-violation detection},
 abstract = {We present a runtime technique for checking that a concurrently-accessed data structure implementation, such as a file system or the storage management module of a database, conforms to an executable specification that contains an atomic method per data structure operation. The specification can be provided separately or a non-concurrent, "atomized" interpretation of the implementation can serve as the specification. The technique consists of two phases. In the first phase, the implementation is instrumented in order to record information into a log during execution. In the second, a separate verification thread uses the logged information to drive an instance of the specification and to check whether the logged execution conforms to it. We paid special attention to the general applicability and scalability of the techniques and to minimizing their concurrency and performance impact. The result is a lightweight verification method that provides a significant improvement over testing for concurrent programs.We formalize conformance to a specification using the notion of refinement: Each trace of the implementation must be equivalent to some trace of the specification. Among the novel features of our work are two variations on the definition of refinement appropriate for runtime checking: I/O and "view" refinement. These definitions were motivated by our experience with two industrial-scale concurrent data structure implementations: the Boxwood project, a B-link tree data structure built on a novel storage infrastructure [10] and the Scan file system [9]. I/O and view refinement checking were implemented as a verification tool named VRYD (VerifYing concurrent programs by Runtime Refinement-violation Detection). VYRD was applied to the verification of Boxwood, Java class libraries, and, previously, to the Scan filesystem. It was able to detect previously unnoticed subtle concurrency bugs in Boxwood and the Scan file system, and the known bugs in the Java class libraries and manually constructed examples. Experimental results indicate that our techniques have modest computational cost.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {27--37},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065015},
 doi = {http://doi.acm.org/10.1145/1065010.1065015},
 acmid = {1065015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent data structures, refinement, runtime verification},
} 

@article{Elmas:2005:VVC:1064978.1065015,
 author = {Elmas, Tayfun and Tasiran, Serdar and Qadeer, Shaz},
 title = {VYRD: verifYing concurrent programs by runtime refinement-violation detection},
 abstract = {We present a runtime technique for checking that a concurrently-accessed data structure implementation, such as a file system or the storage management module of a database, conforms to an executable specification that contains an atomic method per data structure operation. The specification can be provided separately or a non-concurrent, "atomized" interpretation of the implementation can serve as the specification. The technique consists of two phases. In the first phase, the implementation is instrumented in order to record information into a log during execution. In the second, a separate verification thread uses the logged information to drive an instance of the specification and to check whether the logged execution conforms to it. We paid special attention to the general applicability and scalability of the techniques and to minimizing their concurrency and performance impact. The result is a lightweight verification method that provides a significant improvement over testing for concurrent programs.We formalize conformance to a specification using the notion of refinement: Each trace of the implementation must be equivalent to some trace of the specification. Among the novel features of our work are two variations on the definition of refinement appropriate for runtime checking: I/O and "view" refinement. These definitions were motivated by our experience with two industrial-scale concurrent data structure implementations: the Boxwood project, a B-link tree data structure built on a novel storage infrastructure [10] and the Scan file system [9]. I/O and view refinement checking were implemented as a verification tool named VRYD (VerifYing concurrent programs by Runtime Refinement-violation Detection). VYRD was applied to the verification of Boxwood, Java class libraries, and, previously, to the Scan filesystem. It was able to detect previously unnoticed subtle concurrency bugs in Boxwood and the Scan file system, and the known bugs in the Java class libraries and manually constructed examples. Experimental results indicate that our techniques have modest computational cost.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {27--37},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065015},
 doi = {http://doi.acm.org/10.1145/1064978.1065015},
 acmid = {1065015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent data structures, refinement, runtime verification},
} 

@article{Jhala:2005:PS:1064978.1065016,
 author = {Jhala, Ranjit and Majumdar, Rupak},
 title = {Path slicing},
 abstract = {We present a new technique, path slicing</i>, that takes as input a possibly infeasible path to a target location, and eliminates all the operations that are irrelevant towards the reachability of the target location. A path slice is a subsequence of the original path whose infeasibility guarantees the infeasibility of the original path, and whose feasibility guarantees the existence of some feasible variant of the given path that reaches the target location even though the given path may itself be infeasible. Our method combines the ability of program slicing to look at several program paths, with the precision that dynamic slicing enjoys by focusing on a single path. We have implemented Path Slicing to analyze possible counterexamples returned by the software model checker <sc>Blast</sc>. We show its effectiveness in drastically reducing the size of the counterexamples to less than 1\% of their original size. This enables the precise verification of application programs (upto 100KLOC), by allowing the analysis to focus on the part of the counterexample that is relevant to the property being checked.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {38--47},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064978.1065016},
 doi = {http://doi.acm.org/10.1145/1064978.1065016},
 acmid = {1065016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {counterexample analysis, program slicing},
} 

@inproceedings{Jhala:2005:PS:1065010.1065016,
 author = {Jhala, Ranjit and Majumdar, Rupak},
 title = {Path slicing},
 abstract = {We present a new technique, path slicing</i>, that takes as input a possibly infeasible path to a target location, and eliminates all the operations that are irrelevant towards the reachability of the target location. A path slice is a subsequence of the original path whose infeasibility guarantees the infeasibility of the original path, and whose feasibility guarantees the existence of some feasible variant of the given path that reaches the target location even though the given path may itself be infeasible. Our method combines the ability of program slicing to look at several program paths, with the precision that dynamic slicing enjoys by focusing on a single path. We have implemented Path Slicing to analyze possible counterexamples returned by the software model checker <sc>Blast</sc>. We show its effectiveness in drastically reducing the size of the counterexamples to less than 1\% of their original size. This enables the precise verification of application programs (upto 100KLOC), by allowing the analysis to focus on the part of the counterexample that is relevant to the property being checked.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {38--47},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1065010.1065016},
 doi = {http://doi.acm.org/10.1145/1065010.1065016},
 acmid = {1065016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {counterexample analysis, program slicing},
} 

@article{Mandelin:2005:JMH:1064978.1065018,
 author = {Mandelin, David and Xu, Lin and Bod\'{\i}k, Rastislav and Kimelman, Doug},
 title = {Jungloid mining: helping to navigate the API jungle},
 abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid</i> code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined</i> from a corpus of sample client programs.We implemented a tool, <sc>prospector</sc>, based on these techniques. <sc>prospector</sc> is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested <sc>prospector</sc> on a set of real programming problems involving APIs; <sc>prospector</sc> found the desired solution for 18 of 20 problems. We also evaluated <sc>prospector</sc> in a user study, finding that programmers solved programming problems more quickly and with more reuse when using <sc>prospector</sc> than without <sc>prospector</sc>.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {48--61},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1064978.1065018},
 doi = {http://doi.acm.org/10.1145/1064978.1065018},
 acmid = {1065018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mining, program synthesis, reuse},
} 

@inproceedings{Mandelin:2005:JMH:1065010.1065018,
 author = {Mandelin, David and Xu, Lin and Bod\'{\i}k, Rastislav and Kimelman, Doug},
 title = {Jungloid mining: helping to navigate the API jungle},
 abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid</i> code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined</i> from a corpus of sample client programs.We implemented a tool, <sc>prospector</sc>, based on these techniques. <sc>prospector</sc> is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested <sc>prospector</sc> on a set of real programming problems involving APIs; <sc>prospector</sc> found the desired solution for 18 of 20 problems. We also evaluated <sc>prospector</sc> in a user study, finding that programmers solved programming problems more quickly and with more reuse when using <sc>prospector</sc> than without <sc>prospector</sc>.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {48--61},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1065010.1065018},
 doi = {http://doi.acm.org/10.1145/1065010.1065018},
 acmid = {1065018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mining, program synthesis, reuse},
} 

@article{Furr:2005:CTS:1064978.1065019,
 author = {Furr, Michael and Foster, Jeffrey S.},
 title = {Checking type safety of foreign function calls},
 abstract = {We present a multi-lingual type inference system for checking type safety across a foreign function interface. The goal of our system is to prevent foreign function calls from introducing type and memory safety violations into an otherwise safe language. Our system targets OCaml's FFI to C, which is relatively lightweight and illustrates some interesting challenges in multi-lingual type inference. The type language in our system embeds OCaml types in C types and vice-versa, which allows us to track type information accurately even through the foreign language, where the original types are lost. Our system uses representational</i> types that can model multiple OCaml types, because C programs can observe that many OCaml types have the same physical representation. Furthermore, because C has a low-level view of OCaml data, our inference system includes a dataflow analysis to track memory offsets and tag information. Finally, our type system includes garbage collection information to ensure that pointers from the FFI to the OCaml heap are tracked properly. We have implemented our inference system and applied it to a small set of benchmarks. Our results show that programmers do misuse these interfaces, and our implementation has found several bugs and questionable coding practices in our benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {62--72},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065019},
 doi = {http://doi.acm.org/10.1145/1064978.1065019},
 acmid = {1065019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FFI, OCaml, dataflow analysis, flow-sensitive type system, foreign function calls, foreign function interface, multi-lingual type inference, multi-lingual type system, representational type},
} 

@inproceedings{Furr:2005:CTS:1065010.1065019,
 author = {Furr, Michael and Foster, Jeffrey S.},
 title = {Checking type safety of foreign function calls},
 abstract = {We present a multi-lingual type inference system for checking type safety across a foreign function interface. The goal of our system is to prevent foreign function calls from introducing type and memory safety violations into an otherwise safe language. Our system targets OCaml's FFI to C, which is relatively lightweight and illustrates some interesting challenges in multi-lingual type inference. The type language in our system embeds OCaml types in C types and vice-versa, which allows us to track type information accurately even through the foreign language, where the original types are lost. Our system uses representational</i> types that can model multiple OCaml types, because C programs can observe that many OCaml types have the same physical representation. Furthermore, because C has a low-level view of OCaml data, our inference system includes a dataflow analysis to track memory offsets and tag information. Finally, our type system includes garbage collection information to ensure that pointers from the FFI to the OCaml heap are tracked properly. We have implemented our inference system and applied it to a small set of benchmarks. Our results show that programmers do misuse these interfaces, and our implementation has found several bugs and questionable coding practices in our benchmarks.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {62--72},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065019},
 doi = {http://doi.acm.org/10.1145/1065010.1065019},
 acmid = {1065019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FFI, OCaml, dataflow analysis, flow-sensitive type system, foreign function calls, foreign function interface, multi-lingual type inference, multi-lingual type system, representational type},
} 

@inproceedings{Siek:2005:ELS:1065010.1065021,
 author = {Siek, Jeremy G. and Lumsdaine, Andrew},
 title = {Essential language support for generic programming},
 abstract = {Concepts</i> are an essential language feature for generic programming in the large. Concepts allow for succinct expression of constraints on type parameters of generic algorithms, enable systematic organization of problem domain abstractions, and make generic algorithms easier to use. In this paper we present the design of a type system and semantics for concepts that is suitable for non-type-inferencing languages. Our design shares much in common with the type classes of Haskell, though our primary influence is from best practices in the C++ community, where concepts are used to document type requirements for templates in generic libraries. Concepts include a novel combination of associated types and same-type constraints that do not appear in type classes, but that are similar to nested types and type sharing in ML.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065021},
 doi = {http://doi.acm.org/10.1145/1065010.1065021},
 acmid = {1065021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C++, Haskell, generic programming, polymorphism, standard ML},
} 

@article{Siek:2005:ELS:1064978.1065021,
 author = {Siek, Jeremy G. and Lumsdaine, Andrew},
 title = {Essential language support for generic programming},
 abstract = {Concepts</i> are an essential language feature for generic programming in the large. Concepts allow for succinct expression of constraints on type parameters of generic algorithms, enable systematic organization of problem domain abstractions, and make generic algorithms easier to use. In this paper we present the design of a type system and semantics for concepts that is suitable for non-type-inferencing languages. Our design shares much in common with the type classes of Haskell, though our primary influence is from best practices in the C++ community, where concepts are used to document type requirements for templates in generic libraries. Concepts include a novel combination of associated types and same-type constraints that do not appear in type classes, but that are similar to nested types and type sharing in ML.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065021},
 doi = {http://doi.acm.org/10.1145/1064978.1065021},
 acmid = {1065021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C++, Haskell, generic programming, polymorphism, standard ML},
} 

@inproceedings{Chin:2005:STQ:1065010.1065022,
 author = {Chin, Brian and Markstrum, Shane and Millstein, Todd},
 title = {Semantic type qualifiers},
 abstract = {We present a new approach for supporting user-defined type refinements, which augment existing types to specify and check additional invariants of interest to programmers. We provide an expressive language in which users define new refinements and associated type rules. These rules are automatically incorporated by an extensible typechecker</i> during static typechecking of programs. Separately, a soundness checker</i>automatically proves that each refinement's type rules ensure the intended invariant, for all possible programs. We have formalized our approach and have instantiated it as a framework for adding new type qualifiers to C programs. We have used this framework to define and automatically prove sound a host of type qualifiers of different sorts, including pos and neg for integers, tainted and untainted for strings, and nonnull and unique for pointers, and we have applied our qualifiers to ensure important invariants on open-source C programs.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065022},
 doi = {http://doi.acm.org/10.1145/1065010.1065022},
 acmid = {1065022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extensible typechecking, type qualifiers, type soundness},
} 

@article{Chin:2005:STQ:1064978.1065022,
 author = {Chin, Brian and Markstrum, Shane and Millstein, Todd},
 title = {Semantic type qualifiers},
 abstract = {We present a new approach for supporting user-defined type refinements, which augment existing types to specify and check additional invariants of interest to programmers. We provide an expressive language in which users define new refinements and associated type rules. These rules are automatically incorporated by an extensible typechecker</i> during static typechecking of programs. Separately, a soundness checker</i>automatically proves that each refinement's type rules ensure the intended invariant, for all possible programs. We have formalized our approach and have instantiated it as a framework for adding new type qualifiers to C programs. We have used this framework to define and automatically prove sound a host of type qualifiers of different sorts, including pos and neg for integers, tainted and untainted for strings, and nonnull and unique for pointers, and we have applied our qualifiers to ensure important invariants on open-source C programs.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065022},
 doi = {http://doi.acm.org/10.1145/1064978.1065022},
 acmid = {1065022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extensible typechecking, type qualifiers, type soundness},
} 

@article{Krishnaswami:2005:POE:1064978.1065023,
 author = {Krishnaswami, Neel and Aldrich, Jonathan},
 title = {Permission-based ownership: encapsulating state in higher-order typed languages},
 abstract = {Today's module systems do not effectively support information hiding in the presence of shared mutable objects, causing serious problems in the development and evolution of large software systems. Ownership types have been proposed as a solution to this problem, but current systems have ad-hoc access restrictions and are limited to Java-like languages.In this paper, we describe System F<inf>own</inf></i>, an extension of System F with references and ownership. Our design shows both how ownership fits into standard type theory and the encapsulation benefits it can provide in languages with first-class functions, abstract data types, and parametric polymorphism. By looking at ownership in the setting of SystemF, we were able to develop a design that is more principled and flexible than previous ownership type systems, while also providing stronger encapsulation guarantees.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {96--106},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065023},
 doi = {http://doi.acm.org/10.1145/1064978.1065023},
 acmid = {1065023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {domains, lambda calculus, modularity, ownership types, permissions, state, system f, type theory},
} 

@inproceedings{Krishnaswami:2005:POE:1065010.1065023,
 author = {Krishnaswami, Neel and Aldrich, Jonathan},
 title = {Permission-based ownership: encapsulating state in higher-order typed languages},
 abstract = {Today's module systems do not effectively support information hiding in the presence of shared mutable objects, causing serious problems in the development and evolution of large software systems. Ownership types have been proposed as a solution to this problem, but current systems have ad-hoc access restrictions and are limited to Java-like languages.In this paper, we describe System F<inf>own</inf></i>, an extension of System F with references and ownership. Our design shows both how ownership fits into standard type theory and the encapsulation benefits it can provide in languages with first-class functions, abstract data types, and parametric polymorphism. By looking at ownership in the setting of SystemF, we were able to develop a design that is more principled and flexible than previous ownership type systems, while also providing stronger encapsulation guarantees.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {96--106},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065023},
 doi = {http://doi.acm.org/10.1145/1065010.1065023},
 acmid = {1065023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {domains, lambda calculus, modularity, ownership types, permissions, state, system f, type theory},
} 

@article{Jimenez:2005:CPI:1064978.1065025,
 author = {Jim\'{e}nez, Daniel A.},
 title = {Code placement for improving dynamic branch prediction accuracy},
 abstract = {Code placement techniques have traditionally improved instruction fetch bandwidth by increasing instruction locality and decreasing the number of taken branches. However, traditional code placement techniques have less benefit in the presence of a trace cache that alters the placement of instructions in the instruction cache. Moreover, as pipelines have become deeper to accommodate increasing clock rates, branch misprediction penalties have become a significant impediment to performance. We evaluate pattern history table partitioning</i>, a feedback directed code placement technique that explicitly places conditional branches so that they are less likely to interfere destructively with one another in branch prediction tables. On SPEC CPU benchmarks running on an Intel Pentium 4, branch mispredictions are reduced by up to 22\% and 3.5\% on average. This reduction yields a speedup of up to 16.0\% and 4.5\% on average. By contrast, branch alignment, a previous code placement technique, yields only up to a 4.7\% speedup and less than 1\% on average.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064978.1065025},
 doi = {http://doi.acm.org/10.1145/1064978.1065025},
 acmid = {1065025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, compilers},
} 

@inproceedings{Jimenez:2005:CPI:1065010.1065025,
 author = {Jim\'{e}nez, Daniel A.},
 title = {Code placement for improving dynamic branch prediction accuracy},
 abstract = {Code placement techniques have traditionally improved instruction fetch bandwidth by increasing instruction locality and decreasing the number of taken branches. However, traditional code placement techniques have less benefit in the presence of a trace cache that alters the placement of instructions in the instruction cache. Moreover, as pipelines have become deeper to accommodate increasing clock rates, branch misprediction penalties have become a significant impediment to performance. We evaluate pattern history table partitioning</i>, a feedback directed code placement technique that explicitly places conditional branches so that they are less likely to interfere destructively with one another in branch prediction tables. On SPEC CPU benchmarks running on an Intel Pentium 4, branch mispredictions are reduced by up to 22\% and 3.5\% on average. This reduction yields a speedup of up to 16.0\% and 4.5\% on average. By contrast, branch alignment, a previous code placement technique, yields only up to a 4.7\% speedup and less than 1\% on average.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1065010.1065025},
 doi = {http://doi.acm.org/10.1145/1065010.1065025},
 acmid = {1065025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, compilers},
} 

@inproceedings{Avgustinov:2005:OA:1065010.1065026,
 author = {Avgustinov, Pavel and Christensen, Aske Simon and Hendren, Laurie and Kuzins, Sascha and Lhot\'{a}k, Jennifer and Lhot\'{a}k, Ond\v{r}ej and de Moor, Oege and Sereni, Damien and Sittampalam, Ganesh and Tibble, Julian},
 title = {Optimising aspectJ},
 abstract = {AspectJ, an aspect-oriented extension of Java, is becoming increasingly popular. However, not much work has been directed at optimising compilers for AspectJ. Optimising AOP languages provides many new and interesting challenges for compiler writers, and this paper identifies and addresses three such challenges.First, compiling around</i> advice efficiently is particularly challenging. We provide a new code generation strategy for around</i> advice, which (unlike previous implementations) both avoids the use of excessive inlining and the use of closures. We show it leads to more compact code, and can also improve run-time performance. Second, woven code sometimes includes run-time tests to determine whether advice should execute. One important case is the cflow</i> pointcut which uses information about the dynamic calling context. Previous techniques for cflow</i> were very costly in terms of both time and space. We present new techniques to minimise or eliminate the overhead of cflow</i> using both intra- and inter-procedural analyses. Third, we have addressed the general problem of how to structure an optimising compiler so that traditional analyses can be easily adapted to the AOP setting.We have implemented all of the techniques in this paper in abc</i>, our AspectBench Compiler for AspectJ, and we demonstrate significant speedups with empirical results. Some of our techniques have already been integrated into the production AspectJ compiler, ajc</i> 1.2.1.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065026},
 doi = {http://doi.acm.org/10.1145/1065010.1065026},
 acmid = {1065026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {around advice, aspect-oriented programming language, aspectJ, cflow pointcut, optimization},
} 

@article{Avgustinov:2005:OA:1064978.1065026,
 author = {Avgustinov, Pavel and Christensen, Aske Simon and Hendren, Laurie and Kuzins, Sascha and Lhot\'{a}k, Jennifer and Lhot\'{a}k, Ond\v{r}ej and de Moor, Oege and Sereni, Damien and Sittampalam, Ganesh and Tibble, Julian},
 title = {Optimising aspectJ},
 abstract = {AspectJ, an aspect-oriented extension of Java, is becoming increasingly popular. However, not much work has been directed at optimising compilers for AspectJ. Optimising AOP languages provides many new and interesting challenges for compiler writers, and this paper identifies and addresses three such challenges.First, compiling around</i> advice efficiently is particularly challenging. We provide a new code generation strategy for around</i> advice, which (unlike previous implementations) both avoids the use of excessive inlining and the use of closures. We show it leads to more compact code, and can also improve run-time performance. Second, woven code sometimes includes run-time tests to determine whether advice should execute. One important case is the cflow</i> pointcut which uses information about the dynamic calling context. Previous techniques for cflow</i> were very costly in terms of both time and space. We present new techniques to minimise or eliminate the overhead of cflow</i> using both intra- and inter-procedural analyses. Third, we have addressed the general problem of how to structure an optimising compiler so that traditional analyses can be easily adapted to the AOP setting.We have implemented all of the techniques in this paper in abc</i>, our AspectBench Compiler for AspectJ, and we demonstrate significant speedups with empirical results. Some of our techniques have already been integrated into the production AspectJ compiler, ajc</i> 1.2.1.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065026},
 doi = {http://doi.acm.org/10.1145/1064978.1065026},
 acmid = {1065026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {around advice, aspect-oriented programming language, aspectJ, cflow pointcut, optimization},
} 

@inproceedings{Lattner:2005:APA:1065010.1065027,
 author = {Lattner, Chris and Adve, Vikram},
 title = {Automatic pool allocation: improving performance by controlling data structure layout in the heap},
 abstract = {This paper describes Automatic Pool Allocation</i>, a transformation framework that segregates distinct instances of heap-based data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. The primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. The key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a context-sensitive pointer analysis, including a novel strategy for correct handling of indirect (and potentially unsafe) function calls. The transformation does not require type safe programs and works for the full generality of C and C++. Second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. Third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. Using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by 10-25\% in many cases, about 2x in two cases, and more than 10x in two small benchmarks. Overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heap-based data structures.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {129--142},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1065010.1065027},
 doi = {http://doi.acm.org/10.1145/1065010.1065027},
 acmid = {1065027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, data layout, pool allocation, recursive data structure, static analysis},
} 

@article{Lattner:2005:APA:1064978.1065027,
 author = {Lattner, Chris and Adve, Vikram},
 title = {Automatic pool allocation: improving performance by controlling data structure layout in the heap},
 abstract = {This paper describes Automatic Pool Allocation</i>, a transformation framework that segregates distinct instances of heap-based data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. The primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. The key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a context-sensitive pointer analysis, including a novel strategy for correct handling of indirect (and potentially unsafe) function calls. The transformation does not require type safe programs and works for the full generality of C and C++. Second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. Third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. Using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by 10-25\% in many cases, about 2x in two cases, and more than 10x in two small benchmarks. Overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heap-based data structures.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {129--142},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1064978.1065027},
 doi = {http://doi.acm.org/10.1145/1064978.1065027},
 acmid = {1065027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, data layout, pool allocation, recursive data structure, static analysis},
} 

@inproceedings{Hertz:2005:GCW:1065010.1065028,
 author = {Hertz, Matthew and Feng, Yi and Berger, Emery D.},
 title = {Garbage collection without paging},
 abstract = {Garbage collection offers numerous software engineering advantages, but interacts poorly with virtual memory managers. Existing garbage collectors require far more pages than the application's working set and touch pages without regard to which ones are in memory, especially during full-heap garbage collection. The resulting paging can cause throughput to plummet and pause times to spike up to seconds or even minutes. We present a garbage collector that avoids paging. This bookmarking collector</i> cooperates with the virtual memory manager to guide its eviction decisions. Using summary information ("bookmarks") recorded from evicted pages, the collector can perform in-memory full-heap collections. In the absence of memory pressure, the bookmarking collector matches the throughput of the best collector we tested while running in smaller heaps. In the face of memory pressure, it improves throughput by up to a factor of five and reduces pause times by up to a factor of 45 over the next best collector. Compared to a collector that consistently provides high throughput (generational mark-sweep), the bookmarking collector reduces pause times by up to 218x and improves throughput by up to 41x. Bookmarking collection thus provides greater utilization of available physical memory than other collectors while matching or exceeding their throughput.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065028},
 doi = {http://doi.acm.org/10.1145/1065010.1065028},
 acmid = {1065028},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bookmarking collection, garbage collection, generational collection, memory pressure, paging, virtual memory},
} 

@article{Hertz:2005:GCW:1064978.1065028,
 author = {Hertz, Matthew and Feng, Yi and Berger, Emery D.},
 title = {Garbage collection without paging},
 abstract = {Garbage collection offers numerous software engineering advantages, but interacts poorly with virtual memory managers. Existing garbage collectors require far more pages than the application's working set and touch pages without regard to which ones are in memory, especially during full-heap garbage collection. The resulting paging can cause throughput to plummet and pause times to spike up to seconds or even minutes. We present a garbage collector that avoids paging. This bookmarking collector</i> cooperates with the virtual memory manager to guide its eviction decisions. Using summary information ("bookmarks") recorded from evicted pages, the collector can perform in-memory full-heap collections. In the absence of memory pressure, the bookmarking collector matches the throughput of the best collector we tested while running in smaller heaps. In the face of memory pressure, it improves throughput by up to a factor of five and reduces pause times by up to a factor of 45 over the next best collector. Compared to a collector that consistently provides high throughput (generational mark-sweep), the bookmarking collector reduces pause times by up to 218x and improves throughput by up to 41x. Bookmarking collection thus provides greater utilization of available physical memory than other collectors while matching or exceeding their throughput.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065028},
 doi = {http://doi.acm.org/10.1145/1064978.1065028},
 acmid = {1065028},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bookmarking collection, garbage collection, generational collection, memory pressure, paging, virtual memory},
} 

@article{Rong:2005:RAS:1064978.1065030,
 author = {Rong, Hongbo and Douillet, Alban and Gao, Guang R.},
 title = {Register allocation for software pipelined multi-dimensional loops},
 abstract = {Software pipelining of a multi-dimensional loop is an important optimization that overlaps the execution of successive outermost loop iterations to explore instruction-level parallelism from the entire n-dimensional iteration space. This paper investigates register allocation for software pipelined multi-dimensional loops.For single loop software pipelining, the lifetime instances of a loop variant in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime</i> using Rau's terminology) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multi-dimensional loop is considerably more complex, and so are the vector lifetimes in it.In this paper, we develop a way to normalize and represent vector lifetimes in multi-dimensional loop software pipelining, which capture their complexity, while exposing their regularity that enables us to develop a simple, yet powerful solution. Our algorithm is based on the development of a metric, called distance</i>, that quantitatively determines the degree of potential overlapping (conflicts) between two vector lifetimes. We show how to calculate and use the distance, conservatively or aggressively, to guide the register allocation of the vector lifetimes under a bin-packing algorithm framework. The classical register allocation for software pipelined single loops is subsumed by our method as a special case.The method has been implemented in the ORC compiler and produced code for the Itanium architecture. We report the effectiveness of our method on 134 loop nests with 348 loop levels. Several strategies for register allocation are compared and analyzed.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {154--167},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1064978.1065030},
 doi = {http://doi.acm.org/10.1145/1064978.1065030},
 acmid = {1065030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {register allocation, software pipelining},
} 

@inproceedings{Rong:2005:RAS:1065010.1065030,
 author = {Rong, Hongbo and Douillet, Alban and Gao, Guang R.},
 title = {Register allocation for software pipelined multi-dimensional loops},
 abstract = {Software pipelining of a multi-dimensional loop is an important optimization that overlaps the execution of successive outermost loop iterations to explore instruction-level parallelism from the entire n-dimensional iteration space. This paper investigates register allocation for software pipelined multi-dimensional loops.For single loop software pipelining, the lifetime instances of a loop variant in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime</i> using Rau's terminology) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multi-dimensional loop is considerably more complex, and so are the vector lifetimes in it.In this paper, we develop a way to normalize and represent vector lifetimes in multi-dimensional loop software pipelining, which capture their complexity, while exposing their regularity that enables us to develop a simple, yet powerful solution. Our algorithm is based on the development of a metric, called distance</i>, that quantitatively determines the degree of potential overlapping (conflicts) between two vector lifetimes. We show how to calculate and use the distance, conservatively or aggressively, to guide the register allocation of the vector lifetimes under a bin-packing algorithm framework. The classical register allocation for software pipelined single loops is subsumed by our method as a special case.The method has been implemented in the ORC compiler and produced code for the Itanium architecture. We report the effectiveness of our method on 134 loop nests with 348 loop levels. Several strategies for register allocation are compared and analyzed.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {154--167},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1065010.1065030},
 doi = {http://doi.acm.org/10.1145/1065010.1065030},
 acmid = {1065030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {register allocation, software pipelining},
} 

@inproceedings{Zhuang:2005:DRA:1065010.1065031,
 author = {Zhuang, Xiaotong and Pande, Santosh},
 title = {Differential register allocation},
 abstract = {Micro-architecture designers are very cautious about expanding the number of architected registers (also the register field), because increasing the register field adds to the code size, raises I-cache and memory pressure, complicates processor pipeline. Especially for low-end processors, encoding space could be extremely limited due to area and power considerations. On the other hand, the number of architected registers exposed to the compiler could directly affect the effectiveness of compiler analysis and optimization. For high performance computers, register pressure can be higher than the available registers in some regions, e.g. due to optimizations like aggressive function inlining, software pipelining etc. The compiler cannot effectively perform compilation and optimization if only a small number of registers are exposed through the ISA. Therefore, it is crucial that more architected registers are available at the compiler's disposal without expanding the code size significantly.In this paper, we look at a new register encoding scheme called differential encoding that allows more registers to be addressed in the operand field of instructions than the direct encoding currently being used. We show it can be implemented with very low overhead. Based upon differential encoding, we apply it in several ways such that the extra architected registers can benefit the performance. Three schemes are devised to integrate differential encoding with register allocation. We demonstrate that differential register allocation is helpful in improving the performance of both high-end and low-end processors. Moreover, We can combine it with software pipelining to provide more registers and reduce spills.Our results show that differential encoding significantly reduces the number of spills and speeds up program execution. For a low-end configuration, we achieve over 12\% speedup while keeping code size almost unaffected. For optimization on loops, it significantly speeds up loops with high register pressure (over 70\% speedup).},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {168--179},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065031},
 doi = {http://doi.acm.org/10.1145/1065010.1065031},
 acmid = {1065031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architected register, differential dncoding, register allocation},
} 

@article{Zhuang:2005:DRA:1064978.1065031,
 author = {Zhuang, Xiaotong and Pande, Santosh},
 title = {Differential register allocation},
 abstract = {Micro-architecture designers are very cautious about expanding the number of architected registers (also the register field), because increasing the register field adds to the code size, raises I-cache and memory pressure, complicates processor pipeline. Especially for low-end processors, encoding space could be extremely limited due to area and power considerations. On the other hand, the number of architected registers exposed to the compiler could directly affect the effectiveness of compiler analysis and optimization. For high performance computers, register pressure can be higher than the available registers in some regions, e.g. due to optimizations like aggressive function inlining, software pipelining etc. The compiler cannot effectively perform compilation and optimization if only a small number of registers are exposed through the ISA. Therefore, it is crucial that more architected registers are available at the compiler's disposal without expanding the code size significantly.In this paper, we look at a new register encoding scheme called differential encoding that allows more registers to be addressed in the operand field of instructions than the direct encoding currently being used. We show it can be implemented with very low overhead. Based upon differential encoding, we apply it in several ways such that the extra architected registers can benefit the performance. Three schemes are devised to integrate differential encoding with register allocation. We demonstrate that differential register allocation is helpful in improving the performance of both high-end and low-end processors. Moreover, We can combine it with software pipelining to provide more registers and reduce spills.Our results show that differential encoding significantly reduces the number of spills and speeds up program execution. For a low-end configuration, we achieve over 12\% speedup while keeping code size almost unaffected. For optimization on loops, it significantly speeds up loops with high register pressure (over 70\% speedup).},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {168--179},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065031},
 doi = {http://doi.acm.org/10.1145/1064978.1065031},
 acmid = {1065031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architected register, differential dncoding, register allocation},
} 

@inproceedings{Aleta:2005:DOS:1065010.1065032,
 author = {Alet\`{a}, Alex and Codina, Josep M. and Gonz\`{a}lez, Antonio and Kaeli, David},
 title = {Demystifying on-the-fly spill code},
 abstract = {Modulo scheduling is an effective code generation technique that exploits the parallelism in program loops by overlapping iterations. One drawback of this optimization is that register requirements increase significantly because values across different loop iterations can be live concurrently. One possible solution to reduce register pressure is to insert spill code to release registers. Spill code stores values to memory between the producer and consumer instructions.Spilling heuristics can be divided into two classes: 1) a posteriori</i> approaches (spill code is inserted after scheduling the loop) or 2) on-the-fly</i> approaches (spill code is inserted during loop scheduling). Recent studies have reported obtaining better results for spilling on-the-fly</i>. In this work, we study both approaches and propose two new techniques, one for each approach. Our new algorithms try to address the drawbacks observed in previous proposals. We show that the new algorithms outperform previous techniques and, at the same time, reduce compilation time. We also show that, much to our surprise, a posteriori</i> spilling can be in fact slitghtly more effective than on-the-fly</i> spilling.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {180--189},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1065010.1065032},
 doi = {http://doi.acm.org/10.1145/1065010.1065032},
 acmid = {1065032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modulo scheduling, register allocation, spill code},
} 

@article{Aleta:2005:DOS:1064978.1065032,
 author = {Alet\`{a}, Alex and Codina, Josep M. and Gonz\`{a}lez, Antonio and Kaeli, David},
 title = {Demystifying on-the-fly spill code},
 abstract = {Modulo scheduling is an effective code generation technique that exploits the parallelism in program loops by overlapping iterations. One drawback of this optimization is that register requirements increase significantly because values across different loop iterations can be live concurrently. One possible solution to reduce register pressure is to insert spill code to release registers. Spill code stores values to memory between the producer and consumer instructions.Spilling heuristics can be divided into two classes: 1) a posteriori</i> approaches (spill code is inserted after scheduling the loop) or 2) on-the-fly</i> approaches (spill code is inserted during loop scheduling). Recent studies have reported obtaining better results for spilling on-the-fly</i>. In this work, we study both approaches and propose two new techniques, one for each approach. Our new algorithms try to address the drawbacks observed in previous proposals. We show that the new algorithms outperform previous techniques and, at the same time, reduce compilation time. We also show that, much to our surprise, a posteriori</i> spilling can be in fact slitghtly more effective than on-the-fly</i> spilling.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {180--189},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064978.1065032},
 doi = {http://doi.acm.org/10.1145/1064978.1065032},
 acmid = {1065032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modulo scheduling, register allocation, spill code},
} 

@article{Luk:2005:PBC:1064978.1065034,
 author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
 title = {Pin: building customized program analysis tools with dynamic instrumentation},
 abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin</i>. Our goals are to provide easy-to-use, portable, transparent</i>, and efficient</i> instrumentation. Instrumentation tools (called Pintools</i>) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent</i> whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent</i> as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation</i> to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium\&#174;, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {190--200},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065034},
 doi = {http://doi.acm.org/10.1145/1064978.1065034},
 acmid = {1065034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilation, instrumentation, program analysis tools},
} 

@inproceedings{Luk:2005:PBC:1065010.1065034,
 author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
 title = {Pin: building customized program analysis tools with dynamic instrumentation},
 abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin</i>. Our goals are to provide easy-to-use, portable, transparent</i>, and efficient</i> instrumentation. Instrumentation tools (called Pintools</i>) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent</i> whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent</i> as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation</i> to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium\&#174;, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {190--200},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065034},
 doi = {http://doi.acm.org/10.1145/1065010.1065034},
 acmid = {1065034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilation, instrumentation, program analysis tools},
} 

@article{Ayers:2005:TFF:1064978.1065035,
 author = {Ayers, Andrew and Schooler, Richard and Metcalf, Chris and Agarwal, Anant and Rhee, Junghwan and Witchel, Emmett},
 title = {TraceBack: first fault diagnosis by reconstruction of distributed control flow},
 abstract = {Faults that occur in production systems are the most important faults to fix, but most production systems lack the debugging facilities present in development environments. TraceBack provides debugging information for production systems by providing execution history data about program problems (such as crashes, hangs, and exceptions). TraceBack supports features commonly found in production environments such as multiple threads, dynamically loaded modules, multiple source languages (e.g., Java applications running with JNI modules written in C++), and distributed execution across multiple computers. TraceBack supports first fault diagnosis</i>-discovering what went wrong the first time a fault is encountered. The user can see how the program reached the fault state without having to re-run the computation; in effect enabling a limited form of a debugger in production code.TraceBack uses static, binary program analysis to inject low-overhead runtime instrumentation at control-flow block granularity. Post-facto reconstruction of the records written by the instrumentation code produces a source-statement trace for user diagnosis. The trace shows the dynamic instruction sequence leading up to the fault state, even when the program took exceptions or terminated abruptly (e.g., kill -9).We have implemented TraceBack on a variety of architectures and operating systems, and present examples from a variety of platforms. Performance overhead is variable, from 5\% for Apache running SPECweb99, to 16\%-25\% for the Java SPECJbb benchmark, to 60\% average for SPECint2000. We show examples of TraceBack's cross-language and cross-machine abilities, and report its use in diagnosing problems in production software.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {201--212},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065035},
 doi = {http://doi.acm.org/10.1145/1064978.1065035},
 acmid = {1065035},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault diagnosis, instrumentation},
} 

@inproceedings{Ayers:2005:TFF:1065010.1065035,
 author = {Ayers, Andrew and Schooler, Richard and Metcalf, Chris and Agarwal, Anant and Rhee, Junghwan and Witchel, Emmett},
 title = {TraceBack: first fault diagnosis by reconstruction of distributed control flow},
 abstract = {Faults that occur in production systems are the most important faults to fix, but most production systems lack the debugging facilities present in development environments. TraceBack provides debugging information for production systems by providing execution history data about program problems (such as crashes, hangs, and exceptions). TraceBack supports features commonly found in production environments such as multiple threads, dynamically loaded modules, multiple source languages (e.g., Java applications running with JNI modules written in C++), and distributed execution across multiple computers. TraceBack supports first fault diagnosis</i>-discovering what went wrong the first time a fault is encountered. The user can see how the program reached the fault state without having to re-run the computation; in effect enabling a limited form of a debugger in production code.TraceBack uses static, binary program analysis to inject low-overhead runtime instrumentation at control-flow block granularity. Post-facto reconstruction of the records written by the instrumentation code produces a source-statement trace for user diagnosis. The trace shows the dynamic instruction sequence leading up to the fault state, even when the program took exceptions or terminated abruptly (e.g., kill -9).We have implemented TraceBack on a variety of architectures and operating systems, and present examples from a variety of platforms. Performance overhead is variable, from 5\% for Apache running SPECweb99, to 16\%-25\% for the Java SPECJbb benchmark, to 60\% average for SPECint2000. We show examples of TraceBack's cross-language and cross-machine abilities, and report its use in diagnosing problems in production software.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {201--212},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065035},
 doi = {http://doi.acm.org/10.1145/1065010.1065035},
 acmid = {1065035},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault diagnosis, instrumentation},
} 

@inproceedings{Godefroid:2005:DDA:1065010.1065036,
 author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
 title = {DART: directed automated random testing},
 abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated</i> extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random</i> testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct</i> systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing</i>, or DART</i> for short. The main strength of DART is thus that testing can be performed completely automatically</i> on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {213--223},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065036},
 doi = {http://doi.acm.org/10.1145/1065010.1065036},
 acmid = {1065036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test generation, interfaces, program verification, random testing, software testing},
} 

@article{Godefroid:2005:DDA:1064978.1065036,
 author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
 title = {DART: directed automated random testing},
 abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated</i> extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random</i> testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct</i> systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing</i>, or DART</i> for short. The main strength of DART is thus that testing can be performed completely automatically</i> on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {213--223},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065036},
 doi = {http://doi.acm.org/10.1145/1064978.1065036},
 acmid = {1065036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test generation, interfaces, program verification, random testing, software testing},
} 

@article{Chen:2005:SAH:1064978.1065038,
 author = {Chen, Michael K. and Li, Xiao Feng and Lian, Ruiqi and Lin, Jason H. and Liu, Lixia and Liu, Tao and Ju, Roy},
 title = {Shangri-La: achieving high performance from compiled network applications while enabling ease of programming},
 abstract = {Programming network processors is challenging. To sustain high line rates, network processors have extremely tight memory access and instruction budgets. Achieving desired performance has traditionally required hand-coded assembly. Researchers have recently proposed high-level programming languages for packet processing, but the challenges of compiling these languages into code that is competitive with hand-tuned assembly remain unanswered.This paper describes the Shangri-La compiler, which accepts a packet program written in a C-like high-level language and applies scalar and specialized optimizations to generate a highly optimized binary. Hot code paths identified by profiling are mapped across processing elements to maximize processor utilization. Since our compilation target has no hardware caches, software-controlled caches are generated for frequently accessed application data structures. Packet handling optimizations significantly reduce per-packet memory access and instruction counts. Finally, a custom stack model maps stack frames to the fastest levels of the target processor's heterogeneous memory hierarchy.Binaries generated by the compiler were evaluated on the Intel IXP2400 network processor with eight packet processing cores and eight threads per core. Our results show the importance of both traditional and specialized optimization techniques for achieving the maximum forwarding rates on three network applications, L3-Switch, MPLS</i> and Firewall</i>.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {224--236},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1064978.1065038},
 doi = {http://doi.acm.org/10.1145/1064978.1065038},
 acmid = {1065038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, dataflow programming, network processors, packet processing, program partitioning, throughput-oriented computing},
} 

@inproceedings{Chen:2005:SAH:1065010.1065038,
 author = {Chen, Michael K. and Li, Xiao Feng and Lian, Ruiqi and Lin, Jason H. and Liu, Lixia and Liu, Tao and Ju, Roy},
 title = {Shangri-La: achieving high performance from compiled network applications while enabling ease of programming},
 abstract = {Programming network processors is challenging. To sustain high line rates, network processors have extremely tight memory access and instruction budgets. Achieving desired performance has traditionally required hand-coded assembly. Researchers have recently proposed high-level programming languages for packet processing, but the challenges of compiling these languages into code that is competitive with hand-tuned assembly remain unanswered.This paper describes the Shangri-La compiler, which accepts a packet program written in a C-like high-level language and applies scalar and specialized optimizations to generate a highly optimized binary. Hot code paths identified by profiling are mapped across processing elements to maximize processor utilization. Since our compilation target has no hardware caches, software-controlled caches are generated for frequently accessed application data structures. Packet handling optimizations significantly reduce per-packet memory access and instruction counts. Finally, a custom stack model maps stack frames to the fastest levels of the target processor's heterogeneous memory hierarchy.Binaries generated by the compiler were evaluated on the Intel IXP2400 network processor with eight packet processing cores and eight threads per core. Our results show the importance of both traditional and specialized optimization techniques for achieving the maximum forwarding rates on three network applications, L3-Switch, MPLS</i> and Firewall</i>.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {224--236},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1065010.1065038},
 doi = {http://doi.acm.org/10.1145/1065010.1065038},
 acmid = {1065038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, dataflow programming, network processors, packet processing, program partitioning, throughput-oriented computing},
} 

@inproceedings{Dai:2005:APP:1065010.1065039,
 author = {Dai, Jinquan and Huang, Bo and Li, Long and Harrison, Luddy},
 title = {Automatically partitioning packet processing applications for pipelined architectures},
 abstract = {Modern network processors employs parallel processing engines (PEs) to keep up with explosive internet packet processing demands. Most network processors further allow processing engines to be organized in a pipelined fashion to enable higher processing throughput and flexibility. In this paper, we present a novel program transformation technique to exploit parallel and pipelined computing power of modern network processors. Our proposed method automatically partitions a sequential packet processing application into coordinated pipelined parallel subtasks which can be naturally mapped to contemporary high-performance network processors. Our transformation technique ensures that packet processing tasks are balanced among pipeline stages and that data transmission between pipeline stages is minimized. We have implemented the proposed transformation method in an auto-partitioning C compiler product for Intel Network Processors. Experimental results show that our method provides impressive speed up for the commonly used NPF IPv4 forwarding and IP forwarding benchmarks. For a 9-stage pipeline, our auto-partitioning C compiler obtained more than 4X speedup for the IPv4 forwarding PPS and the IP forwarding PPS (for both the IPv4 traffic and IPv6 traffic).},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065039},
 doi = {http://doi.acm.org/10.1145/1065010.1065039},
 acmid = {1065039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {live-set transmission, network processor, packet processing, parallel, pipelining transformation, program partition},
} 

@article{Dai:2005:APP:1064978.1065039,
 author = {Dai, Jinquan and Huang, Bo and Li, Long and Harrison, Luddy},
 title = {Automatically partitioning packet processing applications for pipelined architectures},
 abstract = {Modern network processors employs parallel processing engines (PEs) to keep up with explosive internet packet processing demands. Most network processors further allow processing engines to be organized in a pipelined fashion to enable higher processing throughput and flexibility. In this paper, we present a novel program transformation technique to exploit parallel and pipelined computing power of modern network processors. Our proposed method automatically partitions a sequential packet processing application into coordinated pipelined parallel subtasks which can be naturally mapped to contemporary high-performance network processors. Our transformation technique ensures that packet processing tasks are balanced among pipeline stages and that data transmission between pipeline stages is minimized. We have implemented the proposed transformation method in an auto-partitioning C compiler product for Intel Network Processors. Experimental results show that our method provides impressive speed up for the commonly used NPF IPv4 forwarding and IP forwarding benchmarks. For a 9-stage pipeline, our auto-partitioning C compiler obtained more than 4X speedup for the IPv4 forwarding PPS and the IP forwarding PPS (for both the IPv4 traffic and IPv6 traffic).},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065039},
 doi = {http://doi.acm.org/10.1145/1064978.1065039},
 acmid = {1065039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {live-set transmission, network processor, packet processing, parallel, pipelining transformation, program partition},
} 

@inproceedings{Ni:2005:PAN:1065010.1065040,
 author = {Ni, Yang and Kremer, Ulrich and Stere, Adrian and Iftode, Liviu},
 title = {Programming ad-hoc networks of mobile and resource-constrained devices},
 abstract = {Ad-hoc networks of mobile devices such as smart phones and PDAs represent a new and exciting distributed system architecture. Building distributed applications on such an architecture poses new design challenges in programming models, languages, compilers, and runtime systems. This paper discusses SpatialViews, a high-level language designed for programming mobile devices connected through a wireless ad-hoc network. SpatialViews allows specification of virtual networks with nodes providing desired services and residing in interesting spaces. These nodes are discovered dynamically with user-specified time constraints and quality of result (QoR). The programming model supports "best-effort" semantics, i.e., different executions of the same program may result in "correct" answers of different quality. It is the responsibility of the compiler and runtime system to produce a high-quality answer for the particular network and resource conditions encountered during program execution. Four applications, which exercise different features of the SpatialViews language, are presented to demonstrate the expressiveness of the language and the efficiency of the compiler generated code. The applications are an application that collects and aggregates sensor data in network, an application that performs dynamic service installation, a mobile camera application that supports computation offloading for image understanding, and an augmented-reality (AR) Pacman game. The efficiency of the compiler generated code is verified through simulation and physical measurements. The reported results show that SpatialViews is an expressive and effective language for ad-hoc networks. In addition, compiler optimizations can significantly improve response times and energy consumption.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065040},
 doi = {http://doi.acm.org/10.1145/1065010.1065040},
 acmid = {1065040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MANET, ad-hoc networks, location-awareness, quality of result, service discovery},
} 

@article{Ni:2005:PAN:1064978.1065040,
 author = {Ni, Yang and Kremer, Ulrich and Stere, Adrian and Iftode, Liviu},
 title = {Programming ad-hoc networks of mobile and resource-constrained devices},
 abstract = {Ad-hoc networks of mobile devices such as smart phones and PDAs represent a new and exciting distributed system architecture. Building distributed applications on such an architecture poses new design challenges in programming models, languages, compilers, and runtime systems. This paper discusses SpatialViews, a high-level language designed for programming mobile devices connected through a wireless ad-hoc network. SpatialViews allows specification of virtual networks with nodes providing desired services and residing in interesting spaces. These nodes are discovered dynamically with user-specified time constraints and quality of result (QoR). The programming model supports "best-effort" semantics, i.e., different executions of the same program may result in "correct" answers of different quality. It is the responsibility of the compiler and runtime system to produce a high-quality answer for the particular network and resource conditions encountered during program execution. Four applications, which exercise different features of the SpatialViews language, are presented to demonstrate the expressiveness of the language and the efficiency of the compiler generated code. The applications are an application that collects and aggregates sensor data in network, an application that performs dynamic service installation, a mobile camera application that supports computation offloading for image understanding, and an augmented-reality (AR) Pacman game. The efficiency of the compiler generated code is verified through simulation and physical measurements. The reported results show that SpatialViews is an expressive and effective language for ad-hoc networks. In addition, compiler optimizations can significantly improve response times and energy consumption.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065040},
 doi = {http://doi.acm.org/10.1145/1064978.1065040},
 acmid = {1065040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MANET, ad-hoc networks, location-awareness, quality of result, service discovery},
} 

@inproceedings{Boehm:2005:TCI:1065010.1065042,
 author = {Boehm, Hans-J.},
 title = {Threads cannot be implemented as a library},
 abstract = {In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure library-based approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure type-safety, which would entail even stronger constraints. The issues we raise are not specific to that context.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {261--268},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1065010.1065042},
 doi = {http://doi.acm.org/10.1145/1065010.1065042},
 acmid = {1065042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data race, optimization, pthreads, register promotion, threads},
} 

@article{Boehm:2005:TCI:1064978.1065042,
 author = {Boehm, Hans-J.},
 title = {Threads cannot be implemented as a library},
 abstract = {In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure library-based approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure type-safety, which would entail even stronger constraints. The issues we raise are not specific to that context.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {261--268},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1064978.1065042},
 doi = {http://doi.acm.org/10.1145/1064978.1065042},
 acmid = {1065042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data race, optimization, pthreads, register promotion, threads},
} 

@inproceedings{Quinones:2005:MCI:1065010.1065043,
 author = {Qui\~{n}ones, Carlos Garc\'{\i}a and Madriles, Carlos and S\'{a}nchez, Jes\'{u}s and Marcuello, Pedro and Gonz\'{a}lez, Antonio and Tullsen, Dean M.},
 title = {Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices},
 abstract = {Speculative parallelization can provide significant sources of additional thread-level parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail.The management of inter-thread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. P-slices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing.Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {269--279},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1065010.1065043},
 doi = {http://doi.acm.org/10.1145/1065010.1065043},
 acmid = {1065043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, pre-computation slices, speculative multithreading, thread-level parallelism},
} 

@article{Quinones:2005:MCI:1064978.1065043,
 author = {Qui\~{n}ones, Carlos Garc\'{\i}a and Madriles, Carlos and S\'{a}nchez, Jes\'{u}s and Marcuello, Pedro and Gonz\'{a}lez, Antonio and Tullsen, Dean M.},
 title = {Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices},
 abstract = {Speculative parallelization can provide significant sources of additional thread-level parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail.The management of inter-thread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. P-slices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing.Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {269--279},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064978.1065043},
 doi = {http://doi.acm.org/10.1145/1064978.1065043},
 acmid = {1065043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, pre-computation slices, speculative multithreading, thread-level parallelism},
} 

@inproceedings{Herlihy:2005:TMS:1065010.1065011,
 author = {Herlihy, Maurice},
 title = {The transactional manifesto: software engineering and non-blocking synchronization},
 abstract = {Computer architecture is about to undergo, if not another revolution, then a vigorous shaking-up. The major chip manufacturers have, for the time being, simply given up trying to make processors run faster. Instead, they have recently started shipping "multicore" architectures, in which multiple processors (cores) communicate directly through shared hardware caches, providing increased concurrency instead of increased clock speed.As a result, system designers and software engineers can no longer rely on increasing clock speed to hide software bloat. Instead, they must somehow learn to make effective use of increasing parallelism. This adaptation will not be easy. Conventional synchronization techniques based on locks and conditions are unlikely to be effective in such a demanding environment. Coarse-grained locks, which protect relatively large amounts of data, do not scale, and fine-grained locks introduce substantial software engineering problems.Transactional memory is a computational model in which threads synchronize by optimistic, lock-free transactions. This synchronization model promises to alleviate many (perhaps not all) of the problems associated with locking, and there is a growing community of researchers working on both software and hardware support for this approach. This talk will survey the area, with a focus on open research problems.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {280--280},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1065010.1065011},
 doi = {http://doi.acm.org/10.1145/1065010.1065011},
 acmid = {1065011},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Solar-Lezama:2005:PSB:1065010.1065045,
 author = {Solar-Lezama, Armando and Rabbah, Rodric and Bod\'{\i}k, Rastislav and Ebcio\u{g}lu, Kemal},
 title = {Programming by sketching for bit-streaming programs},
 abstract = {This paper introduces the concept of programming with sketches</i>, an approach for the rapid development of high-performance applications. This approach allows a programmer to write clean and portable reference code, and then obtain a high-quality implementation by simply sketching</i> the outlines of the desired implementation. Subsequently, a compiler automatically fills in the missing details while also ensuring that a completed sketch is faithful to the input reference code. In this paper, we develop StreamBit as a sketching methodology for the important class of bit-streaming programs (e.g., coding and cryptography).A sketch is a partial</i> specification of the implementation, and as such, it affords several benefits to programmer in terms of productivity and code robustness. First, a sketch is easier to write compared to a complete implementation. Second, sketching allows the programmer to focus on exploiting algorithmic properties rather than on orchestrating low-level details. Third, a sketch-aware compiler rejects "buggy" sketches, thus improving reliability while allowing the programmer to quickly evaluate sophisticated implementation ideas.We evaluated the productivity and performance benefits of our programming methodology in a user-study, where a group of novice StreamBit programmers competed with a group of experienced C programmers on implementing a cipher. We learned that, given the same time budget, the ciphers developed in StreamBit ran 2.5x faster than ciphers coded in C. We also produced implementations of DES and Serpent that were competitive with hand optimized implementations available in the public domain.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {281--294},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1065010.1065045},
 doi = {http://doi.acm.org/10.1145/1065010.1065045},
 acmid = {1065045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {StreamIt, domain specific compiler, domain specific language, sketching, stream programming, synchronous dataflow},
} 

@article{Solar-Lezama:2005:PSB:1064978.1065045,
 author = {Solar-Lezama, Armando and Rabbah, Rodric and Bod\'{\i}k, Rastislav and Ebcio\u{g}lu, Kemal},
 title = {Programming by sketching for bit-streaming programs},
 abstract = {This paper introduces the concept of programming with sketches</i>, an approach for the rapid development of high-performance applications. This approach allows a programmer to write clean and portable reference code, and then obtain a high-quality implementation by simply sketching</i> the outlines of the desired implementation. Subsequently, a compiler automatically fills in the missing details while also ensuring that a completed sketch is faithful to the input reference code. In this paper, we develop StreamBit as a sketching methodology for the important class of bit-streaming programs (e.g., coding and cryptography).A sketch is a partial</i> specification of the implementation, and as such, it affords several benefits to programmer in terms of productivity and code robustness. First, a sketch is easier to write compared to a complete implementation. Second, sketching allows the programmer to focus on exploiting algorithmic properties rather than on orchestrating low-level details. Third, a sketch-aware compiler rejects "buggy" sketches, thus improving reliability while allowing the programmer to quickly evaluate sophisticated implementation ideas.We evaluated the productivity and performance benefits of our programming methodology in a user-study, where a group of novice StreamBit programmers competed with a group of experienced C programmers on implementing a cipher. We learned that, given the same time budget, the ciphers developed in StreamBit ran 2.5x faster than ciphers coded in C. We also produced implementations of DES and Serpent that were competitive with hand optimized implementations available in the public domain.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {281--294},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1064978.1065045},
 doi = {http://doi.acm.org/10.1145/1064978.1065045},
 acmid = {1065045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {StreamIt, domain specific compiler, domain specific language, sketching, stream programming, synchronous dataflow},
} 

@article{Fisher:2005:PDL:1064978.1065046,
 author = {Fisher, Kathleen and Gruber, Robert},
 title = {PADS: a domain-specific language for processing ad hoc data},
 abstract = {PADS is a declarative data description language that allows data analysts to describe both the physical layout of ad hoc data sources and semantic properties of that data. From such descriptions, the PADS compiler generates libraries and tools for manipulating the data, including parsing routines, statistical profiling tools, translation programs to produce well-behaved formats such as Xml or those required for loading relational databases, and tools for running XQueries over raw PADS data sources. The descriptions are concise enough to serve as "living" documentation while flexible enough to describe most of the ASCII, binary, and Cobol formats that we have seen in practice. The generated parsing library provides for robust, application-specific error handling.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {295--304},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064978.1065046},
 doi = {http://doi.acm.org/10.1145/1064978.1065046},
 acmid = {1065046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data description language, domain-specific languages},
} 

@inproceedings{Fisher:2005:PDL:1065010.1065046,
 author = {Fisher, Kathleen and Gruber, Robert},
 title = {PADS: a domain-specific language for processing ad hoc data},
 abstract = {PADS is a declarative data description language that allows data analysts to describe both the physical layout of ad hoc data sources and semantic properties of that data. From such descriptions, the PADS compiler generates libraries and tools for manipulating the data, including parsing routines, statistical profiling tools, translation programs to produce well-behaved formats such as Xml or those required for loading relational databases, and tools for running XQueries over raw PADS data sources. The descriptions are concise enough to serve as "living" documentation while flexible enough to describe most of the ASCII, binary, and Cobol formats that we have seen in practice. The generated parsing library provides for robust, application-specific error handling.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {295--304},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1065010.1065046},
 doi = {http://doi.acm.org/10.1145/1065010.1065046},
 acmid = {1065046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data description language, domain-specific languages},
} 

@article{Bauer:2005:CSP:1064978.1065047,
 author = {Bauer, Lujo and Ligatti, Jay and Walker, David},
 title = {Composing security policies with polymer},
 abstract = {We introduce a language and system that supports definition and composition of complex run-time security policies for Java applications. Our policies are comprised of two sorts of methods. The first is query</i> methods that are called whenever an untrusted application tries to execute a security-sensitive action. A query method returns a suggestion</i> indicating how the security-sensitive action should be handled. The second sort of methods are those that perform state updates as the policy's suggestions are followed.The structure of our policies facilitates composition, as policies can query other policies for suggestions. In order to give programmers control over policy composition, we have designed the system so that policies, suggestions, and application events are all first-class objects that a higher-order policy may manipulate. We show how to use these programming features by developing a library of policy combinators.Our system is fully implemented, and we have defined a formal semantics for an idealized subset of the language containing all of the key features. We demonstrate the effectiveness of our system by implementing a large-scale security policy for an email client.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {305--314},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064978.1065047},
 doi = {http://doi.acm.org/10.1145/1064978.1065047},
 acmid = {1065047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composable security policies, edit automata, program monitors, run-time enforcement, security automata},
} 

@inproceedings{Bauer:2005:CSP:1065010.1065047,
 author = {Bauer, Lujo and Ligatti, Jay and Walker, David},
 title = {Composing security policies with polymer},
 abstract = {We introduce a language and system that supports definition and composition of complex run-time security policies for Java applications. Our policies are comprised of two sorts of methods. The first is query</i> methods that are called whenever an untrusted application tries to execute a security-sensitive action. A query method returns a suggestion</i> indicating how the security-sensitive action should be handled. The second sort of methods are those that perform state updates as the policy's suggestions are followed.The structure of our policies facilitates composition, as policies can query other policies for suggestions. In order to give programmers control over policy composition, we have designed the system so that policies, suggestions, and application events are all first-class objects that a higher-order policy may manipulate. We show how to use these programming features by developing a library of policy combinators.Our system is fully implemented, and we have defined a formal semantics for an idealized subset of the language containing all of the key features. We demonstrate the effectiveness of our system by implementing a large-scale security policy for an email client.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {305--314},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1065010.1065047},
 doi = {http://doi.acm.org/10.1145/1065010.1065047},
 acmid = {1065047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composable security policies, edit automata, program monitors, run-time enforcement, security automata},
} 

@article{Franchetti:2005:FLM:1064978.1065048,
 author = {Franchetti, Franz and Voronenko, Yevgen and P\"{u}schel, Markus},
 title = {Formal loop merging for signal transforms},
 abstract = {A critical optimization in the domain of linear signal transforms, such as the discrete Fourier transform (DFT), is loop merging, which increases data locality and reuse and thus performance. In particular, this includes the conversion of shuffle operations into array reindexings. To date, loop merging is well understood only for the DFT, and only for Cooley-Tukey FFT based algorithms, which excludes DFT sizes divisible by large primes. In this paper, we present a formal loop merging framework for general signal transforms and its implementation within the SPIRAL code generator. The framework consists of \&#917;-SPL, a mathematical language to express loops and index mappings; a rewriting system to merge loops in \&#917;-SPL and a compiler that translates \&#917;-SPL into code. We apply the framework to DFT sizes that cannot be handled using only the Cooley-Tukey FFT and compare our method to FFTW 3.0.1 and the vendor library Intel MKL 7.2.1. Compared to FFTW our generated code is a factor of 2--4 faster under equal implementation conditions (same algorithms, same unrolling threshold). For some sizes we show a speed-up of a factor of 9 using Bluestein's algorithm. Further, we give a detailed comparison against the Intel vendor library MKL; our generated code is between 2 times faster and 4.5 times slower.},
 journal = {SIGPLAN Not.},
 volume = {40},
 issue = {6},
 month = {June},
 year = {2005},
 issn = {0362-1340},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064978.1065048},
 doi = {http://doi.acm.org/10.1145/1064978.1065048},
 acmid = {1065048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DFT, automatic performance tuning, discrete Fourier transform, domain-specific language, linear signal transform, loop optimization},
} 

@inproceedings{Franchetti:2005:FLM:1065010.1065048,
 author = {Franchetti, Franz and Voronenko, Yevgen and P\"{u}schel, Markus},
 title = {Formal loop merging for signal transforms},
 abstract = {A critical optimization in the domain of linear signal transforms, such as the discrete Fourier transform (DFT), is loop merging, which increases data locality and reuse and thus performance. In particular, this includes the conversion of shuffle operations into array reindexings. To date, loop merging is well understood only for the DFT, and only for Cooley-Tukey FFT based algorithms, which excludes DFT sizes divisible by large primes. In this paper, we present a formal loop merging framework for general signal transforms and its implementation within the SPIRAL code generator. The framework consists of \&#917;-SPL, a mathematical language to express loops and index mappings; a rewriting system to merge loops in \&#917;-SPL and a compiler that translates \&#917;-SPL into code. We apply the framework to DFT sizes that cannot be handled using only the Cooley-Tukey FFT and compare our method to FFTW 3.0.1 and the vendor library Intel MKL 7.2.1. Compared to FFTW our generated code is a factor of 2--4 faster under equal implementation conditions (same algorithms, same unrolling threshold). For some sizes we show a speed-up of a factor of 9 using Bluestein's algorithm. Further, we give a detailed comparison against the Intel vendor library MKL; our generated code is between 2 times faster and 4.5 times slower.},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
 series = {PLDI '05},
 year = {2005},
 isbn = {1-59593-056-6},
 location = {Chicago, IL, USA},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1065010.1065048},
 doi = {http://doi.acm.org/10.1145/1065010.1065048},
 acmid = {1065048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DFT, automatic performance tuning, discrete Fourier transform, domain-specific language, linear signal transform, loop optimization},
} 

@inproceedings{Gay:2003:NLH:781131.781133,
 author = {Gay, David and Levis, Philip and von Behren, Robert and Welsh, Matt and Brewer, Eric and Culler, David},
 title = {The <i>nesC</i> language: A holistic approach to networked embedded systems},
 abstract = {We present nesC</i>, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, low-power "motes," each of which execute concurrent, reactive programs that must operate with severe memory and power constraints.nesC's contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption).nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781131.781133},
 doi = {http://doi.acm.org/10.1145/781131.781133},
 acmid = {781133},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, TinyOS, components, concurrency, data races, first-order, modules, nesC, programming languages},
} 

@article{Gay:2003:NLH:780822.781133,
 author = {Gay, David and Levis, Philip and von Behren, Robert and Welsh, Matt and Brewer, Eric and Culler, David},
 title = {The <i>nesC</i> language: A holistic approach to networked embedded systems},
 abstract = {We present nesC</i>, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, low-power "motes," each of which execute concurrent, reactive programs that must operate with severe memory and power constraints.nesC's contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption).nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/780822.781133},
 doi = {http://doi.acm.org/10.1145/780822.781133},
 acmid = {781133},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, TinyOS, components, concurrency, data races, first-order, modules, nesC, programming languages},
} 

@article{Lamb:2003:LAO:780822.781134,
 author = {Lamb, Andrew A. and Thies, William and Amarasinghe, Saman},
 title = {Linear analysis and optimization of stream programs},
 abstract = {As more complex DSP algorithms are realized in practice, there is an increasing need for high-level stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the C-like code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fully-automatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450\% performance improvement over our benchmark suite.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {12--25},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781134},
 doi = {http://doi.acm.org/10.1145/780822.781134},
 acmid = {781134},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DSP, FFT, StreamIt, algebraic simplification, embedded, linear systems, optimization, stream programming},
} 

@inproceedings{Lamb:2003:LAO:781131.781134,
 author = {Lamb, Andrew A. and Thies, William and Amarasinghe, Saman},
 title = {Linear analysis and optimization of stream programs},
 abstract = {As more complex DSP algorithms are realized in practice, there is an increasing need for high-level stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the C-like code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fully-automatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450\% performance improvement over our benchmark suite.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {12--25},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781134},
 doi = {http://doi.acm.org/10.1145/781131.781134},
 acmid = {781134},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DSP, FFT, StreamIt, algebraic simplification, embedded, linear systems, optimization, stream programming},
} 

@article{George:2003:TIN:780822.781135,
 author = {George, Lal and Blu Matthias},
 title = {Taming the IXP network processor},
 abstract = {We compile Nova, a new language designed for writing network processing applications, using a back end based on integer-linear programming (ILP) for register allocation, optimal bank assignment, and spills. The compiler's optimizer employs CPS as its intermediate representation; some of the invariants that this IR guarantees are essential for the formulation of a practical ILP model.Appel and George used a similar ILP-based technique for the IA32 to decide which variables reside in registers but deferred the actual assignment of colors to a later phase. We demonstrate how to carry over their idea to an architecture with many more banks, register aggregates, variables with multiple simultaneous register assignments, and, very importantly, one where bank- and register-assignment cannot be done in isolation from each other. Our approach performs well in practise---without causing an explosion in size or solve time of the generated integer linear programs.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781135},
 doi = {http://doi.acm.org/10.1145/780822.781135},
 acmid = {781135},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Intel IXA, bank assignment, code generation, integer linear programming, network processors, programming languages, register allocation},
} 

@inproceedings{George:2003:TIN:781131.781135,
 author = {George, Lal and Blu Matthias},
 title = {Taming the IXP network processor},
 abstract = {We compile Nova, a new language designed for writing network processing applications, using a back end based on integer-linear programming (ILP) for register allocation, optimal bank assignment, and spills. The compiler's optimizer employs CPS as its intermediate representation; some of the invariants that this IR guarantees are essential for the formulation of a practical ILP model.Appel and George used a similar ILP-based technique for the IA32 to decide which variables reside in registers but deferred the actual assignment of colors to a later phase. We demonstrate how to carry over their idea to an architecture with many more banks, register aggregates, variables with multiple simultaneous register assignments, and, very importantly, one where bank- and register-assignment cannot be done in isolation from each other. Our approach performs well in practise---without causing an explosion in size or solve time of the generated integer linear programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781135},
 doi = {http://doi.acm.org/10.1145/781131.781135},
 acmid = {781135},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Intel IXA, bank assignment, code generation, integer linear programming, network processors, programming languages, register allocation},
} 

@inproceedings{Hsu:2003:DIE:781131.781137,
 author = {Hsu, Chung-Hsing and Kremer, Ulrich},
 title = {The design, implementation, and evaluation of a compiler algorithm for CPU energy reduction},
 abstract = {This paper presents the design and implementation of a compiler algorithm that effectively optimizes programs for energy usage using dynamic voltage scaling (DVS). The algorithm identifies program regions where the CPU can be slowed down with negligible performance loss. It is implemented as a source-to-source level transformation using the SUIF2 compiler infrastructure. Physical measurements on a high-performance laptop show that total system</i> (i.e., laptop) energy savings of up to 28\% can be achieved with performance degradation of less than 5\% for the <b>SPECfp95</b> benchmarks. On average, the system energy and energy-delay product are reduced by 11\% and 9\%, respectively, with a performance slowdown of 2\%. It was also discovered that the energy usage of the programs using our DVS algorithm is within 6\% from the theoretical lower bound. To the best of our knowledge, this is one of the first work that evaluates DVS algorithms by physical measurements.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {38--48},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781131.781137},
 doi = {http://doi.acm.org/10.1145/781131.781137},
 acmid = {781137},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage scaling, energy savings},
} 

@article{Hsu:2003:DIE:780822.781137,
 author = {Hsu, Chung-Hsing and Kremer, Ulrich},
 title = {The design, implementation, and evaluation of a compiler algorithm for CPU energy reduction},
 abstract = {This paper presents the design and implementation of a compiler algorithm that effectively optimizes programs for energy usage using dynamic voltage scaling (DVS). The algorithm identifies program regions where the CPU can be slowed down with negligible performance loss. It is implemented as a source-to-source level transformation using the SUIF2 compiler infrastructure. Physical measurements on a high-performance laptop show that total system</i> (i.e., laptop) energy savings of up to 28\% can be achieved with performance degradation of less than 5\% for the <b>SPECfp95</b> benchmarks. On average, the system energy and energy-delay product are reduced by 11\% and 9\%, respectively, with a performance slowdown of 2\%. It was also discovered that the energy usage of the programs using our DVS algorithm is within 6\% from the theoretical lower bound. To the best of our knowledge, this is one of the first work that evaluates DVS algorithms by physical measurements.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {38--48},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/780822.781137},
 doi = {http://doi.acm.org/10.1145/780822.781137},
 acmid = {781137},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage scaling, energy savings},
} 

@article{Xie:2003:CDV:780822.781138,
 author = {Xie, Fen and Martonosi, Margaret and Malik, Sharad},
 title = {Compile-time dynamic voltage scaling settings: opportunities and limits},
 abstract = {With power-related concerns becoming dominant aspects of hardware and software design, significant research effort has been devoted towards system power minimization. Among run-time power-management techniques, dynamic voltage scaling (DVS) has emerged as an important approach, with the ability to provide significant power savings. DVS exploits the ability to control the power consumption by varying a processor's supply voltage (V) and clock frequency (f). DVS controls energy by scheduling different parts of the computation to different (V, f) pairs; the goal is to minimize energy while meeting performance needs. Although processors like the Intel XScale and Transmeta Crusoe allow software DVS control, such control has thus far largely been used at the process/task level under operating system control. This is mainly because the energy and time overhead for switching DVS modes is considered too large and difficult to manage within a single program.In this paper we explore the opportunities and limits of compile-time DVS scheduling. We derive an analytical model for the maximum energy savings that can be obtained using DVS given a few known program and processor parameters. We use this model to determine scenarios where energy consumption benefits from compile-time DVS and those where there is no benefit. The model helps us extrapolate the benefits of compile-time DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through optimal settings of DVS modes. This is done by extending the existing Mixed-integer Linear Program (MILP) formulation for this problem by accurately accounting for DVS energy switching overhead, by providing finer-grained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of compile-time DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {49--62},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781138},
 doi = {http://doi.acm.org/10.1145/780822.781138},
 acmid = {781138},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model, compiler, dynamic voltage scaling, low power, mixed-integer linear programming},
} 

@inproceedings{Xie:2003:CDV:781131.781138,
 author = {Xie, Fen and Martonosi, Margaret and Malik, Sharad},
 title = {Compile-time dynamic voltage scaling settings: opportunities and limits},
 abstract = {With power-related concerns becoming dominant aspects of hardware and software design, significant research effort has been devoted towards system power minimization. Among run-time power-management techniques, dynamic voltage scaling (DVS) has emerged as an important approach, with the ability to provide significant power savings. DVS exploits the ability to control the power consumption by varying a processor's supply voltage (V) and clock frequency (f). DVS controls energy by scheduling different parts of the computation to different (V, f) pairs; the goal is to minimize energy while meeting performance needs. Although processors like the Intel XScale and Transmeta Crusoe allow software DVS control, such control has thus far largely been used at the process/task level under operating system control. This is mainly because the energy and time overhead for switching DVS modes is considered too large and difficult to manage within a single program.In this paper we explore the opportunities and limits of compile-time DVS scheduling. We derive an analytical model for the maximum energy savings that can be obtained using DVS given a few known program and processor parameters. We use this model to determine scenarios where energy consumption benefits from compile-time DVS and those where there is no benefit. The model helps us extrapolate the benefits of compile-time DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through optimal settings of DVS modes. This is done by extending the existing Mixed-integer Linear Program (MILP) formulation for this problem by accurately accounting for DVS energy switching overhead, by providing finer-grained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of compile-time DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {49--62},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781138},
 doi = {http://doi.acm.org/10.1145/781131.781138},
 acmid = {781138},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model, compiler, dynamic voltage scaling, low power, mixed-integer linear programming},
} 

@article{Yotov:2003:CEM:780822.781140,
 author = {Yotov, Kamen and Li, Xiaoming and Ren, Gang and Cibulskis, Michael and DeJong, Gerald and Garzaran, Maria and Padua, David and Pingali, Keshav and Stodghill, Paul and Wu, Peng},
 title = {A comparison of empirical and model-driven optimization},
 abstract = {Empirical program optimizers estimate the values of key optimization parameters by generating different program versions and running them on the actual hardware to determine which values give the best performance. In contrast, conventional compilers use models of programs and machines to choose these parameters. It is widely believed that model-driven optimization does not compete with empirical optimization, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS (a system for generating a dense numerical linear algebra library called the BLAS) with a model-driven optimization engine that used detailed models to estimate values for optimization parameters, and then measured the relative performance of the two systems on three different hardware platforms. Our experiments show that model-driven optimization can be surprisingly effective, and can generate code whose performance is comparable to that of code generated by empirical optimizers for the BLAS.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {63--76},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781140},
 doi = {http://doi.acm.org/10.1145/780822.781140},
 acmid = {781140},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BLAS, blocking, code generation, compilers, empirical optimization, memory hierarchy, model-driven optimization, program transformation, tiling, unrolling},
} 

@inproceedings{Yotov:2003:CEM:781131.781140,
 author = {Yotov, Kamen and Li, Xiaoming and Ren, Gang and Cibulskis, Michael and DeJong, Gerald and Garzaran, Maria and Padua, David and Pingali, Keshav and Stodghill, Paul and Wu, Peng},
 title = {A comparison of empirical and model-driven optimization},
 abstract = {Empirical program optimizers estimate the values of key optimization parameters by generating different program versions and running them on the actual hardware to determine which values give the best performance. In contrast, conventional compilers use models of programs and machines to choose these parameters. It is widely believed that model-driven optimization does not compete with empirical optimization, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS (a system for generating a dense numerical linear algebra library called the BLAS) with a model-driven optimization engine that used detailed models to estimate values for optimization parameters, and then measured the relative performance of the two systems on three different hardware platforms. Our experiments show that model-driven optimization can be surprisingly effective, and can generate code whose performance is comparable to that of code generated by empirical optimizers for the BLAS.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {63--76},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781140},
 doi = {http://doi.acm.org/10.1145/781131.781140},
 acmid = {781140},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BLAS, blocking, code generation, compilers, empirical optimization, memory hierarchy, model-driven optimization, program transformation, tiling, unrolling},
} 

@article{Stephenson:2003:MOI:780822.781141,
 author = {Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O'Reilly, Una-May},
 title = {Meta optimization: improving compiler heuristics with machine learning},
 abstract = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23\% (up to 73\%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25\% on our training set, and 9\% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781141},
 doi = {http://doi.acm.org/10.1145/780822.781141},
 acmid = {781141},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler heuristics, genetic programming, machine learning, priority functions},
} 

@inproceedings{Stephenson:2003:MOI:781131.781141,
 author = {Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O'Reilly, Una-May},
 title = {Meta optimization: improving compiler heuristics with machine learning},
 abstract = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23\% (up to 73\%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25\% on our training set, and 9\% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781141},
 doi = {http://doi.acm.org/10.1145/781131.781141},
 acmid = {781141},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler heuristics, genetic programming, machine learning, priority functions},
} 

@inproceedings{Strout:2003:CCR:781131.781142,
 author = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne},
 title = {Compile-time composition of run-time data and iteration reorderings},
 abstract = {Many important applications, such as those using sparse data structures, have memory reference patterns that are unknown at compile-time. Prior work has developed run-time reorderings of data and computation that enhance locality in such applications.This paper presents a compile-time framework that allows the explicit composition of run-time data and iteration-reordering transformations. Our framework builds on the iteration-reordering framework of Kelly and Pugh to represent the effects of a given composition. To motivate our extension, we show that new compositions of run-time reordering transformations can result in better performance on three benchmarks.We show how to express a number of run-time data and iteration-reordering transformations that focus on improving data locality. We also describe the space of possible run-time reordering transformations and how existing transformations fit within it. Since sparse tiling techniques are included in our framework, they become more generally applicable, both to a larger class of applications, and in their composition with other reordering transformations. Finally, within the presented framework data need be remapped only once</i> at runtime for a given composition thus exhibiting one example of overhead reductions the framework can express.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781142},
 doi = {http://doi.acm.org/10.1145/781131.781142},
 acmid = {781142},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data remapping, inspector/executor, iteration reordering, optimization, run-time transformations, sparse tiling},
} 

@article{Strout:2003:CCR:780822.781142,
 author = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne},
 title = {Compile-time composition of run-time data and iteration reorderings},
 abstract = {Many important applications, such as those using sparse data structures, have memory reference patterns that are unknown at compile-time. Prior work has developed run-time reorderings of data and computation that enhance locality in such applications.This paper presents a compile-time framework that allows the explicit composition of run-time data and iteration-reordering transformations. Our framework builds on the iteration-reordering framework of Kelly and Pugh to represent the effects of a given composition. To motivate our extension, we show that new compositions of run-time reordering transformations can result in better performance on three benchmarks.We show how to express a number of run-time data and iteration-reordering transformations that focus on improving data locality. We also describe the space of possible run-time reordering transformations and how existing transformations fit within it. Since sparse tiling techniques are included in our framework, they become more generally applicable, both to a larger class of applications, and in their composition with other reordering transformations. Finally, within the presented framework data need be remapped only once</i> at runtime for a given composition thus exhibiting one example of overhead reductions the framework can express.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781142},
 doi = {http://doi.acm.org/10.1145/780822.781142},
 acmid = {781142},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data remapping, inspector/executor, iteration reordering, optimization, run-time transformations, sparse tiling},
} 

@inproceedings{Berndl:2003:PAU:781131.781144,
 author = {Berndl, Marc and Lhot\'{a}k, Ondrej and Qian, Feng and Hendren, Laurie and Umanee, Navindra},
 title = {Points-to analysis using BDDs},
 abstract = {This paper reports on a new approach to solving a subset-based points-to analysis for Java using Binary Decision Diagrams (BDDs). In the model checking community, BDDs have been shown very effective for representing large sets and solving very large verification problems. Our work shows that BDDs can also be very effective for developing a points-to analysis that is simple to implement and that scales well, in both space and time, to large programs.The paper first introduces BDDs and operations on BDDs using some simple points-to examples. Then, a complete subset-based points-to algorithm is presented, expressed completely using BDDs and BDD operations. This algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. Experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the BDD-based solver to an efficient hand-coded graph-based solver. Finally, based on the results of the BDD-based solver, a variety of BDD-based queries are presented, including the points-to query.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781144},
 doi = {http://doi.acm.org/10.1145/781131.781144},
 acmid = {781144},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, points-to analysis},
} 

@article{Berndl:2003:PAU:780822.781144,
 author = {Berndl, Marc and Lhot\'{a}k, Ondrej and Qian, Feng and Hendren, Laurie and Umanee, Navindra},
 title = {Points-to analysis using BDDs},
 abstract = {This paper reports on a new approach to solving a subset-based points-to analysis for Java using Binary Decision Diagrams (BDDs). In the model checking community, BDDs have been shown very effective for representing large sets and solving very large verification problems. Our work shows that BDDs can also be very effective for developing a points-to analysis that is simple to implement and that scales well, in both space and time, to large programs.The paper first introduces BDDs and operations on BDDs using some simple points-to examples. Then, a complete subset-based points-to algorithm is presented, expressed completely using BDDs and BDD operations. This algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. Experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the BDD-based solver to an efficient hand-coded graph-based solver. Finally, based on the results of the BDD-based solver, a variety of BDD-based queries are presented, including the points-to query.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781144},
 doi = {http://doi.acm.org/10.1145/780822.781144},
 acmid = {781144},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary decision diagrams, points-to analysis},
} 

@article{von Praun:2003:SCA:780822.781145,
 author = {von Praun, Christoph and Gross, Thomas R.},
 title = {Static conflict analysis for multi-threaded object-oriented programs},
 abstract = {A compiler for multi-threaded object-oriented programs needs information about the sharing of objects for a variety of reasons: to implement optimizations, to issue warnings, to add instrumentation to detect access violations that occur at runtime. An Object Use Graph (OUG) statically captures accesses from different threads to objects. An OUG extends the Heap Shape Graph (HSG), which is a compile-time abstraction for runtime objects (nodes) and their reference relations (edges). An OUG specifies for a specific node in the HSG a partial order of events relevant to the corresponding runtime object(s). Relevant events include read and write access, object escape, thread start and join.OUGs have been implemented in a Java compiler. Initial experience shows that OUGs are effective to identify object accesses that potentially conflict at runtime and isolate accesses that never cause a problem at runtime. The capabilities of OUGs are compared with an advanced program analysis that has been used for lock elimination. For the set of benchmarks investigated here, OUGs report only a fraction of shared objects as conflicting and reduce the number of compile-time reports in terms of allocation sites of conflicting objects by 28--92\% (average 64\%). For benchmarks of up to 30 KLOC, the time taken to construct OUGs is, with one exception, in the order of seconds.The information collected in the OUG has been used to instrument Java programs with checks for object races. OUGs provide precise information about object sharing and static protection, so runtime instrumentation that checks those cases that cannot be disambiguated at compile-time is sparse, and the total runtime overhead of checking for object races is only 3--86\% (average 47\%).},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {115--128},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781145},
 doi = {http://doi.acm.org/10.1145/780822.781145},
 acmid = {781145},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heap shape graph, object use graph, program analysis, race detection, representations for concurrent programs},
} 

@inproceedings{von Praun:2003:SCA:781131.781145,
 author = {von Praun, Christoph and Gross, Thomas R.},
 title = {Static conflict analysis for multi-threaded object-oriented programs},
 abstract = {A compiler for multi-threaded object-oriented programs needs information about the sharing of objects for a variety of reasons: to implement optimizations, to issue warnings, to add instrumentation to detect access violations that occur at runtime. An Object Use Graph (OUG) statically captures accesses from different threads to objects. An OUG extends the Heap Shape Graph (HSG), which is a compile-time abstraction for runtime objects (nodes) and their reference relations (edges). An OUG specifies for a specific node in the HSG a partial order of events relevant to the corresponding runtime object(s). Relevant events include read and write access, object escape, thread start and join.OUGs have been implemented in a Java compiler. Initial experience shows that OUGs are effective to identify object accesses that potentially conflict at runtime and isolate accesses that never cause a problem at runtime. The capabilities of OUGs are compared with an advanced program analysis that has been used for lock elimination. For the set of benchmarks investigated here, OUGs report only a fraction of shared objects as conflicting and reduce the number of compile-time reports in terms of allocation sites of conflicting objects by 28--92\% (average 64\%). For benchmarks of up to 30 KLOC, the time taken to construct OUGs is, with one exception, in the order of seconds.The information collected in the OUG has been used to instrument Java programs with checks for object races. OUGs provide precise information about object sharing and static protection, so runtime instrumentation that checks those cases that cannot be disambiguated at compile-time is sparse, and the total runtime overhead of checking for object races is only 3--86\% (average 47\%).},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {115--128},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781145},
 doi = {http://doi.acm.org/10.1145/781131.781145},
 acmid = {781145},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heap shape graph, object use graph, program analysis, race detection, representations for concurrent programs},
} 

@article{Aiken:2003:CIL:780822.781146,
 author = {Aiken, Alex and Foster, Jeffrey S. and Kodumal, John and Terauchi, Tachio},
 title = {Checking and inferring local non-aliasing},
 abstract = {In prior work [15] we studied a language construct \&lt;tt\&gt;restrict\&lt;/tt\&gt; that allows programmers to specify that certain pointers are not aliased to other pointers used within a lexical scope. Among other applications, programming with these constructs helps program analysis tools locally recover strong updates, which can improve the tracking of state in flow-sensitive analyses. In this paper we continue the study of \&lt;tt\&gt;restrict\&lt;/tt\&gt; and introduce the construct \&lt;tt\&gt;confine\&lt;/tt\&gt;. We present a type and effect system for checking the correctness of these annotations, and we develop efficient constraint-based algorithms implementing these type checking systems. To make it easier to use \&lt;tt\&gt;restrict\&lt;/tt\&gt; and \&lt;tt\&gt;confine\&lt;/tt\&gt; in practice, we show how to automatically infer such annotations without programmer assistance. In experiments on locking in 589 Linux device drivers, \&lt;tt\&gt;confine\&lt;/tt\&gt; inference can automatically recover strong updates to eliminate 95\% of the type errors resulting from weak updates.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781146},
 doi = {http://doi.acm.org/10.1145/780822.781146},
 acmid = {781146},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux kernel, alias analysis, confine, constraints, effect inference, flow-sensitivity, locking, restrict, type qualifiers, types},
} 

@inproceedings{Aiken:2003:CIL:781131.781146,
 author = {Aiken, Alex and Foster, Jeffrey S. and Kodumal, John and Terauchi, Tachio},
 title = {Checking and inferring local non-aliasing},
 abstract = {In prior work [15] we studied a language construct \&lt;tt\&gt;restrict\&lt;/tt\&gt; that allows programmers to specify that certain pointers are not aliased to other pointers used within a lexical scope. Among other applications, programming with these constructs helps program analysis tools locally recover strong updates, which can improve the tracking of state in flow-sensitive analyses. In this paper we continue the study of \&lt;tt\&gt;restrict\&lt;/tt\&gt; and introduce the construct \&lt;tt\&gt;confine\&lt;/tt\&gt;. We present a type and effect system for checking the correctness of these annotations, and we develop efficient constraint-based algorithms implementing these type checking systems. To make it easier to use \&lt;tt\&gt;restrict\&lt;/tt\&gt; and \&lt;tt\&gt;confine\&lt;/tt\&gt; in practice, we show how to automatically infer such annotations without programmer assistance. In experiments on locking in 589 Linux device drivers, \&lt;tt\&gt;confine\&lt;/tt\&gt; inference can automatically recover strong updates to eliminate 95\% of the type errors resulting from weak updates.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781146},
 doi = {http://doi.acm.org/10.1145/781131.781146},
 acmid = {781146},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux kernel, alias analysis, confine, constraints, effect inference, flow-sensitivity, locking, restrict, type qualifiers, types},
} 

@article{Liblit:2003:BIV:780822.781148,
 author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
 title = {Bug isolation via remote program sampling},
 abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {141--154},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781148},
 doi = {http://doi.acm.org/10.1145/780822.781148},
 acmid = {781148},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertions, bug isolation, feature selection, logistic regression, random sampling, statistical debugging},
} 

@inproceedings{Liblit:2003:BIV:781131.781148,
 author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
 title = {Bug isolation via remote program sampling},
 abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {141--154},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781148},
 doi = {http://doi.acm.org/10.1145/781131.781148},
 acmid = {781148},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertions, bug isolation, feature selection, logistic regression, random sampling, statistical debugging},
} 

@inproceedings{Dor:2003:CTR:781131.781149,
 author = {Dor, Nurit and Rodeh, Michael and Sagiv, Mooly},
 title = {CSSV: towards a realistic tool for statically detecting all buffer overflows in C},
 abstract = {Erroneous string manipulations are a major source of software defects in C programs yielding vulnerabilities which are exploited by software viruses. We present <b>C</b> <b>S</b>tring <b>S</b>tatic <b>V</b>erifyer (CSSV), a tool that statically uncovers all</i> string manipulation errors. Being a conservative tool, it reports all such errors at the expense of sometimes generating false alarms</i>. Fortunately, only a small number of false alarms are reported, thereby proving that statically reducing software vulnerability is achievable. CSSV handles large programs by analyzing each procedure separately. To this end procedure contracts</i> are allowed which are verified by the tool.We implemented a CSSV prototype and used it to verify the absence of errors in real code from EADS Airbus. When applied to another commonly used string intensive application, CSSV uncovered real bugs with very few false alarms.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {155--167},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/781131.781149},
 doi = {http://doi.acm.org/10.1145/781131.781149},
 acmid = {781149},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, buffer overflow, contracts, error detection, static analysis},
} 

@article{Dor:2003:CTR:780822.781149,
 author = {Dor, Nurit and Rodeh, Michael and Sagiv, Mooly},
 title = {CSSV: towards a realistic tool for statically detecting all buffer overflows in C},
 abstract = {Erroneous string manipulations are a major source of software defects in C programs yielding vulnerabilities which are exploited by software viruses. We present <b>C</b> <b>S</b>tring <b>S</b>tatic <b>V</b>erifyer (CSSV), a tool that statically uncovers all</i> string manipulation errors. Being a conservative tool, it reports all such errors at the expense of sometimes generating false alarms</i>. Fortunately, only a small number of false alarms are reported, thereby proving that statically reducing software vulnerability is achievable. CSSV handles large programs by analyzing each procedure separately. To this end procedure contracts</i> are allowed which are verified by the tool.We implemented a CSSV prototype and used it to verify the absence of errors in real code from EADS Airbus. When applied to another commonly used string intensive application, CSSV uncovered real bugs with very few false alarms.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {155--167},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/780822.781149},
 doi = {http://doi.acm.org/10.1145/780822.781149},
 acmid = {781149},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, buffer overflow, contracts, error detection, static analysis},
} 

@inproceedings{Heine:2003:PFC:781131.781150,
 author = {Heine, David L. and Lam, Monica S.},
 title = {A practical flow-sensitive and context-sensitive C and C++ memory leak detector},
 abstract = {This paper presents a static analysis tool that can automatically find memory leaks and deletions of dangling pointers in large C and C++ applications.We have developed a type system to formalize a practical ownership model of memory management. In this model, every object is pointed to by one and only one owning</i> pointer, which holds the exclusive right and obligation to either delete the object or to transfer the right to another owning pointer. In addition, a pointer-typed class member field is required to either always or never own its pointee at public method boundaries. Programs satisfying this model do not leak memory or delete the same object more than once.We have also developed a flow-sensitive and context-sensitive algorithm to automatically infer the likely ownership interfaces of methods in a program. It identifies statements inconsistent with the model as sources of potential leaks or double deletes. The algorithm is sound with respect to a large subset of the C and C++ language in that it will report all possible errors. It is also practical and useful as it identifies those warnings likely to correspond to errors and helps the user understand the reported errors by showing them the assumed method interfaces.Our techniques are validated with an implementation of a tool we call Clouseau. We applied Clouseau to a suite of applications: two web servers, a chat client, secure shell tools, executable object manipulation tools, and a compiler. The tool found a total of 134 serious memory errors in these applications. The tool analyzes over 50K lines of C++ code in about 9 minutes on a 2 GHz Pentium 4 machine and over 70K lines of C code in just over a minute.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {168--181},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781150},
 doi = {http://doi.acm.org/10.1145/781131.781150},
 acmid = {781150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, memory leaks, memory management, program analysis, type systems},
} 

@article{Heine:2003:PFC:780822.781150,
 author = {Heine, David L. and Lam, Monica S.},
 title = {A practical flow-sensitive and context-sensitive C and C++ memory leak detector},
 abstract = {This paper presents a static analysis tool that can automatically find memory leaks and deletions of dangling pointers in large C and C++ applications.We have developed a type system to formalize a practical ownership model of memory management. In this model, every object is pointed to by one and only one owning</i> pointer, which holds the exclusive right and obligation to either delete the object or to transfer the right to another owning pointer. In addition, a pointer-typed class member field is required to either always or never own its pointee at public method boundaries. Programs satisfying this model do not leak memory or delete the same object more than once.We have also developed a flow-sensitive and context-sensitive algorithm to automatically infer the likely ownership interfaces of methods in a program. It identifies statements inconsistent with the model as sources of potential leaks or double deletes. The algorithm is sound with respect to a large subset of the C and C++ language in that it will report all possible errors. It is also practical and useful as it identifies those warnings likely to correspond to errors and helps the user understand the reported errors by showing them the assumed method interfaces.Our techniques are validated with an implementation of a tool we call Clouseau. We applied Clouseau to a suite of applications: two web servers, a chat client, secure shell tools, executable object manipulation tools, and a compiler. The tool found a total of 134 serious memory errors in these applications. The tool analyzes over 50K lines of C++ code in about 9 minutes on a 2 GHz Pentium 4 machine and over 70K lines of C code in just over a minute.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {168--181},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781150},
 doi = {http://doi.acm.org/10.1145/780822.781150},
 acmid = {781150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, memory leaks, memory management, program analysis, type systems},
} 

@article{Ammons:2003:DTS:780822.781152,
 author = {Ammons, Glenn and Mandelin, David and Bod\'{\i}k, Rastislav and Larus, James R.},
 title = {Debugging temporal specifications with concept analysis},
 abstract = {Program verification tools (such as model checkers and static analyzers) can find many errors in programs. These tools need formal specifications of correct program behavior, but writing a correct specification is difficult, just as writing a correct program is difficult. Thus, just as we need methods for debugging programs, we need methods for debugging specifications.This paper describes a novel method for debugging formal, temporal specifications. Our method exploits the short program execution traces that program verification tools generate from specification violations and that specification miners extract from programs. Manually examining these traces is a straightforward way to debug a specification, but this method is tedious and error-prone because there may be hundreds or thousands of traces to inspect. Our method uses concept analysis to automatically group the traces into highly similar clusters. By examining clusters instead of individual traces, a person can debug a specification with less work.To test our method, we implemented a tool, Cable, for debugging specifications. We have used Cable to debug specifications produced by Strauss, our specification miner. We found that using Cable to debug these specifications requires, on average, less than one third as many user decisions as debugging by examining all traces requires. In one case, using Cable required only 28 decisions, while debugging by examining all traces required 224.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {182--195},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781152},
 doi = {http://doi.acm.org/10.1145/780822.781152},
 acmid = {781152},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concept analysis, hierarchical clustering, specification debuggers, temporal specifications},
} 

@inproceedings{Ammons:2003:DTS:781131.781152,
 author = {Ammons, Glenn and Mandelin, David and Bod\'{\i}k, Rastislav and Larus, James R.},
 title = {Debugging temporal specifications with concept analysis},
 abstract = {Program verification tools (such as model checkers and static analyzers) can find many errors in programs. These tools need formal specifications of correct program behavior, but writing a correct specification is difficult, just as writing a correct program is difficult. Thus, just as we need methods for debugging programs, we need methods for debugging specifications.This paper describes a novel method for debugging formal, temporal specifications. Our method exploits the short program execution traces that program verification tools generate from specification violations and that specification miners extract from programs. Manually examining these traces is a straightforward way to debug a specification, but this method is tedious and error-prone because there may be hundreds or thousands of traces to inspect. Our method uses concept analysis to automatically group the traces into highly similar clusters. By examining clusters instead of individual traces, a person can debug a specification with less work.To test our method, we implemented a tool, Cable, for debugging specifications. We have used Cable to debug specifications produced by Strauss, our specification miner. We found that using Cable to debug these specifications requires, on average, less than one third as many user decisions as debugging by examining all traces requires. In one case, using Cable required only 28 decisions, while debugging by examining all traces required 224.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {182--195},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781152},
 doi = {http://doi.acm.org/10.1145/781131.781152},
 acmid = {781152},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concept analysis, hierarchical clustering, specification debuggers, temporal specifications},
} 

@inproceedings{Blanchet:2003:SAL:781131.781153,
 author = {Blanchet, Bruno and Cousot, Patrick and Cousot, Radhia and Feret, J\'{e}rome and Mauborgne, Laurent and Min\'{e}, Antoine and Monniaux, David and Rival, Xavier},
 title = {A static analyzer for large safety-critical software},
 abstract = {We show that abstract interpretation-based static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the end-user through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software.The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization (Sect. 3 and 7), the symbolic manipulation of expressions to improve the precision of abstract transfer functions (Sect. 6.3), the octagon (Sect. 6.2.2), ellipsoid (Sect. 6.2.3), and decision tree (Sect. 6.2.4) abstract domains, all with sound handling of rounding errors in oating point computations, widening strategies (with thresholds: Sect. 7.1.2, delayed: Sect. 7.1.3) and the automatic determination of the parameters (parametrized packing: Sect. 7.2).},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781153},
 doi = {http://doi.acm.org/10.1145/781131.781153},
 acmid = {781153},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract domains, abstract interpretation, embedded, floating point, reactive, real-time, safety-critical software, static analysis, verification},
} 

@article{Blanchet:2003:SAL:780822.781153,
 author = {Blanchet, Bruno and Cousot, Patrick and Cousot, Radhia and Feret, J\'{e}rome and Mauborgne, Laurent and Min\'{e}, Antoine and Monniaux, David and Rival, Xavier},
 title = {A static analyzer for large safety-critical software},
 abstract = {We show that abstract interpretation-based static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the end-user through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software.The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization (Sect. 3 and 7), the symbolic manipulation of expressions to improve the precision of abstract transfer functions (Sect. 6.3), the octagon (Sect. 6.2.2), ellipsoid (Sect. 6.2.3), and decision tree (Sect. 6.2.4) abstract domains, all with sound handling of rounding errors in oating point computations, widening strategies (with thresholds: Sect. 7.1.2, delayed: Sect. 7.1.3) and the automatic determination of the parameters (parametrized packing: Sect. 7.2).},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781153},
 doi = {http://doi.acm.org/10.1145/780822.781153},
 acmid = {781153},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract domains, abstract interpretation, embedded, floating point, reactive, real-time, safety-critical software, static analysis, verification},
} 

@inproceedings{Chen:2003:PST:781131.781155,
 author = {Chen, Juan and Wu, Dinghao and Appel, Andrew W. and Fang, Hai},
 title = {A provably sound TAL for back-end optimization},
 abstract = {Typed assembly languages provide a way to generate machine-checkable safety proofs for machine-language programs. But the soundness proofs of most existing typed assembly languages are hand-written and cannot be machine-checked, which is worrisome for such large calculi. We have designed and implemented a low-level typed assembly language (LTAL) with a semantic model and established its soundness from the model. Compared to existing typed assembly languages, LTAL is more scalable and more secure; it has no macro instructions that hinder low-level optimizations such as instruction scheduling; its type constructors are expressive enough to capture dataflow information, support the compiler's choice of data representations and permit typed position-independent code; and its type-checking algorithm is completely syntax-directed.We have built a prototype system, based on Standard ML of New Jersey, that compiles most of core ML to Sparc code. We explain how we were able to make the untyped back end in SML/NJ preserve types during instruction selection and register allocation, without restricting low-level optimizations and without knowledge of any type system pervading the instruction selector and register allocator.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {208--219},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781155},
 doi = {http://doi.acm.org/10.1145/781131.781155},
 acmid = {781155},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {proof-carrying code, typed assembly language},
} 

@article{Chen:2003:PST:780822.781155,
 author = {Chen, Juan and Wu, Dinghao and Appel, Andrew W. and Fang, Hai},
 title = {A provably sound TAL for back-end optimization},
 abstract = {Typed assembly languages provide a way to generate machine-checkable safety proofs for machine-language programs. But the soundness proofs of most existing typed assembly languages are hand-written and cannot be machine-checked, which is worrisome for such large calculi. We have designed and implemented a low-level typed assembly language (LTAL) with a semantic model and established its soundness from the model. Compared to existing typed assembly languages, LTAL is more scalable and more secure; it has no macro instructions that hinder low-level optimizations such as instruction scheduling; its type constructors are expressive enough to capture dataflow information, support the compiler's choice of data representations and permit typed position-independent code; and its type-checking algorithm is completely syntax-directed.We have built a prototype system, based on Standard ML of New Jersey, that compiles most of core ML to Sparc code. We explain how we were able to make the untyped back end in SML/NJ preserve types during instruction selection and register allocation, without restricting low-level optimizations and without knowledge of any type system pervading the instruction selector and register allocator.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {208--219},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781155},
 doi = {http://doi.acm.org/10.1145/780822.781155},
 acmid = {781155},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {proof-carrying code, typed assembly language},
} 

@article{Lerner:2003:APC:780822.781156,
 author = {Lerner, Sorin and Millstein, Todd and Chambers, Craig},
 title = {Automatically proving the correctness of compiler optimizations},
 abstract = {We describe a technique for automatically proving compiler optimizations sound</i>, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {220--231},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781156},
 doi = {http://doi.acm.org/10.1145/780822.781156},
 acmid = {781156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated correctness proofs, compiler optimization},
} 

@inproceedings{Lerner:2003:APC:781131.781156,
 author = {Lerner, Sorin and Millstein, Todd and Chambers, Craig},
 title = {Automatically proving the correctness of compiler optimizations},
 abstract = {We describe a technique for automatically proving compiler optimizations sound</i>, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {220--231},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781156},
 doi = {http://doi.acm.org/10.1145/781131.781156},
 acmid = {781156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated correctness proofs, compiler optimization},
} 

@inproceedings{Condit:2003:CRW:781131.781157,
 author = {Condit, Jeremy and Harren, Matthew and McPeak, Scott and Necula, George C. and Weimer, Westley},
 title = {CCured in the real world},
 abstract = {CCured is a program transformation system that adds memory safety guarantees to C programs by verifying statically that memory errors cannot occur and by inserting run-time checks where static verification is insufficient.This paper addresses major usability issues in a previous version of CCured, in which many type casts required the use of pointers whose representation was expensive and incompatible with precompiled libraries. We have extended the CCured type inference algorithm to recognize and verify statically a large number of type casts; this goal is achieved by using physical subtyping and pointers with run-time type information to allow parametric and subtype polymorphism. In addition, we present a new instrumentation scheme that splits CCured's metadata into a separate data structure whose shape mirrors that of the original user data. This scheme allows instrumented programs to invoke external functions directly on the program's data without the use of a wrapper function.With these extensions we were able to use CCured on real-world security-critical network daemons and to produce instrumented versions without memory-safety vulnerabilities.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {232--244},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/781131.781157},
 doi = {http://doi.acm.org/10.1145/781131.781157},
 acmid = {781157},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, compatibility with library code, memory safety, run-time type information, type safety},
} 

@article{Condit:2003:CRW:780822.781157,
 author = {Condit, Jeremy and Harren, Matthew and McPeak, Scott and Necula, George C. and Weimer, Westley},
 title = {CCured in the real world},
 abstract = {CCured is a program transformation system that adds memory safety guarantees to C programs by verifying statically that memory errors cannot occur and by inserting run-time checks where static verification is insufficient.This paper addresses major usability issues in a previous version of CCured, in which many type casts required the use of pointers whose representation was expensive and incompatible with precompiled libraries. We have extended the CCured type inference algorithm to recognize and verify statically a large number of type casts; this goal is achieved by using physical subtyping and pointers with run-time type information to allow parametric and subtype polymorphism. In addition, we present a new instrumentation scheme that splits CCured's metadata into a separate data structure whose shape mirrors that of the original user data. This scheme allows instrumented programs to invoke external functions directly on the program's data without the use of a wrapper function.With these extensions we were able to use CCured on real-world security-critical network daemons and to produce instrumented versions without memory-safety vulnerabilities.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {232--244},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/780822.781157},
 doi = {http://doi.acm.org/10.1145/780822.781157},
 acmid = {781157},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, compatibility with library code, memory safety, run-time type information, type safety},
} 

@article{Ding:2003:PWL:780822.781159,
 author = {Ding, Chen and Zhong, Yutao},
 title = {Predicting whole-program locality through reuse distance analysis},
 abstract = {Profiling can accurately analyze program behavior for select data inputs. We show that profiling can also predict program locality for inputs other than profiled ones. Here locality is defined by the distance of data reuse. Studying whole-program data reuse may reveal global patterns not apparent in short-distance reuses or local control flow. However, the analysis must meet two requirements to be useful. The first is efficiency. It needs to analyze all accesses to all data elements in full-size benchmarks and to measure distance of any length and in any required precision. The second is predication. Based on a few training runs, it needs to classify patterns as regular and irregular and, for regular ones, it should predict their (changing) behavior for other inputs. In this paper, we show that these goals are attainable through three techniques: approximate analysis of reuse distance (originally called LRU stack distance), pattern recognition, and distance-based sampling. When tested on 15 integer and floating-point programs from SPEC and other benchmark suites, our techniques predict with on average 94\% accuracy for data inputs up to hundreds times larger than the training inputs. Based on these results, the paper discusses possible uses of this analysis.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/780822.781159},
 doi = {http://doi.acm.org/10.1145/780822.781159},
 acmid = {781159},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data locality, pattern recognition, prediction, profiling, program locality, reuse distance, sampling, stack distance, training},
} 

@inproceedings{Ding:2003:PWL:781131.781159,
 author = {Ding, Chen and Zhong, Yutao},
 title = {Predicting whole-program locality through reuse distance analysis},
 abstract = {Profiling can accurately analyze program behavior for select data inputs. We show that profiling can also predict program locality for inputs other than profiled ones. Here locality is defined by the distance of data reuse. Studying whole-program data reuse may reveal global patterns not apparent in short-distance reuses or local control flow. However, the analysis must meet two requirements to be useful. The first is efficiency. It needs to analyze all accesses to all data elements in full-size benchmarks and to measure distance of any length and in any required precision. The second is predication. Based on a few training runs, it needs to classify patterns as regular and irregular and, for regular ones, it should predict their (changing) behavior for other inputs. In this paper, we show that these goals are attainable through three techniques: approximate analysis of reuse distance (originally called LRU stack distance), pattern recognition, and distance-based sampling. When tested on 15 integer and floating-point programs from SPEC and other benchmark suites, our techniques predict with on average 94\% accuracy for data inputs up to hundreds times larger than the training inputs. Based on these results, the paper discusses possible uses of this analysis.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/781131.781159},
 doi = {http://doi.acm.org/10.1145/781131.781159},
 acmid = {781159},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data locality, pattern recognition, prediction, profiling, program locality, reuse distance, sampling, stack distance, training},
} 

@article{Joisha:2003:SAS:780822.781160,
 author = {Joisha, Pramod G. and Banerjee, Prithviraj},
 title = {Static array storage optimization in MATLAB},
 abstract = {Static array storage optimization in MATLAB.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {258--268},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/780822.781160},
 doi = {http://doi.acm.org/10.1145/780822.781160},
 acmid = {781160},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Joisha:2003:SAS:781131.781160,
 author = {Joisha, Pramod G. and Banerjee, Prithviraj},
 title = {Static array storage optimization in MATLAB},
 abstract = {Static array storage optimization in MATLAB.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {258--268},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781131.781160},
 doi = {http://doi.acm.org/10.1145/781131.781160},
 acmid = {781160},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Inagaki:2003:SPD:781131.781161,
 author = {Inagaki, Tatsushi and Onodera, Tamiya and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Stride prefetching by dynamically inspecting objects},
 abstract = {Software prefetching is a promising technique to hide cache miss latencies, but it remains challenging to effectively prefetch pointer-based data structures because obtaining the memory address to be prefetched requires pointer dereferences. The recently proposed stride prefetching overcomes this problem, but it only exploits  inter-iteration</i> stride patterns and relies on an off-line profiling method.We propose a new algorithm for stride prefetching which is intended for use in a dynamic compiler. We exploit both inter-</i> and intra-iteration</i> stride patterns, which we discover using an ultra-lightweight profiling technique, called object inspection</i>. This is a kind of partial interpretation that only a dynamic compiler can perform. During the compilation of a method, the dynamic compiler gathers the profile information by partially interpreting the method using the actual values of parameters and causing no side effects.We evaluated an implementation of our prefetching algorithm in a production-level Java just-in time compiler. The results show that the algorithm achieved up to an 18.9\%</i> and 25.1\%</i> speedup in industry-standard benchmarks on the Pentium 4 and the Athlon MP, respectively, while it increased the compilation time by less than 3.0\%</i>.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {269--277},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/781131.781161},
 doi = {http://doi.acm.org/10.1145/781131.781161},
 acmid = {781161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java just-in-time compiler, object inspection, stride prefetching},
} 

@article{Inagaki:2003:SPD:780822.781161,
 author = {Inagaki, Tatsushi and Onodera, Tamiya and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Stride prefetching by dynamically inspecting objects},
 abstract = {Software prefetching is a promising technique to hide cache miss latencies, but it remains challenging to effectively prefetch pointer-based data structures because obtaining the memory address to be prefetched requires pointer dereferences. The recently proposed stride prefetching overcomes this problem, but it only exploits  inter-iteration</i> stride patterns and relies on an off-line profiling method.We propose a new algorithm for stride prefetching which is intended for use in a dynamic compiler. We exploit both inter-</i> and intra-iteration</i> stride patterns, which we discover using an ultra-lightweight profiling technique, called object inspection</i>. This is a kind of partial interpretation that only a dynamic compiler can perform. During the compilation of a method, the dynamic compiler gathers the profile information by partially interpreting the method using the actual values of parameters and causing no side effects.We evaluated an implementation of our prefetching algorithm in a production-level Java just-in time compiler. The results show that the algorithm achieved up to an 18.9\%</i> and 25.1\%</i> speedup in industry-standard benchmarks on the Pentium 4 and the Athlon MP, respectively, while it increased the compilation time by less than 3.0\%</i>.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {269--277},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/780822.781161},
 doi = {http://doi.acm.org/10.1145/780822.781161},
 acmid = {781161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java just-in-time compiler, object inspection, stride prefetching},
} 

@article{Ertl:2003:OIB:780822.781162,
 author = {Ertl, M. Anton and Gregg, David},
 title = {Optimizing indirect branch prediction accuracy in virtual machine interpreters},
 abstract = {Interpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers are the best widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2\%--50\%. In this paper we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter build-time) and dynamic (interpreter run-time) variants of these techniques and compare them and several combinations of these techniques. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 3.17 over efficient threaded-code interpreters, and speedups by a factor of up to 1.3 over techniques relying on superinstructions alone.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {278--288},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/780822.781162},
 doi = {http://doi.acm.org/10.1145/780822.781162},
 acmid = {781162},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, branch target buffer, code replication, interpreter, superinstruction},
} 

@inproceedings{Ertl:2003:OIB:781131.781162,
 author = {Ertl, M. Anton and Gregg, David},
 title = {Optimizing indirect branch prediction accuracy in virtual machine interpreters},
 abstract = {Interpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers are the best widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2\%--50\%. In this paper we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter build-time) and dynamic (interpreter run-time) variants of these techniques and compare them and several combinations of these techniques. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 3.17 over efficient threaded-code interpreters, and speedups by a factor of up to 1.3 over techniques relying on superinstructions alone.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {278--288},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781131.781162},
 doi = {http://doi.acm.org/10.1145/781131.781162},
 acmid = {781162},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, branch target buffer, code replication, interpreter, superinstruction},
} 

@inproceedings{Lin:2003:CFS:781131.781164,
 author = {Lin, Jin and Chen, Tong and Hsu, Wei-Chung and Yew, Pen-Chung and Ju, Roy Dz-Ching and Ngai, Tin-Fook and Chan, Sun},
 title = {A compiler framework for speculative analysis and optimizations},
 abstract = {Speculative execution, such as control speculation and data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative SSA form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing program analysis frameworks to be easily extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide checking (such as advanced load address table</i> (ALAT) [10]) on data speculation to guarantee the correctness of program execution. We use SSAPRE [21] as one example to illustrate how to incorporate data speculation in those important compiler optimizations such as partial redundancy elimination (PRE), register promotion, strength reduction and linear function test replacement. Our extended framework allows both control and data speculation to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler (ORC). We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework and how data speculation benefits partial redundancy elimination.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {289--299},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781131.781164},
 doi = {http://doi.acm.org/10.1145/781131.781164},
 acmid = {781164},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data speculation, partial redundancy elimination, register promotion, speculative SSA form, speculative weak update},
} 

@article{Lin:2003:CFS:780822.781164,
 author = {Lin, Jin and Chen, Tong and Hsu, Wei-Chung and Yew, Pen-Chung and Ju, Roy Dz-Ching and Ngai, Tin-Fook and Chan, Sun},
 title = {A compiler framework for speculative analysis and optimizations},
 abstract = {Speculative execution, such as control speculation and data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative SSA form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing program analysis frameworks to be easily extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide checking (such as advanced load address table</i> (ALAT) [10]) on data speculation to guarantee the correctness of program execution. We use SSAPRE [21] as one example to illustrate how to incorporate data speculation in those important compiler optimizations such as partial redundancy elimination (PRE), register promotion, strength reduction and linear function test replacement. Our extended framework allows both control and data speculation to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler (ORC). We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework and how data speculation benefits partial redundancy elimination.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {289--299},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/780822.781164},
 doi = {http://doi.acm.org/10.1145/780822.781164},
 acmid = {781164},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data speculation, partial redundancy elimination, register promotion, speculative SSA form, speculative weak update},
} 

@article{Chu:2003:RHO:780822.781165,
 author = {Chu, Michael and Fan, Kevin and Mahlke, Scott},
 title = {Region-based hierarchical operation partitioning for multicluster processors},
 abstract = {Clustered architectures are a solution to the bottleneck of centralized register files in superscalar and VLIW processors. The main challenge associated with clustered architectures is compiler support to effectively partition operations across the available resources on each cluster. In this work, we present a novel technique for clustering operations based on graph partitioning methods. Our approach incorporates new methods of assigning weights to nodes and edges within the dataflow graph to guide the partitioner. Nodes are assigned weights to reflect their resource usage within a cluster, while a slack distribution method intelligently assigns weights to edges to reflect the cost of inserting moves across clusters. A multilevel graph partitioning algorithm, which globally divides a dataflow graph into multiple parts in a hierarchical manner, uses these weights to efficiently generate estimates for the quality of partitions. We found that our algorithm was able to achieve an average of 20\% improvement in DSP kernels and 5\% improvement in SPECint2000 for a four-cluster architecture.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {300--311},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781165},
 doi = {http://doi.acm.org/10.1145/780822.781165},
 acmid = {781165},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, instruction scheduling, instruction-level parallelism, multicluster processor, operation partitioning, region-based compilation},
} 

@inproceedings{Chu:2003:RHO:781131.781165,
 author = {Chu, Michael and Fan, Kevin and Mahlke, Scott},
 title = {Region-based hierarchical operation partitioning for multicluster processors},
 abstract = {Clustered architectures are a solution to the bottleneck of centralized register files in superscalar and VLIW processors. The main challenge associated with clustered architectures is compiler support to effectively partition operations across the available resources on each cluster. In this work, we present a novel technique for clustering operations based on graph partitioning methods. Our approach incorporates new methods of assigning weights to nodes and edges within the dataflow graph to guide the partitioner. Nodes are assigned weights to reflect their resource usage within a cluster, while a slack distribution method intelligently assigns weights to edges to reflect the cost of inserting moves across clusters. A multilevel graph partitioning algorithm, which globally divides a dataflow graph into multiple parts in a hierarchical manner, uses these weights to efficiently generate estimates for the quality of partitions. We found that our algorithm was able to achieve an average of 20\% improvement in DSP kernels and 5\% improvement in SPECint2000 for a four-cluster architecture.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {300--311},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781165},
 doi = {http://doi.acm.org/10.1145/781131.781165},
 acmid = {781165},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, instruction scheduling, instruction-level parallelism, multicluster processor, operation partitioning, region-based compilation},
} 

@article{Suganuma:2003:RCT:780822.781166,
 author = {Suganuma, Toshio and Yasue, Toshiaki and Nakatani, Toshio},
 title = {A region-based compilation technique for a Java just-in-time compiler},
 abstract = {Method inlining and data flow analysis are two major optimization components for effective program transformations, however they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This paper describes the design and implementation of a region-based compilation technique in our dynamic compilation system, in which the compiled regions are selected as code portions without rarely executed code. The key part of this technique is the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify rare sections of code. The region selection process and method inlining decision are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, leading to more method inlining. Thus the inlining process can be performed for parts of a method, not for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then rely on on-stack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that the approach of region-based compilation achieves approximately 5\% performance improvement on average, while reducing the compilation overhead by 20 to 30\%, in comparison to the traditional function-based compilation techniques.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {312--323},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781166},
 doi = {http://doi.acm.org/10.1145/780822.781166},
 acmid = {781166},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilers, on-stack replacement, partial inlining, region-based compilation},
} 

@inproceedings{Suganuma:2003:RCT:781131.781166,
 author = {Suganuma, Toshio and Yasue, Toshiaki and Nakatani, Toshio},
 title = {A region-based compilation technique for a Java just-in-time compiler},
 abstract = {Method inlining and data flow analysis are two major optimization components for effective program transformations, however they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This paper describes the design and implementation of a region-based compilation technique in our dynamic compilation system, in which the compiled regions are selected as code portions without rarely executed code. The key part of this technique is the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify rare sections of code. The region selection process and method inlining decision are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, leading to more method inlining. Thus the inlining process can be performed for parts of a method, not for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then rely on on-stack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that the approach of region-based compilation achieves approximately 5\% performance improvement on average, while reducing the compilation overhead by 20 to 30\%, in comparison to the traditional function-based compilation techniques.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {312--323},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781166},
 doi = {http://doi.acm.org/10.1145/781131.781166},
 acmid = {781166},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilers, on-stack replacement, partial inlining, region-based compilation},
} 

@inproceedings{Boyapati:2003:OTS:781131.781168,
 author = {Boyapati, Chandrasekhar and Salcianu, Alexandru and Beebee,Jr., William and Rinard, Martin},
 title = {Ownership types for safe region-based memory management in real-time Java},
 abstract = {The Real Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real-time constraints. Real-time threads use region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead.Our system also makes several contributions over previous work on region types. For object-oriented programs, it combines the benefits of region types and ownership types in a unified type system framework. For multithreaded programs, it allows long-lived threads to share objects without using the heap and without memory leaks. For real-time programs, it ensures that real-time threads do not interfere with the garbage collector. Our experience indicates that our type system is sufficiently expressive and requires little programming overhead, and that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of real-time programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {324--337},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/781131.781168},
 doi = {http://doi.acm.org/10.1145/781131.781168},
 acmid = {781168},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {encapsulation, ownership types, real-time, regions},
} 

@article{Boyapati:2003:OTS:780822.781168,
 author = {Boyapati, Chandrasekhar and Salcianu, Alexandru and Beebee,Jr., William and Rinard, Martin},
 title = {Ownership types for safe region-based memory management in real-time Java},
 abstract = {The Real Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real-time constraints. Real-time threads use region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead.Our system also makes several contributions over previous work on region types. For object-oriented programs, it combines the benefits of region types and ownership types in a unified type system framework. For multithreaded programs, it allows long-lived threads to share objects without using the heap and without memory leaks. For real-time programs, it ensures that real-time threads do not interfere with the garbage collector. Our experience indicates that our type system is sufficiently expressive and requires little programming overhead, and that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of real-time programs.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {324--337},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/780822.781168},
 doi = {http://doi.acm.org/10.1145/780822.781168},
 acmid = {781168},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {encapsulation, ownership types, real-time, regions},
} 

@inproceedings{Flanagan:2003:TES:781131.781169,
 author = {Flanagan, Cormac and Qadeer, Shaz},
 title = {A type and effect system for atomicity},
 abstract = {Ensuring the correctness of multithreaded programs is difficult, due to the potential for unexpected and nondeterministic interactions between threads. Previous work addressed this problem by devising tools for detecting race conditions</i>, a situation where two threads simultaneously access the same data variable, and at least one of the accesses is a write. However, verifying the absence of such simultaneous-access race conditions is neither necessary nor sufficient to ensure the absence of errors due to unexpected thread interactions.We propose that a stronger non-interference property is required, namely atomicity</i>. Atomic methods can be assumed to execute serially, without interleaved steps of other threads. Thus, atomic methods are amenable to sequential reasoning techniques, which significantly simplifies both formal and informal reasoning about program correctness.This paper presents a type system for specifying and verifying the atomicity of methods in multithreaded Java programs. The atomic type system is a synthesis of Lipton's theory of reduction and type systems for race detection.We have implemented this atomic type system for Java and used it to check a variety of standard Java library classes. The type checker uncovered subtle atomicity violations in classes such as \&lt;tt\&gt;java.lang.String\&lt;/tt\&gt; and \&lt;tt\&gt;java.lang.String-Buffer\&lt;/tt\&gt; that cause crashes under certain thread interleavings.This paper proposes that a stronger non-interference property is required, namely atomicity</i>, and presents a type system for verifying the atomicity of methods in multithreaded Java programs. Methods in a class can be annotated with the keyword \&lt;tt\&gt;atomic\&lt;/tt\&gt;. Clients of a well-typed class can then assume that each atomic method is executed in one step, thus significantly simplifying both formal and informal reasoning about the client's correctness.},
 booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
 series = {PLDI '03},
 year = {2003},
 isbn = {1-58113-662-5},
 location = {San Diego, California, USA},
 pages = {338--349},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781131.781169},
 doi = {http://doi.acm.org/10.1145/781131.781169},
 acmid = {781169},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, multithreading, race conditions, static checking},
} 

@article{Flanagan:2003:TES:780822.781169,
 author = {Flanagan, Cormac and Qadeer, Shaz},
 title = {A type and effect system for atomicity},
 abstract = {Ensuring the correctness of multithreaded programs is difficult, due to the potential for unexpected and nondeterministic interactions between threads. Previous work addressed this problem by devising tools for detecting race conditions</i>, a situation where two threads simultaneously access the same data variable, and at least one of the accesses is a write. However, verifying the absence of such simultaneous-access race conditions is neither necessary nor sufficient to ensure the absence of errors due to unexpected thread interactions.We propose that a stronger non-interference property is required, namely atomicity</i>. Atomic methods can be assumed to execute serially, without interleaved steps of other threads. Thus, atomic methods are amenable to sequential reasoning techniques, which significantly simplifies both formal and informal reasoning about program correctness.This paper presents a type system for specifying and verifying the atomicity of methods in multithreaded Java programs. The atomic type system is a synthesis of Lipton's theory of reduction and type systems for race detection.We have implemented this atomic type system for Java and used it to check a variety of standard Java library classes. The type checker uncovered subtle atomicity violations in classes such as \&lt;tt\&gt;java.lang.String\&lt;/tt\&gt; and \&lt;tt\&gt;java.lang.String-Buffer\&lt;/tt\&gt; that cause crashes under certain thread interleavings.This paper proposes that a stronger non-interference property is required, namely atomicity</i>, and presents a type system for verifying the atomicity of methods in multithreaded Java programs. Methods in a class can be annotated with the keyword \&lt;tt\&gt;atomic\&lt;/tt\&gt;. Clients of a well-typed class can then assume that each atomic method is executed in one step, thus significantly simplifying both formal and informal reasoning about the client's correctness.},
 journal = {SIGPLAN Not.},
 volume = {38},
 issue = {5},
 month = {May},
 year = {2003},
 issn = {0362-1340},
 pages = {338--349},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/780822.781169},
 doi = {http://doi.acm.org/10.1145/780822.781169},
 acmid = {781169},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, multithreading, race conditions, static checking},
} 

@inproceedings{Foster:2002:FTQ:512529.512531,
 author = {Foster, Jeffrey S. and Terauchi, Tachio and Aiken, Alex},
 title = {Flow-sensitive type qualifiers},
 abstract = {We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively---the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512531},
 doi = {http://doi.acm.org/10.1145/512529.512531},
 acmid = {512531},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, constraints, effect inference, flow-sensitivity, linux kernel, locking, restrict, type qualifiers, types},
} 

@article{Foster:2002:FTQ:543552.512531,
 author = {Foster, Jeffrey S. and Terauchi, Tachio and Aiken, Alex},
 title = {Flow-sensitive type qualifiers},
 abstract = {We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively---the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512531},
 doi = {http://doi.acm.org/10.1145/543552.512531},
 acmid = {512531},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, constraints, effect inference, flow-sensitivity, linux kernel, locking, restrict, type qualifiers, types},
} 

@article{Fahndrich:2002:AFP:543552.512532,
 author = {Fahndrich, Manuel and DeLine, Robert},
 title = {Adoption and focus: practical linear types for imperative programming},
 abstract = {A type system with linearity is useful for checking software protocols andresource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512532},
 doi = {http://doi.acm.org/10.1145/543552.512532},
 acmid = {512532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heap aliasing, linear types, region-based memory management},
} 

@inproceedings{Fahndrich:2002:AFP:512529.512532,
 author = {Fahndrich, Manuel and DeLine, Robert},
 title = {Adoption and focus: practical linear types for imperative programming},
 abstract = {A type system with linearity is useful for checking software protocols andresource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512532},
 doi = {http://doi.acm.org/10.1145/512529.512532},
 acmid = {512532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heap aliasing, linear types, region-based memory management},
} 

@article{Budimlic:2002:FCC:543552.512534,
 author = {Budimlic, Zoran and Cooper, Keith D. and Harvey, Timothy J. and Kennedy, Ken and Oberg, Timothy S. and Reeves, Steven W.},
 title = {Fast copy coalescing and live-range identification},
 abstract = {This paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for \&fgr;-node instantiation during the conversion of the static single assignment (SSA) form into the control-flow graph (CFG), effectively yielding a new, very fast copy coalescing and live-range identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is where-is the number of instructions in the program.Performing copy folding during the SSA-to-CFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graph-coloring register allocation more practical in just in time (JIT) and other time-critical compilers For example, Sun's Hotspot Server Compiler already employs a graph-coloring register allocator[10].This paper also presents an improvement to the classical interference-graph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interference-graph-based coalescing algorithm, while requiring three times less compilation time.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {25--32},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/543552.512534},
 doi = {http://doi.acm.org/10.1145/543552.512534},
 acmid = {512534},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, copy coalescing, interference graph, live-range identification, register allocation},
} 

@inproceedings{Budimlic:2002:FCC:512529.512534,
 author = {Budimlic, Zoran and Cooper, Keith D. and Harvey, Timothy J. and Kennedy, Ken and Oberg, Timothy S. and Reeves, Steven W.},
 title = {Fast copy coalescing and live-range identification},
 abstract = {This paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for \&fgr;-node instantiation during the conversion of the static single assignment (SSA) form into the control-flow graph (CFG), effectively yielding a new, very fast copy coalescing and live-range identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is where-is the number of instructions in the program.Performing copy folding during the SSA-to-CFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graph-coloring register allocation more practical in just in time (JIT) and other time-critical compilers For example, Sun's Hotspot Server Compiler already employs a graph-coloring register allocator[10].This paper also presents an improvement to the classical interference-graph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interference-graph-based coalescing algorithm, while requiring three times less compilation time.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {25--32},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/512529.512534},
 doi = {http://doi.acm.org/10.1145/512529.512534},
 acmid = {512534},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, copy coalescing, interference graph, live-range identification, register allocation},
} 

@inproceedings{Koseki:2002:PGC:512529.512535,
 author = {Koseki, Akira and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Preference-directed graph coloring},
 abstract = {This paper describes a new framework of register allocation based on Chaitin-style coloring. Our focus is on maximizing the chances for live ranges to be allocated to the most preferred registers while not destroying the colorability obtained by graph simplification. Our coloring algorithm uses a graph representation of preferences called a Register Preference Graph, which helps find a good register selection. We then try to relax the register selection order created by the graph simplification. The relaxed order is defined as a partial order, represented using a graph called a Coloring Precedence Graph. Our algorithm utilizes such a partial order for the register selection instead of using the traditional simplification-driven order so that the chances of honoring the preferences are effectively increased. Experimental results show that our coloring algorithm is powerful to simultaneously handle spill decisions, register coalescing, and preference resolutions.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512535},
 doi = {http://doi.acm.org/10.1145/512529.512535},
 acmid = {512535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, irregular-register architectures, register allocation, register coalescing},
} 

@article{Koseki:2002:PGC:543552.512535,
 author = {Koseki, Akira and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Preference-directed graph coloring},
 abstract = {This paper describes a new framework of register allocation based on Chaitin-style coloring. Our focus is on maximizing the chances for live ranges to be allocated to the most preferred registers while not destroying the colorability obtained by graph simplification. Our coloring algorithm uses a graph representation of preferences called a Register Preference Graph, which helps find a good register selection. We then try to relax the register selection order created by the graph simplification. The relaxed order is defined as a partial order, represented using a graph called a Coloring Precedence Graph. Our algorithm utilizes such a partial order for the register selection instead of using the traditional simplification-driven order so that the chances of honoring the preferences are effectively increased. Experimental results show that our coloring algorithm is powerful to simultaneously handle spill decisions, register coalescing, and preference resolutions.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512535},
 doi = {http://doi.acm.org/10.1145/543552.512535},
 acmid = {512535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, irregular-register architectures, register allocation, register coalescing},
} 

@article{Gargi:2002:SAP:543552.512536,
 author = {Gargi, Karthik},
 title = {A sparse algorithm for predicated global value numbering},
 abstract = {This paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing global reassociation</i>, it exploits the congruences induced by the predicates of conditional jumps (predicate inference</i> and value inference</i>), and it associates the arguments of acyclic \&oslash;</i> functions with the predicates controlling their arrival (\&oslash; predication</i>), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient sparse</i> formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {45--56},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512536},
 doi = {http://doi.acm.org/10.1145/543552.512536},
 acmid = {512536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {global value numbering, static single assignment},
} 

@inproceedings{Gargi:2002:SAP:512529.512536,
 author = {Gargi, Karthik},
 title = {A sparse algorithm for predicated global value numbering},
 abstract = {This paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing global reassociation</i>, it exploits the congruences induced by the predicates of conditional jumps (predicate inference</i> and value inference</i>), and it associates the arguments of acyclic \&oslash;</i> functions with the predicates controlling their arrival (\&oslash; predication</i>), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient sparse</i> formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {45--56},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512536},
 doi = {http://doi.acm.org/10.1145/512529.512536},
 acmid = {512536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {global value numbering, static single assignment},
} 

@article{Das:2002:EPP:543552.512538,
 author = {Das, Manuvir and Lerner, Sorin and Seigle, Mark},
 title = {ESP: path-sensitive program verification in polynomial time},
 abstract = {In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full path-sensitive analysis.We have implemented this "property simulation" algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to <b>.fprintf</b> in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {57--68},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512538},
 doi = {http://doi.acm.org/10.1145/543552.512538},
 acmid = {512538},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow analysis, error detection, path-sensitive analysis},
} 

@inproceedings{Das:2002:EPP:512529.512538,
 author = {Das, Manuvir and Lerner, Sorin and Seigle, Mark},
 title = {ESP: path-sensitive program verification in polynomial time},
 abstract = {In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full path-sensitive analysis.We have implemented this "property simulation" algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to <b>.fprintf</b> in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {57--68},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512538},
 doi = {http://doi.acm.org/10.1145/512529.512538},
 acmid = {512538},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow analysis, error detection, path-sensitive analysis},
} 

@inproceedings{Hallem:2002:SLB:512529.512539,
 author = {Hallem, Seth and Chelf, Benjamin and Xie, Yichen and Engler, Dawson},
 title = {A system and language for building system-specific, static analyses},
 abstract = {This paper presents a novel approach to bug-finding analysis and an implementation of that approach. Our goal is to find as many serious bugs as possible. To do so, we designed a flexible, easy-to-use extension language for specifying analyses and an efficent algorithm for executing these extensions. The language, metal</i>, allows the users of our system to specify a broad class of analyses in terms that resemble the intuitive description of the rules that they check. The system, xgcc</i>, executes these analyses efficiently using a context-sensitive, interprocedural analysis. Our prior work has shown that the approach described in this paper is effective: it has successfully found thousands of bugs in real systems code. This paper describes the underlying system used to achieve these results. We believe that our system is an effective framework for deploying new bug-finding analyses quickly and easily.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {69--82},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/512529.512539},
 doi = {http://doi.acm.org/10.1145/512529.512539},
 acmid = {512539},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, extensible compilation},
} 

@article{Hallem:2002:SLB:543552.512539,
 author = {Hallem, Seth and Chelf, Benjamin and Xie, Yichen and Engler, Dawson},
 title = {A system and language for building system-specific, static analyses},
 abstract = {This paper presents a novel approach to bug-finding analysis and an implementation of that approach. Our goal is to find as many serious bugs as possible. To do so, we designed a flexible, easy-to-use extension language for specifying analyses and an efficent algorithm for executing these extensions. The language, metal</i>, allows the users of our system to specify a broad class of analyses in terms that resemble the intuitive description of the rules that they check. The system, xgcc</i>, executes these analyses efficiently using a context-sensitive, interprocedural analysis. Our prior work has shown that the approach described in this paper is effective: it has successfully found thousands of bugs in real systems code. This paper describes the underlying system used to achieve these results. We believe that our system is an effective framework for deploying new bug-finding analyses quickly and easily.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {69--82},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/543552.512539},
 doi = {http://doi.acm.org/10.1145/543552.512539},
 acmid = {512539},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error detection, extensible compilation},
} 

@article{Ramalingam:2002:DSP:543552.512540,
 author = {Ramalingam, G. and Warshavsky, Alex and Field, John and Goyal, Deepak and Sagiv, Mooly},
 title = {Deriving specialized program analyses for certifying component-client conformance},
 abstract = {We are concerned with the problem of statically certifying</i> (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged</i> fashion for certain classes of first-order safety</i> (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize first-order predicates</i>, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier</i>. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the client violates the component's constraints. Unlike verification approaches that analyze a specification and client code together, our technique can take advantage of computationally-intensive symbolic techniques during the abstraction generation phase, without affecting the performance of client analysis. Using as a running example the Concurrent Modification Problem</i> (CMP), which arises when certain classes defined by the Java Collections Framework are misused, we describe several different classes of certifiers with varying time/space/precision tradeoffs. Of particular note are precise, polynomial-time, flow- and context-sensitive certifiers for certain classes of FOS specifications and client programs. Finally, we evaluate a prototype implementation of a certifier for CMP on a variety of test programs. The results of the evaluation show that our approach, though conservative, yields very few "false alarms," with acceptable performance.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512540},
 doi = {http://doi.acm.org/10.1145/543552.512540},
 acmid = {512540},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, model checking, predicate abstraction, software components, static analysis},
} 

@inproceedings{Ramalingam:2002:DSP:512529.512540,
 author = {Ramalingam, G. and Warshavsky, Alex and Field, John and Goyal, Deepak and Sagiv, Mooly},
 title = {Deriving specialized program analyses for certifying component-client conformance},
 abstract = {We are concerned with the problem of statically certifying</i> (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged</i> fashion for certain classes of first-order safety</i> (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize first-order predicates</i>, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier</i>. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the client violates the component's constraints. Unlike verification approaches that analyze a specification and client code together, our technique can take advantage of computationally-intensive symbolic techniques during the abstraction generation phase, without affecting the performance of client analysis. Using as a running example the Concurrent Modification Problem</i> (CMP), which arises when certain classes defined by the Java Collections Framework are misused, we describe several different classes of certifiers with varying time/space/precision tradeoffs. Of particular note are precise, polynomial-time, flow- and context-sensitive certifiers for certain classes of FOS specifications and client programs. Finally, we evaluate a prototype implementation of a certifier for CMP on a variety of test programs. The results of the evaluation show that our approach, though conservative, yields very few "false alarms," with acceptable performance.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512540},
 doi = {http://doi.acm.org/10.1145/512529.512540},
 acmid = {512540},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, model checking, predicate abstraction, software components, static analysis},
} 

@inproceedings{Debray:2002:PCC:512529.512542,
 author = {Debray, Saumya and Evans, William},
 title = {Profile-guided code compression},
 abstract = {As computers are increasingly used in contexts where the amount of available memory is limited, it becomes important to devise techniques that reduce the memory footprint of application programs while leaving them in an executable form. This paper describes an approach to applying data compression techniques to reduce the size of infrequently executed portions of a program. The compressed code is decompressed dynamically (via software) if needed, prior to execution. The use of data compression techniques increases the amount of code size reduction that can be achieved; their application to infrequently executed code limits the runtime overhead due to dynamic decompression; and the use of software decompression renders the approach generally applicable, without requiring specialized hardware. The code size reductions obtained depend on the threshold used to determine what code is "infrequently executed" and hence should be compressed: for low thresholds, we see size reductions of 13.7\% to 18.8\%, on average, for a set of embedded applications, without excessive runtime overhead.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/512529.512542},
 doi = {http://doi.acm.org/10.1145/512529.512542},
 acmid = {512542},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code compaction, code compression, code size reduction, dynamic decompression},
} 

@article{Debray:2002:PCC:543552.512542,
 author = {Debray, Saumya and Evans, William},
 title = {Profile-guided code compression},
 abstract = {As computers are increasingly used in contexts where the amount of available memory is limited, it becomes important to devise techniques that reduce the memory footprint of application programs while leaving them in an executable form. This paper describes an approach to applying data compression techniques to reduce the size of infrequently executed portions of a program. The compressed code is decompressed dynamically (via software) if needed, prior to execution. The use of data compression techniques increases the amount of code size reduction that can be achieved; their application to infrequently executed code limits the runtime overhead due to dynamic decompression; and the use of software decompression renders the approach generally applicable, without requiring specialized hardware. The code size reductions obtained depend on the threshold used to determine what code is "infrequently executed" and hence should be compressed: for low thresholds, we see size reductions of 13.7\% to 18.8\%, on average, for a set of embedded applications, without excessive runtime overhead.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/543552.512542},
 doi = {http://doi.acm.org/10.1145/543552.512542},
 acmid = {512542},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code compaction, code compression, code size reduction, dynamic decompression},
} 

@inproceedings{Rajagopalan:2002:POE:512529.512543,
 author = {Rajagopalan, Mohan and Debray, Saumya K. and Hiltunen, Matti A. and Schlichting, Richard D.},
 title = {Profile-directed optimization of event-based programs},
 abstract = {Events are used as a fundamental abstraction in programs ranging from graphical user interfaces (GUIs) to systems for building customized network protocols. While providing a flexible structuring and execution paradigm, events have the potentially serious drawback of extra execution overhead due to the indirection between modules that raise events and those that handle them. This paper describes an approach to addressing this issue using static optimization techniques. This approach, which exploits the underlying predictability often exhibited by event-based programs, is based on first profiling the program to identify commonly occurring event sequences. A variety of techniques that use the resulting profile information are then applied to the program to reduce the overheads associated with such mechanisms as indirect function calls and argument marshaling. In addition to describing the overall approach, experimental results are given that demonstrate the effectiveness of the techniques. These results are from event-based programs written for X Windows, a system for building GUIs, and Cactus, a system for constructing highly configurable distributed services and network protocols.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/512529.512543},
 doi = {http://doi.acm.org/10.1145/512529.512543},
 acmid = {512543},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {events, handlers, profiling},
} 

@article{Rajagopalan:2002:POE:543552.512543,
 author = {Rajagopalan, Mohan and Debray, Saumya K. and Hiltunen, Matti A. and Schlichting, Richard D.},
 title = {Profile-directed optimization of event-based programs},
 abstract = {Events are used as a fundamental abstraction in programs ranging from graphical user interfaces (GUIs) to systems for building customized network protocols. While providing a flexible structuring and execution paradigm, events have the potentially serious drawback of extra execution overhead due to the indirection between modules that raise events and those that handle them. This paper describes an approach to addressing this issue using static optimization techniques. This approach, which exploits the underlying predictability often exhibited by event-based programs, is based on first profiling the program to identify commonly occurring event sequences. A variety of techniques that use the resulting profile information are then applied to the program to reduce the overheads associated with such mechanisms as indirect function calls and argument marshaling. In addition to describing the overall approach, experimental results are given that demonstrate the effectiveness of the techniques. These results are from event-based programs written for X Windows, a system for building GUIs, and Cactus, a system for constructing highly configurable distributed services and network protocols.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/543552.512543},
 doi = {http://doi.acm.org/10.1145/543552.512543},
 acmid = {512543},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {events, handlers, profiling},
} 

@article{Liao:2002:PBA:543552.512544,
 author = {Liao, Steve S.W. and Wang, Perry H. and Wang, Hong and Hoflehner, Gerolf and Lavery, Daniel and Shen, John P.},
 title = {Post-pass binary adaptation for software-based speculative precomputation},
 abstract = {Recently, a number of thread-based prefetching techniques have been proposed. These techniques aim at improving the latency of single-threaded applications by leveraging multithreading resources to perform memory prefetching via speculative prefetch threads. Software-based speculative precomputation (SSP) is one such technique, proposed for multithreaded Itanium models. SSP does not require expensive hardware support-instead it relies on the compiler to adapt binaries to perform prefetching on otherwise idle hardware thread contexts at run time. This paper presents a post-pass compilation tool for generating SSP-enhanced binaries. The tool is able to: (1) analyze a single-threaded application to generate prefetch threads; (2) identify and embed trigger points in the original binary; and (3) produce a new binary that has the prefetch threads attached. The execution of the new binary spawns the speculative prefetch threads, which are executed concurrently with the main thread. Our results indicate that for a set of pointer-intensive benchmarks, the prefetching performed by the speculative threads achieves an average of 87\% speedup on an in-order processor and 5\% speedup on an out-of-order processor.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512544},
 doi = {http://doi.acm.org/10.1145/543552.512544},
 acmid = {512544},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining speculative precomputation, delay minimization, dependence reduction, long-range thread-based prefetching, loop rotation, pointer, post-pass, prediction, scheduling, slack, slicing, speculation, triggering},
} 

@inproceedings{Liao:2002:PBA:512529.512544,
 author = {Liao, Steve S.W. and Wang, Perry H. and Wang, Hong and Hoflehner, Gerolf and Lavery, Daniel and Shen, John P.},
 title = {Post-pass binary adaptation for software-based speculative precomputation},
 abstract = {Recently, a number of thread-based prefetching techniques have been proposed. These techniques aim at improving the latency of single-threaded applications by leveraging multithreading resources to perform memory prefetching via speculative prefetch threads. Software-based speculative precomputation (SSP) is one such technique, proposed for multithreaded Itanium models. SSP does not require expensive hardware support-instead it relies on the compiler to adapt binaries to perform prefetching on otherwise idle hardware thread contexts at run time. This paper presents a post-pass compilation tool for generating SSP-enhanced binaries. The tool is able to: (1) analyze a single-threaded application to generate prefetch threads; (2) identify and embed trigger points in the original binary; and (3) produce a new binary that has the prefetch threads attached. The execution of the new binary spawns the speculative prefetch threads, which are executed concurrently with the main thread. Our results indicate that for a set of pointer-intensive benchmarks, the prefetching performed by the speculative threads achieves an average of 87\% speedup on an in-order processor and 5\% speedup on an out-of-order processor.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512544},
 doi = {http://doi.acm.org/10.1145/512529.512544},
 acmid = {512544},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining speculative precomputation, delay minimization, dependence reduction, long-range thread-based prefetching, loop rotation, pointer, post-pass, prediction, scheduling, slack, slicing, speculation, triggering},
} 

@inproceedings{Ossia:2002:PIC:512529.512546,
 author = {Ossia, Yoav and Ben-Yitzhak, Ori and Goft, Irit and Kolodner, Elliot K. and Leikehman, Victor and Owshanko, Avi},
 title = {A parallel, incremental and concurrent GC for servers},
 abstract = {Multithreaded applications with multi-gigabyte heaps running on modern servers provide new challenges for garbage collection (GC). The challenges for "server-oriented" GC include: ensuring short pause times on a multi-gigabyte heap, while minimizing throughput penalty, good scaling on multiprocessor hardware, and keeping the number of expensive multi-cycle fence instructions required by weak ordering to a minimum. We designed and implemented a fully parallel, incremental, mostly concurrent collector, which employs several novel techniques to meet these challenges. First, it combines incremental GC to ensure short pause times with concurrent low-priority background GC threads to take advantage of processor idle time. Second, it employs a low-overhead work packet mechanism to enable full parallelism among the incremental and concurrent collecting threads and ensure load balancing. Third, it reduces memory fence instructions by using batching techniques: one fence for each block of small objects allocated, one fence for each group of objects marked, and no fence at all in the write barrier. When compared to the mature well-optimized parallel stop-the-world mark-sweep collector already in the IBM JVM, our collector prototype reduces the maximum pause time from 284 ms to 101 ms, and the average pause time from 266 ms to 66 ms while only losing 10\% throughput when running the SPECjbb2000 benchmark on a 256 MB heap on a 4-way 550 MHz Pentium multiprocessor.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512546},
 doi = {http://doi.acm.org/10.1145/512529.512546},
 acmid = {512546},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JVM, Java, concurrent garbage collection, garbage collection, incremental garbage collection, weak ordering},
} 

@article{Ossia:2002:PIC:543552.512546,
 author = {Ossia, Yoav and Ben-Yitzhak, Ori and Goft, Irit and Kolodner, Elliot K. and Leikehman, Victor and Owshanko, Avi},
 title = {A parallel, incremental and concurrent GC for servers},
 abstract = {Multithreaded applications with multi-gigabyte heaps running on modern servers provide new challenges for garbage collection (GC). The challenges for "server-oriented" GC include: ensuring short pause times on a multi-gigabyte heap, while minimizing throughput penalty, good scaling on multiprocessor hardware, and keeping the number of expensive multi-cycle fence instructions required by weak ordering to a minimum. We designed and implemented a fully parallel, incremental, mostly concurrent collector, which employs several novel techniques to meet these challenges. First, it combines incremental GC to ensure short pause times with concurrent low-priority background GC threads to take advantage of processor idle time. Second, it employs a low-overhead work packet mechanism to enable full parallelism among the incremental and concurrent collecting threads and ensure load balancing. Third, it reduces memory fence instructions by using batching techniques: one fence for each block of small objects allocated, one fence for each group of objects marked, and no fence at all in the write barrier. When compared to the mature well-optimized parallel stop-the-world mark-sweep collector already in the IBM JVM, our collector prototype reduces the maximum pause time from 284 ms to 101 ms, and the average pause time from 266 ms to 66 ms while only losing 10\% throughput when running the SPECjbb2000 benchmark on a 256 MB heap on a 4-way 550 MHz Pentium multiprocessor.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512546},
 doi = {http://doi.acm.org/10.1145/543552.512546},
 acmid = {512546},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JVM, Java, concurrent garbage collection, garbage collection, incremental garbage collection, weak ordering},
} 

@article{Hallenberg:2002:CRI:543552.512547,
 author = {Hallenberg, Niels and Elsman, Martin and Tofte, Mads},
 title = {Combining region inference and garbage collection},
 abstract = {This paper describes a memory discipline that combines region-based memory management and copying garbage collection by extending Cheney's copying garbage collection algorithm to work with regions. The paper presents empirical evidence that region inference very significantly reduces the number of garbage collections; and evidence that the fastest execution is obtained by using regions alone, without garbage collection. The memory discipline is implemented for Standard ML in the ML Kit compiler and measurements show that for a variety of benchmark programs, code generated by the compiler is as efficient, both with respect to execution time and memory usage, as programs compiled with Standard ML of New Jersey, another state-of-the-art Standard ML compiler.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {141--152},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512547},
 doi = {http://doi.acm.org/10.1145/543552.512547},
 acmid = {512547},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, region interface, standard ML},
} 

@inproceedings{Hallenberg:2002:CRI:512529.512547,
 author = {Hallenberg, Niels and Elsman, Martin and Tofte, Mads},
 title = {Combining region inference and garbage collection},
 abstract = {This paper describes a memory discipline that combines region-based memory management and copying garbage collection by extending Cheney's copying garbage collection algorithm to work with regions. The paper presents empirical evidence that region inference very significantly reduces the number of garbage collections; and evidence that the fastest execution is obtained by using regions alone, without garbage collection. The memory discipline is implemented for Standard ML in the ML Kit compiler and measurements show that for a variety of benchmark programs, code generated by the compiler is as efficient, both with respect to execution time and memory usage, as programs compiled with Standard ML of New Jersey, another state-of-the-art Standard ML compiler.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {141--152},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512547},
 doi = {http://doi.acm.org/10.1145/512529.512547},
 acmid = {512547},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, region interface, standard ML},
} 

@article{Blackburn:2002:BGA:543552.512548,
 author = {Blackburn, Stephen M and Jones, Richard and McKinley, Kathryn S. and Moss, J Eliot B},
 title = {Beltway: getting around garbage collection gridlock},
 abstract = {We present the design and implementation of a new garbage collection framework that significantly generalizes existing copying collectors. The Beltway</i> framework exploits and separates object age and incrementality. It groups objects in one or more increments on queues called belts</i>, collects belts independently, and collects increments on a belt in first-in-first-out order. We show that Beltway configurations, selected by command line options, act and perform the same as semi-space, generational, and older-first collectors, and encompass all previous copying collectors of which we are aware. The increasing reliance on garbage collected languages such as Java requires that the collector perform well. We show that the generality of Beltway enables us to design and implement new collectors that are robust to variations in heap size and improve total execution time over the best generational copying collectors of which we are aware by up to 40\%, and on average by 5 to 10\%, for small to moderate heap sizes. New garbage collection algorithms are rare, and yet we define not just one, but a new family of collectors that subsumes previous work. This generality enables us to explore a larger design space and build better collectors.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {153--164},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512548},
 doi = {http://doi.acm.org/10.1145/543552.512548},
 acmid = {512548},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, beltway, copying collection, generational collection},
} 

@inproceedings{Blackburn:2002:BGA:512529.512548,
 author = {Blackburn, Stephen M and Jones, Richard and McKinley, Kathryn S. and Moss, J Eliot B},
 title = {Beltway: getting around garbage collection gridlock},
 abstract = {We present the design and implementation of a new garbage collection framework that significantly generalizes existing copying collectors. The Beltway</i> framework exploits and separates object age and incrementality. It groups objects in one or more increments on queues called belts</i>, collects belts independently, and collects increments on a belt in first-in-first-out order. We show that Beltway configurations, selected by command line options, act and perform the same as semi-space, generational, and older-first collectors, and encompass all previous copying collectors of which we are aware. The increasing reliance on garbage collected languages such as Java requires that the collector perform well. We show that the generality of Beltway enables us to design and implement new collectors that are robust to variations in heap size and improve total execution time over the best generational copying collectors of which we are aware by up to 40\%, and on average by 5 to 10\%, for small to moderate heap sizes. New garbage collection algorithms are rare, and yet we define not just one, but a new family of collectors that subsumes previous work. This generality enables us to explore a larger design space and build better collectors.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {153--164},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512548},
 doi = {http://doi.acm.org/10.1145/512529.512548},
 acmid = {512548},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, beltway, copying collection, generational collection},
} 

@article{So:2002:CAF:543552.512550,
 author = {So, Byoungro and Hall, Mary W. and Diniz, Pedro C.},
 title = {A compiler approach to fast hardware design space exploration in FPGA-based systems},
 abstract = {The current practice of mapping computations to custom hardware implementations requires programmers to assume the role of hardware designers. In tuning the performance of their hardware implementation, designers manually apply loop transformations such as loop unrolling. designers manually apply loop transformations. For example, loop unrolling is used to expose instruction-level parallelism at the expense of more hardware resources for concurrent operator evaluation. Because unrolling also increases the amount of data a computation requires, too much unrolling can lead to a memory bound implementation where resources are idle. To negotiate inherent hardware space-time trade-offs, designers must engage in an iterative refinement cycle, at each step manually applying transformations and evaluating their impact. This process is not only error-prone and tedious but also prohibitively expensive given the large search spaces and with long synthesis times. This paper describes an automated approach to hardware design space exploration, through a collaboration between parallelizing compiler technology and high-level synthesis tools. We present a compiler algorithm that automatically explores the large design spaces resulting from the application of several program transformations commonly used in application-specific hardware designs. Our approach uses synthesis estimation techniques to quantitatively evaluate alternate designs for a loop nest computation. We have implemented this design space exploration algorithm in the context of a compilation and synthesis system called DEFACTO, and present results of this implementation on five multimedia kernels. Our algorithm derives an implementation that closely matches the performance of the fastest design in the design space, and among implementations with comparable performance, selects the smallest design. We search on average only 0.3\% of the design space. This technology thus significantly raises the level of abstraction for hardware design and explores a design space much larger than is feasible for a human designer.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512550},
 doi = {http://doi.acm.org/10.1145/543552.512550},
 acmid = {512550},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data dependence analysis, design space exploration, loop transformations, reuse analysis},
} 

@inproceedings{So:2002:CAF:512529.512550,
 author = {So, Byoungro and Hall, Mary W. and Diniz, Pedro C.},
 title = {A compiler approach to fast hardware design space exploration in FPGA-based systems},
 abstract = {The current practice of mapping computations to custom hardware implementations requires programmers to assume the role of hardware designers. In tuning the performance of their hardware implementation, designers manually apply loop transformations such as loop unrolling. designers manually apply loop transformations. For example, loop unrolling is used to expose instruction-level parallelism at the expense of more hardware resources for concurrent operator evaluation. Because unrolling also increases the amount of data a computation requires, too much unrolling can lead to a memory bound implementation where resources are idle. To negotiate inherent hardware space-time trade-offs, designers must engage in an iterative refinement cycle, at each step manually applying transformations and evaluating their impact. This process is not only error-prone and tedious but also prohibitively expensive given the large search spaces and with long synthesis times. This paper describes an automated approach to hardware design space exploration, through a collaboration between parallelizing compiler technology and high-level synthesis tools. We present a compiler algorithm that automatically explores the large design spaces resulting from the application of several program transformations commonly used in application-specific hardware designs. Our approach uses synthesis estimation techniques to quantitatively evaluate alternate designs for a loop nest computation. We have implemented this design space exploration algorithm in the context of a compilation and synthesis system called DEFACTO, and present results of this implementation on five multimedia kernels. Our algorithm derives an implementation that closely matches the performance of the fastest design in the design space, and among implementations with comparable performance, selects the smallest design. We search on average only 0.3\% of the design space. This technology thus significantly raises the level of abstraction for hardware design and explores a design space much larger than is feasible for a human designer.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512550},
 doi = {http://doi.acm.org/10.1145/512529.512550},
 acmid = {512550},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data dependence analysis, design space exploration, loop transformations, reuse analysis},
} 

@article{Cociorva:2002:STO:543552.512551,
 author = {Cociorva, Daniel and Baumgartner, Gerald and Lam, Chi-Chung and Sadayappan, P. and Ramanujam, J. and Nooijen, Marcel and Bernholdt, David E. and Harrison, Robert},
 title = {Space-time trade-off optimization for a class of electronic structure calculations},
 abstract = {The accurate modeling of the electronic structure of atoms and molecules is very computationally intensive. Many models of electronic structure, such as the Coupled Cluster approach, involve collections of tensor contractions. There are usually a large number of alternative ways of implementing the tensor contractions, representing different trade-offs between the space required for temporary intermediates and the total number of arithmetic operations. In this paper, we present an algorithm that starts with an operation-minimal form of the computation and systematically explores the possible space-time trade-offs to identify the form with lowest cost that fits within a specified memory limit. Its utility is demonstrated by applying it to a computation representative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Pacific Northwest National Laboratory.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/543552.512551},
 doi = {http://doi.acm.org/10.1145/543552.512551},
 acmid = {512551},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop fusion, loop transformation, tile size selection},
} 

@inproceedings{Cociorva:2002:STO:512529.512551,
 author = {Cociorva, Daniel and Baumgartner, Gerald and Lam, Chi-Chung and Sadayappan, P. and Ramanujam, J. and Nooijen, Marcel and Bernholdt, David E. and Harrison, Robert},
 title = {Space-time trade-off optimization for a class of electronic structure calculations},
 abstract = {The accurate modeling of the electronic structure of atoms and molecules is very computationally intensive. Many models of electronic structure, such as the Coupled Cluster approach, involve collections of tensor contractions. There are usually a large number of alternative ways of implementing the tensor contractions, representing different trade-offs between the space required for temporary intermediates and the total number of arithmetic operations. In this paper, we present an algorithm that starts with an operation-minimal form of the computation and systematically explores the possible space-time trade-offs to identify the form with lowest cost that fits within a specified memory limit. Its utility is demonstrated by applying it to a computation representative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Pacific Northwest National Laboratory.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/512529.512551},
 doi = {http://doi.acm.org/10.1145/512529.512551},
 acmid = {512551},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop fusion, loop transformation, tile size selection},
} 

@article{Kawahito:2002:ESE:543552.512552,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Effective sign extension elimination},
 abstract = {Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most of the programs available today are still designed for 32-bit architectures. Java\&trade;, for example, specifies the frequently used int" as a 32-bit data type. If such Java programs are executed on a 64-bit architecture, many 32-bit values must be sign-extended to 64-bit values for integer operations. This causes serious performance overhead. In this paper, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64\&trade;. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it significantly improves performance, while it increases JIT compilation time by only 0.11\%. We implemented our algorithm for programs in Java, but it can be applied to any language requiring sign extensions.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {187--198},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512552},
 doi = {http://doi.acm.org/10.1145/543552.512552},
 acmid = {512552},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {64-bit architectures, IA-64, JIT compilers, Java, sign extension},
} 

@inproceedings{Kawahito:2002:ESE:512529.512552,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Effective sign extension elimination},
 abstract = {Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most of the programs available today are still designed for 32-bit architectures. Java\&trade;, for example, specifies the frequently used int" as a 32-bit data type. If such Java programs are executed on a 64-bit architecture, many 32-bit values must be sign-extended to 64-bit values for integer operations. This causes serious performance overhead. In this paper, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64\&trade;. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it significantly improves performance, while it increases JIT compilation time by only 0.11\%. We implemented our algorithm for programs in Java, but it can be applied to any language requiring sign extensions.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {187--198},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512552},
 doi = {http://doi.acm.org/10.1145/512529.512552},
 acmid = {512552},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {64-bit architectures, IA-64, JIT compilers, Java, sign extension},
} 

@article{Chilimbi:2002:DHD:543552.512554,
 author = {Chilimbi, Trishul M. and Hirzel, Martin},
 title = {Dynamic hot data stream prefetching for general-purpose programs},
 abstract = {Prefetching data ahead of use has the potential to tolerate the grow ing processor-memory performance gap by overlapping long latency memory accesses with useful computation. While sophisti cated prefetching techniques have been automated for limited domains, such as scientific codes that access dense arrays in loop nests, a similar level of success has eluded general-purpose pro grams, especially pointer-chasing codes written in languages such as C and C++. We address this problem by describing, implementing and evaluating a dynamic prefetching scheme. Our technique runs on stock hardware, is completely automatic, and works for general-purpose programs, including pointer-chasing codes written in weakly-typed languages, such as C and C++. It operates in three phases. First, the profiling phase gathers a temporal data reference profile from a running program with low-overhead. Next, the profiling is turned off and a fast analysis algorithm extracts hot data streams, which are data reference sequences that frequently repeat in the same order, from the temporal profile. Then, the system dynamically injects code at appropriate program points to detect and prefetch these hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis is performed, and the program continues to execute with the added prefetch instructions. At the end of the hibernation phase, the program is de-optimized to remove the inserted checks and prefetch instructions, and control returns to the profiling phase. For long-running programs, this profile, analyze and optimize, hibernate, cycle will repeat multiple times. Our initial results from applying dynamic prefetching are promising, indicating overall execution time improvements of 5.19\% for several memory-performance-limited SPECint2000 benchmarks running their largest (ref) inputs.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {199--209},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/543552.512554},
 doi = {http://doi.acm.org/10.1145/543552.512554},
 acmid = {512554},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data reference profiling, dynamic optimization, dynamic profiling, memory performance optimization, prefetching, temporal profiling},
} 

@inproceedings{Chilimbi:2002:DHD:512529.512554,
 author = {Chilimbi, Trishul M. and Hirzel, Martin},
 title = {Dynamic hot data stream prefetching for general-purpose programs},
 abstract = {Prefetching data ahead of use has the potential to tolerate the grow ing processor-memory performance gap by overlapping long latency memory accesses with useful computation. While sophisti cated prefetching techniques have been automated for limited domains, such as scientific codes that access dense arrays in loop nests, a similar level of success has eluded general-purpose pro grams, especially pointer-chasing codes written in languages such as C and C++. We address this problem by describing, implementing and evaluating a dynamic prefetching scheme. Our technique runs on stock hardware, is completely automatic, and works for general-purpose programs, including pointer-chasing codes written in weakly-typed languages, such as C and C++. It operates in three phases. First, the profiling phase gathers a temporal data reference profile from a running program with low-overhead. Next, the profiling is turned off and a fast analysis algorithm extracts hot data streams, which are data reference sequences that frequently repeat in the same order, from the temporal profile. Then, the system dynamically injects code at appropriate program points to detect and prefetch these hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis is performed, and the program continues to execute with the added prefetch instructions. At the end of the hibernation phase, the program is de-optimized to remove the inserted checks and prefetch instructions, and control returns to the profiling phase. For long-running programs, this profile, analyze and optimize, hibernate, cycle will repeat multiple times. Our initial results from applying dynamic prefetching are promising, indicating overall execution time improvements of 5.19\% for several memory-performance-limited SPECint2000 benchmarks running their largest (ref) inputs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {199--209},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/512529.512554},
 doi = {http://doi.acm.org/10.1145/512529.512554},
 acmid = {512554},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data reference profiling, dynamic optimization, dynamic profiling, memory performance optimization, prefetching, temporal profiling},
} 

@inproceedings{Wu:2002:EDR:512529.512555,
 author = {Wu, Youfeng},
 title = {Efficient discovery of regular stride patterns in irregular programs and its use in compiler prefetching},
 abstract = {Irregular data references are difficult to prefetch, as the future memory address of a load instruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that some important load instructions in irregular programs contain stride access patterns. Although the load instructions with stride patterns are difficult to identify with static compiler techniques, we developed an efficient profiling method to discover these load instructions. The new profiling method integrates the profiling for stride information and the traditional profiling for edge frequency into a single profiling pass. The integrated profiling pass runs only 17\% slower than the frequency profiling alone. The collected stride information helps the compiler to identify load instructions with stride patterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching techniques in a research compiler for Itanium Processor Family (IPF), and obtained significant performance improvement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x speedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance gain is stable across input data sets. These benefits make the new profiling and prefetching techniques suitable for production compilers.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {210--221},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512555},
 doi = {http://doi.acm.org/10.1145/512529.512555},
 acmid = {512555},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data prefetching, integrated stride and frequency profiling, performance evaluation, phased multi-strided loads, strongly single-strided loads},
} 

@article{Wu:2002:EDR:543552.512555,
 author = {Wu, Youfeng},
 title = {Efficient discovery of regular stride patterns in irregular programs and its use in compiler prefetching},
 abstract = {Irregular data references are difficult to prefetch, as the future memory address of a load instruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that some important load instructions in irregular programs contain stride access patterns. Although the load instructions with stride patterns are difficult to identify with static compiler techniques, we developed an efficient profiling method to discover these load instructions. The new profiling method integrates the profiling for stride information and the traditional profiling for edge frequency into a single profiling pass. The integrated profiling pass runs only 17\% slower than the frequency profiling alone. The collected stride information helps the compiler to identify load instructions with stride patterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching techniques in a research compiler for Itanium Processor Family (IPF), and obtained significant performance improvement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x speedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance gain is stable across input data sets. These benefits make the new profiling and prefetching techniques suitable for production compilers.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {210--221},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512555},
 doi = {http://doi.acm.org/10.1145/543552.512555},
 acmid = {512555},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data prefetching, integrated stride and frequency profiling, performance evaluation, phased multi-strided loads, strongly single-strided loads},
} 

@inproceedings{Burtscher:2002:SLC:512529.512556,
 author = {Burtscher, Martin and Diwan, Amer and Hauswirth, Matthias},
 title = {Static load classification for improving the value predictability of data-cache misses},
 abstract = {While caches are effective at avoiding most main-memory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the data-cache miss latency, architects have proposed various speculation mechanisms, including load-value prediction. A load-value predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware- and profile-based methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512556},
 doi = {http://doi.acm.org/10.1145/512529.512556},
 acmid = {512556},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {load-value prediction, type-based analysis},
} 

@article{Burtscher:2002:SLC:543552.512556,
 author = {Burtscher, Martin and Diwan, Amer and Hauswirth, Matthias},
 title = {Static load classification for improving the value predictability of data-cache misses},
 abstract = {While caches are effective at avoiding most main-memory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the data-cache miss latency, architects have proposed various speculation mechanisms, including load-value prediction. A load-value predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware- and profile-based methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512556},
 doi = {http://doi.acm.org/10.1145/543552.512556},
 acmid = {512556},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {load-value prediction, type-based analysis},
} 

@article{Flanagan:2002:ESC:543552.512558,
 author = {Flanagan, Cormac and Leino, K. Rustan M. and Lillibridge, Mark and Nelson, Greg and Saxe, James B. and Stata, Raymie},
 title = {Extended static checking for Java},
 abstract = {Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that finds common programming errors. The checker is powered by verification-condition generation and automatic theorem-proving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {234--245},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512558},
 doi = {http://doi.acm.org/10.1145/543552.512558},
 acmid = {512558},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compile-time program checking},
} 

@inproceedings{Flanagan:2002:ESC:512529.512558,
 author = {Flanagan, Cormac and Leino, K. Rustan M. and Lillibridge, Mark and Nelson, Greg and Saxe, James B. and Stata, Raymie},
 title = {Extended static checking for Java},
 abstract = {Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that finds common programming errors. The checker is powered by verification-condition generation and automatic theorem-proving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {234--245},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512558},
 doi = {http://doi.acm.org/10.1145/512529.512558},
 acmid = {512558},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compile-time program checking},
} 

@inproceedings{Leino:2002:UDG:512529.512559,
 author = {Leino, K. Rustan M. and Poetzsch-Heffter, Arnd and Zhou, Yunhong},
 title = {Using data groups to specify and check side effects},
 abstract = {Reasoning precisely about the side effects of procedure calls is important to many program analyses. This paper introduces a technique for specifying and statically checking the side effects of methods in an object-oriented language. The technique uses data groups</i>, which abstract over variables that are not in scope, and limits program behavior by two alias-confining restrictions, pivot uniqueness</i> and owner exclusion</i>. The technique is shown to achieve modular soundness and is simpler than previous attempts at solving this problem.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512559},
 doi = {http://doi.acm.org/10.1145/512529.512559},
 acmid = {512559},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias confinement, data groups, frame conditions, modifies lists, modular soundness, owner exclusion, pivot uniqueness, side effects, verification},
} 

@article{Leino:2002:UDG:543552.512559,
 author = {Leino, K. Rustan M. and Poetzsch-Heffter, Arnd and Zhou, Yunhong},
 title = {Using data groups to specify and check side effects},
 abstract = {Reasoning precisely about the side effects of procedure calls is important to many program analyses. This paper introduces a technique for specifying and statically checking the side effects of methods in an object-oriented language. The technique uses data groups</i>, which abstract over variables that are not in scope, and limits program behavior by two alias-confining restrictions, pivot uniqueness</i> and owner exclusion</i>. The technique is shown to achieve modular soundness and is simpler than previous attempts at solving this problem.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512559},
 doi = {http://doi.acm.org/10.1145/543552.512559},
 acmid = {512559},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias confinement, data groups, frame conditions, modifies lists, modular soundness, owner exclusion, pivot uniqueness, side effects, verification},
} 

@inproceedings{Choi:2002:EPD:512529.512560,
 author = {Choi, Jong-Deok and Lee, Keunwoo and Loginov, Alexey and O'Callahan, Robert and Sarkar, Vivek and Sridharan, Manu},
 title = {Efficient and precise datarace detection for multithreaded object-oriented programs},
 abstract = {We present a novel approach to dynamic datarace detection for multithreaded object-oriented programs. Past techniques for on-the-fly datarace detection either sacrificed precision for performance, leading to many false positive datarace reports, or maintained precision but incurred significant overheads in the range of 3<sc>x</sc> to 30<sc>x</sc>. In contrast, our approach results in very few false positives and runtime overhead in the 13\% to 42\% range, making it both efficient and</i> precise. This performance improvement is the result of a unique combination of complementary static and dynamic optimization techniques.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512560},
 doi = {http://doi.acm.org/10.1145/512529.512560},
 acmid = {512560},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataraces, debugging, multithreaded programming, object-oriented programming, parallel programs, race conditions, static-dynamic co-analysis, synchronization},
} 

@article{Choi:2002:EPD:543552.512560,
 author = {Choi, Jong-Deok and Lee, Keunwoo and Loginov, Alexey and O'Callahan, Robert and Sarkar, Vivek and Sridharan, Manu},
 title = {Efficient and precise datarace detection for multithreaded object-oriented programs},
 abstract = {We present a novel approach to dynamic datarace detection for multithreaded object-oriented programs. Past techniques for on-the-fly datarace detection either sacrificed precision for performance, leading to many false positive datarace reports, or maintained precision but incurred significant overheads in the range of 3<sc>x</sc> to 30<sc>x</sc>. In contrast, our approach results in very few false positives and runtime overhead in the 13\% to 42\% range, making it both efficient and</i> precise. This performance improvement is the result of a unique combination of complementary static and dynamic optimization techniques.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512560},
 doi = {http://doi.acm.org/10.1145/543552.512560},
 acmid = {512560},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataraces, debugging, multithreaded programming, object-oriented programming, parallel programs, race conditions, static-dynamic co-analysis, synchronization},
} 

@inproceedings{Baker:2002:MMS:512529.512562,
 author = {Baker, Jason and Hsieh, Wilson C.},
 title = {Maya: multiple-dispatch syntax extension in Java},
 abstract = {We have designed and implemented Maya, a version of Java that allows programmers to extend and reinterpret its syntax. Maya generalizes macro systems by treating grammar productions as generic functions, and semantic actions on productions as multimethods on the corresponding generic functions. Programmers can write new generic functions (i.e., grammar productions) and new multimethods (i.e., semantic actions), through which they can extend the grammar of the language and change the semantics of its syntactic constructs, respectively. Maya's multimethods are compile-time metaprograms that transform abstract syntax: they execute at program compile-time, because they are semantic actions executed by the parser. Maya's multimethods can be dispatched on the syntactic structure of the input, as well as the static, source-level types of expressions in the input. In this paper we describe what Maya can do and how it works. We describe how its novel parsing techniques work and how Maya can statically detect certain kinds of errors, such as code that generates references to free variables. Finally, to demonstrate Maya's expressiveness, we describe how Maya can be used to implement the MultiJava language, which was described by Clifton et al. at OOPSLA 2000.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {270--281},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512562},
 doi = {http://doi.acm.org/10.1145/512529.512562},
 acmid = {512562},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, generative programming, macros, metaprogramming},
} 

@article{Baker:2002:MMS:543552.512562,
 author = {Baker, Jason and Hsieh, Wilson C.},
 title = {Maya: multiple-dispatch syntax extension in Java},
 abstract = {We have designed and implemented Maya, a version of Java that allows programmers to extend and reinterpret its syntax. Maya generalizes macro systems by treating grammar productions as generic functions, and semantic actions on productions as multimethods on the corresponding generic functions. Programmers can write new generic functions (i.e., grammar productions) and new multimethods (i.e., semantic actions), through which they can extend the grammar of the language and change the semantics of its syntactic constructs, respectively. Maya's multimethods are compile-time metaprograms that transform abstract syntax: they execute at program compile-time, because they are semantic actions executed by the parser. Maya's multimethods can be dispatched on the syntactic structure of the input, as well as the static, source-level types of expressions in the input. In this paper we describe what Maya can do and how it works. We describe how its novel parsing techniques work and how Maya can statically detect certain kinds of errors, such as code that generates references to free variables. Finally, to demonstrate Maya's expressiveness, we describe how Maya can be used to implement the MultiJava language, which was described by Clifton et al. at OOPSLA 2000.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {270--281},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512562},
 doi = {http://doi.acm.org/10.1145/543552.512562},
 acmid = {512562},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, generative programming, macros, metaprogramming},
} 

@article{Grossman:2002:RMM:543552.512563,
 author = {Grossman, Dan and Morrisett, Greg and Jim, Trevor and Hicks, Michael and Wang, Yanling and Cheney, James},
 title = {Region-based memory management in cyclone},
 abstract = {Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8\% of the code; of the changes, only 6\% (of the 8\%) were region annotations.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {282--293},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512563},
 doi = {http://doi.acm.org/10.1145/543552.512563},
 acmid = {512563},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Grossman:2002:RMM:512529.512563,
 author = {Grossman, Dan and Morrisett, Greg and Jim, Trevor and Hicks, Michael and Wang, Yanling and Cheney, James},
 title = {Region-based memory management in cyclone},
 abstract = {Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8\% of the code; of the changes, only 6\% (of the 8\%) were region annotations.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {282--293},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512563},
 doi = {http://doi.acm.org/10.1145/512529.512563},
 acmid = {512563},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Almasi:2002:MCM:543552.512564,
 author = {Alm\'{a}si, George and Padua, David},
 title = {MaJIC: compiling MATLAB for speed and responsiveness},
 abstract = {This paper presents and evaluates techniques to improve the execution performance of MATLAB. Previous efforts concentrated on source to source translation and batch compilation; <b>MaJIC</b> provides an interactive frontend that looks like MATLAB and compiles/optimizes code behind the scenes in real time, employing a combination of just-in-time and speculative ahead-of-time compilation. Performance results show that the proper mixture of these two techniques can yield near-zero response time as well as performance gains previously achieved only by batch compilers.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {294--303},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/543552.512564},
 doi = {http://doi.acm.org/10.1145/543552.512564},
 acmid = {512564},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Almasi:2002:MCM:512529.512564,
 author = {Alm\'{a}si, George and Padua, David},
 title = {MaJIC: compiling MATLAB for speed and responsiveness},
 abstract = {This paper presents and evaluates techniques to improve the execution performance of MATLAB. Previous efforts concentrated on source to source translation and batch compilation; <b>MaJIC</b> provides an interactive frontend that looks like MATLAB and compiles/optimizes code behind the scenes in real time, employing a combination of just-in-time and speculative ahead-of-time compilation. Performance results show that the proper mixture of these two techniques can yield near-zero response time as well as performance gains previously achieved only by batch compilers.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {294--303},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/512529.512564},
 doi = {http://doi.acm.org/10.1145/512529.512564},
 acmid = {512564},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Joshi:2002:DGS:543552.512566,
 author = {Joshi, Rajeev and Nelson, Greg and Randall, Keith},
 title = {Denali: a goal-directed superoptimizer},
 abstract = {This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {304--314},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/543552.512566},
 doi = {http://doi.acm.org/10.1145/543552.512566},
 acmid = {512566},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {optimizing compiler, superoptimizer},
} 

@inproceedings{Joshi:2002:DGS:512529.512566,
 author = {Joshi, Rajeev and Nelson, Greg and Randall, Keith},
 title = {Denali: a goal-directed superoptimizer},
 abstract = {This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {304--314},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/512529.512566},
 doi = {http://doi.acm.org/10.1145/512529.512566},
 acmid = {512566},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {optimizing compiler, superoptimizer},
} 

@inproceedings{Henzinger:2002:EMP:512529.512567,
 author = {Henzinger, Thomas A. and Kirsch, Christoph M.},
 title = {The embedded machine: predictable, portable real-time code},
 abstract = {The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first, platform-independent compiler phase generates E code (code executed by the Embedded Machine), which supervises the timing ---not the scheduling--- of application tasks relative to external events, such as clock ticks and sensor interrupts. E~code is portable and exhibits, given an input behavior, predictable (i.e., deterministic) timing and output behavior. The second, platform-dependent compiler phase checks the time safety</i> of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.},
 booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation},
 series = {PLDI '02},
 year = {2002},
 isbn = {1-58113-463-0},
 location = {Berlin, Germany},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/512529.512567},
 doi = {http://doi.acm.org/10.1145/512529.512567},
 acmid = {512567},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {real time, virtual machine},
} 

@article{Henzinger:2002:EMP:543552.512567,
 author = {Henzinger, Thomas A. and Kirsch, Christoph M.},
 title = {The embedded machine: predictable, portable real-time code},
 abstract = {The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first, platform-independent compiler phase generates E code (code executed by the Embedded Machine), which supervises the timing ---not the scheduling--- of application tasks relative to external events, such as clock ticks and sensor interrupts. E~code is portable and exhibits, given an input behavior, predictable (i.e., deterministic) timing and output behavior. The second, platform-dependent compiler phase checks the time safety</i> of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {5},
 month = {May},
 year = {2002},
 issn = {0362-1340},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/543552.512567},
 doi = {http://doi.acm.org/10.1145/543552.512567},
 acmid = {512567},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {real time, virtual machine},
} 

@inproceedings{Kennedy:2001:DIG:378795.378797,
 author = {Kennedy, Andrew and Sy Don},
 title = {Design and implementation of generics for the .NET Common language runtime},
 abstract = {The Microsoft.NET Common Language Runtime provides a shared type system, intermediate language and dynamic execution environment for the implementation and inter-operation of multiple source languages. In this paper we extend it with direct support for parametric polymorphism (also known as generics), describing the design through examples written in an extended version of the C# programming language, and explaining aspects of implementation by reference to a prototype extension to the runtime.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378797},
 doi = {http://doi.acm.org/10.1145/378795.378797},
 acmid = {378797},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kennedy:2001:DIG:381694.378797,
 author = {Kennedy, Andrew and Sy Don},
 title = {Design and implementation of generics for the .NET Common language runtime},
 abstract = {The Microsoft.NET Common Language Runtime provides a shared type system, intermediate language and dynamic execution environment for the implementation and inter-operation of multiple source languages. In this paper we extend it with direct support for parametric polymorphism (also known as generics), describing the design through examples written in an extended version of the C# programming language, and explaining aspects of implementation by reference to a prototype extension to the runtime.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378797},
 doi = {http://doi.acm.org/10.1145/381694.378797},
 acmid = {378797},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hicks:2001:DSU:381694.378798,
 author = {Hicks, Michael and Moore, Jonathan T. and Nettles, Scott},
 title = {Dynamic software updating},
 abstract = {Many important applications must run continuously and without interruption, yet must be changed to fix bugs or upgrade functionality. No prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, and ease of use.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378798},
 doi = {http://doi.acm.org/10.1145/381694.378798},
 acmid = {378798},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hicks:2001:DSU:378795.378798,
 author = {Hicks, Michael and Moore, Jonathan T. and Nettles, Scott},
 title = {Dynamic software updating},
 abstract = {Many important applications must run continuously and without interruption, yet must be changed to fix bugs or upgrade functionality. No prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, and ease of use.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378798},
 doi = {http://doi.acm.org/10.1145/378795.378798},
 acmid = {378798},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heintze:2001:DPA:378795.378802,
 author = {Heintze, Nevin and Tardieu, Olivier},
 title = {Demand-driven pointer analysis},
 abstract = {Known algorithms for pointer analysis are ``global" in the sense that they perform an exhaustive analysis of a program or program component. In this paper we introduce a demand-driven approach for pointer analysis. Specifically, we describe a demand-driven</i> flow-insensitive, subset-based, con text-insensitive points-to analysis. Given a list of pointer variables (a query), our analysis performs just enough computation to determine the points-to sets for these query variables. Using deductive reachability formulations of both the exhaustive and the demand-driven analyses, we prove that our algorithm is correct. We also show that our analysis is optimal in the sense that it does not do more work than necessary. We illustrate the feasibility and efficiency of our analysis with an implementation of demand-driven points-to analysis for computing the call-graphs of C programs with function pointers. The performance of our system varies substantially across benchmarks - the main factor is how much of the points-to graph must be computed to determine the call-graph. For some benchmarks, only a small part of the points-to graph is needed (e.g pouray emacs</i> and gcc</i>), and here we see more than a 10x speedup. For other benchmarks (e.g. burlap</i> and gimp</i>), we need to compute most (> 95\%) of the points-to graph, and here the demand-driven algorithm is considerably slower, because using the demand-driven algorithm is a slow method of computing the full points-to graph.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {24--34},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378802},
 doi = {http://doi.acm.org/10.1145/378795.378802},
 acmid = {378802},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heintze:2001:DPA:381694.378802,
 author = {Heintze, Nevin and Tardieu, Olivier},
 title = {Demand-driven pointer analysis},
 abstract = {Known algorithms for pointer analysis are ``global" in the sense that they perform an exhaustive analysis of a program or program component. In this paper we introduce a demand-driven approach for pointer analysis. Specifically, we describe a demand-driven</i> flow-insensitive, subset-based, con text-insensitive points-to analysis. Given a list of pointer variables (a query), our analysis performs just enough computation to determine the points-to sets for these query variables. Using deductive reachability formulations of both the exhaustive and the demand-driven analyses, we prove that our algorithm is correct. We also show that our analysis is optimal in the sense that it does not do more work than necessary. We illustrate the feasibility and efficiency of our analysis with an implementation of demand-driven points-to analysis for computing the call-graphs of C programs with function pointers. The performance of our system varies substantially across benchmarks - the main factor is how much of the points-to graph must be computed to determine the call-graph. For some benchmarks, only a small part of the points-to graph is needed (e.g pouray emacs</i> and gcc</i>), and here we see more than a 10x speedup. For other benchmarks (e.g. burlap</i> and gimp</i>), we need to compute most (> 95\%) of the points-to graph, and here the demand-driven algorithm is considerably slower, because using the demand-driven algorithm is a slow method of computing the full points-to graph.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {24--34},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378802},
 doi = {http://doi.acm.org/10.1145/381694.378802},
 acmid = {378802},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vivien:2001:IPE:381694.378804,
 author = {Vivien, Fr\'{e}d\'{e}ric and Rinard, Martin},
 title = {Incrementalized pointer and escape analysis},
 abstract = {We present a new pointer and escape analysis. Instead of analyzing the whole program, the algorithm incrementally analyzes only those parts of the program that may deliver useful results. An analysis policy monitors the analysis results to direct the incremental investment of analysis resources to those parts of the program that offer the highest expected optimization return.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378804},
 doi = {http://doi.acm.org/10.1145/381694.378804},
 acmid = {378804},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vivien:2001:IPE:378795.378804,
 author = {Vivien, Fr\'{e}d\'{e}ric and Rinard, Martin},
 title = {Incrementalized pointer and escape analysis},
 abstract = {We present a new pointer and escape analysis. Instead of analyzing the whole program, the algorithm incrementally analyzes only those parts of the program that may deliver useful results. An analysis policy monitors the analysis results to direct the incremental investment of analysis resources to those parts of the program that offer the highest expected optimization return.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378804},
 doi = {http://doi.acm.org/10.1145/378795.378804},
 acmid = {378804},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghiya:2001:IPA:381694.378806,
 author = {Ghiya, Rakesh and Lavery, Daniel and Sehr, David},
 title = {On the importance of points-to analysis and other memory disambiguation methods for C programs},
 abstract = {In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel\&reg; Itanium\&trade; processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378806},
 doi = {http://doi.acm.org/10.1145/381694.378806},
 acmid = {378806},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghiya:2001:IPA:378795.378806,
 author = {Ghiya, Rakesh and Lavery, Daniel and Sehr, David},
 title = {On the importance of points-to analysis and other memory disambiguation methods for C programs},
 abstract = {In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel\&reg; Itanium\&trade; processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378806},
 doi = {http://doi.acm.org/10.1145/378795.378806},
 acmid = {378806},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{DeLine:2001:EHP:378795.378811,
 author = {DeLine, Robert and F\"{a}hndrich, Manuel},
 title = {Enforcing high-level protocols in low-level software},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {59--69},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378811},
 doi = {http://doi.acm.org/10.1145/378795.378811},
 acmid = {378811},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{DeLine:2001:EHP:381694.378811,
 author = {DeLine, Robert and F\"{a}hndrich, Manuel},
 title = {Enforcing high-level protocols in low-level software},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {59--69},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378811},
 doi = {http://doi.acm.org/10.1145/381694.378811},
 acmid = {378811},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gay:2001:LSR:381694.378815,
 author = {Gay, David and Aiken, Alex},
 title = {Language support for regions},
 abstract = {Region-based memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27\% to a maximum of 11\% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www.cs.berkeley.edu/~dgay/rc.tar.gz.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378815},
 doi = {http://doi.acm.org/10.1145/381694.378815},
 acmid = {378815},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gay:2001:LSR:378795.378815,
 author = {Gay, David and Aiken, Alex},
 title = {Language support for regions},
 abstract = {Region-based memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27\% to a maximum of 11\% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www.cs.berkeley.edu/~dgay/rc.tar.gz.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378815},
 doi = {http://doi.acm.org/10.1145/378795.378815},
 acmid = {378815},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Monnier:2001:PS:381694.378817,
 author = {Monnier, Stefan and Saha, Bratin and Shao, Zhong},
 title = {Principled scavenging},
 abstract = {Proof-carrying code and typed assembly languages aim to minimize the trusted computing base by directly certifying the actual machine code. Unfortunately, these systems cannot get rid of the dependency on a trusted garbage collector. Indeed, constructing a provably type-safe garbage collector is one of the major open problems in the area of certifying compilation.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {81--91},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378817},
 doi = {http://doi.acm.org/10.1145/381694.378817},
 acmid = {378817},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Monnier:2001:PS:378795.378817,
 author = {Monnier, Stefan and Saha, Bratin and Shao, Zhong},
 title = {Principled scavenging},
 abstract = {Proof-carrying code and typed assembly languages aim to minimize the trusted computing base by directly certifying the actual machine code. Unfortunately, these systems cannot get rid of the dependency on a trusted garbage collector. Indeed, constructing a provably type-safe garbage collector is one of the major open problems in the area of certifying compilation.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {81--91},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378817},
 doi = {http://doi.acm.org/10.1145/378795.378817},
 acmid = {378817},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bacon:2001:JWC:378795.378819,
 author = {Bacon, David F. and Attanasio, Clement R. and Lee, Han B. and Rajan, V. T. and Smith, Stephen},
 title = {Java without the coffee breaks: a nonintrusive multiprocessor garbage collector},
 abstract = {The deployment of Java as a concurrent programming language has created a critical need for high-performance, concurrent, and incremental multiprocessor garbage collection. We present the Recycler</i>, a fully concurrent pure reference counting garbage collector that we have implemented in the Jalape\&ntilde;o Java virtual machine running on shared memory multiprocessors.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {92--103},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378819},
 doi = {http://doi.acm.org/10.1145/378795.378819},
 acmid = {378819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bacon:2001:JWC:381694.378819,
 author = {Bacon, David F. and Attanasio, Clement R. and Lee, Han B. and Rajan, V. T. and Smith, Stephen},
 title = {Java without the coffee breaks: a nonintrusive multiprocessor garbage collector},
 abstract = {The deployment of Java as a concurrent programming language has created a critical need for high-performance, concurrent, and incremental multiprocessor garbage collection. We present the Recycler</i>, a fully concurrent pure reference counting garbage collector that we have implemented in the Jalape\&ntilde;o Java virtual machine running on shared memory multiprocessors.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {92--103},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378819},
 doi = {http://doi.acm.org/10.1145/381694.378819},
 acmid = {378819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shaham:2001:HPS:381694.378820,
 author = {Shaham, Ran and Kolodner, Elliot K. and Sagiv, Mooly},
 title = {Heap profiling for space-efficient Java},
 abstract = {We present a heap-profiling tool for exploring the potential for space savings in Java programs. The output of the tool is used to direct rewriting of application source code in a way that allows more timely garbage collection (GC) of objects, thus saving space. The rewriting can also avoid allocating some objects that are never used.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {104--113},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381694.378820},
 doi = {http://doi.acm.org/10.1145/381694.378820},
 acmid = {378820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shaham:2001:HPS:378795.378820,
 author = {Shaham, Ran and Kolodner, Elliot K. and Sagiv, Mooly},
 title = {Heap profiling for space-efficient Java},
 abstract = {We present a heap-profiling tool for exploring the potential for space savings in Java programs. The output of the tool is used to direct rewriting of application source code in a way that allows more timely garbage collection (GC) of objects, thus saving space. The rewriting can also avoid allocating some objects that are never used.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {104--113},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378795.378820},
 doi = {http://doi.acm.org/10.1145/378795.378820},
 acmid = {378820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berger:2001:CHM:378795.378821,
 author = {Berger, Emery D. and Zorn, Benjamin G. and McKinley, Kathryn S.},
 title = {Composing high-performance memory allocators},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {114--124},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378821},
 doi = {http://doi.acm.org/10.1145/378795.378821},
 acmid = {378821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berger:2001:CHM:381694.378821,
 author = {Berger, Emery D. and Zorn, Benjamin G. and McKinley, Kathryn S.},
 title = {Composing high-performance memory allocators},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {114--124},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378821},
 doi = {http://doi.acm.org/10.1145/381694.378821},
 acmid = {378821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cheng:2001:PRG:381694.378823,
 author = {Cheng, Perry and Blelloch, Guy E.},
 title = {A parallel, real-time garbage collector},
 abstract = {We describe a parallel, real-time garbage collector and present experimental results that demonstrate good scalability and good real-time bounds. The collector is designed for shared-memory multiprocessors and is based on an earlier collector algorithm [2], which provided fixed bounds on the time any thread must pause for collection. However, since our earlier algorithm was designed for simple analysis, it had some impractical features. This paper presents the extensions necessary for a practical implementation: reducing excessive interleaving, handling stacks and global variables, reducing double allocation, and special treatment of large and small objects. An implementation based on the modified algorithm is evaluated on a set of 15 SML benchmarks on a Sun Enterprise 10000, a 64-way UltraSparc-II multiprocessor. To the best of our knowledge, this is the first implementation of a parallel, real-time garbage collector.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {125--136},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378823},
 doi = {http://doi.acm.org/10.1145/381694.378823},
 acmid = {378823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cheng:2001:PRG:378795.378823,
 author = {Cheng, Perry and Blelloch, Guy E.},
 title = {A parallel, real-time garbage collector},
 abstract = {We describe a parallel, real-time garbage collector and present experimental results that demonstrate good scalability and good real-time bounds. The collector is designed for shared-memory multiprocessors and is based on an earlier collector algorithm [2], which provided fixed bounds on the time any thread must pause for collection. However, since our earlier algorithm was designed for simple analysis, it had some impractical features. This paper presents the extensions necessary for a practical implementation: reducing excessive interleaving, handling stacks and global variables, reducing double allocation, and special treatment of large and small objects. An implementation based on the modified algorithm is evaluated on a set of 15 SML benchmarks on a Sun Enterprise 10000, a 64-way UltraSparc-II multiprocessor. To the best of our knowledge, this is the first implementation of a parallel, real-time garbage collector.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {125--136},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378823},
 doi = {http://doi.acm.org/10.1145/378795.378823},
 acmid = {378823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Amme:2001:STS:381694.378825,
 author = {Am Wolfram and Dalton, Niall and von Ronne, Jeffery and Franz, Michael},
 title = {SafeTSA: a type safe and referentially secure mobile-code representation based on static single assignment form},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {137--147},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378825},
 doi = {http://doi.acm.org/10.1145/381694.378825},
 acmid = {378825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Amme:2001:STS:378795.378825,
 author = {Am Wolfram and Dalton, Niall and von Ronne, Jeffery and Franz, Michael},
 title = {SafeTSA: a type safe and referentially secure mobile-code representation based on static single assignment form},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {137--147},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378825},
 doi = {http://doi.acm.org/10.1145/378795.378825},
 acmid = {378825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Evans:2001:BCV:381694.378827,
 author = {Evans, William S. and Fraser, Christopher W.},
 title = {Bytecode compression via profiled grammar rewriting},
 abstract = {This paper describes the design and implementation of a method for producing compact, bytecoded instruction sets and interpreters for them. It accepts a grammar for programs written using a simple bytecoded stack-based instruction set, as well as a training set of sample programs. The system transforms the grammar, creating an expanded grammar that represents the same language as the original grammar, but permits a shorter derivation of the sample programs and others like them. A program's derivation under the expanded grammar forms the compressed bytecode representation of the program. The interpreter for this bytecode is automatically generated from the original bytecode interpreter and the expanded grammar. Programs expressed using compressed bytecode can be substantially smaller than their original bytecode representation and even their machine code representation. For example, compression cuts the bytecode for lcc from 199KB to 58KB but increases the size of the interpreter by just over 11KB.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {148--155},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/381694.378827},
 doi = {http://doi.acm.org/10.1145/381694.378827},
 acmid = {378827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bytecode interpretation, context-free grammars, program compression, variable-to-fixed length codes},
} 

@inproceedings{Evans:2001:BCV:378795.378827,
 author = {Evans, William S. and Fraser, Christopher W.},
 title = {Bytecode compression via profiled grammar rewriting},
 abstract = {This paper describes the design and implementation of a method for producing compact, bytecoded instruction sets and interpreters for them. It accepts a grammar for programs written using a simple bytecoded stack-based instruction set, as well as a training set of sample programs. The system transforms the grammar, creating an expanded grammar that represents the same language as the original grammar, but permits a shorter derivation of the sample programs and others like them. A program's derivation under the expanded grammar forms the compressed bytecode representation of the program. The interpreter for this bytecode is automatically generated from the original bytecode interpreter and the expanded grammar. Programs expressed using compressed bytecode can be substantially smaller than their original bytecode representation and even their machine code representation. For example, compression cuts the bytecode for lcc from 199KB to 58KB but increases the size of the interpreter by just over 11KB.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {148--155},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/378795.378827},
 doi = {http://doi.acm.org/10.1145/378795.378827},
 acmid = {378827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bytecode interpretation, context-free grammars, program compression, variable-to-fixed length codes},
} 

@article{Krintz:2001:UAR:381694.378831,
 author = {Krintz, Chandra and Calder, Brad},
 title = {Using annotations to reduce dynamic optimization time},
 abstract = {Dynamic compilation and optimization are widely used in heterogenous computing environments, in which an intermediate form of the code is compiled to native code during execution. An important trade off exists between the amount of time spent dynamically optimizing the program and the running time of the program. The time to perform dynamic optimizations can cause significant delays during execution and also prohibit performance gains that result from more complex optimization.</i>
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {156--167},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378831},
 doi = {http://doi.acm.org/10.1145/381694.378831},
 acmid = {378831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krintz:2001:UAR:378795.378831,
 author = {Krintz, Chandra and Calder, Brad},
 title = {Using annotations to reduce dynamic optimization time},
 abstract = {Dynamic compilation and optimization are widely used in heterogenous computing environments, in which an intermediate form of the code is compiled to native code during execution. An important trade off exists between the amount of time spent dynamically optimizing the program and the running time of the program. The time to perform dynamic optimizations can cause significant delays during execution and also prohibit performance gains that result from more complex optimization.</i>
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {156--167},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378831},
 doi = {http://doi.acm.org/10.1145/378795.378831},
 acmid = {378831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arnold:2001:FRC:378795.378832,
 author = {Arnold, Matthew and Ryder, Barbara G.},
 title = {A framework for reducing the cost of instrumented code},
 abstract = {Instrumenting code to collect profiling information can cause substantial execution overhead. This overhead makes instrumentation difficult to perform at runtime, often preventing many known offline</i> feedback-directed optimizations from being used in online systems. This paper presents a general framework for performing instrumentation sampling</i> to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using code-duplication and counter-based sampling</i> to allow switching between instrumented and non-instrumented code.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {168--179},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378832},
 doi = {http://doi.acm.org/10.1145/378795.378832},
 acmid = {378832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arnold:2001:FRC:381694.378832,
 author = {Arnold, Matthew and Ryder, Barbara G.},
 title = {A framework for reducing the cost of instrumented code},
 abstract = {Instrumenting code to collect profiling information can cause substantial execution overhead. This overhead makes instrumentation difficult to perform at runtime, often preventing many known offline</i> feedback-directed optimizations from being used in online systems. This paper presents a general framework for performing instrumentation sampling</i> to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using code-duplication and counter-based sampling</i> to allow switching between instrumented and non-instrumented code.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {168--179},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378832},
 doi = {http://doi.acm.org/10.1145/381694.378832},
 acmid = {378832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhang:2001:TWP:381694.378835,
 author = {Zhang, Youtao and Gupta, Rajiv},
 title = {Timestamped whole program path representation and its applications},
 abstract = {A whole program path</i> (WPP) is a complete control flow trace of a program's execution. Recently Larus [18] showed that although WPP is expected to be very large (100's of MBytes), it can be greatly compressed (to 10's of MBytes) and therefore saved for future analysis. While the compression algorithm proposed by Larus is highly effective, the compression is accompanied with a loss in the ease with which subsets of information can be accessed. In particular, path traces pertaining to a particular function cannot generally be obtained without examining the entire compressed WPP representation. To solve this problem we advocate the application of compaction</i> techniques aimed at providing easy access to path traces on a per function basis.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378835},
 doi = {http://doi.acm.org/10.1145/381694.378835},
 acmid = {378835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhang:2001:TWP:378795.378835,
 author = {Zhang, Youtao and Gupta, Rajiv},
 title = {Timestamped whole program path representation and its applications},
 abstract = {A whole program path</i> (WPP) is a complete control flow trace of a program's execution. Recently Larus [18] showed that although WPP is expected to be very large (100's of MBytes), it can be greatly compressed (to 10's of MBytes) and therefore saved for future analysis. While the compression algorithm proposed by Larus is highly effective, the compression is accompanied with a loss in the ease with which subsets of information can be accessed. In particular, path traces pertaining to a particular function cannot generally be obtained without examining the entire compressed WPP representation. To solve this problem we advocate the application of compaction</i> techniques aimed at providing easy access to path traces on a per function basis.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378835},
 doi = {http://doi.acm.org/10.1145/378795.378835},
 acmid = {378835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chilimbi:2001:ERA:381694.378840,
 author = {Chilimbi, Trishul M.},
 title = {Efficient representations and abstractions for quantifying and exploiting data reference locality},
 abstract = {With the growing processor-memory performance gap, understanding and optimizing a program's reference locality, and consequently, its cache performance, is becoming increasingly important. Unfortunately, current reference locality optimizations rely on heuristics and are fairly ad-hoc. In addition, while optimization technology for improving instruction cache performance is fairly mature (though heuristic-based), data cache optimizations are still at an early stage. We believe the primary reason for this imbalance is the lack of a suitable representation of a program's dynamic data reference behavior and a quantitative basis for understanding this behavior.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378840},
 doi = {http://doi.acm.org/10.1145/381694.378840},
 acmid = {378840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chilimbi:2001:ERA:378795.378840,
 author = {Chilimbi, Trishul M.},
 title = {Efficient representations and abstractions for quantifying and exploiting data reference locality},
 abstract = {With the growing processor-memory performance gap, understanding and optimizing a program's reference locality, and consequently, its cache performance, is becoming increasingly important. Unfortunately, current reference locality optimizations rely on heuristics and are fairly ad-hoc. In addition, while optimization technology for improving instruction cache performance is fairly mature (though heuristic-based), data cache optimizations are still at an early stage. We believe the primary reason for this imbalance is the lack of a suitable representation of a program's dynamic data reference behavior and a quantitative basis for understanding this behavior.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378840},
 doi = {http://doi.acm.org/10.1145/378795.378840},
 acmid = {378840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ball:2001:APA:378795.378846,
 author = {Ball, Thomas and Majumdar, Rupak and Millstein, Todd and Rajamani, Sriram K.},
 title = {Automatic predicate abstraction of C programs},
 abstract = {Model checking has been widely successful in validating and debugging designs in the hardware and protocol domains. However, state-space explosion limits the applicability of model checking tools, so model checkers typically operate on abstractions of systems.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378846},
 doi = {http://doi.acm.org/10.1145/378795.378846},
 acmid = {378846},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ball:2001:APA:381694.378846,
 author = {Ball, Thomas and Majumdar, Rupak and Millstein, Todd and Rajamani, Sriram K.},
 title = {Automatic predicate abstraction of C programs},
 abstract = {Model checking has been widely successful in validating and debugging designs in the hardware and protocol domains. However, state-space explosion limits the applicability of model checking tools, so model checkers typically operate on abstractions of systems.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378846},
 doi = {http://doi.acm.org/10.1145/381694.378846},
 acmid = {378846},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aggarwal:2001:RFA:381694.378850,
 author = {Aggarwal, Aneesh and Randall, Keith H.},
 title = {Related field analysis},
 abstract = {We present an extension of field analysis (sec [4]) called related field analysis</i> which is a general technique for proving relationships between two or more fields of an object. We demonstrate the feasibility and applicability of related field analysis by applying it to the problem of removing array bounds checks. For array bounds check removal, we define a pair of related fields to be an integer field and an array field for which the integer field has a known relationship to the length of the array. This related field information can then be used to remove array bounds checks from accesses to the array field. Our results show that related field analysis can remove an average of 50\% of the dynamic array bounds checks on a wide range of applications.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {214--220},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/381694.378850},
 doi = {http://doi.acm.org/10.1145/381694.378850},
 acmid = {378850},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aggarwal:2001:RFA:378795.378850,
 author = {Aggarwal, Aneesh and Randall, Keith H.},
 title = {Related field analysis},
 abstract = {We present an extension of field analysis (sec [4]) called related field analysis</i> which is a general technique for proving relationships between two or more fields of an object. We demonstrate the feasibility and applicability of related field analysis by applying it to the problem of removing array bounds checks. For array bounds check removal, we define a pair of related fields to be an integer field and an array field for which the integer field has a known relationship to the length of the array. This related field information can then be used to remove array bounds checks from accesses to the array field. Our results show that related field analysis can remove an average of 50\% of the dynamic array bounds checks on a wide range of applications.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {214--220},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/378795.378850},
 doi = {http://doi.acm.org/10.1145/378795.378850},
 acmid = {378850},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Moller:2001:PAL:378795.378851,
 author = {M{\o}ller, Anders and Schwartzbach, Michael I.},
 title = {The pointer assertion logic engine},
 abstract = {We present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as graph types</i>. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {221--231},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378851},
 doi = {http://doi.acm.org/10.1145/378795.378851},
 acmid = {378851},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Moller:2001:PAL:381694.378851,
 author = {M{\o}ller, Anders and Schwartzbach, Michael I.},
 title = {The pointer assertion logic engine},
 abstract = {We present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as graph types</i>. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {221--231},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378851},
 doi = {http://doi.acm.org/10.1145/381694.378851},
 acmid = {378851},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thies:2001:UFS:378795.378852,
 author = {Thies, William and Vivien, Fr\'{e}d\'{e}ric and Sheldon, Jeffrey and Amarasinghe, Saman},
 title = {A unified framework for schedule and storage optimization},
 abstract = {We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multi-dimensional array, and programs that are in a single assignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {232--242},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378852},
 doi = {http://doi.acm.org/10.1145/378795.378852},
 acmid = {378852},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thies:2001:UFS:381694.378852,
 author = {Thies, William and Vivien, Fr\'{e}d\'{e}ric and Sheldon, Jeffrey and Amarasinghe, Saman},
 title = {A unified framework for schedule and storage optimization},
 abstract = {We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multi-dimensional array, and programs that are in a single assignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {232--242},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378852},
 doi = {http://doi.acm.org/10.1145/381694.378852},
 acmid = {378852},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Appel:2001:OSC:378795.378854,
 author = {Appel, Andrew W. and George, Lal},
 title = {Optimal spilling for CISC machines with few registers},
 abstract = {Many graph-coloring register-allocation algorithms don't work well for machines with few registers. Heuristics for live-range splitting are complex or suboptimal; heuristics for register assignment rarely factor the presence of fancy addressing modes; these problems are more severe the fewer registers there are to work with. We show how to optimally split live ranges and optimally use addressing modes, where the optimality condition measures dynamically weighted loads and stores but not register-register moves. Our algorithm uses integer linear programming but is much more efficient than previous ILP-based approaches to register allocation. We then show a variant of Park and Moon's optimistic coalescing algorithm that does a very good (though not provably optimal) job of removing the register-register moves. The result is Pentium code that is 9.5\% faster than code generated by SSA-based splitting with iterated register coalescing.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {243--253},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378854},
 doi = {http://doi.acm.org/10.1145/378795.378854},
 acmid = {378854},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:2001:OSC:381694.378854,
 author = {Appel, Andrew W. and George, Lal},
 title = {Optimal spilling for CISC machines with few registers},
 abstract = {Many graph-coloring register-allocation algorithms don't work well for machines with few registers. Heuristics for live-range splitting are complex or suboptimal; heuristics for register assignment rarely factor the presence of fancy addressing modes; these problems are more severe the fewer registers there are to work with. We show how to optimally split live ranges and optimally use addressing modes, where the optimality condition measures dynamically weighted loads and stores but not register-register moves. Our algorithm uses integer linear programming but is much more efficient than previous ILP-based approaches to register allocation. We then show a variant of Park and Moon's optimistic coalescing algorithm that does a very good (though not provably optimal) job of removing the register-register moves. The result is Pentium code that is 9.5\% faster than code generated by SSA-based splitting with iterated register coalescing.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {243--253},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378854},
 doi = {http://doi.acm.org/10.1145/381694.378854},
 acmid = {378854},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heintze:2001:UAA:378795.378855,
 author = {Heintze, Nevin and Tardieu, Olivier},
 title = {Ultra-fast aliasing analysis using CLA: a million lines of C code in a second},
 abstract = {We describe the design and implementation of a system for very fast points-to analysis. On code bases of about million lines of unpreprocessed C code, our system performs field-based Andersen-style points-to analysis in less than a second and uses less than 10MB of memory. Our two main contributions are a database-centric analysis architecture called compile-link-analyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our points-to analysis system is built into a forward data-dependence analysis tool that is deployed within Lucent to help with consistent type modifications to large legacy C code bases.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {254--263},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378795.378855},
 doi = {http://doi.acm.org/10.1145/378795.378855},
 acmid = {378855},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heintze:2001:UAA:381694.378855,
 author = {Heintze, Nevin and Tardieu, Olivier},
 title = {Ultra-fast aliasing analysis using CLA: a million lines of C code in a second},
 abstract = {We describe the design and implementation of a system for very fast points-to analysis. On code bases of about million lines of unpreprocessed C code, our system performs field-based Andersen-style points-to analysis in less than a second and uses less than 10MB of memory. Our two main contributions are a database-centric analysis architecture called compile-link-analyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our points-to analysis system is built into a forward data-dependence analysis tool that is deployed within Lucent to help with consistent type modifications to large legacy C code bases.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {254--263},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381694.378855},
 doi = {http://doi.acm.org/10.1145/381694.378855},
 acmid = {378855},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hanson:2001:DV:378795.378857,
 author = {Hanson, David R. and Proebsting, Todd A.},
 title = {Dynamic variables},
 abstract = {Most programming languages use static scope rules for associating uses of identifiers with their declarations. Static scope helps catch errors at compile time, and it can be implemented efficiently. Some popular languages\&mdash;Perl, Tel, TeX, and Postscript\&mdash;offer dynamic scope, because dynamic scope works well for variables that ``customize" the execution environment, for example. Programmers must simulate dynamic scope to implement this kind of usage in statically scoped languages. This paper describes the design and implementation of imperative language constructs for introducing and referencing dynamically scoped variables\&mdash;dynamic variables for short. The design is a minimalist one, because dynamic variables are best used sparingly, much like exceptions. The facility does, however, cater to the typical uses for dynamic scope, and it provides a cleaner mechanism for so-called thread-local variables. A particularly simple implementation suffices for languages without exception handling. For languages with exception handling, a more efficient implementation builds on existing compiler infrastructure. Exception handling can be viewed as a control construct with dynamic scope. Likewise, dynamic variables are a data construct with dynamic scope.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378795.378857},
 doi = {http://doi.acm.org/10.1145/378795.378857},
 acmid = {378857},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hanson:2001:DV:381694.378857,
 author = {Hanson, David R. and Proebsting, Todd A.},
 title = {Dynamic variables},
 abstract = {Most programming languages use static scope rules for associating uses of identifiers with their declarations. Static scope helps catch errors at compile time, and it can be implemented efficiently. Some popular languages\&mdash;Perl, Tel, TeX, and Postscript\&mdash;offer dynamic scope, because dynamic scope works well for variables that ``customize" the execution environment, for example. Programmers must simulate dynamic scope to implement this kind of usage in statically scoped languages. This paper describes the design and implementation of imperative language constructs for introducing and referencing dynamically scoped variables\&mdash;dynamic variables for short. The design is a minimalist one, because dynamic variables are best used sparingly, much like exceptions. The facility does, however, cater to the typical uses for dynamic scope, and it provides a cleaner mechanism for so-called thread-local variables. A particularly simple implementation suffices for languages without exception handling. For languages with exception handling, a more efficient implementation builds on existing compiler infrastructure. Exception handling can be viewed as a control construct with dynamic scope. Likewise, dynamic variables are a data construct with dynamic scope.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381694.378857},
 doi = {http://doi.acm.org/10.1145/381694.378857},
 acmid = {378857},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marlow:2001:AEH:381694.378858,
 author = {Marlow, Simon and Jones, Simon Peyton and Moran, Andrew and Reppy, John},
 title = {Asynchronous exceptions in Haskell},
 abstract = {Asynchronous exceptions, such as timeouts are important for robust, modular programs, but are extremely difficult to program with \&mdash; so much so that most programming languages either heavily restrict them or ban them altogether. We extend our earlier work, in which we added synchronous exceptions to Haskell, to support asynchronous exceptions too. Our design introduces scoped combinators for blocking and unblocking asynchronous interrupts, along with a somewhat surprising semantics for operations that can suspend. Uniquely, we also give a formal semantics for our system.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {274--285},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378858},
 doi = {http://doi.acm.org/10.1145/381694.378858},
 acmid = {378858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marlow:2001:AEH:378795.378858,
 author = {Marlow, Simon and Jones, Simon Peyton and Moran, Andrew and Reppy, John},
 title = {Asynchronous exceptions in Haskell},
 abstract = {Asynchronous exceptions, such as timeouts are important for robust, modular programs, but are extremely difficult to program with \&mdash; so much so that most programming languages either heavily restrict them or ban them altogether. We extend our earlier work, in which we added synchronous exceptions to Haskell, to support asynchronous exceptions too. Our design introduces scoped combinators for blocking and unblocking asynchronous interrupts, along with a somewhat surprising semantics for operations that can suspend. Uniquely, we also give a formal semantics for our system.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {274--285},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378858},
 doi = {http://doi.acm.org/10.1145/378795.378858},
 acmid = {378858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chatterjee:2001:EAC:381694.378859,
 author = {Chatterjee, Siddhartha and Parker, Erin and Hanlon, Philip J. and Lebeck, Alvin R.},
 title = {Exact analysis of the cache behavior of nested loops},
 abstract = {We develop from first principles an exact model of the behavior of loop nests executing in a memory hicrarchy, by using a nontraditional classification of misses that has the key property of composability. We use Presburger formulas to express various kinds of misses as well as the state of the cache at the end of the loop nest. We use existing tools to simplify these formulas and to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors of non-linear array layouts based on bit interleaving of array indices. We also indicate how to handle modest levels of associativity, and how to perform limited symbolic analysis of cache behavior. The complexity of the formulas relates to the static structure of the loop nest rather than to its dynamic trip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns of cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our method can serve as the basis for a static performance predictor to guide program and data transformations to improve performance.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {286--297},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378859},
 doi = {http://doi.acm.org/10.1145/381694.378859},
 acmid = {378859},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chatterjee:2001:EAC:378795.378859,
 author = {Chatterjee, Siddhartha and Parker, Erin and Hanlon, Philip J. and Lebeck, Alvin R.},
 title = {Exact analysis of the cache behavior of nested loops},
 abstract = {We develop from first principles an exact model of the behavior of loop nests executing in a memory hicrarchy, by using a nontraditional classification of misses that has the key property of composability. We use Presburger formulas to express various kinds of misses as well as the state of the cache at the end of the loop nest. We use existing tools to simplify these formulas and to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors of non-linear array layouts based on bit interleaving of array indices. We also indicate how to handle modest levels of associativity, and how to perform limited symbolic analysis of cache behavior. The complexity of the formulas relates to the static structure of the loop nest rather than to its dynamic trip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns of cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our method can serve as the basis for a static performance predictor to guide program and data transformations to improve performance.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {286--297},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378859},
 doi = {http://doi.acm.org/10.1145/378795.378859},
 acmid = {378859},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Xiong:2001:SLC:381694.378860,
 author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
 title = {SPL: a language and compiler for DSP algorithms},
 abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as ``hard-wired" systems like FFTW.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {298--308},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378860},
 doi = {http://doi.acm.org/10.1145/381694.378860},
 acmid = {378860},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Xiong:2001:SLC:378795.378860,
 author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
 title = {SPL: a language and compiler for DSP algorithms},
 abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as ``hard-wired" systems like FFTW.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {298--308},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378860},
 doi = {http://doi.acm.org/10.1145/378795.378860},
 acmid = {378860},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kumar:2001:ELP:378795.378862,
 author = {Kumar, Sanjeev and Mandelbaum, Yitzhak and Yu, Xiang and Li, Kai},
 title = {ESP: a language for programmable devices},
 abstract = {This paper presents the design and implementation of Event-driven State-machines Programming (ESP)\&mdash;a language for programmable devices. In traditional languages, like C, using event-driven state-machine forces a tradeoff that requires giving up ease of development and reliability to achieve high performance. ESP is designed to provide all of these three properties simultaneously.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {309--320},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378795.378862},
 doi = {http://doi.acm.org/10.1145/378795.378862},
 acmid = {378862},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kumar:2001:ELP:381694.378862,
 author = {Kumar, Sanjeev and Mandelbaum, Yitzhak and Yu, Xiang and Li, Kai},
 title = {ESP: a language for programmable devices},
 abstract = {This paper presents the design and implementation of Event-driven State-machines Programming (ESP)\&mdash;a language for programmable devices. In traditional languages, like C, using event-driven state-machine forces a tradeoff that requires giving up ease of development and reliability to achieve high performance. ESP is designed to provide all of these three properties simultaneously.
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {309--320},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381694.378862},
 doi = {http://doi.acm.org/10.1145/381694.378862},
 acmid = {378862},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schnarr:2001:FLC:378795.378864,
 author = {Schnarr, Eric C. and Hill, Mark D. and Larus, James R.},
 title = {Facile: a language and compiler for high-performance processor simulators},
 abstract = {Architectural simulators are essential tools for computer architecture and systems research and development. Simulators, however, are becoming frustratingly slow, because they must now model increasingly complex micro-architectures running realistic workloads. Previously, we developed a technique called fast-forwarding, which applied partial evaluation and mermoization to improve the performance of detailed architectural simulations by as much as an order of magnitude [14].
},
 booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
 series = {PLDI '01},
 year = {2001},
 isbn = {1-58113-414-2},
 location = {Snowbird, Utah, United States},
 pages = {321--331},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378795.378864},
 doi = {http://doi.acm.org/10.1145/378795.378864},
 acmid = {378864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memoization, micro-architecture simulation, out-of-order processor simulation, partial evaluation},
} 

@article{Schnarr:2001:FLC:381694.378864,
 author = {Schnarr, Eric C. and Hill, Mark D. and Larus, James R.},
 title = {Facile: a language and compiler for high-performance processor simulators},
 abstract = {Architectural simulators are essential tools for computer architecture and systems research and development. Simulators, however, are becoming frustratingly slow, because they must now model increasingly complex micro-architectures running realistic workloads. Previously, we developed a technique called fast-forwarding, which applied partial evaluation and mermoization to improve the performance of detailed architectural simulations by as much as an order of magnitude [14].
},
 journal = {SIGPLAN Not.},
 volume = {36},
 issue = {5},
 month = {May},
 year = {2001},
 issn = {0362-1340},
 pages = {321--331},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378864},
 doi = {http://doi.acm.org/10.1145/381694.378864},
 acmid = {378864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memoization, micro-architecture simulation, out-of-order processor simulation, partial evaluation},
} 

@article{Chilimbi:1999:CSL:301631.301633,
 author = {Chilimbi, Trishul M. and Hill, Mark D. and Larus, James R.},
 title = {Cache-conscious structure layout},
 abstract = {Hardware trends have produced an increasing disparity between processor speeds and memory access times. While a variety of techniques for tolerating or reducing memory latency have been proposed, these are rarely successful for pointer-manipulating programs.This paper explores a complementary approach that attacks the source (poor reference locality) of the problem rather than its manifestation (memory latency). It demonstrates that careful data organization and layout provides an essential mechanism to improve the cache locality of pointer-manipulating programs and consequently, their performance. It explores two placement techniques---clustering</i> and coloring</i>---that improve cache performance by increasing a pointer structure's spatial and temporal locality, and by reducing cache-conflicts.To reduce the cost of applying these techniques, this paper discusses two strategies---cache-conscious reorganization</i> and cache-conscious allocation</i>---and describes two semi-automatic tools---ccmorph and ccmalloc---that use these strategies to produce cache-conscious pointer structure layouts. ccmorph is a transparent tree reorganizer that utilizes topology information to cluster and color the structure. ccmalloc is a cache-conscious heap allocator that attempts to co-locate contemporaneously accessed data elements in the same physical cache block. Our evaluations, with microbenchmarks, several small benchmarks, and a couple of large real-world applications, demonstrate that the cache-conscious structure layouts produced by ccmorph and ccmalloc offer large performance benefits---in most cases, significantly outperforming state-of-the-art prefetching.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301633},
 doi = {http://doi.acm.org/10.1145/301631.301633},
 acmid = {301633},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache-conscious allocation, cache-conscious data placement, cache-conscious reorganization, clustering, coloring},
} 

@inproceedings{Chilimbi:1999:CSL:301618.301633,
 author = {Chilimbi, Trishul M. and Hill, Mark D. and Larus, James R.},
 title = {Cache-conscious structure layout},
 abstract = {Hardware trends have produced an increasing disparity between processor speeds and memory access times. While a variety of techniques for tolerating or reducing memory latency have been proposed, these are rarely successful for pointer-manipulating programs.This paper explores a complementary approach that attacks the source (poor reference locality) of the problem rather than its manifestation (memory latency). It demonstrates that careful data organization and layout provides an essential mechanism to improve the cache locality of pointer-manipulating programs and consequently, their performance. It explores two placement techniques---clustering</i> and coloring</i>---that improve cache performance by increasing a pointer structure's spatial and temporal locality, and by reducing cache-conflicts.To reduce the cost of applying these techniques, this paper discusses two strategies---cache-conscious reorganization</i> and cache-conscious allocation</i>---and describes two semi-automatic tools---ccmorph and ccmalloc---that use these strategies to produce cache-conscious pointer structure layouts. ccmorph is a transparent tree reorganizer that utilizes topology information to cluster and color the structure. ccmalloc is a cache-conscious heap allocator that attempts to co-locate contemporaneously accessed data elements in the same physical cache block. Our evaluations, with microbenchmarks, several small benchmarks, and a couple of large real-world applications, demonstrate that the cache-conscious structure layouts produced by ccmorph and ccmalloc offer large performance benefits---in most cases, significantly outperforming state-of-the-art prefetching.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301633},
 doi = {http://doi.acm.org/10.1145/301618.301633},
 acmid = {301633},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache-conscious allocation, cache-conscious data placement, cache-conscious reorganization, clustering, coloring},
} 

@inproceedings{Chilimbi:1999:CSD:301618.301635,
 author = {Chilimbi, Trishul M. and Davidson, Bob and Larus, James R.},
 title = {Cache-conscious structure definition},
 abstract = {A program's cache performance can be improved by changing the organization and layout of its data---even complex, pointer-based data structures. Previous techniques improved the cache performance of these structures by arranging distinct instances to increase reference locality. These techniques produced significant performance improvements, but worked best for small structures that could be packed into a cache block.This paper extends that work by concentrating on the internal organization of fields in a data structure. It describes two techniques---structure splitting</i> and field reordering</i>---that improve the cache behavior of structures larger than a cache block. For structures comparable in size to a cache block, structure splitting can increase the number of hot fields that can be placed in a cache block. In five Java programs, structure splitting reduced cache miss rates 10--27\% and improved performance 6--18\% beyond the benefits of previously described cache-conscious reorganization techniques.For large structures, which span many cache blocks, reordering fields, to place those with high temporal affinity in the same cache block can also improve cache utilization. This paper describes bbcache, a tool that recommends C structure field reorderings. Preliminary measurements indicate that reordering fields in 5 active structures improves the performance of Microsoft SQL Server 7.0 2--3\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301635},
 doi = {http://doi.acm.org/10.1145/301618.301635},
 acmid = {301635},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache-conscious definition, class splitting, field reorganization, structure splitting},
} 

@article{Chilimbi:1999:CSD:301631.301635,
 author = {Chilimbi, Trishul M. and Davidson, Bob and Larus, James R.},
 title = {Cache-conscious structure definition},
 abstract = {A program's cache performance can be improved by changing the organization and layout of its data---even complex, pointer-based data structures. Previous techniques improved the cache performance of these structures by arranging distinct instances to increase reference locality. These techniques produced significant performance improvements, but worked best for small structures that could be packed into a cache block.This paper extends that work by concentrating on the internal organization of fields in a data structure. It describes two techniques---structure splitting</i> and field reordering</i>---that improve the cache behavior of structures larger than a cache block. For structures comparable in size to a cache block, structure splitting can increase the number of hot fields that can be placed in a cache block. In five Java programs, structure splitting reduced cache miss rates 10--27\% and improved performance 6--18\% beyond the benefits of previously described cache-conscious reorganization techniques.For large structures, which span many cache blocks, reordering fields, to place those with high temporal affinity in the same cache block can also improve cache utilization. This paper describes bbcache, a tool that recommends C structure field reorderings. Preliminary measurements indicate that reordering fields in 5 active structures improves the performance of Microsoft SQL Server 7.0 2--3\%.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301635},
 doi = {http://doi.acm.org/10.1145/301631.301635},
 acmid = {301635},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache-conscious definition, class splitting, field reorganization, structure splitting},
} 

@article{Jones:1999:SIE:301631.301637,
 author = {Jones, Simon Peyton and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
 title = {A semantics for imprecise exceptions},
 abstract = {Some modern superscalar microprocessors provide only imprecise exceptions</i>. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301637},
 doi = {http://doi.acm.org/10.1145/301631.301637},
 acmid = {301637},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jones:1999:SIE:301618.301637,
 author = {Jones, Simon Peyton and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
 title = {A semantics for imprecise exceptions},
 abstract = {Some modern superscalar microprocessors provide only imprecise exceptions</i>. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301637},
 doi = {http://doi.acm.org/10.1145/301618.301637},
 acmid = {301637},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fisher:1999:DCM:301631.301638,
 author = {Fisher, Kathleen and Reppy, John},
 title = {The design of a class mechanism for Moby},
 abstract = {Typical class-based languages, such as C++ and JAVA, provide complex class mechanisms but only weak module systems. In fact, classes in these languages incorporate many of the features found in richer module mechanisms. In this paper, we describe an alternative approach to designing a language that has both classes and modules. In our design, we rely on a rich ML-style module system to provide features such as visibility control and parameterization, while providing a minimal class mechanism that includes only those features needed to support inheritance. Programmers can then use the combination of modules and classes to implement the full range of class-based features and idioms. Our approach has the advantage that it provides a full-featured module system (useful in its own right), while keeping the class mechanism quite simple.We have incorporated this design in MOBY, which is an ML-style language that supports class-based object-oriented programming. In this paper, we describe our design via a series of simple examples, show how various class-based features and idioms are realized in MOBY, compare our design with others, and sketch its formal semantics.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {37--49},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301631.301638},
 doi = {http://doi.acm.org/10.1145/301631.301638},
 acmid = {301638},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fisher:1999:DCM:301618.301638,
 author = {Fisher, Kathleen and Reppy, John},
 title = {The design of a class mechanism for Moby},
 abstract = {Typical class-based languages, such as C++ and JAVA, provide complex class mechanisms but only weak module systems. In fact, classes in these languages incorporate many of the features found in richer module mechanisms. In this paper, we describe an alternative approach to designing a language that has both classes and modules. In our design, we rely on a rich ML-style module system to provide features such as visibility control and parameterization, while providing a minimal class mechanism that includes only those features needed to support inheritance. Programmers can then use the combination of modules and classes to implement the full range of class-based features and idioms. Our approach has the advantage that it provides a full-featured module system (useful in its own right), while keeping the class mechanism quite simple.We have incorporated this design in MOBY, which is an ML-style language that supports class-based object-oriented programming. In this paper, we describe our design via a series of simple examples, show how various class-based features and idioms are realized in MOBY, compare our design with others, and sketch its formal semantics.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {37--49},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301618.301638},
 doi = {http://doi.acm.org/10.1145/301618.301638},
 acmid = {301638},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Crary:1999:RM:301618.301641,
 author = {Crary, Karl and Harper, Robert and Puri, Sidd},
 title = {What is a recursive module?},
 abstract = {A hierarchical module system is an effective tool for structuring large programs. Strictly hierarchical module systems impose an acyclic ordering on import dependencies among program units. This can impede modular programming by forcing mutually-dependent components to be consolidated into a single module. Recently there have been several proposals for module systems that admit cyclic dependencies, but it is not clear how these proposals relate to one another, nor how one might integrate them into an expressive module system such as that of ML.To address this question we provide a type-theoretic analysis of the notion of a recursive module in the context of a "phase-distinction" formalism for higher-order module systems. We extend this calculus with a recursive module mechanism and a new form of signature, called a recursively dependent signature</i>, to support the definition of recursive modules. These extensions are justified by an interpretation in terms of more primitive language constructs. This interpretation may also serve as a guide for implementation.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {50--63},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301618.301641},
 doi = {http://doi.acm.org/10.1145/301618.301641},
 acmid = {301641},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Crary:1999:RM:301631.301641,
 author = {Crary, Karl and Harper, Robert and Puri, Sidd},
 title = {What is a recursive module?},
 abstract = {A hierarchical module system is an effective tool for structuring large programs. Strictly hierarchical module systems impose an acyclic ordering on import dependencies among program units. This can impede modular programming by forcing mutually-dependent components to be consolidated into a single module. Recently there have been several proposals for module systems that admit cyclic dependencies, but it is not clear how these proposals relate to one another, nor how one might integrate them into an expressive module system such as that of ML.To address this question we provide a type-theoretic analysis of the notion of a recursive module in the context of a "phase-distinction" formalism for higher-order module systems. We extend this calculus with a recursive module mechanism and a new form of signature, called a recursively dependent signature</i>, to support the definition of recursive modules. These extensions are justified by an interpretation in terms of more primitive language constructs. This interpretation may also serve as a guide for implementation.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {50--63},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301631.301641},
 doi = {http://doi.acm.org/10.1145/301631.301641},
 acmid = {301641},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bodik:1999:LAD:301631.301643,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Load-reuse analysis: design and evaluation},
 abstract = {Load-reuse analysis finds instructions that repeatedly access the same memory location. This location can be promoted to a register, eliminating redundant loads by reusing the results of prior memory accesses. This paper develops a load-reuse analysis and designs a method for evaluating its precision.In designing the analysis, we aspire for completeness</i>---the goal of exposing all reuse that can be harvested by a subsequent program transformation. For register promotion, a suitable transformation is partial redundancy elimination (PRE). To approach the ideal goal of PRE-completeness, the load-reuse analysis is phrased as a data-flow problem on a program representation that is path-sensitive</i>, as it detects reuse even when it originates in a different instruction along each control flow path. Furthermore, the analysis is comprehensive</i>, as it treats scalar, array and pointer-based loads uniformly.In evaluating the analysis, we compare it with an ideal analysis. By observing the run-time stream of memory references, we collect all PRE-exploitable reuse and treat it as the ideal analysis performance. To compare the (static) load-reuse analysis with the (dynamic) ideal reuse, we use an estimator</i> algorithm that computes, given a data-flow solution and a program profile, the dynamic amount of reuse detected by the analysis. We developed a family of estimators that differ in how well they bound the profiling error inherent in the edge</i> profile. By bounding the error, the estimators offer a precise and practical method for determining the run-time optimization benefit.Our experiments show that about 55\% of loads executed in Spec95 exhibit reuse. Of those, our analysis exposes about 80\%.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {64--76},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301631.301643},
 doi = {http://doi.acm.org/10.1145/301631.301643},
 acmid = {301643},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-flow analysis, profile-guided optimizations, program representations, register promotion},
} 

@inproceedings{Bodik:1999:LAD:301618.301643,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Load-reuse analysis: design and evaluation},
 abstract = {Load-reuse analysis finds instructions that repeatedly access the same memory location. This location can be promoted to a register, eliminating redundant loads by reusing the results of prior memory accesses. This paper develops a load-reuse analysis and designs a method for evaluating its precision.In designing the analysis, we aspire for completeness</i>---the goal of exposing all reuse that can be harvested by a subsequent program transformation. For register promotion, a suitable transformation is partial redundancy elimination (PRE). To approach the ideal goal of PRE-completeness, the load-reuse analysis is phrased as a data-flow problem on a program representation that is path-sensitive</i>, as it detects reuse even when it originates in a different instruction along each control flow path. Furthermore, the analysis is comprehensive</i>, as it treats scalar, array and pointer-based loads uniformly.In evaluating the analysis, we compare it with an ideal analysis. By observing the run-time stream of memory references, we collect all PRE-exploitable reuse and treat it as the ideal analysis performance. To compare the (static) load-reuse analysis with the (dynamic) ideal reuse, we use an estimator</i> algorithm that computes, given a data-flow solution and a program profile, the dynamic amount of reuse detected by the analysis. We developed a family of estimators that differ in how well they bound the profiling error inherent in the edge</i> profile. By bounding the error, the estimators offer a precise and practical method for determining the run-time optimization benefit.Our experiments show that about 55\% of loads executed in Spec95 exhibit reuse. Of those, our analysis exposes about 80\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {64--76},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301618.301643},
 doi = {http://doi.acm.org/10.1145/301618.301643},
 acmid = {301643},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-flow analysis, profile-guided optimizations, program representations, register promotion},
} 

@inproceedings{Rugina:1999:PAM:301618.301645,
 author = {Rugina, Radu and Rinard, Martin},
 title = {Pointer analysis for multithreaded programs},
 abstract = {This paper presents a novel interprocedural, flow-sensitive, and context-sensitive pointer analysis algorithm for multithreaded programs that may concurrently update shared pointers. For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point. The algorithm correctly handles a full range of constructs in multithreaded programs, including recursive functions, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between pointer variables of different types, heap and stack allocated memory, shared global variables, and thread-private global variables.We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a sizable set of multithreaded programs written in the Cilk multithreaded programming language. Our experimental results show that the analysis has good precision and converges quickly for our set of Cilk programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301618.301645},
 doi = {http://doi.acm.org/10.1145/301618.301645},
 acmid = {301645},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rugina:1999:PAM:301631.301645,
 author = {Rugina, Radu and Rinard, Martin},
 title = {Pointer analysis for multithreaded programs},
 abstract = {This paper presents a novel interprocedural, flow-sensitive, and context-sensitive pointer analysis algorithm for multithreaded programs that may concurrently update shared pointers. For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point. The algorithm correctly handles a full range of constructs in multithreaded programs, including recursive functions, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between pointer variables of different types, heap and stack allocated memory, shared global variables, and thread-private global variables.We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a sizable set of multithreaded programs written in the Cilk multithreaded programming language. Our experimental results show that the analysis has good precision and converges quickly for our set of Cilk programs.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301631.301645},
 doi = {http://doi.acm.org/10.1145/301631.301645},
 acmid = {301645},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yong:1999:PAP:301631.301647,
 author = {Yong, Suan Hsi and Horwitz, Susan and Reps, Thomas},
 title = {Pointer analysis for programs with structures and casting},
 abstract = {Type casting allows a program to access an object as if it had a type different from its declared type. This complicates the design of a pointer-analysis algorithm that treats structure fields as separate objects; therefore, some previous pointer-analysis algorithms "collapse" a structure into a single variable. The disadvantage of this approach is that it can lead to very imprecise points-to information. Other algorithms treat each field as a separate object based on its offset and size. While this approach leads to more precise results, the results are not portable because the memory layout of structures is implementation dependent.This paper first describes the complications introduced by type casting, then presents a tunable pointer-analysis framework for handling structures in the presence of casting. Different instances of this framework produce algorithms with different levels of precision, portability, and efficiency. Experimental results from running our implementations of four instances of this framework show that (i) it is important to distinguish fields of structures in pointer analysis, but (ii) making conservative approximations when casting is involved usually does not cost much in terms of time, space, or the precision of the results.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {91--103},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301631.301647},
 doi = {http://doi.acm.org/10.1145/301631.301647},
 acmid = {301647},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yong:1999:PAP:301618.301647,
 author = {Yong, Suan Hsi and Horwitz, Susan and Reps, Thomas},
 title = {Pointer analysis for programs with structures and casting},
 abstract = {Type casting allows a program to access an object as if it had a type different from its declared type. This complicates the design of a pointer-analysis algorithm that treats structure fields as separate objects; therefore, some previous pointer-analysis algorithms "collapse" a structure into a single variable. The disadvantage of this approach is that it can lead to very imprecise points-to information. Other algorithms treat each field as a separate object based on its offset and size. While this approach leads to more precise results, the results are not portable because the memory layout of structures is implementation dependent.This paper first describes the complications introduced by type casting, then presents a tunable pointer-analysis framework for handling structures in the presence of casting. Different instances of this framework produce algorithms with different levels of precision, portability, and efficiency. Experimental results from running our implementations of four instances of this framework show that (i) it is important to distinguish fields of structures in pointer analysis, but (ii) making conservative approximations when casting is involved usually does not cost much in terms of time, space, or the precision of the results.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {91--103},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301618.301647},
 doi = {http://doi.acm.org/10.1145/301618.301647},
 acmid = {301647},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blelloch:1999:BTS:301631.301648,
 author = {Blelloch, Guy E. and Cheng, Perry},
 title = {On bounding time and space for multiprocessor garbage collection},
 abstract = {This paper presents the first multiprocessor garbage collection algorithm with provable bounds on time and space. The algorithm is a real-time shared-memory copying collector. We prove that the algorithm requires at most 2(R</i>(l + 2/k</i>) + N</i> + 5PD</i>) memory locations, where P</i> is the number of processors, R</i> is the maximum reachable space during a computation (number of locations accessible from the root set), N</i> is the maximum number of reachable objects, D</i> is the maximum depth of any data object, and k</i> is a parameter specifying how many locations are copied each time a location is allocated. Furthermore we show that client threads are never stopped for more than time proportional to k</i> non-blocking machine instructions. The bounds are guaranteed even with arbitrary length arrays. The collector only requires write-barriers (reads are unaffected by the collector), makes few assumptions about the threads that are generating the garbage, and allows them to run mostly asynchronously.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {104--117},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301631.301648},
 doi = {http://doi.acm.org/10.1145/301631.301648},
 acmid = {301648},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blelloch:1999:BTS:301618.301648,
 author = {Blelloch, Guy E. and Cheng, Perry},
 title = {On bounding time and space for multiprocessor garbage collection},
 abstract = {This paper presents the first multiprocessor garbage collection algorithm with provable bounds on time and space. The algorithm is a real-time shared-memory copying collector. We prove that the algorithm requires at most 2(R</i>(l + 2/k</i>) + N</i> + 5PD</i>) memory locations, where P</i> is the number of processors, R</i> is the maximum reachable space during a computation (number of locations accessible from the root set), N</i> is the maximum number of reachable objects, D</i> is the maximum depth of any data object, and k</i> is a parameter specifying how many locations are copied each time a location is allocated. Furthermore we show that client threads are never stopped for more than time proportional to k</i> non-blocking machine instructions. The bounds are guaranteed even with arbitrary length arrays. The collector only requires write-barriers (reads are unaffected by the collector), makes few assumptions about the threads that are generating the garbage, and allows them to run mostly asynchronously.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {104--117},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301618.301648},
 doi = {http://doi.acm.org/10.1145/301618.301648},
 acmid = {301648},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stichnoth:1999:SGC:301631.301652,
 author = {Stichnoth, James M. and Lueh, Guei-Yuan and Cierniak, Micha\l},
 title = {Support for garbage collection at every instruction in a Java compiler},
 abstract = {A high-performance implementation of a Java Virtual Machine<sup>1</sup> requires a compiler to translate Java bytecodes into native instructions, as well as an advanced garbage collector (e.g., copying or generational). When the Java heap is exhausted and the garbage collector executes, the compiler must report to the garbage collector all live object references contained in physical registers and stack locations. Typical compilers only allow certain instructions (e.g., call instructions and backward branches) to be GC-safe</i>; if GC happens at some other instruction, the compiler may need to advance execution to the next GC-safe point. Until now, no one has ever attempted to make every</i> compiler-generated instruction GC-safe, due to the perception that recording this information would require too much space. This kind of support could improve the GC performance in multithreaded applications. We show how to use simple compression techniques to reduce the size of the GC map to about 20\% of the generated code size, a result that is competitive with the best previously published results. In addition, we extend the work of Agesen, Detlefs, and Moss, regarding the so-called "JSR Problem" (the single exception to Java's type safety property), in a way that eliminates the need for extra runtime overhead in the generated code.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {118--127},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301631.301652},
 doi = {http://doi.acm.org/10.1145/301631.301652},
 acmid = {301652},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, compilers, garbage collection},
} 

@inproceedings{Stichnoth:1999:SGC:301618.301652,
 author = {Stichnoth, James M. and Lueh, Guei-Yuan and Cierniak, Micha\l},
 title = {Support for garbage collection at every instruction in a Java compiler},
 abstract = {A high-performance implementation of a Java Virtual Machine<sup>1</sup> requires a compiler to translate Java bytecodes into native instructions, as well as an advanced garbage collector (e.g., copying or generational). When the Java heap is exhausted and the garbage collector executes, the compiler must report to the garbage collector all live object references contained in physical registers and stack locations. Typical compilers only allow certain instructions (e.g., call instructions and backward branches) to be GC-safe</i>; if GC happens at some other instruction, the compiler may need to advance execution to the next GC-safe point. Until now, no one has ever attempted to make every</i> compiler-generated instruction GC-safe, due to the perception that recording this information would require too much space. This kind of support could improve the GC performance in multithreaded applications. We show how to use simple compression techniques to reduce the size of the GC map to about 20\% of the generated code size, a result that is competitive with the best previously published results. In addition, we extend the work of Agesen, Detlefs, and Moss, regarding the so-called "JSR Problem" (the single exception to Java's type safety property), in a way that eliminates the need for extra runtime overhead in the generated code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {118--127},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301618.301652},
 doi = {http://doi.acm.org/10.1145/301618.301652},
 acmid = {301652},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, compilers, garbage collection},
} 

@article{Rao:1999:SAO:301631.301653,
 author = {Rao, Amit and Pande, Santosh},
 title = {Storage assignment optimizations to generate compact and efficient code on embedded DSPs},
 abstract = {DSP architectures typically provide dedicated memory address generation units and indirect addressing modes with auto-increment and auto-decrement that subsume address arithmetic calculation. The heavy use of auto-increment and auto-decrement indirect addressing require DSP compilers to perform a careful placement of variables in storage to minimize address arithmetic instructions to generate compact and efficient DSP code. Liao et al. formulated the problem of storage assignment as the simple offset assignment problem (SOA) and the general offset assignment problem (GOA), and proposed heuristic solutions.The storage allocation of variables critically depends on the sequence of variable accesses. In this paper we present techniques to optimize the access sequence of variables by applying algebraic transformations (such as commutativity and associativity) on expression trees to obtain the least cost offset assignment. We develop a new formulation of this problem as the least cost access sequence problem (LCAS). Based on the proposed framework, we develop heuristic algorithms that determine empirically near-optimal solutions resulting in fewer address arithmetic instructions. We have implemented the proposed heuristic algorithms by extending the storage assignment optimization in the SPAM compiler back-end targeted for the TMS320C25 DSP. In the case of SOA, experimental results for programs from the DSPstone benchmark suite show an average improvement of 3.36\% in static code size and an average relative speed-up of 7.28\% over results obtained using existing SOA algorithms. The average code size reduction over code compiled with a naive storage assignment algorithm is 7.04\%. The proposed framework has also been applied to the GOA problem and shows average code size reductions of 2.04\% over results obtained using existing GOA algorithms, and average code size reductions of 10.84\% over a naive GOA algorithm. Code size reduction and improvement in dynamic instruction counts could be valuable given limited memory and real-time response requirements placed on embedded systems.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {128--138},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301653},
 doi = {http://doi.acm.org/10.1145/301631.301653},
 acmid = {301653},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rao:1999:SAO:301618.301653,
 author = {Rao, Amit and Pande, Santosh},
 title = {Storage assignment optimizations to generate compact and efficient code on embedded DSPs},
 abstract = {DSP architectures typically provide dedicated memory address generation units and indirect addressing modes with auto-increment and auto-decrement that subsume address arithmetic calculation. The heavy use of auto-increment and auto-decrement indirect addressing require DSP compilers to perform a careful placement of variables in storage to minimize address arithmetic instructions to generate compact and efficient DSP code. Liao et al. formulated the problem of storage assignment as the simple offset assignment problem (SOA) and the general offset assignment problem (GOA), and proposed heuristic solutions.The storage allocation of variables critically depends on the sequence of variable accesses. In this paper we present techniques to optimize the access sequence of variables by applying algebraic transformations (such as commutativity and associativity) on expression trees to obtain the least cost offset assignment. We develop a new formulation of this problem as the least cost access sequence problem (LCAS). Based on the proposed framework, we develop heuristic algorithms that determine empirically near-optimal solutions resulting in fewer address arithmetic instructions. We have implemented the proposed heuristic algorithms by extending the storage assignment optimization in the SPAM compiler back-end targeted for the TMS320C25 DSP. In the case of SOA, experimental results for programs from the DSPstone benchmark suite show an average improvement of 3.36\% in static code size and an average relative speed-up of 7.28\% over results obtained using existing SOA algorithms. The average code size reduction over code compiled with a naive storage assignment algorithm is 7.04\%. The proposed framework has also been applied to the GOA problem and shows average code size reductions of 2.04\% over results obtained using existing GOA algorithms, and average code size reductions of 10.84\% over a naive GOA algorithm. Code size reduction and improvement in dynamic instruction counts could be valuable given limited memory and real-time response requirements placed on embedded systems.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {128--138},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301653},
 doi = {http://doi.acm.org/10.1145/301618.301653},
 acmid = {301653},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooper:1999:ECC:301631.301655,
 author = {Cooper, Keith D. and McIntosh, Nathaniel},
 title = {Enhanced code compression for embedded RISC processors},
 abstract = {This paper explores compiler techniques for reducing the memory needed to load and run program executables. In embedded systems, where economic incentives to reduce both RAM and ROM are strong, the size of compiled code is increasingly important. Similarly, in mobile and network computing, the need to transmit an executable before running it places a premium on code size. Our work focuses on reducing the size of a program's code segment, using pattern-matching techniques to identify and coalesce together repeated instruction sequences. In contrast to other methods, our framework preserves the ability to run program executables directly, without an intervening decompression stage. Our compression framework is integrated into an industrial-strength optimizing compiler, which allows us to explore the interaction between code compression and classical code optimization techniques, and requires that we contend with the difficulties of compressing previously optimized code. The specific contributions in this paper include a comprehensive experimental evaluation of code compression for a RISC-like architecture, a more powerful pattern-matching scheme for improved identification of repeated code fragments, and a new form of profile-driven code compression that reduces the speed penalty arising from compression.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301655},
 doi = {http://doi.acm.org/10.1145/301631.301655},
 acmid = {301655},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cooper:1999:ECC:301618.301655,
 author = {Cooper, Keith D. and McIntosh, Nathaniel},
 title = {Enhanced code compression for embedded RISC processors},
 abstract = {This paper explores compiler techniques for reducing the memory needed to load and run program executables. In embedded systems, where economic incentives to reduce both RAM and ROM are strong, the size of compiled code is increasingly important. Similarly, in mobile and network computing, the need to transmit an executable before running it places a premium on code size. Our work focuses on reducing the size of a program's code segment, using pattern-matching techniques to identify and coalesce together repeated instruction sequences. In contrast to other methods, our framework preserves the ability to run program executables directly, without an intervening decompression stage. Our compression framework is integrated into an industrial-strength optimizing compiler, which allows us to explore the interaction between code compression and classical code optimization techniques, and requires that we contend with the difficulties of compressing previously optimized code. The specific contributions in this paper include a comprehensive experimental evaluation of code compression for a RISC-like architecture, a more powerful pattern-matching scheme for improved identification of repeated code fragments, and a new form of profile-driven code compression that reduces the speed penalty arising from compression.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301655},
 doi = {http://doi.acm.org/10.1145/301618.301655},
 acmid = {301655},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vegdahl:1999:UNM:301631.301657,
 author = {Vegdahl, Steven R.},
 title = {Using node merging to enhance graph coloring},
 abstract = {A Chaitin-style register allocator often blocks during its simplification phase because no node in the interference graph has a degree that is sufficiently small. Typically, this is handled by node-splitting, or by optimistically continuing---and hoping that a legal N-coloring will still be found. We observe that the merging of two nodes in a graph causes a reduction in the degree of any node that had been adjacent to both. We have enhanced Chaitin's coloring algorithm so that it attempts node-merging during graph simplification; this often allows simplification to continue, while still guaranteeing a coloring for the graph. We have tested this algorithm using Appel's database of register-coloring graphs, and have compared it with Chaitin's algorithm. The merge-enhanced algorithm yields a better coloring about 8\% of the time, and a worse coloring less than 0.1\% of the time.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {150--154},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/301631.301657},
 doi = {http://doi.acm.org/10.1145/301631.301657},
 acmid = {301657},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation, register coalescing},
} 

@inproceedings{Vegdahl:1999:UNM:301618.301657,
 author = {Vegdahl, Steven R.},
 title = {Using node merging to enhance graph coloring},
 abstract = {A Chaitin-style register allocator often blocks during its simplification phase because no node in the interference graph has a degree that is sufficiently small. Typically, this is handled by node-splitting, or by optimistically continuing---and hoping that a legal N-coloring will still be found. We observe that the merging of two nodes in a graph causes a reduction in the degree of any node that had been adjacent to both. We have enhanced Chaitin's coloring algorithm so that it attempts node-merging during graph simplification; this often allows simplification to continue, while still guaranteeing a coloring for the graph. We have tested this algorithm using Appel's database of register-coloring graphs, and have compared it with Chaitin's algorithm. The merge-enhanced algorithm yields a better coloring about 8\% of the time, and a worse coloring less than 0.1\% of the time.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {150--154},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/301618.301657},
 doi = {http://doi.acm.org/10.1145/301618.301657},
 acmid = {301657},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph coloring, register allocation, register coalescing},
} 

@article{Schlansker:1999:CCB:301631.301659,
 author = {Schlansker, Michael and Mahlke, Scott and Johnson, Richard},
 title = {Control CPR: a branch height reduction optimization for EPIC architectures},
 abstract = {The challenge of exploiting high degrees of instruction-level parallelism is often hampered by frequent branching. Both exposed branch latency and low branch throughput can restrict parallelism. Control critical path reduction</i> (control CPR) is a compilation technique to address these problems. Control CPR can reduce the dependence height of critical paths through branch operations as well as decrease the number of executed branches. In this paper, we present an approach to control CPR that recognizes sequences of branches using profiling statistics. The control CPR transformation is applied to the predominant path through this sequence. Our approach, its implementation, and experimental results are presented. This work demonstrates that control CPR enhances instruction-level parallelism for a variety of application programs and improves their performance across a range of processors.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {155--168},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301631.301659},
 doi = {http://doi.acm.org/10.1145/301631.301659},
 acmid = {301659},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schlansker:1999:CCB:301618.301659,
 author = {Schlansker, Michael and Mahlke, Scott and Johnson, Richard},
 title = {Control CPR: a branch height reduction optimization for EPIC architectures},
 abstract = {The challenge of exploiting high degrees of instruction-level parallelism is often hampered by frequent branching. Both exposed branch latency and low branch throughput can restrict parallelism. Control critical path reduction</i> (control CPR) is a compilation technique to address these problems. Control CPR can reduce the dependence height of critical paths through branch operations as well as decrease the number of executed branches. In this paper, we present an approach to control CPR that recognizes sequences of branches using profiling statistics. The control CPR transformation is applied to the predominant path through this sequence. Our approach, its implementation, and experimental results are presented. This work demonstrates that control CPR enhances instruction-level parallelism for a variety of application programs and improves their performance across a range of processors.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {155--168},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301618.301659},
 doi = {http://doi.acm.org/10.1145/301618.301659},
 acmid = {301659},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Frigo:1999:FFT:301631.301661,
 author = {Frigo, Matteo},
 title = {A fast Fourier transform compiler},
 abstract = {The FFTW library for computing the discrete Fourier transform (DFT) has gained a wide acceptance in both academia and industry, because it provides excellent performance on a variety of machines (even competitive with or faster than equivalent libraries supplied by vendors). In FFTW, most of the performance-critical code was generated automatically by a special-purpose compiler, called genfft, that outputs C code. Written in Objective Caml, genfft can produce DFT programs for any input length, and it can specialize the DFT program for the common case where the input data are real instead of complex. Unexpectedly, genfft "discovered" algorithms that were previously unknown, and it was able to reduce the arithmetic complexity of some other existing algorithms. This paper describes the internals of this special-purpose compiler in some detail, and it argues that a specialized compiler is a valuable tool.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301661},
 doi = {http://doi.acm.org/10.1145/301631.301661},
 acmid = {301661},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Frigo:1999:FFT:301618.301661,
 author = {Frigo, Matteo},
 title = {A fast Fourier transform compiler},
 abstract = {The FFTW library for computing the discrete Fourier transform (DFT) has gained a wide acceptance in both academia and industry, because it provides excellent performance on a variety of machines (even competitive with or faster than equivalent libraries supplied by vendors). In FFTW, most of the performance-critical code was generated automatically by a special-purpose compiler, called genfft, that outputs C code. Written in Objective Caml, genfft can produce DFT programs for any input length, and it can specialize the DFT program for the common case where the input data are real instead of complex. Unexpectedly, genfft "discovered" algorithms that were previously unknown, and it was able to reduce the arithmetic complexity of some other existing algorithms. This paper describes the internals of this special-purpose compiler in some detail, and it argues that a specialized compiler is a valuable tool.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301661},
 doi = {http://doi.acm.org/10.1145/301618.301661},
 acmid = {301661},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wu:1999:NFD:301631.301663,
 author = {Wu, Le-Chun and Mirani, Rajiv and Patil, Harish and Olsen, Bruce and Hwu, Wen-mei W.},
 title = {A new framework for debugging globally optimized code},
 abstract = {With an increasing number of executable binaries generated by optimizing compilers today, providing a clear and correct source-level debugger for programmers to debug optimized code has become a necessity. In this paper, a new framework for debugging globally optimized code is proposed. This framework consists of a new code location mapping scheme, a data location tracking scheme, and an emulation-based forward recovery model. By taking over the control early and emulating instructions selectively, the debugger can preserve and gather the required program state for the recovery of expected variable values at source breakpoints. The framework has been prototyped in the IMPACT compiler and GDB-4.16. Preliminary experiments conducted on several SPEC95 integer programs have yielded encouraging results. The extra time needed for the debugger to calculate the limits of the emulated region and to emulate instructions is hardly noticeable, while the increase in executable file size due to the extra debug information is on average 76\% of that of the executable file with no debug information.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {181--191},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301663},
 doi = {http://doi.acm.org/10.1145/301631.301663},
 acmid = {301663},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wu:1999:NFD:301618.301663,
 author = {Wu, Le-Chun and Mirani, Rajiv and Patil, Harish and Olsen, Bruce and Hwu, Wen-mei W.},
 title = {A new framework for debugging globally optimized code},
 abstract = {With an increasing number of executable binaries generated by optimizing compilers today, providing a clear and correct source-level debugger for programmers to debug optimized code has become a necessity. In this paper, a new framework for debugging globally optimized code is proposed. This framework consists of a new code location mapping scheme, a data location tracking scheme, and an emulation-based forward recovery model. By taking over the control early and emulating instructions selectively, the debugger can preserve and gather the required program state for the recovery of expected variable values at source breakpoints. The framework has been prototyped in the IMPACT compiler and GDB-4.16. Preliminary experiments conducted on several SPEC95 integer programs have yielded encouraging results. The extra time needed for the debugger to calculate the limits of the emulated region and to emulate instructions is hardly noticeable, while the increase in executable file size due to the extra debug information is on average 76\% of that of the executable file with no debug information.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {181--191},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301663},
 doi = {http://doi.acm.org/10.1145/301618.301663},
 acmid = {301663},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Foster:1999:TTQ:301618.301665,
 author = {Foster, Jeffrey S. and F\"{a}hndrich, Manuel and Aiken, Alexander},
 title = {A theory of type qualifiers},
 abstract = {We describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a const-inference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301665},
 doi = {http://doi.acm.org/10.1145/301618.301665},
 acmid = {301665},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Foster:1999:TTQ:301631.301665,
 author = {Foster, Jeffrey S. and F\"{a}hndrich, Manuel and Aiken, Alexander},
 title = {A theory of type qualifiers},
 abstract = {We describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a const-inference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301665},
 doi = {http://doi.acm.org/10.1145/301631.301665},
 acmid = {301665},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Leung:1999:SSA:301618.301667,
 author = {Leung, Allen and George, Lal},
 title = {Static single assignment form for machine code},
 abstract = {Static Single Assignment (SSA) is an effective intermediate representation in optimizing compilers. However, traditional SSA form and optimizations are not applicable to programs represented as native machine instructions because the use of dedicated registers imposed by calling conventions, the runtime system, and target architecture must be made explicit. We present a simple scheme for converting between programs in machine code and in SSA, such that references to dedicated physical registers in machine code are preserved. Our scheme ignores all output- and anti-dependences imposed by physical registers while a program is in SSA form, but inserts compensation code during machine code reconstruction if any naming requirements have been violated. By resolving all mismatches between the two representations in separate phases, we are able to utilize existing SSA algorithms unaltered to perform machine code optimizations.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {204--214},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301667},
 doi = {http://doi.acm.org/10.1145/301618.301667},
 acmid = {301667},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Leung:1999:SSA:301631.301667,
 author = {Leung, Allen and George, Lal},
 title = {Static single assignment form for machine code},
 abstract = {Static Single Assignment (SSA) is an effective intermediate representation in optimizing compilers. However, traditional SSA form and optimizations are not applicable to programs represented as native machine instructions because the use of dedicated registers imposed by calling conventions, the runtime system, and target architecture must be made explicit. We present a simple scheme for converting between programs in machine code and in SSA, such that references to dedicated physical registers in machine code are preserved. Our scheme ignores all output- and anti-dependences imposed by physical registers while a program is in SSA form, but inserts compensation code during machine code reconstruction if any naming requirements have been violated. By resolving all mismatches between the two representations in separate phases, we are able to utilize existing SSA algorithms unaltered to perform machine code optimizations.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {204--214},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301667},
 doi = {http://doi.acm.org/10.1145/301631.301667},
 acmid = {301667},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Song:1999:NTT:301631.301668,
 author = {Song, Yonghong and Li, Zhiyuan},
 title = {New tiling techniques to improve cache temporal locality},
 abstract = {Tiling is a well-known loop transformation to improve temporal locality of nested loops. Current compiler algorithms for tiling are limited to loops which are perfectly nested or can be transformed, in trivial ways, into a perfect nest. This paper presents a number of program transformations to enable tiling for a class of nontrivial imperfectly-nested loops such that cache locality is improved. We define a program model for such loops and develop compiler algorithms for their tiling. We propose to adopt odd-even variable duplication</i> to break anti- and output dependences without unduly increasing the working-set size, and to adopt speculative execution</i> to enable tiling of loops which may terminate prematurely due to, e.g. convergence tests in iterative algorithms. We have implemented these techniques in a research compiler, Panorama. Initial experiments with several benchmark programs are performed on SGI workstations based on MIPS R5K and R10K processors. Overall, the transformed programs run faster by 9\% to 164\%.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {215--228},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301631.301668},
 doi = {http://doi.acm.org/10.1145/301631.301668},
 acmid = {301668},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, loop transformations, optimizing compilers},
} 

@inproceedings{Song:1999:NTT:301618.301668,
 author = {Song, Yonghong and Li, Zhiyuan},
 title = {New tiling techniques to improve cache temporal locality},
 abstract = {Tiling is a well-known loop transformation to improve temporal locality of nested loops. Current compiler algorithms for tiling are limited to loops which are perfectly nested or can be transformed, in trivial ways, into a perfect nest. This paper presents a number of program transformations to enable tiling for a class of nontrivial imperfectly-nested loops such that cache locality is improved. We define a program model for such loops and develop compiler algorithms for their tiling. We propose to adopt odd-even variable duplication</i> to break anti- and output dependences without unduly increasing the working-set size, and to adopt speculative execution</i> to enable tiling of loops which may terminate prematurely due to, e.g. convergence tests in iterative algorithms. We have implemented these techniques in a research compiler, Panorama. Initial experiments with several benchmark programs are performed on SGI workstations based on MIPS R5K and R10K processors. Overall, the transformed programs run faster by 9\% to 164\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {215--228},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/301618.301668},
 doi = {http://doi.acm.org/10.1145/301618.301668},
 acmid = {301668},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, loop transformations, optimizing compilers},
} 

@article{Ding:1999:ICP:301631.301670,
 author = {Ding, Chen and Kennedy, Ken},
 title = {Improving cache performance in dynamic applications through data and computation reorganization at run time},
 abstract = {With the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been static</i>---applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a dynamic</i> approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated run-time overhead.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {229--241},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301631.301670},
 doi = {http://doi.acm.org/10.1145/301631.301670},
 acmid = {301670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ding:1999:ICP:301618.301670,
 author = {Ding, Chen and Kennedy, Ken},
 title = {Improving cache performance in dynamic applications through data and computation reorganization at run time},
 abstract = {With the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been static</i>---applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a dynamic</i> approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated run-time overhead.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {229--241},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/301618.301670},
 doi = {http://doi.acm.org/10.1145/301618.301670},
 acmid = {301670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fraser:1999:AIM:301631.301672,
 author = {Fraser, Christopher W.},
 title = {Automatic inference of models for statistical code compression},
 abstract = {This paper describes experiments that apply machine learning to compress computer programs, formalizing and automating decisions about instruction encoding that have traditionally been made by humans in a more ad hoc manner. A program accepts a large training set of program material in a conventional compiler intermediate representation (IR) and automatically infers a decision tree that separates IR code into streams that compress much better than the undifferentiated whole. Driving a conventional arithmetic compressor with this model yields code 30\% smaller than the previous record for IR code compression, and 24\% smaller than an ambitious optimizing compiler feeding an ambitious general-purpose data compressor.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {242--246},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/301631.301672},
 doi = {http://doi.acm.org/10.1145/301631.301672},
 acmid = {301672},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract machines, code compaction, code compression, compiler intermediate languages and representations, data compression, decision trees, machine learning, statistical models, virtual machines},
} 

@inproceedings{Fraser:1999:AIM:301618.301672,
 author = {Fraser, Christopher W.},
 title = {Automatic inference of models for statistical code compression},
 abstract = {This paper describes experiments that apply machine learning to compress computer programs, formalizing and automating decisions about instruction encoding that have traditionally been made by humans in a more ad hoc manner. A program accepts a large training set of program material in a conventional compiler intermediate representation (IR) and automatically infers a decision tree that separates IR code into streams that compress much better than the undifferentiated whole. Driving a conventional arithmetic compressor with this model yields code 30\% smaller than the previous record for IR code compression, and 24\% smaller than an ambitious optimizing compiler feeding an ambitious general-purpose data compressor.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {242--246},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/301618.301672},
 doi = {http://doi.acm.org/10.1145/301618.301672},
 acmid = {301672},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract machines, code compaction, code compression, compiler intermediate languages and representations, data compression, decision trees, machine learning, statistical models, virtual machines},
} 

@article{Pugh:1999:CJC:301631.301676,
 author = {Pugh, William},
 title = {Compressing Java class files},
 abstract = {Java class files are often distributed as jar files, which are collections of individually compressed class files (and possibility other files). Jar files are typically about 1/2 the size of the original class files due to compression. I have developed a wire-code format for collections of Java class files. This format is typically 1/2 to 1/5 of the size of the corresponding compressed jar file (1/4 to 1/10 the size of the original class files).},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {247--258},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301676},
 doi = {http://doi.acm.org/10.1145/301631.301676},
 acmid = {301676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pugh:1999:CJC:301618.301676,
 author = {Pugh, William},
 title = {Compressing Java class files},
 abstract = {Java class files are often distributed as jar files, which are collections of individually compressed class files (and possibility other files). Jar files are typically about 1/2 the size of the original class files due to compression. I have developed a wire-code format for collections of Java class files. This format is typically 1/2 to 1/5 of the size of the corresponding compressed jar file (1/4 to 1/10 the size of the original class files).},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {247--258},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301676},
 doi = {http://doi.acm.org/10.1145/301618.301676},
 acmid = {301676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Larus:1999:WPP:301618.301678,
 author = {Larus, James R.},
 title = {Whole program paths},
 abstract = {Whole program paths (WPP)</i> are a new approach to capturing and representing a program's dynamic---actually executed---control flow. Unlike other path profiling techniques, which record intraprocedural or acyclic paths, WPPs produce a single, compact description of a program's entire control flow, including loop iteration and interprocedural paths.This paper explains how to collect and represent WPPs. It also shows how to use WPPs to find hot subpaths</i>, which are the heavily executed sequences of code that should be the focus of performance tuning and compiler optimization.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {259--269},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301678},
 doi = {http://doi.acm.org/10.1145/301618.301678},
 acmid = {301678},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data compression, dynamic program measurement, path profiling, program control flow, program tracing},
} 

@article{Larus:1999:WPP:301631.301678,
 author = {Larus, James R.},
 title = {Whole program paths},
 abstract = {Whole program paths (WPP)</i> are a new approach to capturing and representing a program's dynamic---actually executed---control flow. Unlike other path profiling techniques, which record intraprocedural or acyclic paths, WPPs produce a single, compact description of a program's entire control flow, including loop iteration and interprocedural paths.This paper explains how to collect and represent WPPs. It also shows how to use WPPs to find hot subpaths</i>, which are the heavily executed sequences of code that should be the focus of performance tuning and compiler optimization.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {259--269},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301678},
 doi = {http://doi.acm.org/10.1145/301631.301678},
 acmid = {301678},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data compression, dynamic program measurement, path profiling, program control flow, program tracing},
} 

@inproceedings{Fraser:1999:FCG:301618.301680,
 author = {Fraser, Christopher W. and Proebsting, Todd A.},
 title = {Finite-state code generation},
 abstract = {This paper describes GBURG, which generates tiny, fast code generators based on finite-state machine pattern matching. The code generators translate postfix intermediate code into machine instructions in one pass (except, of course, for backpatching addresses). A stack-based virtual machine---known as the Lean Virtual Machine</i> (LVM)---tuned for fast code generation is also described. GBURG translates the two-page LVM-to-x86 specification into a code generator that fits entirely in an 8 KB I-cache and that emits x86 code at 3.6 MB/set on a 266-MHz P6. Our just-in-time code generator translates and executes small benchmarks at speeds within a factor of two of executables derived from the conventional compile-time code generator on which it is based.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301618.301680},
 doi = {http://doi.acm.org/10.1145/301618.301680},
 acmid = {301680},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fraser:1999:FCG:301631.301680,
 author = {Fraser, Christopher W. and Proebsting, Todd A.},
 title = {Finite-state code generation},
 abstract = {This paper describes GBURG, which generates tiny, fast code generators based on finite-state machine pattern matching. The code generators translate postfix intermediate code into machine instructions in one pass (except, of course, for backpatching addresses). A stack-based virtual machine---known as the Lean Virtual Machine</i> (LVM)---tuned for fast code generation is also described. GBURG translates the two-page LVM-to-x86 specification into a code generator that fits entirely in an 8 KB I-cache and that emits x86 code at 3.6 MB/set on a 266-MHz P6. Our just-in-time code generator translates and executes small benchmarks at speeds within a factor of two of executables derived from the conventional compile-time code generator on which it is based.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {270--280},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301631.301680},
 doi = {http://doi.acm.org/10.1145/301631.301680},
 acmid = {301680},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marlet:1999:EIR:301631.301681,
 author = {Marlet, Renaud and Consel, Charles and Boinot, Philippe},
 title = {Efficient incremental run-time specialization for free},
 abstract = {Availability of data in a program determines computation stages. Incremental partial evaluation exploit these stages for optimization: it allows further specialization to be performed as data become available at later stages. The fundamental advantage of incremental specialization is to factorize the specialization process. As a result, specializing a program at a given stage costs considerably less than specializing it once all the data are available.We present a realistic and flexible approach to achieve efficient incremental run-time specialization. Rather than developing specific techniques, as previously proposed, we are able to re-use existing technology by iterating a specialization process. Moreover, in doing so, we do not lose any specialization opportunities. This approach makes it possible to exploit nested quasi-invariants and to speed up the run-time specialization process.This approach has been implemented in Tempo, a specializer for C programs that is publicly available. A preliminary experiment confirm that incremental that incremental specialization can greatly speed up the specialization process.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301681},
 doi = {http://doi.acm.org/10.1145/301631.301681},
 acmid = {301681},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marlet:1999:EIR:301618.301681,
 author = {Marlet, Renaud and Consel, Charles and Boinot, Philippe},
 title = {Efficient incremental run-time specialization for free},
 abstract = {Availability of data in a program determines computation stages. Incremental partial evaluation exploit these stages for optimization: it allows further specialization to be performed as data become available at later stages. The fundamental advantage of incremental specialization is to factorize the specialization process. As a result, specializing a program at a given stage costs considerably less than specializing it once all the data are available.We present a realistic and flexible approach to achieve efficient incremental run-time specialization. Rather than developing specific techniques, as previously proposed, we are able to re-use existing technology by iterating a specialization process. Moreover, in doing so, we do not lose any specialization opportunities. This approach makes it possible to exploit nested quasi-invariants and to speed up the run-time specialization process.This approach has been implemented in Tempo, a specializer for C programs that is publicly available. A preliminary experiment confirm that incremental that incremental specialization can greatly speed up the specialization process.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {281--292},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301681},
 doi = {http://doi.acm.org/10.1145/301618.301681},
 acmid = {301681},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Grant:1999:ESR:301618.301683,
 author = {Grant, Brian and Philipose, Matthai and Mock, Markus and Chambers, Craig and Eggers, Susan J.},
 title = {An evaluation of staged run-time optimizations in DyC},
 abstract = {Previous selective dynamic compilation systems have demonstrated that dynamic compilation can achieve performance improvements at low cost on small kernels, but they have had difficulty scaling to larger programs. To overcome this limitation, we developed DyC, a selective dynamic compilation system that includes more sophisticated and flexible analyses and transformations. DyC is able to achieve good performance improvements on programs that are much larger and more complex than the kernels. We analyze the individual optimizations of DyC and assess their impact on performance collectively and individually.},
 booktitle = {Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation},
 series = {PLDI '99},
 year = {1999},
 isbn = {1-58113-094-5},
 location = {Atlanta, Georgia, United States},
 pages = {293--304},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301618.301683},
 doi = {http://doi.acm.org/10.1145/301618.301683},
 acmid = {301683},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Grant:1999:ESR:301631.301683,
 author = {Grant, Brian and Philipose, Matthai and Mock, Markus and Chambers, Craig and Eggers, Susan J.},
 title = {An evaluation of staged run-time optimizations in DyC},
 abstract = {Previous selective dynamic compilation systems have demonstrated that dynamic compilation can achieve performance improvements at low cost on small kernels, but they have had difficulty scaling to larger programs. To overcome this limitation, we developed DyC, a selective dynamic compilation system that includes more sophisticated and flexible analyses and transformations. DyC is able to achieve good performance improvements on programs that are much larger and more complex than the kernels. We analyze the individual optimizations of DyC and assess their impact on performance collectively and individually.},
 journal = {SIGPLAN Not.},
 volume = {34},
 issue = {5},
 month = {May},
 year = {1999},
 issn = {0362-1340},
 pages = {293--304},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301631.301683},
 doi = {http://doi.acm.org/10.1145/301631.301683},
 acmid = {301683},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bala:2000:DTD:349299.349303,
 author = {Bala, Vasanth and Duesterwald, Evelyn and Banerjia, Sanjeev},
 title = {Dynamo: a transparent dynamic optimization system},
 abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of -O optimized SpecInt95 benchmark binaries created by the HP product C   compiler is improved to a level comparable to their -O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349303},
 doi = {http://doi.acm.org/10.1145/349299.349303},
 acmid = {349303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bala:2000:DTD:358438.349303,
 author = {Bala, Vasanth and Duesterwald, Evelyn and Banerjia, Sanjeev},
 title = {Dynamo: a transparent dynamic optimization system},
 abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of -O optimized SpecInt95 benchmark binaries created by the HP product C   compiler is improved to a level comparable to their -O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349303},
 doi = {http://doi.acm.org/10.1145/358438.349303},
 acmid = {349303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cierniak:2000:PJJ:349299.349306,
 author = {Cierniak, Micha\l and Lueh, Guei-Yuan and Stichnoth, James M.},
 title = {Practicing JUDO: Java under dynamic optimizations},
 abstract = {A high-performance implementation of a Java  Virtual Machine (JVM) consists of efficient implementation of Just-In-Time (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC).  These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA-32 as the hardware platform, but the optimizations can be generalized to other architectures.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {13--26},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/349299.349306},
 doi = {http://doi.acm.org/10.1145/349299.349306},
 acmid = {349306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cierniak:2000:PJJ:358438.349306,
 author = {Cierniak, Micha\l and Lueh, Guei-Yuan and Stichnoth, James M.},
 title = {Practicing JUDO: Java under dynamic optimizations},
 abstract = {A high-performance implementation of a Java  Virtual Machine (JVM) consists of efficient implementation of Just-In-Time (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC).  These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA-32 as the hardware platform, but the optimizations can be generalized to other architectures.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {13--26},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/358438.349306},
 doi = {http://doi.acm.org/10.1145/358438.349306},
 acmid = {349306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lucco:2000:SDP:349299.349307,
 author = {Lucco, Steven},
 title = {Split-stream dictionary program compression},
 abstract = {This paper describes split-stream dictionary (SSD) compression, a new  technique for transforming programs into a compact, interpretable form. We define a compressed program as interpretable when it can be decompressed at basic-block granularity with reasonable efficiency. The granularity requirement enables interpreters or just-in-time (JIT) translators to decompress basic blocks incrementally during program execution. Our previous approach to interpretable compression, the Byte-coded RISC (BRISC) program format [1], achieved unprecedented decompression speed in excess of 5 megabytes per second on a 450MHz Pentium II while compressing benchmark programs to an average of three-fifths the size of their optimized x86 representation. SSD compression combines the key idea behind BRISC with new observations about instruction re-use frequencies to yield four advantages over BRISC and other competing techniques. First, SSD is simple, requiring only a few pages of code for an effective implementation. Second, SSD compresses programs more effectively than any interpretable program compression scheme known to us. For example, SSD compressed a set of programs including the spec95 benchmarks and Microsoft Word97 to less than half the size, on average, of their optimized x86 representation. Third, SSD exceeds BRISC's decompression and JIT translation rates by over 50\%. Finally, SSD's two-phased approach to JIT translation enables a virtual machine to provide graceful degradation of program execution time in the face of increasing RAM constraints. For example, using SSD, we ran   Word97 using a JIT-translation buffer one-third the size of Word97's optimized x86 code, yet incurred only 27\% execution time overhead.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {27--34},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/349299.349307},
 doi = {http://doi.acm.org/10.1145/349299.349307},
 acmid = {349307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compression, runtime system, virtual machine},
} 

@article{Lucco:2000:SDP:358438.349307,
 author = {Lucco, Steven},
 title = {Split-stream dictionary program compression},
 abstract = {This paper describes split-stream dictionary (SSD) compression, a new  technique for transforming programs into a compact, interpretable form. We define a compressed program as interpretable when it can be decompressed at basic-block granularity with reasonable efficiency. The granularity requirement enables interpreters or just-in-time (JIT) translators to decompress basic blocks incrementally during program execution. Our previous approach to interpretable compression, the Byte-coded RISC (BRISC) program format [1], achieved unprecedented decompression speed in excess of 5 megabytes per second on a 450MHz Pentium II while compressing benchmark programs to an average of three-fifths the size of their optimized x86 representation. SSD compression combines the key idea behind BRISC with new observations about instruction re-use frequencies to yield four advantages over BRISC and other competing techniques. First, SSD is simple, requiring only a few pages of code for an effective implementation. Second, SSD compresses programs more effectively than any interpretable program compression scheme known to us. For example, SSD compressed a set of programs including the spec95 benchmarks and Microsoft Word97 to less than half the size, on average, of their optimized x86 representation. Third, SSD exceeds BRISC's decompression and JIT translation rates by over 50\%. Finally, SSD's two-phased approach to JIT translation enables a virtual machine to provide graceful degradation of program execution time in the face of increasing RAM constraints. For example, using SSD, we ran   Word97 using a JIT-translation buffer one-third the size of Word97's optimized x86 code, yet incurred only 27\% execution time overhead.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {27--34},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/358438.349307},
 doi = {http://doi.acm.org/10.1145/358438.349307},
 acmid = {349307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compression, runtime system, virtual machine},
} 

@article{Das:2000:UPA:358438.349309,
 author = {Das, Manuvir},
 title = {Unification-based pointer analysis with directional assignments},
 abstract = {This paper describes a new algorithm for flow and context insensitive  pointer analysis of C programs. Our studies show that the most common use of pointers in C programs is in passing the addresses of composite objects or updateable values as arguments to procedures. Therefore, we have designed a low-cost algorithm that handles this common case accurately. In terms of both precision and running time, this algorithm lies between  Steensgaard's algorithm, which treats assignments bi-directionally using unification, and Andersen's algorithm, which treats assignments directionally using subtyping. Our ``one level flow" algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the points-to graph, while using   unification  elsewhere in the graph. The method scales easily to large programs. For instance, we are able to analyze a 1.4 MLOC (million lines of code) program in two minutes, using less than 200MB of memory. At the same time, the precision of our algorithm is very close to that of Andersen's algorithm. On all of the integer benchmark programs from SPEC95, the one level flow algorithm and Andersen's algorithm produce either identical or essentially identical points-to information. Therefore, we claim that our algorithm provides a method for obtaining precise flow-insensitive points-to information for large C programs.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349309},
 doi = {http://doi.acm.org/10.1145/358438.349309},
 acmid = {349309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Das:2000:UPA:349299.349309,
 author = {Das, Manuvir},
 title = {Unification-based pointer analysis with directional assignments},
 abstract = {This paper describes a new algorithm for flow and context insensitive  pointer analysis of C programs. Our studies show that the most common use of pointers in C programs is in passing the addresses of composite objects or updateable values as arguments to procedures. Therefore, we have designed a low-cost algorithm that handles this common case accurately. In terms of both precision and running time, this algorithm lies between  Steensgaard's algorithm, which treats assignments bi-directionally using unification, and Andersen's algorithm, which treats assignments directionally using subtyping. Our ``one level flow" algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the points-to graph, while using   unification  elsewhere in the graph. The method scales easily to large programs. For instance, we are able to analyze a 1.4 MLOC (million lines of code) program in two minutes, using less than 200MB of memory. At the same time, the precision of our algorithm is very close to that of Andersen's algorithm. On all of the integer benchmark programs from SPEC95, the one level flow algorithm and Andersen's algorithm produce either identical or essentially identical points-to information. Therefore, we claim that our algorithm provides a method for obtaining precise flow-insensitive points-to information for large C programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349309},
 doi = {http://doi.acm.org/10.1145/349299.349309},
 acmid = {349309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rountev:2000:OVS:349299.349310,
 author = {Rountev, Atanas and Chandra, Satish},
 title = {Off-line variable substitution for scaling points-to analysis},
 abstract = {Most compiler optimizations and software productivity tools rely on
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/349299.349310},
 doi = {http://doi.acm.org/10.1145/349299.349310},
 acmid = {349310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rountev:2000:OVS:358438.349310,
 author = {Rountev, Atanas and Chandra, Satish},
 title = {Off-line variable substitution for scaling points-to analysis},
 abstract = {Most compiler optimizations and software productivity tools rely on
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/358438.349310},
 doi = {http://doi.acm.org/10.1145/358438.349310},
 acmid = {349310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cheng:2000:MIP:349299.349311,
 author = {Cheng, Ben-Chung and Hwu, Wen-Mei W.},
 title = {Modular interprocedural pointer analysis using access paths: design, implementation, and evaluation},
 abstract = {In this paper we present a modular interprocedural pointer analysis 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {57--69},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349311},
 doi = {http://doi.acm.org/10.1145/349299.349311},
 acmid = {349311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cheng:2000:MIP:358438.349311,
 author = {Cheng, Ben-Chung and Hwu, Wen-Mei W.},
 title = {Modular interprocedural pointer analysis using access paths: design, implementation, and evaluation},
 abstract = {In this paper we present a modular interprocedural pointer analysis 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {57--69},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349311},
 doi = {http://doi.acm.org/10.1145/358438.349311},
 acmid = {349311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Xu:2000:SCM:349299.349313,
 author = {Xu, Zhichen and Miller, Barton P. and Reps, Thomas},
 title = {Safety checking of machine code},
 abstract = {We show how to determine statically whether it is safe for untrusted 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {70--82},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349313},
 doi = {http://doi.acm.org/10.1145/349299.349313},
 acmid = {349313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Xu:2000:SCM:358438.349313,
 author = {Xu, Zhichen and Miller, Barton P. and Reps, Thomas},
 title = {Safety checking of machine code},
 abstract = {We show how to determine statically whether it is safe for untrusted 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {70--82},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349313},
 doi = {http://doi.acm.org/10.1145/358438.349313},
 acmid = {349313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Necula:2000:TVO:349299.349314,
 author = {Necula, George C.},
 title = {Translation validation for an optimizing compiler},
 abstract = {We describe a translation validation infrastructure for the GNU C 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349314},
 doi = {http://doi.acm.org/10.1145/349299.349314},
 acmid = {349314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Necula:2000:TVO:358438.349314,
 author = {Necula, George C.},
 title = {Translation validation for an optimizing compiler},
 abstract = {We describe a translation validation infrastructure for the GNU C 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349314},
 doi = {http://doi.acm.org/10.1145/358438.349314},
 acmid = {349314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Colby:2000:CCJ:349299.349315,
 author = {Colby, Christopher and Lee, Peter and Necula, George C. and Blau, Fred and Plesko, Mark and Cline, Kenneth},
 title = {A certifying compiler for Java},
 abstract = {This paper presents the initial results of a project to determine if
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {95--107},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349315},
 doi = {http://doi.acm.org/10.1145/349299.349315},
 acmid = {349315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Colby:2000:CCJ:358438.349315,
 author = {Colby, Christopher and Lee, Peter and Necula, George C. and Blau, Fred and Plesko, Mark and Cline, Kenneth},
 title = {A certifying compiler for Java},
 abstract = {This paper presents the initial results of a project to determine if
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {95--107},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349315},
 doi = {http://doi.acm.org/10.1145/358438.349315},
 acmid = {349315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stephenson:2000:BAA:358438.349317,
 author = {Stephenson, Mark and Babb, Jonathan and Amarasinghe, Saman},
 title = {Bidwidth analysis with application to silicon compilation},
 abstract = {This paper introduces Bitwise, a compiler that 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349317},
 doi = {http://doi.acm.org/10.1145/358438.349317},
 acmid = {349317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stephenson:2000:BAA:349299.349317,
 author = {Stephenson, Mark and Babb, Jonathan and Amarasinghe, Saman},
 title = {Bidwidth analysis with application to silicon compilation},
 abstract = {This paper introduces Bitwise, a compiler that 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349317},
 doi = {http://doi.acm.org/10.1145/349299.349317},
 acmid = {349317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wilken:2000:OIS:349299.349318,
 author = {Wilken, Kent and Liu, Jack and Heffernan, Mark},
 title = {Optimal instruction scheduling using integer programming},
 abstract = {This paper presents a new approach to local instruction scheduling  based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integer-programming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integer-program variables, fewer integer-program constraints and fewer terms in some of the remaining constraints, thus reducing integer-program solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {121--133},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349318},
 doi = {http://doi.acm.org/10.1145/349299.349318},
 acmid = {349318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilken:2000:OIS:358438.349318,
 author = {Wilken, Kent and Liu, Jack and Heffernan, Mark},
 title = {Optimal instruction scheduling using integer programming},
 abstract = {This paper presents a new approach to local instruction scheduling  based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integer-programming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integer-program variables, fewer integer-program constraints and fewer terms in some of the remaining constraints, thus reducing integer-program solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14\%.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {121--133},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349318},
 doi = {http://doi.acm.org/10.1145/358438.349318},
 acmid = {349318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zalamea:2000:ISC:358438.349319,
 author = {Zalamea, Javier and Llosa, Josep and Ayguad\'{e}, Eduard and Valero, Mateo},
 title = {Improved spill code generation for software pipelined loops},
 abstract = {Software pipelining is a loop scheduling technique that extracts
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {134--144},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349319},
 doi = {http://doi.acm.org/10.1145/358438.349319},
 acmid = {349319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction-level parallelism, register allocation, software pipelining, spill code},
} 

@inproceedings{Zalamea:2000:ISC:349299.349319,
 author = {Zalamea, Javier and Llosa, Josep and Ayguad\'{e}, Eduard and Valero, Mateo},
 title = {Improved spill code generation for software pipelined loops},
 abstract = {Software pipelining is a loop scheduling technique that extracts
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {134--144},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349319},
 doi = {http://doi.acm.org/10.1145/349299.349319},
 acmid = {349319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction-level parallelism, register allocation, software pipelining, spill code},
} 

@article{Larsen:2000:ESL:358438.349320,
 author = {Larsen, Samuel and Amarasinghe, Saman},
 title = {Exploiting superword level parallelism with multimedia instruction sets},
 abstract = {Increasing focus on multimedia applications has prompted the addition
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349320},
 doi = {http://doi.acm.org/10.1145/358438.349320},
 acmid = {349320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Larsen:2000:ESL:349299.349320,
 author = {Larsen, Samuel and Amarasinghe, Saman},
 title = {Exploiting superword level parallelism with multimedia instruction sets},
 abstract = {Increasing focus on multimedia applications has prompted the addition
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349320},
 doi = {http://doi.acm.org/10.1145/349299.349320},
 acmid = {349320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lin:2000:CAI:349299.349322,
 author = {Lin, Yuan and Padua, David},
 title = {Compiler analysis of irregular memory accesses},
 abstract = {Irregular array accesses are array accesses whose array subscripts  do  not have closed-form expressions in terms of loop indices. Traditional array analysis and loop transformation techniques cannot handle irregular array accesses. In this paper, we study two kinds of simple and common cases of irregular array accesses: single-indexed access and indirect array access. We present techniques to analyze these two cases at compile-time, and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compile-time and improved speedups.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349322},
 doi = {http://doi.acm.org/10.1145/349299.349322},
 acmid = {349322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lin:2000:CAI:358438.349322,
 author = {Lin, Yuan and Padua, David},
 title = {Compiler analysis of irregular memory accesses},
 abstract = {Irregular array accesses are array accesses whose array subscripts  do  not have closed-form expressions in terms of loop indices. Traditional array analysis and loop transformation techniques cannot handle irregular array accesses. In this paper, we study two kinds of simple and common cases of irregular array accesses: single-indexed access and indirect array access. We present techniques to analyze these two cases at compile-time, and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compile-time and improved speedups.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349322},
 doi = {http://doi.acm.org/10.1145/358438.349322},
 acmid = {349322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yi:2000:TLR:358438.349323,
 author = {Yi, Qing and Adve, Vikram and Kennedy, Ken},
 title = {Transforming loops to recursion for multi-level memory hierarchies},
 abstract = {Recently, there have been several experimental and theoretical results 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {169--181},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349323},
 doi = {http://doi.acm.org/10.1145/358438.349323},
 acmid = {349323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yi:2000:TLR:349299.349323,
 author = {Yi, Qing and Adve, Vikram and Kennedy, Ken},
 title = {Transforming loops to recursion for multi-level memory hierarchies},
 abstract = {Recently, there have been several experimental and theoretical results 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {169--181},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349323},
 doi = {http://doi.acm.org/10.1145/349299.349323},
 acmid = {349323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rugina:2000:SBA:358438.349325,
 author = {Rugina, Radu and Rinard, Martin},
 title = {Symbolic bounds analysis of pointers, array indices, and accessed memory regions},
 abstract = { This paper presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions.  Our
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {182--195},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/358438.349325},
 doi = {http://doi.acm.org/10.1145/358438.349325},
 acmid = {349325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rugina:2000:SBA:349299.349325,
 author = {Rugina, Radu and Rinard, Martin},
 title = {Symbolic bounds analysis of pointers, array indices, and accessed memory regions},
 abstract = { This paper presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions.  Our
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {182--195},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/349299.349325},
 doi = {http://doi.acm.org/10.1145/349299.349325},
 acmid = {349325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sreedhar:2000:FIO:349299.349326,
 author = {Sreedhar, Vugranam C. and Burke, Michael and Choi, Jong-Deok},
 title = {A framework for interprocedural optimization in the presence of dynamic class loading},
 abstract = {Dynamic class loading during program execution in the Java Programming Language is an impediment for generating code that is as efficient as code generated using static whole-program analysis and optimization. Whole-program analysis and optimization is possible for languages, such as C++, that do not allow new classes and/or methods to be  loaded during program execution. One solution for performing whole-program analysis and
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349326},
 doi = {http://doi.acm.org/10.1145/349299.349326},
 acmid = {349326},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sreedhar:2000:FIO:358438.349326,
 author = {Sreedhar, Vugranam C. and Burke, Michael and Choi, Jong-Deok},
 title = {A framework for interprocedural optimization in the presence of dynamic class loading},
 abstract = {Dynamic class loading during program execution in the Java Programming Language is an impediment for generating code that is as efficient as code generated using static whole-program analysis and optimization. Whole-program analysis and optimization is possible for languages, such as C++, that do not allow new classes and/or methods to be  loaded during program execution. One solution for performing whole-program analysis and
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349326},
 doi = {http://doi.acm.org/10.1145/358438.349326},
 acmid = {349326},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ruf:2000:ESR:358438.349327,
 author = {Ruf, Erik},
 title = {Effective synchronization removal for Java},
 abstract = {We present a new technique for removing unnecessary synchronization operations from statically compiled Java programs. Our approach improves upon current efforts based on escape analysis, as it can eliminate synchronization operations even on objects that escape their allocating threads. It makes use of a compact, equivalence-class-based representation that eliminates the need for fixed point operations during the analysis. 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {208--218},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349327},
 doi = {http://doi.acm.org/10.1145/358438.349327},
 acmid = {349327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ruf:2000:ESR:349299.349327,
 author = {Ruf, Erik},
 title = {Effective synchronization removal for Java},
 abstract = {We present a new technique for removing unnecessary synchronization operations from statically compiled Java programs. Our approach improves upon current efforts based on escape analysis, as it can eliminate synchronization operations even on objects that escape their allocating threads. It makes use of a compact, equivalence-class-based representation that eliminates the need for fixed point operations during the analysis. 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {208--218},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349327},
 doi = {http://doi.acm.org/10.1145/349299.349327},
 acmid = {349327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flanagan:2000:TRD:349299.349328,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {Type-based race detection for Java},
 abstract = { This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes thatrequire client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {219--232},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/349299.349328},
 doi = {http://doi.acm.org/10.1145/349299.349328},
 acmid = {349328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flanagan:2000:TRD:358438.349328,
 author = {Flanagan, Cormac and Freund, Stephen N.},
 title = {Type-based race detection for Java},
 abstract = { This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes thatrequire client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {219--232},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/358438.349328},
 doi = {http://doi.acm.org/10.1145/358438.349328},
 acmid = {349328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramalingam:2000:LDD:349299.349330,
 author = {Ramalingam, G.},
 title = {On loops, dominators, and dominance frontier},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {233--241},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/349299.349330},
 doi = {http://doi.acm.org/10.1145/349299.349330},
 acmid = {349330},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramalingam:2000:LDD:358438.349330,
 author = {Ramalingam, G.},
 title = {On loops, dominators, and dominance frontier},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {233--241},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/358438.349330},
 doi = {http://doi.acm.org/10.1145/358438.349330},
 acmid = {349330},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wan:2000:FRP:349299.349331,
 author = {Wan, Zhanyong and Hudak, Paul},
 title = {Functional reactive programming from first principles},
 abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {242--252},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349331},
 doi = {http://doi.acm.org/10.1145/349299.349331},
 acmid = {349331},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wan:2000:FRP:358438.349331,
 author = {Wan, Zhanyong and Hudak, Paul},
 title = {Functional reactive programming from first principles},
 abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {242--252},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349331},
 doi = {http://doi.acm.org/10.1145/358438.349331},
 acmid = {349331},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fahndrich:2000:SCF:349299.349332,
 author = {F\"{a}hndrich, Manuel and Rehof, Jakob and Das, Manuvir},
 title = {Scalable context-sensitive flow analysis using instantiation constraints},
 abstract = {This paper shows that a type graph (obtained via polymorphic type
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {253--263},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349332},
 doi = {http://doi.acm.org/10.1145/349299.349332},
 acmid = {349332},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fahndrich:2000:SCF:358438.349332,
 author = {F\"{a}hndrich, Manuel and Rehof, Jakob and Das, Manuvir},
 title = {Scalable context-sensitive flow analysis using instantiation constraints},
 abstract = {This paper shows that a type graph (obtained via polymorphic type
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {253--263},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349332},
 doi = {http://doi.acm.org/10.1145/358438.349332},
 acmid = {349332},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cannarozzi:2000:CGC:358438.349334,
 author = {Cannarozzi, Dante J. and Plezbert, Michael P. and Cytron, Ron K.},
 title = {Contaminated garbage collection},
 abstract = {We describe a new method for determining when an object can be
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/358438.349334},
 doi = {http://doi.acm.org/10.1145/358438.349334},
 acmid = {349334},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cannarozzi:2000:CGC:349299.349334,
 author = {Cannarozzi, Dante J. and Plezbert, Michael P. and Cytron, Ron K.},
 title = {Contaminated garbage collection},
 abstract = {We describe a new method for determining when an object can be
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/349299.349334},
 doi = {http://doi.acm.org/10.1145/349299.349334},
 acmid = {349334},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Domani:2000:GOG:349299.349336,
 author = {Domani, Tamar and Kolodner, Elliot K. and Petrank, Erez},
 title = {A generational on-the-fly garbage collector for Java},
 abstract = {An on-the-fly garbage collector does not stop the program threads to perform the collection. Instead, the collector executes in a separate thread (or process) in parallel to the program. On-the-fly collectors are useful for multi-threaded applications running on multiprocessor servers, where it is important to fully utilize all processors and provide even response time, especially for systems for which 
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349336},
 doi = {http://doi.acm.org/10.1145/349299.349336},
 acmid = {349336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, generational garbage collection, memory management, programming languages},
} 

@article{Domani:2000:GOG:358438.349336,
 author = {Domani, Tamar and Kolodner, Elliot K. and Petrank, Erez},
 title = {A generational on-the-fly garbage collector for Java},
 abstract = {An on-the-fly garbage collector does not stop the program threads to perform the collection. Instead, the collector executes in a separate thread (or process) in parallel to the program. On-the-fly collectors are useful for multi-threaded applications running on multiprocessor servers, where it is important to fully utilize all processors and provide even response time, especially for systems for which 
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349336},
 doi = {http://doi.acm.org/10.1145/358438.349336},
 acmid = {349336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, generational garbage collection, memory management, programming languages},
} 

@article{Ramsey:2000:SIL:358438.349337,
 author = {Ramsey, Norman and Jones, Simon Peyton},
 title = {A single intermediate language that supports multiple implementations of exceptions},
 abstract = {We present mechanisms that enable our compiler-target language, C--, to express four of the best known techniques for implementing exceptions, all within a single, uniform framework. We define the mechanisms precisely, using a formal operational semantics. We also show that exceptions need not require special treatment in the optimizer; by introducing extra dataflow edges, we make standard optimization techniques work even on programs that use exceptions. Our approach clarifies the design space of exception-handling techniques, and it allows a single optimizer to handle a variety of implementation techniques. Our ultimate goal is to allow a source-language compiler the freedom to choose its exception-handling policy, while encapsulating the architecture-dependent mechanisms and their  optimization in an implementation of C--that can be used by compilers for many source languages.},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {285--298},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/358438.349337},
 doi = {http://doi.acm.org/10.1145/358438.349337},
 acmid = {349337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramsey:2000:SIL:349299.349337,
 author = {Ramsey, Norman and Jones, Simon Peyton},
 title = {A single intermediate language that supports multiple implementations of exceptions},
 abstract = {We present mechanisms that enable our compiler-target language, C--, to express four of the best known techniques for implementing exceptions, all within a single, uniform framework. We define the mechanisms precisely, using a formal operational semantics. We also show that exceptions need not require special treatment in the optimizer; by introducing extra dataflow edges, we make standard optimization techniques work even on programs that use exceptions. Our approach clarifies the design space of exception-handling techniques, and it allows a single optimizer to handle a variety of implementation techniques. Our ultimate goal is to allow a source-language compiler the freedom to choose its exception-handling policy, while encapsulating the architecture-dependent mechanisms and their  optimization in an implementation of C--that can be used by compilers for many source languages.},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {285--298},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/349299.349337},
 doi = {http://doi.acm.org/10.1145/349299.349337},
 acmid = {349337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boothe:2000:EAB:358438.349339,
 author = {Boothe, Bob},
 title = {Efficient algorithms for bidirectional debugging},
 abstract = {This paper discusses our research into algorithms for creating an
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/358438.349339},
 doi = {http://doi.acm.org/10.1145/358438.349339},
 acmid = {349339},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boothe:2000:EAB:349299.349339,
 author = {Boothe, Bob},
 title = {Efficient algorithms for bidirectional debugging},
 abstract = {This paper discusses our research into algorithms for creating an
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/349299.349339},
 doi = {http://doi.acm.org/10.1145/349299.349339},
 acmid = {349339},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heydon:2000:CFC:358438.349341,
 author = {Heydon, Allan and Levin, Roy and Yu, Yuan},
 title = {Caching function calls using precise dependencies},
 abstract = {This paper describes the implementation of a purely functional
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {311--320},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/358438.349341},
 doi = {http://doi.acm.org/10.1145/358438.349341},
 acmid = {349341},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heydon:2000:CFC:349299.349341,
 author = {Heydon, Allan and Levin, Roy and Yu, Yuan},
 title = {Caching function calls using precise dependencies},
 abstract = {This paper describes the implementation of a purely functional
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {311--320},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/349299.349341},
 doi = {http://doi.acm.org/10.1145/349299.349341},
 acmid = {349341},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bodik:2000:AEA:358438.349342,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Sarkar, Vivek},
 title = {ABCD: eliminating array bounds checks on demand},
 abstract = {To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting.
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {321--333},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349342},
 doi = {http://doi.acm.org/10.1145/358438.349342},
 acmid = {349342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bodik:2000:AEA:349299.349342,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Sarkar, Vivek},
 title = {ABCD: eliminating array bounds checks on demand},
 abstract = {To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting.
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {321--333},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349342},
 doi = {http://doi.acm.org/10.1145/349299.349342},
 acmid = {349342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghemawat:2000:FAG:358438.349343,
 author = {Ghemawat, Sanjay and Randall, Keith H. and Scales, Daniel J.},
 title = {Field analysis: getting useful and low-cost interprocedural information},
 abstract = {We present a new limited form of interprocedural analysis called   field analysis that can be used by a compiler to reduce the costs of modern language features such as object-oriented programming,
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {334--344},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/358438.349343},
 doi = {http://doi.acm.org/10.1145/358438.349343},
 acmid = {349343},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghemawat:2000:FAG:349299.349343,
 author = {Ghemawat, Sanjay and Randall, Keith H. and Scales, Daniel J.},
 title = {Field analysis: getting useful and low-cost interprocedural information},
 abstract = {We present a new limited form of interprocedural analysis called   field analysis that can be used by a compiler to reduce the costs of modern language features such as object-oriented programming,
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {334--344},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/349299.349343},
 doi = {http://doi.acm.org/10.1145/349299.349343},
 acmid = {349343},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dolby:2000:AOI:358438.349344,
 author = {Dolby, Julian and Chien, Andrew},
 title = {An automatic object inlining optimization and its evaluation},
 abstract = {Automatic object inlining [19, 20] transforms heap
},
 journal = {SIGPLAN Not.},
 volume = {35},
 issue = {5},
 month = {May},
 year = {2000},
 issn = {0362-1340},
 pages = {345--357},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/358438.349344},
 doi = {http://doi.acm.org/10.1145/358438.349344},
 acmid = {349344},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dolby:2000:AOI:349299.349344,
 author = {Dolby, Julian and Chien, Andrew},
 title = {An automatic object inlining optimization and its evaluation},
 abstract = {Automatic object inlining [19, 20] transforms heap
},
 booktitle = {Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
 series = {PLDI '00},
 year = {2000},
 isbn = {1-58113-199-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {345--357},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/349299.349344},
 doi = {http://doi.acm.org/10.1145/349299.349344},
 acmid = {349344},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bodik:1998:CRR:277650.277653,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Complete removal of redundant expressions},
 abstract = {Partial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loop-invariant computations. Because existing PRE implementations are based on code motion</i>, they fail to completely remove the redundancies. In fact, we observed that 73\% of loop-invariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete</i> PRE, control flow restructuring</i> must be applied. However, the resulting code duplication may cause code size explosion.This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete</i> removal of partial redundancies, based on the integration</i> of code motion and control flow restructuring. In contrast to existing complete techniques, we resort to restructuring merely to remove obstacles to code motion, rather than to carry out the actual optimization.Guiding the optimization with a profile enables additional code growth reduction through selecting those duplications whose cost is justified by sufficient execution-time gains. The paper develops two methods for determining the optimization benefit of restructuring a program region, one based on path-profiles and the other on data-flow frequency analysis. Furthermore, the abstraction underlying the new PRE algorithm enables a simple formulation of speculative code motion guaranteed to have positive dynamic improvements. Finally, we show how to balance the three transformations (code motion, restructuring, and speculation) to achieve a near-complete PRE with very little code growth.We also present algorithms for efficiently computing dynamic benefits. In particular, using an elimination-style data-flow framework, we derive a demand-driven frequency analyzer whose cost can be controlled by permitting a bounded degree of conservative imprecision in the solution.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/277650.277653},
 doi = {http://doi.acm.org/10.1145/277650.277653},
 acmid = {277653},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control flow restructuring, demand-driven frequency data-flow analysis, partial redundancy elimination, profile-guided optimization, speculative execution},
} 

@article{Bodik:1998:CRR:277652.277653,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Complete removal of redundant expressions},
 abstract = {Partial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loop-invariant computations. Because existing PRE implementations are based on code motion</i>, they fail to completely remove the redundancies. In fact, we observed that 73\% of loop-invariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete</i> PRE, control flow restructuring</i> must be applied. However, the resulting code duplication may cause code size explosion.This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete</i> removal of partial redundancies, based on the integration</i> of code motion and control flow restructuring. In contrast to existing complete techniques, we resort to restructuring merely to remove obstacles to code motion, rather than to carry out the actual optimization.Guiding the optimization with a profile enables additional code growth reduction through selecting those duplications whose cost is justified by sufficient execution-time gains. The paper develops two methods for determining the optimization benefit of restructuring a program region, one based on path-profiles and the other on data-flow frequency analysis. Furthermore, the abstraction underlying the new PRE algorithm enables a simple formulation of speculative code motion guaranteed to have positive dynamic improvements. Finally, we show how to balance the three transformations (code motion, restructuring, and speculation) to achieve a near-complete PRE with very little code growth.We also present algorithms for efficiently computing dynamic benefits. In particular, using an elimination-style data-flow framework, we derive a demand-driven frequency analyzer whose cost can be controlled by permitting a bounded degree of conservative imprecision in the solution.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/277652.277653},
 doi = {http://doi.acm.org/10.1145/277652.277653},
 acmid = {277653},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control flow restructuring, demand-driven frequency data-flow analysis, partial redundancy elimination, profile-guided optimization, speculative execution},
} 

@article{Sastry:1998:NAS:277652.277656,
 author = {Sastry, A. V. S. and Ju, Roy D. C.},
 title = {A new algorithm for scalar register promotion based on SSA form},
 abstract = {We present a new register promotion algorithm based on Static Single Assignment (SSA) form. Register promotion is aimed at promoting program names from memory locations to registers. Our algorithm is profile-driven and is based on the scope of intervals. In cases where a complete promotion is not possible because of the presence of function calls or pointer references, the proposed algorithm is capable of eliminating loads and stores on frequently executed paths by placing loads and stores on less frequently executed paths. We also describe an efficient method to incrementally update SSA form when new definitions are cloned from an existing name during register promotion. On SPECInt95 benchmarks, our algorithm removes about ~12\% of memory operations which access scalar variables.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {15--25},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277652.277656},
 doi = {http://doi.acm.org/10.1145/277652.277656},
 acmid = {277656},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sastry:1998:NAS:277650.277656,
 author = {Sastry, A. V. S. and Ju, Roy D. C.},
 title = {A new algorithm for scalar register promotion based on SSA form},
 abstract = {We present a new register promotion algorithm based on Static Single Assignment (SSA) form. Register promotion is aimed at promoting program names from memory locations to registers. Our algorithm is profile-driven and is based on the scope of intervals. In cases where a complete promotion is not possible because of the presence of function calls or pointer references, the proposed algorithm is capable of eliminating loads and stores on frequently executed paths by placing loads and stores on less frequently executed paths. We also describe an efficient method to incrementally update SSA form when new definitions are cloned from an existing name during register promotion. On SPECInt95 benchmarks, our algorithm removes about ~12\% of memory operations which access scalar variables.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {15--25},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277650.277656},
 doi = {http://doi.acm.org/10.1145/277650.277656},
 acmid = {277656},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lo:1998:RPS:277652.277659,
 author = {Lo, Raymond and Chow, Fred and Kennedy, Robert and Liu, Shin-Ming and Tu, Peng},
 title = {Register promotion by sparse partial redundancy elimination of loads and stores},
 abstract = {An algorithm for register promotion is presented based on the observation that the circumstances for promoting a memory location's value to register coincide with situations where the program exhibits partial redundancy between accesses to the memory location. The recent SSAPRE algorithm for eliminating partial redundancy using a sparse SSA representation forms the foundation for the present algorithm to eliminate redundancy among memory accesses, enabling us to achieve both computational and live range optimality in our register promotion results. We discuss how to effect speculative code motion in the SSAPRE framework. We present two different algorithms for performing speculative code motion: the conservative</i> speculation algorithm used in the absence of profile data, and the the profile-driven</i> speculation algorithm used when profile data are available. We define the static single use (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy elimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the effectiveness of our register promotion approach in removing loads and stores. We also study the relative performance of the different speculative code motion strategies when applied to scalar loads and stores.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277659},
 doi = {http://doi.acm.org/10.1145/277652.277659},
 acmid = {277659},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lo:1998:RPS:277650.277659,
 author = {Lo, Raymond and Chow, Fred and Kennedy, Robert and Liu, Shin-Ming and Tu, Peng},
 title = {Register promotion by sparse partial redundancy elimination of loads and stores},
 abstract = {An algorithm for register promotion is presented based on the observation that the circumstances for promoting a memory location's value to register coincide with situations where the program exhibits partial redundancy between accesses to the memory location. The recent SSAPRE algorithm for eliminating partial redundancy using a sparse SSA representation forms the foundation for the present algorithm to eliminate redundancy among memory accesses, enabling us to achieve both computational and live range optimality in our register promotion results. We discuss how to effect speculative code motion in the SSAPRE framework. We present two different algorithms for performing speculative code motion: the conservative</i> speculation algorithm used in the absence of profile data, and the the profile-driven</i> speculation algorithm used when profile data are available. We define the static single use (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy elimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the effectiveness of our register promotion approach in removing loads and stores. We also study the relative performance of the different speculative code motion strategies when applied to scalar loads and stores.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277659},
 doi = {http://doi.acm.org/10.1145/277650.277659},
 acmid = {277659},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rivera:1998:DTE:277650.277661,
 author = {Rivera, Gabriel and Tseng, Chau-Wen},
 title = {Data transformations for eliminating conflict misses},
 abstract = {Many cache misses in scientific programs are due to conflicts caused by limited set associativity. We examine two compile-time data-layout transformations for eliminating conflict misses, concentrating on misses occuring on every loop iteration. Inter-variable padding adjusts variable base addresses, while intra-variable padding modifies array dimension sizes. Two levels of precision are evaluated. PADLITE only uses array and column dimension sizes, relying on assumptions about common array reference patterns. PAD analyzes programs, detecting conflict misses by linearizing array references and calculating conflict distances</i> between uniformly-generated references. The Euclidean algorithm for computing the gcd</i> of two numbers is used to predict conflicts between different array columns for linear algebra codes. Experiments on a range of programs indicate PADLITE can eliminate conflicts for benchmarks, but PAD is more effective over a range of cache and problem sizes. Padding reduces cache miss rates by 16\% on average for a 16K direct-mapped cache. Execution times are reduced by 6\% on average, with some SPEC95 programs improving up to 15\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277661},
 doi = {http://doi.acm.org/10.1145/277650.277661},
 acmid = {277661},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rivera:1998:DTE:277652.277661,
 author = {Rivera, Gabriel and Tseng, Chau-Wen},
 title = {Data transformations for eliminating conflict misses},
 abstract = {Many cache misses in scientific programs are due to conflicts caused by limited set associativity. We examine two compile-time data-layout transformations for eliminating conflict misses, concentrating on misses occuring on every loop iteration. Inter-variable padding adjusts variable base addresses, while intra-variable padding modifies array dimension sizes. Two levels of precision are evaluated. PADLITE only uses array and column dimension sizes, relying on assumptions about common array reference patterns. PAD analyzes programs, detecting conflict misses by linearizing array references and calculating conflict distances</i> between uniformly-generated references. The Euclidean algorithm for computing the gcd</i> of two numbers is used to predict conflicts between different array columns for linear algebra codes. Experiments on a range of programs indicate PADLITE can eliminate conflicts for benchmarks, but PAD is more effective over a range of cache and problem sizes. Padding reduces cache miss rates by 16\% on average for a 16K direct-mapped cache. Execution times are reduced by 6\% on average, with some SPEC95 programs improving up to 15\%.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277661},
 doi = {http://doi.acm.org/10.1145/277652.277661},
 acmid = {277661},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lewis:1998:IEF:277650.277663,
 author = {Lewis, E. Christopher and Lin, Calvin and Snyder, Lawrence},
 title = {The implementation and evaluation of fusion and contraction in array languages},
 abstract = {Array languages such as Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays---both at the source level and during the compilation process---which increase memory usage and pollute the cache. Most compilers address this problem by simply scalarizing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction at the array level</i>. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20\% and sometimes up to 400\%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277650.277663},
 doi = {http://doi.acm.org/10.1145/277650.277663},
 acmid = {277663},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lewis:1998:IEF:277652.277663,
 author = {Lewis, E. Christopher and Lin, Calvin and Snyder, Lawrence},
 title = {The implementation and evaluation of fusion and contraction in array languages},
 abstract = {Array languages such as Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays---both at the source level and during the compilation process---which increase memory usage and pollute the cache. Most compilers address this problem by simply scalarizing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction at the array level</i>. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20\% and sometimes up to 400\%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277652.277663},
 doi = {http://doi.acm.org/10.1145/277652.277663},
 acmid = {277663},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Paek:1998:SAA:277652.277664,
 author = {Paek, Yunheung and Hoeflinger, Jay and Padua, David},
 title = {Simplification of array access patterns for compiler optimizations},
 abstract = {Existing array region representation techniques are sensitive to the complexity of array subscripts. In general, these techniques are very accurate and efficient for simple subscript expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. We found that in scientific applications, many access patterns are simple even when the subscript expressions are complex. In this work, we present a new, general array access representation and define operations for it. This allows us to aggregate and simplify the representation enough that precise region operations may be applied to enable compiler optimizations. Our experiments show that these techniques hold promise for speeding up applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {60--71},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277664},
 doi = {http://doi.acm.org/10.1145/277652.277664},
 acmid = {277664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Paek:1998:SAA:277650.277664,
 author = {Paek, Yunheung and Hoeflinger, Jay and Padua, David},
 title = {Simplification of array access patterns for compiler optimizations},
 abstract = {Existing array region representation techniques are sensitive to the complexity of array subscripts. In general, these techniques are very accurate and efficient for simple subscript expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. We found that in scientific applications, many access patterns are simple even when the subscript expressions are complex. In this work, we present a new, general array access representation and define operations for it. This allows us to aggregate and simplify the representation enough that precise region operations may be applied to enable compiler optimizations. Our experiments show that these techniques hold promise for speeding up applications.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {60--71},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277664},
 doi = {http://doi.acm.org/10.1145/277650.277664},
 acmid = {277664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ammons:1998:IDA:277652.277665,
 author = {Ammons, Glenn and Larus, James R.},
 title = {Improving data-flow analysis with path profiles},
 abstract = {Data-flow analysis computes its solutions over the paths in a control-flow graph. These paths---whether feasible or infeasible, heavily or rarely executed---contribute equally to a solution. However, programs execute only a small fraction of their potential paths and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths</i>.This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a hot path graph</i> in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2--112 times more non-local constants (weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into 1--7\% more dynamic instructions with constant results.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277652.277665},
 doi = {http://doi.acm.org/10.1145/277652.277665},
 acmid = {277665},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ammons:1998:IDA:277650.277665,
 author = {Ammons, Glenn and Larus, James R.},
 title = {Improving data-flow analysis with path profiles},
 abstract = {Data-flow analysis computes its solutions over the paths in a control-flow graph. These paths---whether feasible or infeasible, heavily or rarely executed---contribute equally to a solution. However, programs execute only a small fraction of their potential paths and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths</i>.This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a hot path graph</i> in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2--112 times more non-local constants (weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into 1--7\% more dynamic instructions with constant results.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277650.277665},
 doi = {http://doi.acm.org/10.1145/277650.277665},
 acmid = {277665},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fahndrich:1998:POC:277652.277667,
 author = {F\"{a}hndrich, Manuel and Foster, Jeffrey S. and Su, Zhendong and Aiken, Alexander},
 title = {Partial online cycle elimination in inclusion constraint graphs},
 abstract = {Many program analyses are naturally formulated and implemented using inclusion constraints. We present new results on the scalable implementation of such analyses based on two insights: first, that online elimination of cyclic constraints yields orders-of-magnitude improvements in analysis time for large problems; second, that the choice of constraint representation affects the quality and efficiency of online cycle elimination. We present an analytical model that explains our design choices and show that the model's predictions match well with results from a substantial experiment.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277667},
 doi = {http://doi.acm.org/10.1145/277652.277667},
 acmid = {277667},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fahndrich:1998:POC:277650.277667,
 author = {F\"{a}hndrich, Manuel and Foster, Jeffrey S. and Su, Zhendong and Aiken, Alexander},
 title = {Partial online cycle elimination in inclusion constraint graphs},
 abstract = {Many program analyses are naturally formulated and implemented using inclusion constraints. We present new results on the scalable implementation of such analyses based on two insights: first, that online elimination of cyclic constraints yields orders-of-magnitude improvements in analysis time for large problems; second, that the choice of constraint representation affects the quality and efficiency of online cycle elimination. We present an analytical model that explains our design choices and show that the model's predictions match well with results from a substantial experiment.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277667},
 doi = {http://doi.acm.org/10.1145/277650.277667},
 acmid = {277667},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hasti:1998:USS:277650.277668,
 author = {Hasti, Rebecca and Horwitz, Susan},
 title = {Using static single assignment form to improve flow-insensitive pointer analysis},
 abstract = {A pointer-analysis algorithm can be either flow-sensitive or flow-insensitive. While flow-sensitive analysis usually provides more precise information, it is also usually considerably more costly in terms of time and space. The main contribution of this paper is the presentation of another option in the form of an algorithm that can be 'tuned' to provide a range of results that fall between the results of flow-insensitive and flow-sensitive analysis. The algorithm combines a flow-insensitive pointer analysis with static single assignment (SSA) form and uses an iterative process to obtain progressively better results.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {97--105},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277650.277668},
 doi = {http://doi.acm.org/10.1145/277650.277668},
 acmid = {277668},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hasti:1998:USS:277652.277668,
 author = {Hasti, Rebecca and Horwitz, Susan},
 title = {Using static single assignment form to improve flow-insensitive pointer analysis},
 abstract = {A pointer-analysis algorithm can be either flow-sensitive or flow-insensitive. While flow-sensitive analysis usually provides more precise information, it is also usually considerably more costly in terms of time and space. The main contribution of this paper is the presentation of another option in the form of an algorithm that can be 'tuned' to provide a range of results that fall between the results of flow-insensitive and flow-sensitive analysis. The algorithm combines a flow-insensitive pointer analysis with static single assignment (SSA) form and uses an iterative process to obtain progressively better results.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {97--105},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277652.277668},
 doi = {http://doi.acm.org/10.1145/277652.277668},
 acmid = {277668},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Diwan:1998:TAA:277650.277670,
 author = {Diwan, Amer and McKinley, Kathryn S. and Moss, J. Eliot B.},
 title = {Type-based alias analysis},
 abstract = {This paper evaluates three alias analyses based on programming language types. The first analysis uses type compatibility to determine aliases. The second extends the first by using additional high-level information such as field names. The third extends the second with a flow-insensitive analysis. Although other researchers suggests using types to disambiguate memory references, none evaluates its effectiveness. We perform both static and dynamic evaluations of type-based alias analyses for Modula-3, a statically-typed type-safe language. The static analysis reveals that type compatibility alone yields a very imprecise alias analysis, but the other two analyses significantly improve alias precision. We use redundant load elimination (RLE) to demonstrate the effectiveness of the three alias algorithms in terms of the opportunities for optimization, the impact on simulated execution times, and to compute an upper bound on what a perfect alias analysis would yield. We show modest dynamic improvements for (RLE), and more surprisingly, that on average our alias analysis is within 2.5\% of a perfect alias analysis with respect to RLE on 8 Modula-3 programs. These results illustrate that to explore thoroughly the effectiveness of alias analyses, researchers need static, dynamic, and upper-bound analysis. In addition, we show that for type-safe languages like Modula-3 and Java, a fast and simple alias analysis may be sufficient for many applications.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {106--117},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277670},
 doi = {http://doi.acm.org/10.1145/277650.277670},
 acmid = {277670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Diwan:1998:TAA:277652.277670,
 author = {Diwan, Amer and McKinley, Kathryn S. and Moss, J. Eliot B.},
 title = {Type-based alias analysis},
 abstract = {This paper evaluates three alias analyses based on programming language types. The first analysis uses type compatibility to determine aliases. The second extends the first by using additional high-level information such as field names. The third extends the second with a flow-insensitive analysis. Although other researchers suggests using types to disambiguate memory references, none evaluates its effectiveness. We perform both static and dynamic evaluations of type-based alias analyses for Modula-3, a statically-typed type-safe language. The static analysis reveals that type compatibility alone yields a very imprecise alias analysis, but the other two analyses significantly improve alias precision. We use redundant load elimination (RLE) to demonstrate the effectiveness of the three alias algorithms in terms of the opportunities for optimization, the impact on simulated execution times, and to compute an upper bound on what a perfect alias analysis would yield. We show modest dynamic improvements for (RLE), and more surprisingly, that on average our alias analysis is within 2.5\% of a perfect alias analysis with respect to RLE on 8 Modula-3 programs. These results illustrate that to explore thoroughly the effectiveness of alias analyses, researchers need static, dynamic, and upper-bound analysis. In addition, we show that for type-safe languages like Modula-3 and Java, a fast and simple alias analysis may be sufficient for many applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {106--117},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277670},
 doi = {http://doi.acm.org/10.1145/277652.277670},
 acmid = {277670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sastry:1998:EIF:277652.277709,
 author = {Sastry, S. Subramanya and Palacharla, Subbarao and Smith, James E.},
 title = {Exploiting idle floating-point resources for integer execution},
 abstract = {In conventional superscalar microarchitectures with partitioned integer and floating-point resources, all floating-point resources are idle during execution of integer programs. Palacharla and Smith [26] addressed this drawback and proposed that the floating-point subsystem be augmented to support integer operations. The hardware changes required are expected to be fairly minimal.To exploit these idle floating resources, the compiler must identify integer code that can be profitably offloaded to the augmented floating-point subsystem. In this paper, we present two compiler algorithms to do this. The basic</i> scheme offloads integer computation to the floating-point subsystem using existing program loads/stores for inter-partition communication. For the SPECINT95 benchmarks, we show that this scheme offloads from 5\% to 29\% of the total dynamic instructions to the floating-point subsystem. The advanced</i> scheme inserts copy instructions and duplicates some instructions to further offload computation. We evaluate the effectiveness of the two schemes using timing simulation. We show that the advanced scheme can offload from 9\% to 41\% of the total dynamic instructions to the floating-point subsystem. In doing so, speedups from 3\% to 23\% are achieved over a conventional microarchitecture.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277709},
 doi = {http://doi.acm.org/10.1145/277652.277709},
 acmid = {277709},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sastry:1998:EIF:277650.277709,
 author = {Sastry, S. Subramanya and Palacharla, Subbarao and Smith, James E.},
 title = {Exploiting idle floating-point resources for integer execution},
 abstract = {In conventional superscalar microarchitectures with partitioned integer and floating-point resources, all floating-point resources are idle during execution of integer programs. Palacharla and Smith [26] addressed this drawback and proposed that the floating-point subsystem be augmented to support integer operations. The hardware changes required are expected to be fairly minimal.To exploit these idle floating resources, the compiler must identify integer code that can be profitably offloaded to the augmented floating-point subsystem. In this paper, we present two compiler algorithms to do this. The basic</i> scheme offloads integer computation to the floating-point subsystem using existing program loads/stores for inter-partition communication. For the SPECINT95 benchmarks, we show that this scheme offloads from 5\% to 29\% of the total dynamic instructions to the floating-point subsystem. The advanced</i> scheme inserts copy instructions and duplicates some instructions to further offload computation. We evaluate the effectiveness of the two schemes using timing simulation. We show that the advanced scheme can offload from 9\% to 41\% of the total dynamic instructions to the floating-point subsystem. In doing so, speedups from 3\% to 23\% are achieved over a conventional microarchitecture.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277709},
 doi = {http://doi.acm.org/10.1145/277650.277709},
 acmid = {277709},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:1998:IPB:277650.277711,
 author = {Yang, Minghui and Uh, Gang-Ryung and Whalley, David B.},
 title = {Improving performance by branch reordering},
 abstract = {The conditional branch has long been considered an expensive operation. The relative cost of conditional branches has increased as recently designed machines are now relying on deeper pipelines and higher multiple issue. Reducing the number of conditional branches executed can often result in a substantial performance benefit. This paper describes a code-improving transformation to reorder sequences of conditional branches. First, sequences of branches that can be reordered are detected in the control flow. Second, profiling information is collected to predict the probability that each branch will transfer control out of the sequence. Third, the cost of performing each conditional branch is estimated. Fourth, the most beneficial ordering of the branches based on the estimated probability and cost is selected. The most beneficial ordering often included the insertion of additional conditional branches that did not previously exist in the sequence. Finally, the control flow is restructured to refflect the new ordering. The results of applying the transformation were significant reductions in the dynamic number of instructions and branches, as well as decreases in execution time.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {130--141},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277711},
 doi = {http://doi.acm.org/10.1145/277650.277711},
 acmid = {277711},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yang:1998:IPB:277652.277711,
 author = {Yang, Minghui and Uh, Gang-Ryung and Whalley, David B.},
 title = {Improving performance by branch reordering},
 abstract = {The conditional branch has long been considered an expensive operation. The relative cost of conditional branches has increased as recently designed machines are now relying on deeper pipelines and higher multiple issue. Reducing the number of conditional branches executed can often result in a substantial performance benefit. This paper describes a code-improving transformation to reorder sequences of conditional branches. First, sequences of branches that can be reordered are detected in the control flow. Second, profiling information is collected to predict the probability that each branch will transfer control out of the sequence. Third, the cost of performing each conditional branch is estimated. Fourth, the most beneficial ordering of the branches based on the estimated probability and cost is selected. The most beneficial ordering often included the insertion of additional conditional branches that did not previously exist in the sequence. Finally, the control flow is restructured to refflect the new ordering. The results of applying the transformation were significant reductions in the dynamic number of instructions and branches, as well as decreases in execution time.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {130--141},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277711},
 doi = {http://doi.acm.org/10.1145/277652.277711},
 acmid = {277711},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Traub:1998:QSL:277652.277714,
 author = {Traub, Omri and Holloway, Glenn and Smith, Michael D.},
 title = {Quality and speed in linear-scan register allocation},
 abstract = {A linear-scan</i> algorithm directs the global allocation of register candidates to registers based on a simple linear sweep over the program being compiled. This approach to register allocation makes sense for systems, such as those for dynamic compilation, where compilation speed is important. In contrast, most commercial and research optimizing compilers rely on a graph-coloring approach to global register allocation. In this paper, we compare the performance of a linear-scan method against a modern graph-coloring method. We implement both register allocators within the Machine SUIF extension of the Stanford SUIF compiler system. Experimental results show that linear scan is much faster than coloring on benchmarks with large numbers of register candidates. We also describe improvements to the linear-scan approach that do not change its linear character, but allow it to produce code of a quality near to that produced by graph coloring.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277652.277714},
 doi = {http://doi.acm.org/10.1145/277652.277714},
 acmid = {277714},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binpacking, global register allocation, graph coloring, linear scan},
} 

@inproceedings{Traub:1998:QSL:277650.277714,
 author = {Traub, Omri and Holloway, Glenn and Smith, Michael D.},
 title = {Quality and speed in linear-scan register allocation},
 abstract = {A linear-scan</i> algorithm directs the global allocation of register candidates to registers based on a simple linear sweep over the program being compiled. This approach to register allocation makes sense for systems, such as those for dynamic compilation, where compilation speed is important. In contrast, most commercial and research optimizing compilers rely on a graph-coloring approach to global register allocation. In this paper, we compare the performance of a linear-scan method against a modern graph-coloring method. We implement both register allocators within the Machine SUIF extension of the Stanford SUIF compiler system. Experimental results show that linear scan is much faster than coloring on benchmarks with large numbers of register candidates. We also describe improvements to the linear-scan approach that do not change its linear character, but allow it to produce code of a quality near to that produced by graph coloring.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277650.277714},
 doi = {http://doi.acm.org/10.1145/277650.277714},
 acmid = {277714},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binpacking, global register allocation, graph coloring, linear scan},
} 

@article{Le Fessant:1998:ICA:277652.277715,
 author = {Le Fessant, Fabrice and Piumarta, Ian and Shapiro, Marc},
 title = {An implementation of complete, asynchronous, distributed garbage collection},
 abstract = {Most existing reference-based distributed object systems include some kind of acyclic garbage collection, but fail to provide acceptable collection of cyclic garbage. Those that do provide such GC currently suffer from one or more problems: synchronous operation, the need for expensive global consensus or termination algorithms, susceptibility to communication problems, or an algorithm that does not scale. We present a simple, complete, fault-tolerant, asynchronous extension to the (acyclic) cleanup protocol of the SSP Chains system. This extension is scalable, consumes few resources, and could easily be adapted to work in other reference-based distributed object systems---rendering them usable for very large-scale applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {152--161},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277652.277715},
 doi = {http://doi.acm.org/10.1145/277652.277715},
 acmid = {277715},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed object systems, garbage collection, reference tracking, storage management},
} 

@inproceedings{Le Fessant:1998:ICA:277650.277715,
 author = {Le Fessant, Fabrice and Piumarta, Ian and Shapiro, Marc},
 title = {An implementation of complete, asynchronous, distributed garbage collection},
 abstract = {Most existing reference-based distributed object systems include some kind of acyclic garbage collection, but fail to provide acceptable collection of cyclic garbage. Those that do provide such GC currently suffer from one or more problems: synchronous operation, the need for expensive global consensus or termination algorithms, susceptibility to communication problems, or an algorithm that does not scale. We present a simple, complete, fault-tolerant, asynchronous extension to the (acyclic) cleanup protocol of the SSP Chains system. This extension is scalable, consumes few resources, and could easily be adapted to work in other reference-based distributed object systems---rendering them usable for very large-scale applications.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {152--161},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277650.277715},
 doi = {http://doi.acm.org/10.1145/277650.277715},
 acmid = {277715},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed object systems, garbage collection, reference tracking, storage management},
} 

@inproceedings{Cheng:1998:GSC:277650.277718,
 author = {Cheng, Perry and Harper, Robert and Lee, Peter},
 title = {Generational stack collection and profile-driven pretenuring},
 abstract = {This paper presents two techniques for improving garbage collection performance: generational stack collection and profile-driven pretenuring. The first is applicable to stack-based implementations of functional languages while the second is useful for any generational collector. We have implemented both techniques in a generational collector used by the TIL compiler (Tarditi, Morrisett, Cheng, Stone, Harper, and Lee 1996), and have observed decreases in garbage collection times of as much as 70\% and 30\%, respectively.Functional languages encourage the use of recursion which can lead to a long chain of activation records. When a collection occurs, these activation records must be scanned for roots. We show that scanning many activation records can take so long as to become the dominant cost of garbage collection. However, most deep stacks unwind very infrequently, so most of the root information obtained from the stack remains unchanged across successive garbage collections. Generational stack collection</i> greatly reduces the stack scan cost by reusing information from previous scans.Generational techniques have been successful in reducing the cost of garbage collection (Ungar 1984). Various complex heap arrangements and tenuring policies have been proposed to increase the effectiveness of generational techniques by reducing the cost and frequency of scanning and copying. In contrast, we show that by using profile information to make lifetime predictions, pretenuring</i> can avoid copying data altogether. In essence, this technique uses a refinement of the generational hypothesis (most data die young) with a locality principle concerning the age of data: most allocations sites produce data that immediately dies, while a few allocation sites consistently produce data that survives many collections.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {162--173},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277718},
 doi = {http://doi.acm.org/10.1145/277650.277718},
 acmid = {277718},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cheng:1998:GSC:277652.277718,
 author = {Cheng, Perry and Harper, Robert and Lee, Peter},
 title = {Generational stack collection and profile-driven pretenuring},
 abstract = {This paper presents two techniques for improving garbage collection performance: generational stack collection and profile-driven pretenuring. The first is applicable to stack-based implementations of functional languages while the second is useful for any generational collector. We have implemented both techniques in a generational collector used by the TIL compiler (Tarditi, Morrisett, Cheng, Stone, Harper, and Lee 1996), and have observed decreases in garbage collection times of as much as 70\% and 30\%, respectively.Functional languages encourage the use of recursion which can lead to a long chain of activation records. When a collection occurs, these activation records must be scanned for roots. We show that scanning many activation records can take so long as to become the dominant cost of garbage collection. However, most deep stacks unwind very infrequently, so most of the root information obtained from the stack remains unchanged across successive garbage collections. Generational stack collection</i> greatly reduces the stack scan cost by reusing information from previous scans.Generational techniques have been successful in reducing the cost of garbage collection (Ungar 1984). Various complex heap arrangements and tenuring policies have been proposed to increase the effectiveness of generational techniques by reducing the cost and frequency of scanning and copying. In contrast, we show that by using profile information to make lifetime predictions, pretenuring</i> can avoid copying data altogether. In essence, this technique uses a refinement of the generational hypothesis (most data die young) with a locality principle concerning the age of data: most allocations sites produce data that immediately dies, while a few allocation sites consistently produce data that survives many collections.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {162--173},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277718},
 doi = {http://doi.acm.org/10.1145/277652.277718},
 acmid = {277718},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clinger:1998:PTR:277652.277719,
 author = {Clinger, William D.},
 title = {Proper tail recursion and space efficiency},
 abstract = {The IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive</i>. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation.Proper tail recursion is not the same as ad hoc tail call optimization in stack-based languages. Proper tail recursion often precludes stack allocation of variables, but yields a well-defined asymptotic space complexity that can be relied upon by portable programs.This paper offers a formal and implementation-independent definition of proper tail recursion for Scheme. It also shows how an entire family of reference implementations can be used to characterize related safe-for-space properties, and proves the asymptotic inequalities that hold between them.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277719},
 doi = {http://doi.acm.org/10.1145/277652.277719},
 acmid = {277719},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Clinger:1998:PTR:277650.277719,
 author = {Clinger, William D.},
 title = {Proper tail recursion and space efficiency},
 abstract = {The IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive</i>. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation.Proper tail recursion is not the same as ad hoc tail call optimization in stack-based languages. Proper tail recursion often precludes stack allocation of variables, but yields a well-defined asymptotic space complexity that can be relied upon by portable programs.This paper offers a formal and implementation-independent definition of proper tail recursion for Scheme. It also shows how an entire family of reference implementations can be used to characterize related safe-for-space properties, and proves the asymptotic inequalities that hold between them.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277719},
 doi = {http://doi.acm.org/10.1145/277650.277719},
 acmid = {277719},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adve:1998:UIS:277652.277721,
 author = {Adve, Vikram and Mellor-Crummey, John},
 title = {Using integer sets for data-parallel program analysis and optimization},
 abstract = {In this paper, we describe our experience with using an abstract integer-set framework to develop the Rice dHPF compiler, a compiler for High Performance Fortran. We present simple, yet general formulations of the major computation partitioning and communication analysis tasks as well as a number of important optimizations in terms of abstract operations on sets of integer tuples. This approach has made it possible to implement a comprehensive collection of advanced optimizations in dHPF, and to do so in the context of a more general computation partitioning model than previous compilers. One potential limitation of the approach is that the underlying class of integer set problems is fundamentally unable to represent HPF data distributions on a symbolic number of processors. We describe how we extend the approach to compile codes for a symbolic number of processors, without requiring any changes to the set formulations for the above optimizations. We show experimentally that the set representation is not a dominant factor in compile times on both small and large codes. Finally, we present preliminary performance measurements to show that the generated code achieves good speedups for a few benchmarks. Overall, we believe we are the first to demonstrate by implementation experience that it is practical to build a compiler for HPF using a general and powerful integer-set framework.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {186--198},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277652.277721},
 doi = {http://doi.acm.org/10.1145/277652.277721},
 acmid = {277721},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adve:1998:UIS:277650.277721,
 author = {Adve, Vikram and Mellor-Crummey, John},
 title = {Using integer sets for data-parallel program analysis and optimization},
 abstract = {In this paper, we describe our experience with using an abstract integer-set framework to develop the Rice dHPF compiler, a compiler for High Performance Fortran. We present simple, yet general formulations of the major computation partitioning and communication analysis tasks as well as a number of important optimizations in terms of abstract operations on sets of integer tuples. This approach has made it possible to implement a comprehensive collection of advanced optimizations in dHPF, and to do so in the context of a more general computation partitioning model than previous compilers. One potential limitation of the approach is that the underlying class of integer set problems is fundamentally unable to represent HPF data distributions on a symbolic number of processors. We describe how we extend the approach to compile codes for a symbolic number of processors, without requiring any changes to the set formulations for the above optimizations. We show experimentally that the set representation is not a dominant factor in compile times on both small and large codes. Finally, we present preliminary performance measurements to show that the generated code achieves good speedups for a few benchmarks. Overall, we believe we are the first to demonstrate by implementation experience that it is practical to build a compiler for HPF using a general and powerful integer-set framework.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {186--198},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277650.277721},
 doi = {http://doi.acm.org/10.1145/277650.277721},
 acmid = {277721},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhu:1998:COP:277652.277723,
 author = {Zhu, Yingchun and Hendren, Laurie J.},
 title = {Communication optimizations for parallel C programs},
 abstract = {This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called possible-placement analysis</i>, and a transformation phase called communication selection</i>.The fundamental idea of possible-placement analysis</i> is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the communication selection</i> transformation selects the "best" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16\% over the unoptimized benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {199--211},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277652.277723},
 doi = {http://doi.acm.org/10.1145/277652.277723},
 acmid = {277723},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhu:1998:COP:277650.277723,
 author = {Zhu, Yingchun and Hendren, Laurie J.},
 title = {Communication optimizations for parallel C programs},
 abstract = {This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called possible-placement analysis</i>, and a transformation phase called communication selection</i>.The fundamental idea of possible-placement analysis</i> is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the communication selection</i> transformation selects the "best" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16\% over the unoptimized benchmarks.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {199--211},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277650.277723},
 doi = {http://doi.acm.org/10.1145/277650.277723},
 acmid = {277723},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Frigo:1998:ICM:277652.277725,
 author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
 title = {The implementation of the Cilk-5 multithreaded language},
 abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {212--223},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277725},
 doi = {http://doi.acm.org/10.1145/277652.277725},
 acmid = {277725},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical path, multithreading, parallel computing, programming language, runtime system, work},
} 

@inproceedings{Frigo:1998:ICM:277650.277725,
 author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
 title = {The implementation of the Cilk-5 multithreaded language},
 abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {212--223},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277725},
 doi = {http://doi.acm.org/10.1145/277650.277725},
 acmid = {277725},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical path, multithreading, parallel computing, programming language, runtime system, work},
} 

@article{Wickline:1998:RCG:277652.277727,
 author = {Wickline, Philip and Lee, Peter and Pfenning, Frank},
 title = {Run-time code generation and modal-ML},
 abstract = {This paper presents a typed programming language and compiler for run-time code generation. The language, called ML', extends ML with modal operators in the style of the Mini-ML'<inf>e</i></inf> language of Davies and Pfenning. ML' allows programmers to use types to specify precisely the stages of computation in a program. The types also guide the compiler in generating target code that exploits the staging information through the use of run-time code generation. The target machine is currently a version of the Categorical Abstract Machine, called the CCAM, which we have extended with facilities for run-time code generation.This approach allows the programmer to express the staging that he wants directly to the compiler. It also provides a typed framework in which to verify the correctness of his staging intentions, and to discuss his staging decisions with other programmers. Finally, it supports in a natural way multiple stages of run-time specialization, so that dynamically generated code can be used in the generation of yet further specialized code.This paper presents an overview of the language, with several examples of programs that illustrate key concepts and programming techniques. Then, it discusses the CCAM and the compilation of ML' programs into CCAM code. Finally, the results of some experiments are shown, to demonstrate the benefits of this style of run-time code generation for some applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {224--235},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277727},
 doi = {http://doi.acm.org/10.1145/277652.277727},
 acmid = {277727},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wickline:1998:RCG:277650.277727,
 author = {Wickline, Philip and Lee, Peter and Pfenning, Frank},
 title = {Run-time code generation and modal-ML},
 abstract = {This paper presents a typed programming language and compiler for run-time code generation. The language, called ML', extends ML with modal operators in the style of the Mini-ML'<inf>e</i></inf> language of Davies and Pfenning. ML' allows programmers to use types to specify precisely the stages of computation in a program. The types also guide the compiler in generating target code that exploits the staging information through the use of run-time code generation. The target machine is currently a version of the Categorical Abstract Machine, called the CCAM, which we have extended with facilities for run-time code generation.This approach allows the programmer to express the staging that he wants directly to the compiler. It also provides a typed framework in which to verify the correctness of his staging intentions, and to discuss his staging decisions with other programmers. Finally, it supports in a natural way multiple stages of run-time specialization, so that dynamically generated code can be used in the generation of yet further specialized code.This paper presents an overview of the language, with several examples of programs that illustrate key concepts and programming techniques. Then, it discusses the CCAM and the compilation of ML' programs into CCAM code. Finally, the results of some experiments are shown, to demonstrate the benefits of this style of run-time code generation for some applications.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {224--235},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277727},
 doi = {http://doi.acm.org/10.1145/277650.277727},
 acmid = {277727},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flatt:1998:UCM:277652.277730,
 author = {Flatt, Matthew and Felleisen, Matthias},
 title = {Units: cool modules for HOT languages},
 abstract = {A module system ought to enable assembly-line programming</i> using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of program units</i>, MzScheme's module language for assembly-line programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {236--248},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277652.277730},
 doi = {http://doi.acm.org/10.1145/277652.277730},
 acmid = {277730},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flatt:1998:UCM:277650.277730,
 author = {Flatt, Matthew and Felleisen, Matthias},
 title = {Units: cool modules for HOT languages},
 abstract = {A module system ought to enable assembly-line programming</i> using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of program units</i>, MzScheme's module language for assembly-line programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {236--248},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277650.277730},
 doi = {http://doi.acm.org/10.1145/277650.277730},
 acmid = {277730},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Xi:1998:EAB:277650.277732,
 author = {Xi, Hongwei and Pfenning, Frank},
 title = {Eliminating array bound checking through dependent types},
 abstract = {We present a type-based approach to eliminating array bound checking and list tag checking by conservatively extending Standard ML with a restricted form of dependent types. This enables the programmer to capture more invariants through types while type-checking remains decidable in theory and can still be performed efficiently in practice. We illustrate our approach through concrete examples and present the result of our preliminary experiments which support support the feasibility and effectiveness of our approach.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277650.277732},
 doi = {http://doi.acm.org/10.1145/277650.277732},
 acmid = {277732},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Xi:1998:EAB:277652.277732,
 author = {Xi, Hongwei and Pfenning, Frank},
 title = {Eliminating array bound checking through dependent types},
 abstract = {We present a type-based approach to eliminating array bound checking and list tag checking by conservatively extending Standard ML with a restricted form of dependent types. This enables the programmer to capture more invariants through types while type-checking remains decidable in theory and can still be performed efficiently in practice. We illustrate our approach through concrete examples and present the result of our preliminary experiments which support support the feasibility and effectiveness of our approach.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277652.277732},
 doi = {http://doi.acm.org/10.1145/277652.277732},
 acmid = {277732},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bacon:1998:TLF:277652.277734,
 author = {Bacon, David F. and Konuru, Ravi and Murthy, Chet and Serrano, Mauricio},
 title = {Thin locks: featherweight synchronization for Java},
 abstract = {Language-supported synchronization is a source of serious performance problems in many Java programs. Even single-threaded applications may spend up to half their time performing useless synchronization due to the thread-safe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a factor of 1.7 in real programs.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {258--268},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277652.277734},
 doi = {http://doi.acm.org/10.1145/277652.277734},
 acmid = {277734},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bacon:1998:TLF:277650.277734,
 author = {Bacon, David F. and Konuru, Ravi and Murthy, Chet and Serrano, Mauricio},
 title = {Thin locks: featherweight synchronization for Java},
 abstract = {Language-supported synchronization is a source of serious performance problems in many Java programs. Even single-threaded applications may spend up to half their time performing useless synchronization due to the thread-safe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a factor of 1.7 in real programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {258--268},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277650.277734},
 doi = {http://doi.acm.org/10.1145/277650.277734},
 acmid = {277734},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agesen:1998:GCL:277652.277738,
 author = {Agesen, Ole and Detlefs, David and Moss, J. Eliot},
 title = {Garbage collection and local variable type-precision and liveness in Java virtual machines},
 abstract = {Full precision in garbage collection implies retaining only those heap allocated objects that will actually be used in the future. Since full precision is not computable in general, garbage collectors use safe (i.e., conservative) approximations such as reachability from a set of root references. Ambiguous roots collectors (commonly called "conservative") can be overly conservative because they overestimate the root set, and thereby retain unexpectedly large amounts of garbage. We consider two more precise collection schemes for Java virtual machines (JVMs). One uses a type analysis to obtain a type-precise</i> root set (only those variables that contain references); the other adds a live variable analysis to reduce the root set to only the live reference variables. Even with the Java programming language's strong typing, it turns out that the JVM specification has a feature that makes type-precise root sets difficult to compute. We explain the problem and ways in which it can be solved.Our experimental results include measurements of the costs of the type and liveness analyses at load time, of the incremental benefits at run time of the liveness analysis over the type analysis alone, and of various map sizes and counts. We find that the liveness analysis often produces little or no improvement in heap size, sometimes modest improvements, and occasionally the improvement is dramatic. While further study is in order, we conclude that the main benefit of the liveness analysis is preventing bad surprises.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {269--279},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277652.277738},
 doi = {http://doi.acm.org/10.1145/277652.277738},
 acmid = {277738},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agesen:1998:GCL:277650.277738,
 author = {Agesen, Ole and Detlefs, David and Moss, J. Eliot},
 title = {Garbage collection and local variable type-precision and liveness in Java virtual machines},
 abstract = {Full precision in garbage collection implies retaining only those heap allocated objects that will actually be used in the future. Since full precision is not computable in general, garbage collectors use safe (i.e., conservative) approximations such as reachability from a set of root references. Ambiguous roots collectors (commonly called "conservative") can be overly conservative because they overestimate the root set, and thereby retain unexpectedly large amounts of garbage. We consider two more precise collection schemes for Java virtual machines (JVMs). One uses a type analysis to obtain a type-precise</i> root set (only those variables that contain references); the other adds a live variable analysis to reduce the root set to only the live reference variables. Even with the Java programming language's strong typing, it turns out that the JVM specification has a feature that makes type-precise root sets difficult to compute. We explain the problem and ways in which it can be solved.Our experimental results include measurements of the costs of the type and liveness analyses at load time, of the incremental benefits at run time of the liveness analysis over the type analysis alone, and of various map sizes and counts. We find that the liveness analysis often produces little or no improvement in heap size, sometimes modest improvements, and occasionally the improvement is dramatic. While further study is in order, we conclude that the main benefit of the liveness analysis is preventing bad surprises.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {269--279},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277650.277738},
 doi = {http://doi.acm.org/10.1145/277650.277738},
 acmid = {277738},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adl-Tabatabai:1998:FEC:277652.277740,
 author = {Adl-Tabatabai, Ali-Reza and Cierniak, Micha\l and Lueh, Guei-Yuan and Parikh, Vishesh M. and Stichnoth, James M.},
 title = {Fast, effective code generation in a just-in-time Java compiler},
 abstract = {A "Just-In-Time" (JIT) Java compiler produces native code from Java byte code instructions during program execution. As such, compilation speed is more important in a Java JIT compiler than in a traditional compiler, requiring optimization algorithms to be lightweight and effective. We present the structure of a Java JIT compiler for the Intel Architecture, describe the lightweight implementation of JIT compiler optimizations (e.g., common subexpression elimination, register allocation, and elimination of array bounds checking), and evaluate the performance benefits and tradeoffs of the optimizations. This JIT compiler has been shipped with version 2.5 of Intel's VTune for Java product.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {280--290},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277652.277740},
 doi = {http://doi.acm.org/10.1145/277652.277740},
 acmid = {277740},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adl-Tabatabai:1998:FEC:277650.277740,
 author = {Adl-Tabatabai, Ali-Reza and Cierniak, Micha\l and Lueh, Guei-Yuan and Parikh, Vishesh M. and Stichnoth, James M.},
 title = {Fast, effective code generation in a just-in-time Java compiler},
 abstract = {A "Just-In-Time" (JIT) Java compiler produces native code from Java byte code instructions during program execution. As such, compilation speed is more important in a Java JIT compiler than in a traditional compiler, requiring optimization algorithms to be lightweight and effective. We present the structure of a Java JIT compiler for the Intel Architecture, describe the lightweight implementation of JIT compiler optimizations (e.g., common subexpression elimination, register allocation, and elimination of array bounds checking), and evaluate the performance benefits and tradeoffs of the optimizations. This JIT compiler has been shipped with version 2.5 of Intel's VTune for Java product.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {280--290},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277650.277740},
 doi = {http://doi.acm.org/10.1145/277650.277740},
 acmid = {277740},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Piumarta:1998:ODT:277650.277743,
 author = {Piumarta, Ian and Riccardi, Fabio},
 title = {Optimizing direct threaded code by selective inlining},
 abstract = {Achieving good performance in bytecoded language interpreters is difficult without sacrificing both simplicity and portability. This is due to the complexity of dynamic translation ("just-in-time compilation") of bytecodes into native code, which is the mechanism employed universally by high-performance interpreters.We demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators that can attain as much as 70\% the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower "pure" bytecode interpreters.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277650.277743},
 doi = {http://doi.acm.org/10.1145/277650.277743},
 acmid = {277743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bytecode interpretation, dynamic translation, inlining, just-in-time compilation, threaded code},
} 

@article{Piumarta:1998:ODT:277652.277743,
 author = {Piumarta, Ian and Riccardi, Fabio},
 title = {Optimizing direct threaded code by selective inlining},
 abstract = {Achieving good performance in bytecoded language interpreters is difficult without sacrificing both simplicity and portability. This is due to the complexity of dynamic translation ("just-in-time compilation") of bytecodes into native code, which is the mechanism employed universally by high-performance interpreters.We demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators that can attain as much as 70\% the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower "pure" bytecode interpreters.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277652.277743},
 doi = {http://doi.acm.org/10.1145/277652.277743},
 acmid = {277743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bytecode interpretation, dynamic translation, inlining, just-in-time compilation, threaded code},
} 

@inproceedings{Ayers:1998:SCO:277650.277745,
 author = {Ayers, Andrew and de Jong, Stuart and Peyton, John and Schooler, Richard},
 title = {Scalable cross-module optimization},
 abstract = {Large applications are typically partitioned into separately compiled modules. Large performance gains in these applications are available by optimizing across module boundaries. One barrier to applying crossmodule optimization (CMO) to large applications is the potentially enormous amount of time and space consumed by the optimization process.We describe a framework for scalable CMO that provides large gains in performance on applications that contain millions of lines of code. Two major techniques are described. First, careful management of in-memory data structures results in sub-linear memory occupancy when compared to the number of lines of code being optimized. Second, profile data is used to focus optimization effort on the performance-critical portions of applications. We also present practical issues that arise in deploying this framework in a production environment. These issues include debuggability and compatibility with existing development tools, such as make</i>. Our framework is deployed in Hewlett-Packard's (HP) UNIX compiler products and speeds up shipped independent software vendors' applications by as much as 71\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277745},
 doi = {http://doi.acm.org/10.1145/277650.277745},
 acmid = {277745},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ayers:1998:SCO:277652.277745,
 author = {Ayers, Andrew and de Jong, Stuart and Peyton, John and Schooler, Richard},
 title = {Scalable cross-module optimization},
 abstract = {Large applications are typically partitioned into separately compiled modules. Large performance gains in these applications are available by optimizing across module boundaries. One barrier to applying crossmodule optimization (CMO) to large applications is the potentially enormous amount of time and space consumed by the optimization process.We describe a framework for scalable CMO that provides large gains in performance on applications that contain millions of lines of code. Two major techniques are described. First, careful management of in-memory data structures results in sub-linear memory occupancy when compared to the number of lines of code being optimized. Second, profile data is used to focus optimization effort on the performance-critical portions of applications. We also present practical issues that arise in deploying this framework in a production environment. These issues include debuggability and compatibility with existing development tools, such as make</i>. Our framework is deployed in Hewlett-Packard's (HP) UNIX compiler products and speeds up shipped independent software vendors' applications by as much as 71\%.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277745},
 doi = {http://doi.acm.org/10.1145/277652.277745},
 acmid = {277745},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gay:1998:MME:277652.277748,
 author = {Gay, David and Aiken, Alex},
 title = {Memory management with explicit regions},
 abstract = {Much research has been devoted to studies of and algorithms for memory management based on garbage collection or explicit allocation and deallocation. An alternative approach, region-based memory management, has been known for decades, but has not been well-studied. In a region-based system each allocation specifies a region, and memory is reclaimed by destroying a region, freeing all the storage allocated therein. We show that on a suite of allocation-intensive C programs, regions are competitive with malloc/free and sometimes substantially faster. We also show that regions support safe memory management with low overhead. Experience with our benchmarks suggests that modifying many existing programs to use regions is not difficult.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {313--323},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277652.277748},
 doi = {http://doi.acm.org/10.1145/277652.277748},
 acmid = {277748},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gay:1998:MME:277650.277748,
 author = {Gay, David and Aiken, Alex},
 title = {Memory management with explicit regions},
 abstract = {Much research has been devoted to studies of and algorithms for memory management based on garbage collection or explicit allocation and deallocation. An alternative approach, region-based memory management, has been known for decades, but has not been well-studied. In a region-based system each allocation specifies a region, and memory is reclaimed by destroying a region, freeing all the storage allocated therein. We show that on a suite of allocation-intensive C programs, regions are competitive with malloc/free and sometimes substantially faster. We also show that regions support safe memory management with low overhead. Experience with our benchmarks suggests that modifying many existing programs to use regions is not difficult.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {313--323},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277650.277748},
 doi = {http://doi.acm.org/10.1145/277650.277748},
 acmid = {277748},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sweeney:1998:SDD:277650.277750,
 author = {Sweeney, Peter F. and Tip, Frank},
 title = {A study of dead data members in C++ applications},
 abstract = {Object-oriented applications may contain data members that can be removed from the application without affecting program behavior. Such "dead" data members may occur due to unused functionality in class libraries, or due to the programmer losing track of member usage as the application changes over time. We present a simple and efficient algorithm for detecting dead data members in C++ applications. This algorithm has been implemented using a prototype version of the IBM VisualAge C++ compiler, and applied to a number of realistic benchmark programs ranging from 600 to 58,000 lines of code. For the non-trivial benchmarks, we found that up to 27.3\% of the data members in the benchmarks are dead (average 12.5\%), and that up to 11.6\% of the object space of these applications may be occupied by dead data members at run-time (average 4.4\%).},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {324--332},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277650.277750},
 doi = {http://doi.acm.org/10.1145/277650.277750},
 acmid = {277750},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sweeney:1998:SDD:277652.277750,
 author = {Sweeney, Peter F. and Tip, Frank},
 title = {A study of dead data members in C++ applications},
 abstract = {Object-oriented applications may contain data members that can be removed from the application without affecting program behavior. Such "dead" data members may occur due to unused functionality in class libraries, or due to the programmer losing track of member usage as the application changes over time. We present a simple and efficient algorithm for detecting dead data members in C++ applications. This algorithm has been implemented using a prototype version of the IBM VisualAge C++ compiler, and applied to a number of realistic benchmark programs ranging from 600 to 58,000 lines of code. For the non-trivial benchmarks, we found that up to 27.3\% of the data members in the benchmarks are dead (average 12.5\%), and that up to 11.6\% of the object space of these applications may be occupied by dead data members at run-time (average 4.4\%).},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {324--332},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/277652.277750},
 doi = {http://doi.acm.org/10.1145/277652.277750},
 acmid = {277750},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Necula:1998:DIC:277652.277752,
 author = {Necula, George C. and Lee, Peter},
 title = {The design and implementation of a certifying compiler},
 abstract = {This paper presents the design and implementation of a compiler that translates programs written in a type-safe subset of the C programming language into highly optimized DEC Alpha assembly language programs, and a certifier</i> that automatically checks the type safety and memory safety of any assembly language program produced by the compiler. The result of the certifier is either a formal proof of type safety or a counterexample pointing to a potential violation of the type system by the target program. The ensemble of the compiler and the certifier is called a certifying compiler</i>.Several advantages of certifying compilation over previous approaches can be claimed. The notion of a certifying compiler is significantly easier to employ than a formal compiler verification, in part because it is generally easier to verify the correctness of the result of a computation than to prove the correctness of the computation itself. Also, the approach can be applied even to highly optimizing compilers, as demonstrated by the fact that our compiler generates target code, for a range of realistic C programs, which is competitive with both the cc and gcc compilers with all optimizations enabled. The certifier also drastically improves the effectiveness of compiler testing because, for each test case, it statically signals compilation errors that might otherwise require many executions to detect. Finally, this approach is a practical way to produce the safety proofs for a Proof-Carrying Code system, and thus may be useful in a system for safe mobile code.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {333--344},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277652.277752},
 doi = {http://doi.acm.org/10.1145/277652.277752},
 acmid = {277752},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Necula:1998:DIC:277650.277752,
 author = {Necula, George C. and Lee, Peter},
 title = {The design and implementation of a certifying compiler},
 abstract = {This paper presents the design and implementation of a compiler that translates programs written in a type-safe subset of the C programming language into highly optimized DEC Alpha assembly language programs, and a certifier</i> that automatically checks the type safety and memory safety of any assembly language program produced by the compiler. The result of the certifier is either a formal proof of type safety or a counterexample pointing to a potential violation of the type system by the target program. The ensemble of the compiler and the certifier is called a certifying compiler</i>.Several advantages of certifying compilation over previous approaches can be claimed. The notion of a certifying compiler is significantly easier to employ than a formal compiler verification, in part because it is generally easier to verify the correctness of the result of a computation than to prove the correctness of the computation itself. Also, the approach can be applied even to highly optimizing compilers, as demonstrated by the fact that our compiler generates target code, for a range of realistic C programs, which is competitive with both the cc and gcc compilers with all optimizations enabled. The certifier also drastically improves the effectiveness of compiler testing because, for each test case, it statically signals compilation errors that might otherwise require many executions to detect. Finally, this approach is a practical way to produce the safety proofs for a Proof-Carrying Code system, and thus may be useful in a system for safe mobile code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {333--344},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277650.277752},
 doi = {http://doi.acm.org/10.1145/277650.277752},
 acmid = {277752},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Colby:1998:ACO:277650.277754,
 author = {Colby, Christopher and Godefroid, Patrice and Jagadeesan, Lalita Jategaonkar},
 title = {Automatically closing open reactive programs},
 abstract = {We study in this paper the problem of analyzing implementations of open systems --- systems in which only some of the components are present. We present an algorithm for automatically closing an open concurrent reactive system with its most general environment, i.e., the environment that can provide any input at any time to the system. The result is a nondeterministic closed (i.e., self-executable) system which can exhibit all the possible reactive behaviors of the original open system. These behaviors can then be analyzed using VeriSoft, an existing tool for systematically exploring the state spaces of closed systems composed of multiple (possibly nondeterministic) processes executing arbitrary code. We have implemented the techniques introduced in this paper in a prototype tool for automatically closing open programs written in the C programming language. We discuss preliminary experimental results obtained with a large telephone-switching software application developed at Lucent Technologies.},
 booktitle = {Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation},
 series = {PLDI '98},
 year = {1998},
 isbn = {0-89791-987-4},
 location = {Montreal, Quebec, Canada},
 pages = {345--357},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277650.277754},
 doi = {http://doi.acm.org/10.1145/277650.277754},
 acmid = {277754},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Colby:1998:ACO:277652.277754,
 author = {Colby, Christopher and Godefroid, Patrice and Jagadeesan, Lalita Jategaonkar},
 title = {Automatically closing open reactive programs},
 abstract = {We study in this paper the problem of analyzing implementations of open systems --- systems in which only some of the components are present. We present an algorithm for automatically closing an open concurrent reactive system with its most general environment, i.e., the environment that can provide any input at any time to the system. The result is a nondeterministic closed (i.e., self-executable) system which can exhibit all the possible reactive behaviors of the original open system. These behaviors can then be analyzed using VeriSoft, an existing tool for systematically exploring the state spaces of closed systems composed of multiple (possibly nondeterministic) processes executing arbitrary code. We have implemented the techniques introduced in this paper in a prototype tool for automatically closing open programs written in the C programming language. We discuss preliminary experimental results obtained with a large telephone-switching software application developed at Lucent Technologies.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {5},
 month = {May},
 year = {1998},
 issn = {0362-1340},
 pages = {345--357},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/277652.277754},
 doi = {http://doi.acm.org/10.1145/277652.277754},
 acmid = {277754},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Proebsting:1997:STG:258916.258917,
 author = {Proebsting, Todd A.},
 title = {Simple translation of goal-directed evaluation},
 abstract = {This paper presents a simple, powerful and flexible technique for reasoning about and translating the goal-directed evaluation of programming language constructs that either succeed (and generate sequences of values) or fail. The technique generalizes the Byrd Box</i>, a well-known device for describing Prolog backtracking.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/258916.258917},
 doi = {http://doi.acm.org/10.1145/258916.258917},
 acmid = {258917},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Proebsting:1997:STG:258915.258917,
 author = {Proebsting, Todd A.},
 title = {Simple translation of goal-directed evaluation},
 abstract = {This paper presents a simple, powerful and flexible technique for reasoning about and translating the goal-directed evaluation of programming language constructs that either succeed (and generate sequences of values) or fail. The technique generalizes the Byrd Box</i>, a well-known device for describing Prolog backtracking.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/258915.258917},
 doi = {http://doi.acm.org/10.1145/258915.258917},
 acmid = {258917},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dolby:1997:AIA:258915.258918,
 author = {Dolby, Julian},
 title = {Automatic inline allocation of objects},
 abstract = {Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, function specialization); in this paper, we introduce object inlining</i>, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several object-oriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined codes.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {7--17},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258915.258918},
 doi = {http://doi.acm.org/10.1145/258915.258918},
 acmid = {258918},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dolby:1997:AIA:258916.258918,
 author = {Dolby, Julian},
 title = {Automatic inline allocation of objects},
 abstract = {Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, function specialization); in this paper, we introduce object inlining</i>, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several object-oriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined codes.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {7--17},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258916.258918},
 doi = {http://doi.acm.org/10.1145/258916.258918},
 acmid = {258918},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramalingam:1997:MLA:258915.258919,
 author = {Ramalingam, G. and Srinivasan, Harini},
 title = {A member lookup algorithm for C++},
 abstract = {The member lookup problem in C++ is the problem of resolving a specified member name in the context of a specified class. Member lookup in C++ is complicated by the presence of virtual inheritance and multiple inheritance. In this paper, we present an efficient algorithm for member lookup in C++. We also present a formalism for the multiple inheritance mechanism of C++, which we use as the basis for deriving our algorithm. The formalism may also be of use as a formal basis for deriving other C++ compiler algorithms.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {18--30},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258915.258919},
 doi = {http://doi.acm.org/10.1145/258915.258919},
 acmid = {258919},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramalingam:1997:MLA:258916.258919,
 author = {Ramalingam, G. and Srinivasan, Harini},
 title = {A member lookup algorithm for C++},
 abstract = {The member lookup problem in C++ is the problem of resolving a specified member name in the context of a specified class. Member lookup in C++ is complicated by the presence of virtual inheritance and multiple inheritance. In this paper, we present an efficient algorithm for member lookup in C++. We also present a formalism for the multiple inheritance mechanism of C++, which we use as the basis for deriving our algorithm. The formalism may also be of use as a formal basis for deriving other C++ compiler algorithms.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {18--30},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258916.258919},
 doi = {http://doi.acm.org/10.1145/258916.258919},
 acmid = {258919},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wagner:1997:IAR:258915.258920,
 author = {Wagner, Tim A. and Graham, Susan L.},
 title = {Incremental analysis of real programming languages},
 abstract = {A major research goal for compilers and environments is the automatic derivation of tools from formal specifications. However, the formal model of the language is often inadequate; in particular, LR(k</i>) grammars are unable to describe the natural syntax of many languages, such as C<sup>++</sup> and Fortran, which are inherently non-deterministic. Designers of batch compilers work around such limitations by combining generated components with ad hoc techniques (for instance, performing partial type and scope analysis in tandem with parsing). Unfortunately, the complexity of incremental</i> systems precludes the use of batch solutions. The inability to generate incremental tools for important languages inhibits the widespread use of language-rich interactive environments.We address this problem by extending the language model itself, introducing a program representation based on parse dags</i> that is suitable for both batch and incremental analysis. Ambiguities unresolved by one stage are retained in this representation until further stages can complete the analysis, even if the reaolution depends on further actions by the user. Representing ambiguity explicitly increases the number and variety of languages that can be analyzed incrementally using existing methods.To create this representation, we have developed an efficient incremental parser for general context-free grammars. Our algorithm combines Tomita's generalized LR parser with reuse of entire subtrees via state-matching. Disambiguation can occur statically, during or after parsing, or during semantic analysis (using existing incremental techniques); program errors that preclude disambiguation retsin multiple interpretations indefinitely. Our representation and analyses gain efficiency by exploiting the local nature of ambiguities: for the S<sc>PEC</sc>95 C programs, the explicit representation of ambiguity requires only 0.5\% additional space and less than 1\% additional time during reconstruction.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {31--43},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258915.258920},
 doi = {http://doi.acm.org/10.1145/258915.258920},
 acmid = {258920},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wagner:1997:IAR:258916.258920,
 author = {Wagner, Tim A. and Graham, Susan L.},
 title = {Incremental analysis of real programming languages},
 abstract = {A major research goal for compilers and environments is the automatic derivation of tools from formal specifications. However, the formal model of the language is often inadequate; in particular, LR(k</i>) grammars are unable to describe the natural syntax of many languages, such as C<sup>++</sup> and Fortran, which are inherently non-deterministic. Designers of batch compilers work around such limitations by combining generated components with ad hoc techniques (for instance, performing partial type and scope analysis in tandem with parsing). Unfortunately, the complexity of incremental</i> systems precludes the use of batch solutions. The inability to generate incremental tools for important languages inhibits the widespread use of language-rich interactive environments.We address this problem by extending the language model itself, introducing a program representation based on parse dags</i> that is suitable for both batch and incremental analysis. Ambiguities unresolved by one stage are retained in this representation until further stages can complete the analysis, even if the reaolution depends on further actions by the user. Representing ambiguity explicitly increases the number and variety of languages that can be analyzed incrementally using existing methods.To create this representation, we have developed an efficient incremental parser for general context-free grammars. Our algorithm combines Tomita's generalized LR parser with reuse of entire subtrees via state-matching. Disambiguation can occur statically, during or after parsing, or during semantic analysis (using existing incremental techniques); program errors that preclude disambiguation retsin multiple interpretations indefinitely. Our representation and analyses gain efficiency by exploiting the local nature of ambiguities: for the S<sc>PEC</sc>95 C programs, the explicit representation of ambiguity requires only 0.5\% additional space and less than 1\% additional time during reconstruction.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {31--43},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258916.258920},
 doi = {http://doi.acm.org/10.1145/258916.258920},
 acmid = {258920},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eide:1997:FFO:258915.258921,
 author = {Eide, Eric and Frei, Kevin and Ford, Bryan and Lepreau, Jay and Lindstrom, Gary},
 title = {Flick: a flexible, optimizing IDL compiler},
 abstract = {An interface definition language (<sc>IDL</sc>) is a nontraditional language for describing interfaces between software components. <sc>IDL</sc> compilers generate "stubs" that provide separate communicating processes with the abstraction of local object invocation or procedure call. High-quality stub generation is essential for applications to benefit from component-based designs, whether the components reside on a single computer or on multiple networked hosts. Typical <sc>IDL</sc> compilers, however, do little code optimization, incorrectly assuming that interprocess communication is always the primary bottleneck. More generally, typical <sc>IDL</sc> compilers are "rigid" and limited to supporting only a single <sc>IDL</sc>, a fixed mapping onto a target language, and a narrow range of data encodings and transport mechanisms.Flick</i>, our new <sc>IDL</sc> compiler, is based on the insight that <sc>IDL</sc>s are true languages amenable to modern compilation techniques. Flick exploits concepts from traditional programming language compilers to bring both flexibility and optimization to the domain of <sc>IDL</sc> compilation. Through the use of carefully chosen intermediate representations, Flick supports multiple <sc>IDL</sc>s, diverse data encodings, multiple transport mechanisms, and applies numerous optimizations to all of the code it generates. Our experiments show that Flick-generated stubs marshal data between 2 and 17 times faster than stubs produced by traditional <sc>IDL</sc> compilers, and on today's generic operating systems, increase end-to-end throughput by factors between 1.2 and 3.7.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {44--56},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258915.258921},
 doi = {http://doi.acm.org/10.1145/258915.258921},
 acmid = {258921},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eide:1997:FFO:258916.258921,
 author = {Eide, Eric and Frei, Kevin and Ford, Bryan and Lepreau, Jay and Lindstrom, Gary},
 title = {Flick: a flexible, optimizing IDL compiler},
 abstract = {An interface definition language (<sc>IDL</sc>) is a nontraditional language for describing interfaces between software components. <sc>IDL</sc> compilers generate "stubs" that provide separate communicating processes with the abstraction of local object invocation or procedure call. High-quality stub generation is essential for applications to benefit from component-based designs, whether the components reside on a single computer or on multiple networked hosts. Typical <sc>IDL</sc> compilers, however, do little code optimization, incorrectly assuming that interprocess communication is always the primary bottleneck. More generally, typical <sc>IDL</sc> compilers are "rigid" and limited to supporting only a single <sc>IDL</sc>, a fixed mapping onto a target language, and a narrow range of data encodings and transport mechanisms.Flick</i>, our new <sc>IDL</sc> compiler, is based on the insight that <sc>IDL</sc>s are true languages amenable to modern compilation techniques. Flick exploits concepts from traditional programming language compilers to bring both flexibility and optimization to the domain of <sc>IDL</sc> compilation. Through the use of carefully chosen intermediate representations, Flick supports multiple <sc>IDL</sc>s, diverse data encodings, multiple transport mechanisms, and applies numerous optimizations to all of the code it generates. Our experiments show that Flick-generated stubs marshal data between 2 and 17 times faster than stubs produced by traditional <sc>IDL</sc> compilers, and on today's generic operating systems, increase end-to-end throughput by factors between 1.2 and 3.7.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {44--56},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258916.258921},
 doi = {http://doi.acm.org/10.1145/258916.258921},
 acmid = {258921},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Collberg:1997:RIM:258915.258922,
 author = {Collberg, Christian S.},
 title = {Reverse interpretation + mutation analysis = automatic retargeting},
 abstract = {There are three popular methods for constructing highly retargetable compilers: (1) the compiler emits abstract machine code which is interpreted at run-time, (2) the compiler emits C code which is subsequently compiled to machine code by the native C compiler, or (3) the compiler's code-generator is generated by a back-end generator from a formal machine description produced by the compiler writer.These methods incur high costs at run-time, compile-time, or compiler-construction time, respectively.In this paper we will describe a novel method which promises to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at compiler construction time</i> to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native code-generator can be generated by a back-end generator such as BEG or burg.A prototype Automatic Architecture Discovery Unit</i> has been implemented. The current version is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86. The tool is completely automatic and requires minimal input from the user: principally, the user needs to provide the internet address of the target machine and the command-lines by which the C compiler, assembler, and linker are invoked.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {57--70},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258915.258922},
 doi = {http://doi.acm.org/10.1145/258915.258922},
 acmid = {258922},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Collberg:1997:RIM:258916.258922,
 author = {Collberg, Christian S.},
 title = {Reverse interpretation + mutation analysis = automatic retargeting},
 abstract = {There are three popular methods for constructing highly retargetable compilers: (1) the compiler emits abstract machine code which is interpreted at run-time, (2) the compiler emits C code which is subsequently compiled to machine code by the native C compiler, or (3) the compiler's code-generator is generated by a back-end generator from a formal machine description produced by the compiler writer.These methods incur high costs at run-time, compile-time, or compiler-construction time, respectively.In this paper we will describe a novel method which promises to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at compiler construction time</i> to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native code-generator can be generated by a back-end generator such as BEG or burg.A prototype Automatic Architecture Discovery Unit</i> has been implemented. The current version is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86. The tool is completely automatic and requires minimal input from the user: principally, the user needs to provide the internet address of the target machine and the command-lines by which the C compiler, assembler, and linker are invoked.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {57--70},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258916.258922},
 doi = {http://doi.acm.org/10.1145/258916.258922},
 acmid = {258922},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Diniz:1997:DFE:258915.258923,
 author = {Diniz, Pedro C. and Rinard, Martin C.},
 title = {Dynamic feedback: an effective technique for adaptive computing},
 abstract = {This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.We have implemented dynamic feedback in the context of a parallelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {71--84},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258915.258923},
 doi = {http://doi.acm.org/10.1145/258915.258923},
 acmid = {258923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Diniz:1997:DFE:258916.258923,
 author = {Diniz, Pedro C. and Rinard, Martin C.},
 title = {Dynamic feedback: an effective technique for adaptive computing},
 abstract = {This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.We have implemented dynamic feedback in the context of a parallelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {71--84},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258916.258923},
 doi = {http://doi.acm.org/10.1145/258916.258923},
 acmid = {258923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ammons:1997:EHP:258916.258924,
 author = {Ammons, Glenn and Ball, Thomas and Larus, James R.},
 title = {Exploiting hardware performance counters with flow and context sensitive profiling},
 abstract = {A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98\% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258924},
 doi = {http://doi.acm.org/10.1145/258916.258924},
 acmid = {258924},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ammons:1997:EHP:258915.258924,
 author = {Ammons, Glenn and Ball, Thomas and Larus, James R.},
 title = {Exploiting hardware performance counters with flow and context sensitive profiling},
 abstract = {A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98\% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258924},
 doi = {http://doi.acm.org/10.1145/258915.258924},
 acmid = {258924},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clinger:1997:GGC:258916.258925,
 author = {Clinger, William D. and Hansen, Lars T.},
 title = {Generational garbage collection and the radioactive decay model},
 abstract = {If a fixed exponentially decreasing probability distribution function is used to model every object's lifetime, then the age of an object gives no information about its future life expectancy. This radioactive decay</i> model implies there can be no rational basis for deciding which live objects should be promoted to another generation. Yet there remains a rational basis for deciding how many objects to promote, when to collect garbage, and which generations to collect.Analysis of the model leads to a new kind of generational garbage collector whose effectiveness does not depend upon heuristics that predict which objects will live longer than others.This result provides insight into the computational advantages of generational garbage collection, with implications for the management of objects whose life expectancies are difficult to predict.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258925},
 doi = {http://doi.acm.org/10.1145/258916.258925},
 acmid = {258925},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Clinger:1997:GGC:258915.258925,
 author = {Clinger, William D. and Hansen, Lars T.},
 title = {Generational garbage collection and the radioactive decay model},
 abstract = {If a fixed exponentially decreasing probability distribution function is used to model every object's lifetime, then the age of an object gives no information about its future life expectancy. This radioactive decay</i> model implies there can be no rational basis for deciding which live objects should be promoted to another generation. Yet there remains a rational basis for deciding how many objects to promote, when to collect garbage, and which generations to collect.Analysis of the model leads to a new kind of generational garbage collector whose effectiveness does not depend upon heuristics that predict which objects will live longer than others.This result provides insight into the computational advantages of generational garbage collection, with implications for the management of objects whose life expectancies are difficult to predict.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258925},
 doi = {http://doi.acm.org/10.1145/258915.258925},
 acmid = {258925},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Poletto:1997:TSF:258916.258926,
 author = {Poletto, Massimiliano and Engler, Dawson R. and Kaashoek, M. Frans},
 title = {tcc: a system for fast, flexible, and high-level dynamic code generation},
 abstract = {tcc is a compiler that provides efficient and high-level access to dynamic code generation. It implements the 'C ("Tick-C") programming language, an extension of ANSI C that supports dynamic code generation [15]. 'C gives power and flexibility in specifying dynamically generated code: whereas most other systems use annotations to denote run-time invariants. 'C allows the programmer to specify and compose arbitrary expressions and statements at run time. This degree of control is needed to efficiently implement some of the most important applications of dynamic code generation, such as "just in time" compilers [17] and efficient simulators [10, 48, 46].The paper focuses on the techniques that allow tcc to provide 'C's flexibility and expressiveness without sacrificing run-time code generation efficiency. These techniques include fast register allocation, efficient creation and composition of dynamic code specifications, and link-time analysis to reduce the size of dynamic code generators. tcc also implements two different dynamic code generation strategies, designed to address the tradeoff of dynamic compilation speed versus generated code quality. To characterize the effects of dynamic compilation, we present performance measurements for eleven programs compiled using tcc. On these applications, we measured performance improvements of up to one order of magnitude.To encourage further experimentation and use of dynamic code generation, we are making the tcc compiler available in the public domain. This is, to our knowledge, the first high-level dynamic compilation system to be made available.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {109--121},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258916.258926},
 doi = {http://doi.acm.org/10.1145/258916.258926},
 acmid = {258926},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Poletto:1997:TSF:258915.258926,
 author = {Poletto, Massimiliano and Engler, Dawson R. and Kaashoek, M. Frans},
 title = {tcc: a system for fast, flexible, and high-level dynamic code generation},
 abstract = {tcc is a compiler that provides efficient and high-level access to dynamic code generation. It implements the 'C ("Tick-C") programming language, an extension of ANSI C that supports dynamic code generation [15]. 'C gives power and flexibility in specifying dynamically generated code: whereas most other systems use annotations to denote run-time invariants. 'C allows the programmer to specify and compose arbitrary expressions and statements at run time. This degree of control is needed to efficiently implement some of the most important applications of dynamic code generation, such as "just in time" compilers [17] and efficient simulators [10, 48, 46].The paper focuses on the techniques that allow tcc to provide 'C's flexibility and expressiveness without sacrificing run-time code generation efficiency. These techniques include fast register allocation, efficient creation and composition of dynamic code specifications, and link-time analysis to reduce the size of dynamic code generators. tcc also implements two different dynamic code generation strategies, designed to address the tradeoff of dynamic compilation speed versus generated code quality. To characterize the effects of dynamic compilation, we present performance measurements for eleven programs compiled using tcc. On these applications, we measured performance improvements of up to one order of magnitude.To encourage further experimentation and use of dynamic code generation, we are making the tcc compiler available in the public domain. This is, to our knowledge, the first high-level dynamic compilation system to be made available.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {109--121},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258915.258926},
 doi = {http://doi.acm.org/10.1145/258915.258926},
 acmid = {258926},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goodwin:1997:IDA:258915.258927,
 author = {Goodwin, David W.},
 title = {Interprocedural dataflow analysis in an executable optimizer},
 abstract = {Interprocedural dataflow information enables link-time and post-link-time optimizers to perform analyses and code transformations that are not possible in a traditional compiler. This paper describes the interprocedural dataflow analysis techniques used by Spike, a post-linktime optimizer for Alpha/NT executables. Spike uses dataflow analysis to summarize the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations that require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, the time required to perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike's effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation of a program's intraprocedural and interprocedural control flow that efficiently summarizes the register definitions and uses that occur in the program. Experimental results are presented for the SPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation allows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 integer benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 thousand basic blocks, interprocedural dataflow analysis requires just 12 seconds.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258927},
 doi = {http://doi.acm.org/10.1145/258915.258927},
 acmid = {258927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goodwin:1997:IDA:258916.258927,
 author = {Goodwin, David W.},
 title = {Interprocedural dataflow analysis in an executable optimizer},
 abstract = {Interprocedural dataflow information enables link-time and post-link-time optimizers to perform analyses and code transformations that are not possible in a traditional compiler. This paper describes the interprocedural dataflow analysis techniques used by Spike, a post-linktime optimizer for Alpha/NT executables. Spike uses dataflow analysis to summarize the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations that require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, the time required to perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike's effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation of a program's intraprocedural and interprocedural control flow that efficiently summarizes the register definitions and uses that occur in the program. Experimental results are presented for the SPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation allows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 integer benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 thousand basic blocks, interprocedural dataflow analysis requires just 12 seconds.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258927},
 doi = {http://doi.acm.org/10.1145/258916.258927},
 acmid = {258927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ayers:1997:AI:258916.258928,
 author = {Ayers, Andrew and Schooler, Richard and Gottlieb, Robert},
 title = {Aggressive inlining},
 abstract = {Existing research understates the benefits that can be obtained from inlining and cloning, especially when guided by profile information. Our implementation of inlining and cloning yields excellent results on average and very rarely lowers performance. We believe our good results can be explained by a number of factors: inlining at the intermediate-code level removes most technical restrictions on what can be inlined; the ability to inline across files and incorporate profile information enables us to choose better inline candidates; a high-quality back end can exploit the scheduling and register allocation opportunities presented by larger subroutines; an aggressive processor architecture benefits from more predictable branch behavior; and a large instruction cache mitigates the impact of code expansion. We describe the often dramatic impact of our inlining and cloning on performance: for example, the implementations of our inlining and cloning algorithms in the HP-UX 10.20 compilers boost SPECint95 performance on a PA8000-based workstation by a factor of 1.32.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {134--145},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258928},
 doi = {http://doi.acm.org/10.1145/258916.258928},
 acmid = {258928},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ayers:1997:AI:258915.258928,
 author = {Ayers, Andrew and Schooler, Richard and Gottlieb, Robert},
 title = {Aggressive inlining},
 abstract = {Existing research understates the benefits that can be obtained from inlining and cloning, especially when guided by profile information. Our implementation of inlining and cloning yields excellent results on average and very rarely lowers performance. We believe our good results can be explained by a number of factors: inlining at the intermediate-code level removes most technical restrictions on what can be inlined; the ability to inline across files and incorporate profile information enables us to choose better inline candidates; a high-quality back end can exploit the scheduling and register allocation opportunities presented by larger subroutines; an aggressive processor architecture benefits from more predictable branch behavior; and a large instruction cache mitigates the impact of code expansion. We describe the often dramatic impact of our inlining and cloning on performance: for example, the implementations of our inlining and cloning algorithms in the HP-UX 10.20 compilers boost SPECint95 performance on a PA8000-based workstation by a factor of 1.32.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {134--145},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258928},
 doi = {http://doi.acm.org/10.1145/258915.258928},
 acmid = {258928},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bodik:1997:ICB:258916.258929,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Interprocedural conditional branch elimination},
 abstract = {The existence of statically detectable correlation among conditional branches enables their elimination, an optimization that has a number of benefits. This paper presents techniques to determine whether an interprocedural execution path leading to a conditional branch exists along which the branch outcome is known at compile time, and then to eliminate the branch along this path through code restructuring. The technique consists of a demand driven interprocedural analysis that determines whether a specific branch outcome is correlated with prior statements or branch outcomes. The optimization is performed using a code restructuring algorithm that replicates code to separate out the paths with correlation. When the correlated path is affected by a procedure call, the restructuring is based on procedure entry splitting and exit splitting</i>. The entry splitting</i> transformation creates multiple entries to a procedure, and the exit splitting</i> transformation allows a procedure to return control to one of several return points in the caller. Our technique is efficient in that the correlation detection is demand driven, thus avoiding exhaustive analysis of the entire program, and the restructuring never increases the number of operations along a path through an interprocedural control flow graph. We describe the benefits of our interprocedural branch elimination optimization (ICBE). Our experimental results show that, for the same amount of code growth, the estimated reduction in executed conditional branches is about 2.5 times higher with the ICBE optimization than when only intraprocedural conditional branch elimination is applied.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {146--158},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258916.258929},
 doi = {http://doi.acm.org/10.1145/258916.258929},
 acmid = {258929},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bodik:1997:ICB:258915.258929,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv and Soffa, Mary Lou},
 title = {Interprocedural conditional branch elimination},
 abstract = {The existence of statically detectable correlation among conditional branches enables their elimination, an optimization that has a number of benefits. This paper presents techniques to determine whether an interprocedural execution path leading to a conditional branch exists along which the branch outcome is known at compile time, and then to eliminate the branch along this path through code restructuring. The technique consists of a demand driven interprocedural analysis that determines whether a specific branch outcome is correlated with prior statements or branch outcomes. The optimization is performed using a code restructuring algorithm that replicates code to separate out the paths with correlation. When the correlated path is affected by a procedure call, the restructuring is based on procedure entry splitting and exit splitting</i>. The entry splitting</i> transformation creates multiple entries to a procedure, and the exit splitting</i> transformation allows a procedure to return control to one of several return points in the caller. Our technique is efficient in that the correlation detection is demand driven, thus avoiding exhaustive analysis of the entire program, and the restructuring never increases the number of operations along a path through an interprocedural control flow graph. We describe the benefits of our interprocedural branch elimination optimization (ICBE). Our experimental results show that, for the same amount of code growth, the estimated reduction in executed conditional branches is about 2.5 times higher with the ICBE optimization than when only intraprocedural conditional branch elimination is applied.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {146--158},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258915.258929},
 doi = {http://doi.acm.org/10.1145/258915.258929},
 acmid = {258929},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bodik:1997:PDC:258915.258930,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv},
 title = {Partial dead code elimination using slicing transformations},
 abstract = {We present an approach for optimizing programs that uncovers additional opportunities for optimization of a statement by predicating</i> the statement. In this paper predication algorithms for achieving partial dead code elimination (PDE) are presented. The process of predication embeds a statement in a control flow structure such that the statement is executed only if the execution follows a path along which the value computed by the statement is live. The control flow restructuring performed to achieve predication is expressed through slicing transformations</i>. This approach achieves PDE that is not realizable by existing algorithms. We prove that our algorithm never increases the operation count along any path, and that for acyclic code all partially dead statements are eliminated. The slicing transformation that achieves predication introduces into the program additional conditional branches. These branches are eliminated in a branch deletion step based upon code duplication. We also show how PDE can be used by acyclic schedulers for VLIW processors to reduce critical path lengths along frequently executed paths.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258930},
 doi = {http://doi.acm.org/10.1145/258915.258930},
 acmid = {258930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bodik:1997:PDC:258916.258930,
 author = {Bod\'{\i}k, Rastislav and Gupta, Rajiv},
 title = {Partial dead code elimination using slicing transformations},
 abstract = {We present an approach for optimizing programs that uncovers additional opportunities for optimization of a statement by predicating</i> the statement. In this paper predication algorithms for achieving partial dead code elimination (PDE) are presented. The process of predication embeds a statement in a control flow structure such that the statement is executed only if the execution follows a path along which the value computed by the statement is live. The control flow restructuring performed to achieve predication is expressed through slicing transformations</i>. This approach achieves PDE that is not realizable by existing algorithms. We prove that our algorithm never increases the operation count along any path, and that for acyclic code all partially dead statements are eliminated. The slicing transformation that achieves predication introduces into the program additional conditional branches. These branches are eliminated in a branch deletion step based upon code duplication. We also show how PDE can be used by acyclic schedulers for VLIW processors to reduce critical path lengths along frequently executed paths.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258930},
 doi = {http://doi.acm.org/10.1145/258916.258930},
 acmid = {258930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hashemi:1997:EPM:258915.258931,
 author = {Hashemi, Amir H. and Kaeli, David R. and Calder, Brad},
 title = {Efficient procedure mapping using cache line coloring},
 abstract = {As the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory eflectively. Both hardware and aoftware approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques.In this paper we present a link-time procedure mapping algorithm which can significantly improve the eflectiveness of the instruction cache. Our algorithm produces an improved program layout by performing a color mapping of procedures to cache lines, taking into consideration the procedure size, cache size, cache line size, and call graph. We use cache line coloring to guide the procedure mapping, indicating which cache lines to avoid when placing a procedure in the program layout. Our algorithm reduces on average the instruction cache miss rate by 40\% over the original mapping and by 17\% over the mapping algorithm of Pettis and Hansen [12].},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258931},
 doi = {http://doi.acm.org/10.1145/258915.258931},
 acmid = {258931},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hashemi:1997:EPM:258916.258931,
 author = {Hashemi, Amir H. and Kaeli, David R. and Calder, Brad},
 title = {Efficient procedure mapping using cache line coloring},
 abstract = {As the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory eflectively. Both hardware and aoftware approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques.In this paper we present a link-time procedure mapping algorithm which can significantly improve the eflectiveness of the instruction cache. Our algorithm produces an improved program layout by performing a color mapping of procedures to cache lines, taking into consideration the procedure size, cache size, cache line size, and call graph. We use cache line coloring to guide the procedure mapping, indicating which cache lines to avoid when placing a procedure in the program layout. Our algorithm reduces on average the instruction cache miss rate by 40\% over the original mapping and by 17\% over the mapping algorithm of Pettis and Hansen [12].},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258931},
 doi = {http://doi.acm.org/10.1145/258916.258931},
 acmid = {258931},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Young:1997:NIB:258915.258932,
 author = {Young, Cliff and Johnson, David S. and Smith, Michael D. and Karger, David R.},
 title = {Near-optimal intraprocedural branch alignment},
 abstract = {Branch alignment reorders the basic blocks of a program to minimize pipeline penalties due to control-transfer instructions. Prior work in branch alignment has produced useful heuristic methods. We present a branch alignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks averages within 0.3\% of a provable optimum. We compare the control penalties and running times of our algorithm to an older, greedy approach and observe that both the greedy method and our method are close to the lower bound on control penalties, suggesting that greedy is good enough. Surprisingly, in actual execution our method produces programs that run noticeably faster than the greedy method. We also report results from training and testing on different data sets, validating that our results can be achieved in real-world usage. Training and testing on different data sets slightly reduced the benefits from both branch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits remain.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {183--193},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258915.258932},
 doi = {http://doi.acm.org/10.1145/258915.258932},
 acmid = {258932},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Young:1997:NIB:258916.258932,
 author = {Young, Cliff and Johnson, David S. and Smith, Michael D. and Karger, David R.},
 title = {Near-optimal intraprocedural branch alignment},
 abstract = {Branch alignment reorders the basic blocks of a program to minimize pipeline penalties due to control-transfer instructions. Prior work in branch alignment has produced useful heuristic methods. We present a branch alignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks averages within 0.3\% of a provable optimum. We compare the control penalties and running times of our algorithm to an older, greedy approach and observe that both the greedy method and our method are close to the lower bound on control penalties, suggesting that greedy is good enough. Surprisingly, in actual execution our method produces programs that run noticeably faster than the greedy method. We also report results from training and testing on different data sets, validating that our results can be achieved in real-world usage. Training and testing on different data sets slightly reduced the benefits from both branch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits remain.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {183--193},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258916.258932},
 doi = {http://doi.acm.org/10.1145/258916.258932},
 acmid = {258932},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eichenberger:1997:EFO:258916.258933,
 author = {Eichenberger, Alexandre E. and Davidson, Edward S.},
 title = {Efficient formulation for optimal modulo schedulers},
 abstract = {Modulo scheduling algorithms based on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling heuristics. While recent advances have broadened the scope for which the optimal approach is applicable, this approach increasingly suffers from large execution times. In this paper, we propose a more efficient formulation of the modulo scheduling space that significantly decreases the execution time of solvers based on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 when 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register requirements using the more efficient formulation instead of the traditional formulation. Experimental evidence further indicates that significantly larger loops can be scheduled under realistic machine constraints.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {194--205},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258933},
 doi = {http://doi.acm.org/10.1145/258916.258933},
 acmid = {258933},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eichenberger:1997:EFO:258915.258933,
 author = {Eichenberger, Alexandre E. and Davidson, Edward S.},
 title = {Efficient formulation for optimal modulo schedulers},
 abstract = {Modulo scheduling algorithms based on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling heuristics. While recent advances have broadened the scope for which the optimal approach is applicable, this approach increasingly suffers from large execution times. In this paper, we propose a more efficient formulation of the modulo scheduling space that significantly decreases the execution time of solvers based on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 when 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register requirements using the more efficient formulation instead of the traditional formulation. Experimental evidence further indicates that significantly larger loops can be scheduled under realistic machine constraints.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {194--205},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258933},
 doi = {http://doi.acm.org/10.1145/258915.258933},
 acmid = {258933},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dussart:1997:MPS:258915.258934,
 author = {Dussart, Dirk and Heldal, Rogardt and Hughes, John},
 title = {Module-sensitive program specialisation},
 abstract = {We present an approach for specialising large programs, such as programs consisting of several modules, or libraries. This approach is based on the idea of using a compiler generator (cogen) for creating generating extensions. Generating extensions are specialisers specialised with respect to some input program. When run on some input data the generating extension produces a specialised version of the input program. Here we use the cogen to tailor modules for specialisation. This happens once and for all, independently of all other modules. The resulting module can then be used as a building block for generating extensions for complete programs, in much the same way as the original modules can be put together into complete programs. The result of running the final generating extension is a collection of residual modules, with a module structure derived from the original program.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {206--214},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258915.258934},
 doi = {http://doi.acm.org/10.1145/258915.258934},
 acmid = {258934},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dussart:1997:MPS:258916.258934,
 author = {Dussart, Dirk and Heldal, Rogardt and Hughes, John},
 title = {Module-sensitive program specialisation},
 abstract = {We present an approach for specialising large programs, such as programs consisting of several modules, or libraries. This approach is based on the idea of using a compiler generator (cogen) for creating generating extensions. Generating extensions are specialisers specialised with respect to some input program. When run on some input data the generating extension produces a specialised version of the input program. Here we use the cogen to tailor modules for specialisation. This happens once and for all, independently of all other modules. The resulting module can then be used as a building block for generating extensions for complete programs, in much the same way as the original modules can be put together into complete programs. The result of running the final generating extension is a collection of residual modules, with a module structure derived from the original program.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {206--214},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258916.258934},
 doi = {http://doi.acm.org/10.1145/258916.258934},
 acmid = {258934},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sperber:1997:TPO:258916.258935,
 author = {Sperber, Michael and Thiemann, Peter},
 title = {Two for the price of one: composing partial evaluation and compilation},
 abstract = {One of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a source-to-source transformation for high-level languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several meta-computation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a run-time code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreter-based experiments with a source-to-source version of the partial evaluator before building a realistic compiler which generates object code automatically.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258916.258935},
 doi = {http://doi.acm.org/10.1145/258916.258935},
 acmid = {258935},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sperber:1997:TPO:258915.258935,
 author = {Sperber, Michael and Thiemann, Peter},
 title = {Two for the price of one: composing partial evaluation and compilation},
 abstract = {One of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a source-to-source transformation for high-level languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several meta-computation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a run-time code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreter-based experiments with a source-to-source version of the partial evaluator before building a realistic compiler which generates object code automatically.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258915.258935},
 doi = {http://doi.acm.org/10.1145/258915.258935},
 acmid = {258935},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jensen:1997:AVP:258916.258936,
 author = {Jensen, Jakob L. and J{\o}rgensen, Michael E. and Schwartzbach, Michael I. and Klarlund, Nils},
 title = {Automatic verification of pointer programs using monadic second-order logic},
 abstract = {We present a technique for automatic verification of pointer programs based on a decision procedure for the monadic second-order logic on finite strings.We are concerned with a while-fragment of Pascal, which includes recursively-defined pointer structures but excludes pointer arithmetic.We define a logic of stores with interesting basic predicates such as pointer equality, tests for nil pointers, and garbage cells, as well as reachability along pointers.We present a complete decision procedure for Hoare triples based on this logic over loop-free code. Combined with explicit loop invariants, the decision procedure allows us to answer surprisingly detailed questions about small but non-trivial programs. If a program fails to satisfy a certain property, then we can automatically supply an initial store that provides a counterexample.Our technique had been fully and efficiently implemented for linear linked lists, and it extends in principle to tree structures. The resulting system can be used to verify extensive properties of smaller pointer programs and could be particularly useful in a teaching environment.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {226--234},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258916.258936},
 doi = {http://doi.acm.org/10.1145/258916.258936},
 acmid = {258936},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jensen:1997:AVP:258915.258936,
 author = {Jensen, Jakob L. and J{\o}rgensen, Michael E. and Schwartzbach, Michael I. and Klarlund, Nils},
 title = {Automatic verification of pointer programs using monadic second-order logic},
 abstract = {We present a technique for automatic verification of pointer programs based on a decision procedure for the monadic second-order logic on finite strings.We are concerned with a while-fragment of Pascal, which includes recursively-defined pointer structures but excludes pointer arithmetic.We define a logic of stores with interesting basic predicates such as pointer equality, tests for nil pointers, and garbage cells, as well as reachability along pointers.We present a complete decision procedure for Hoare triples based on this logic over loop-free code. Combined with explicit loop invariants, the decision procedure allows us to answer surprisingly detailed questions about small but non-trivial programs. If a program fails to satisfy a certain property, then we can automatically supply an initial store that provides a counterexample.Our technique had been fully and efficiently implemented for linear linked lists, and it extends in principle to tree structures. The resulting system can be used to verify extensive properties of smaller pointer programs and could be particularly useful in a teaching environment.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {226--234},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258915.258936},
 doi = {http://doi.acm.org/10.1145/258915.258936},
 acmid = {258936},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flanagan:1997:CSA:258916.258937,
 author = {Flanagan, Cormac and Felleisen, Matthias},
 title = {Componential set-based analysis},
 abstract = {Set based analysis is a constraint-based whole program analysis that is applicable to functional and object-oriented programming language. Unfortunately, the analysis is useless for large programs, since it generates descriptions of data flow relationships that grow quadratically in the size of the program.This paper presents componential set-based analysis, which is faster and handles larger programs without any loss of accuracy over set-based analysis. The design of the analysis exploits a number of theoretical results concerning constraint systems, including a completeness result and a decision algorithm concerning the observable equivalance of constraint systems. Experimental results validate the practically of the analysis.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {235--248},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258916.258937},
 doi = {http://doi.acm.org/10.1145/258916.258937},
 acmid = {258937},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flanagan:1997:CSA:258915.258937,
 author = {Flanagan, Cormac and Felleisen, Matthias},
 title = {Componential set-based analysis},
 abstract = {Set based analysis is a constraint-based whole program analysis that is applicable to functional and object-oriented programming language. Unfortunately, the analysis is useless for large programs, since it generates descriptions of data flow relationships that grow quadratically in the size of the program.This paper presents componential set-based analysis, which is faster and handles larger programs without any loss of accuracy over set-based analysis. The design of the analysis exploits a number of theoretical results concerning constraint systems, including a completeness result and a decision algorithm concerning the observable equivalance of constraint systems. Experimental results validate the practically of the analysis.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {235--248},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258915.258937},
 doi = {http://doi.acm.org/10.1145/258915.258937},
 acmid = {258937},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Webber:1997:PAU:258916.258938,
 author = {Webber, Adam Brooks},
 title = {Program analysis using binary relations},
 abstract = {This paper presents a method called relational constraint</i> for finding binary relations among the variables and constants of a program. The method constructs a table of binary relations and treats the program as a collection of constraints on tuples of relations in the table. An experimental optimizer called Thinner uses this method to analyze programs of size n</i> in O</i>(n</i><sup>2</sup>) time.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258938},
 doi = {http://doi.acm.org/10.1145/258916.258938},
 acmid = {258938},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Webber:1997:PAU:258915.258938,
 author = {Webber, Adam Brooks},
 title = {Program analysis using binary relations},
 abstract = {This paper presents a method called relational constraint</i> for finding binary relations among the variables and constants of a program. The method constructs a table of binary relations and treats the program as a collection of constraints on tuples of relations in the table. An experimental optimizer called Thinner uses this method to analyze programs of size n</i> in O</i>(n</i><sup>2</sup>) time.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258938},
 doi = {http://doi.acm.org/10.1145/258915.258938},
 acmid = {258938},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heintze:1997:LSC:258916.258939,
 author = {Heintze, Nevin and McAllester, David},
 title = {Linear-time subtransitive control flow analysis},
 abstract = {We present a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give linear-time algorithms for CFA-consuming applications such as:\&amp;bull; effects analysis: find the side-effecting expressions in a program.\&amp;bull; k</i>-limited CFA: for each call-site, list the functions if there are only a few of them (\&amp;le; k) and otherwise output "many".\&amp;bull; called-once analysis: identify all functions called from only one call-site.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258939},
 doi = {http://doi.acm.org/10.1145/258916.258939},
 acmid = {258939},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heintze:1997:LSC:258915.258939,
 author = {Heintze, Nevin and McAllester, David},
 title = {Linear-time subtransitive control flow analysis},
 abstract = {We present a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give linear-time algorithms for CFA-consuming applications such as:\&amp;bull; effects analysis: find the side-effecting expressions in a program.\&amp;bull; k</i>-limited CFA: for each call-site, list the functions if there are only a few of them (\&amp;le; k) and otherwise output "many".\&amp;bull; called-once analysis: identify all functions called from only one call-site.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258939},
 doi = {http://doi.acm.org/10.1145/258915.258939},
 acmid = {258939},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chow:1997:NAP:258915.258940,
 author = {Chow, Fred and Chan, Sun and Kennedy, Robert and Liu, Shin-Ming and Lo, Raymond and Tu, Peng},
 title = {A new algorithm for partial redundancy elimination based on SSA form},
 abstract = {A new algorithm, SSAPRE, for performing partial redundancy elimination based entirely on SSA form is presented. It achieves optimal code motion similar to lazy code motion [KRS94a, DS93], but is formulated independently and does not involve iterative data flow analysis and bit vectors in its solution. It not only exhibits the characteristics common to other sparse approaches, but also inherits the advantages shared by other SSA-based optimization techniques. SSAPRE also maintains its output in the same SSA form as its input. In describing the algorithm, we state theorems with proofs giving our claims about SSAPRE. We also give additional description about our practical implementation of SSAPRE, and analyze and compare its performance with a bit-vector-based implementation of PRE. We conclude with some discussion of the implications of this work.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {273--286},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258915.258940},
 doi = {http://doi.acm.org/10.1145/258915.258940},
 acmid = {258940},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chow:1997:NAP:258916.258940,
 author = {Chow, Fred and Chan, Sun and Kennedy, Robert and Liu, Shin-Ming and Lo, Raymond and Tu, Peng},
 title = {A new algorithm for partial redundancy elimination based on SSA form},
 abstract = {A new algorithm, SSAPRE, for performing partial redundancy elimination based entirely on SSA form is presented. It achieves optimal code motion similar to lazy code motion [KRS94a, DS93], but is formulated independently and does not involve iterative data flow analysis and bit vectors in its solution. It not only exhibits the characteristics common to other sparse approaches, but also inherits the advantages shared by other SSA-based optimization techniques. SSAPRE also maintains its output in the same SSA form as its input. In describing the algorithm, we state theorems with proofs giving our claims about SSAPRE. We also give additional description about our practical implementation of SSAPRE, and analyze and compare its performance with a bit-vector-based implementation of PRE. We conclude with some discussion of the implications of this work.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {273--286},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258916.258940},
 doi = {http://doi.acm.org/10.1145/258916.258940},
 acmid = {258940},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bergner:1997:SCM:258916.258941,
 author = {Bergner, Peter and Dahl, Peter and Engebretsen, David and O'Keefe, Matthew},
 title = {Spill code minimization via interference region spilling},
 abstract = {Many optimizing compilers perform global register allocation using a Chaitin-style graph coloring algorithm. Live ranges that cannot be allocated to registers are spilled to memory. The amount of code required to spill the live range depends on the spilling heuristic used. Chaitin's spilling heuristic offers some guidance in reducing the amount of spill code produced. However, this heuristic does not allow the partial spilling of live ranges and the reduction in spill code is limited to a local level. In this paper, we present a global technique called interference region</i> spilling that improves the spilling granularity of any local spilling heuristic. Our technique works above the local spilling heuristic, limiting the normal insertion of spill code to a portion of each spilled live range. By partially spilling live ranges, we can achieve large reductions in dynamically executed spill code; up to 75\% in some cases and an average of 33.6\% across the benchmarks tested.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {287--295},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258916.258941},
 doi = {http://doi.acm.org/10.1145/258916.258941},
 acmid = {258941},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bergner:1997:SCM:258915.258941,
 author = {Bergner, Peter and Dahl, Peter and Engebretsen, David and O'Keefe, Matthew},
 title = {Spill code minimization via interference region spilling},
 abstract = {Many optimizing compilers perform global register allocation using a Chaitin-style graph coloring algorithm. Live ranges that cannot be allocated to registers are spilled to memory. The amount of code required to spill the live range depends on the spilling heuristic used. Chaitin's spilling heuristic offers some guidance in reducing the amount of spill code produced. However, this heuristic does not allow the partial spilling of live ranges and the reduction in spill code is limited to a local level. In this paper, we present a global technique called interference region</i> spilling that improves the spilling granularity of any local spilling heuristic. Our technique works above the local spilling heuristic, limiting the normal insertion of spill code to a portion of each spilled live range. By partially spilling live ranges, we can achieve large reductions in dynamically executed spill code; up to 75\% in some cases and an average of 33.6\% across the benchmarks tested.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {287--295},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/258915.258941},
 doi = {http://doi.acm.org/10.1145/258915.258941},
 acmid = {258941},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lueh:1997:CDR:258916.258942,
 author = {Lueh, Guei-Yuan and Gross, Thomas},
 title = {Call-cost directed register allocation},
 abstract = {Choosing the right kind of register for a live range plays a major role in eliminating the register-allocation overhead when the compiled function is frequently executed or function tails are on the most frequently executed paths. Picking the wrong kind of register for a live range incurs a high penalty that may dominate the total overhead of register allocation. In this paper, we present three improvements, storage-class analysis, benefit-driven simplification, and preference decision that are effective in selecting the right kind of register for a live range. Then we compare an enhanced Chaitin-style register allocator (with these three improvements) with priority-based and optimistic coloring.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {296--307},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258942},
 doi = {http://doi.acm.org/10.1145/258916.258942},
 acmid = {258942},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lueh:1997:CDR:258915.258942,
 author = {Lueh, Guei-Yuan and Gross, Thomas},
 title = {Call-cost directed register allocation},
 abstract = {Choosing the right kind of register for a live range plays a major role in eliminating the register-allocation overhead when the compiled function is frequently executed or function tails are on the most frequently executed paths. Picking the wrong kind of register for a live range incurs a high penalty that may dominate the total overhead of register allocation. In this paper, we present three improvements, storage-class analysis, benefit-driven simplification, and preference decision that are effective in selecting the right kind of register for a live range. Then we compare an enhanced Chaitin-style register allocator (with these three improvements) with priority-based and optimistic coloring.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {296--307},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258942},
 doi = {http://doi.acm.org/10.1145/258915.258942},
 acmid = {258942},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lu:1997:RPC:258916.258943,
 author = {Lu, John and Cooper, Keith D.},
 title = {Register promotion in C programs},
 abstract = {The combination of pointers and pointer arithmetic in C makes the task of improving C programs somewhat more difficult than improving programs written in simpler languages like Fortran. While much work has been published that focuses on the analysis of pointers, little has appeared that uses the results of such analysis to improve the code compiled for C. This paper examines the problem of register promotion in C and presents experimental results showing that it can have dramatic effects on memory traffic.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {308--319},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258943},
 doi = {http://doi.acm.org/10.1145/258916.258943},
 acmid = {258943},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lu:1997:RPC:258915.258943,
 author = {Lu, John and Cooper, Keith D.},
 title = {Register promotion in C programs},
 abstract = {The combination of pointers and pointer arithmetic in C makes the task of improving C programs somewhat more difficult than improving programs written in simpler languages like Fortran. While much work has been published that focuses on the analysis of pointers, little has appeared that uses the results of such analysis to improve the code compiled for C. This paper examines the problem of register promotion in C and presents experimental results showing that it can have dramatic effects on memory traffic.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {308--319},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258943},
 doi = {http://doi.acm.org/10.1145/258915.258943},
 acmid = {258943},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Taura:1997:FMM:258916.258944,
 author = {Taura, Kenjiro and Yonezawa, Akinori},
 title = {Fine-grain multithreading with minimal compiler support\&mdash;a cost effective approach to implementing efficient multithreading languages},
 abstract = {It is difficult to map the execution model of multithreading languages (languages which support fine-grain dynamic thread creation) onto the single stack execution model of C. Consequently, previous work on efficient multithreading uses elaborate frame formats and allocation strategy, with compilers customized for them. This paper presents an alternative cost-effective implementation strategy for multithreading languages which can maximally exploit current sequential C compilers. We identify a set of primitives whereby efficient dynamic thread creation and switch can be achieved and clarify implementation issues and solutions which work under the stack frame layout and calling conventions of current C compilers. The primitives are implemented as a C library and named StackThreads. In StackThreads, a thread creation is done just by a C procedure call, maximizing thread creation performance. When a procedure suspends an execution, the context of the procedure, which is roughly a stack frame of the procedure, is saved into heap and resumed later. With StackThreads, the compiler writer can straightforwardly translate sequential constructs of the source language into corresponding C statements or expressions, while using StackThreads primitives as a blackbox mechanism which switches execution between C procedures.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {320--333},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258916.258944},
 doi = {http://doi.acm.org/10.1145/258916.258944},
 acmid = {258944},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Taura:1997:FMM:258915.258944,
 author = {Taura, Kenjiro and Yonezawa, Akinori},
 title = {Fine-grain multithreading with minimal compiler support\&mdash;a cost effective approach to implementing efficient multithreading languages},
 abstract = {It is difficult to map the execution model of multithreading languages (languages which support fine-grain dynamic thread creation) onto the single stack execution model of C. Consequently, previous work on efficient multithreading uses elaborate frame formats and allocation strategy, with compilers customized for them. This paper presents an alternative cost-effective implementation strategy for multithreading languages which can maximally exploit current sequential C compilers. We identify a set of primitives whereby efficient dynamic thread creation and switch can be achieved and clarify implementation issues and solutions which work under the stack frame layout and calling conventions of current C compilers. The primitives are implemented as a C library and named StackThreads. In StackThreads, a thread creation is done just by a C procedure call, maximizing thread creation performance. When a procedure suspends an execution, the context of the procedure, which is roughly a stack frame of the procedure, is saved into heap and resumed later. With StackThreads, the compiler writer can straightforwardly translate sequential constructs of the source language into corresponding C statements or expressions, while using StackThreads primitives as a blackbox mechanism which switches execution between C procedures.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {320--333},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/258915.258944},
 doi = {http://doi.acm.org/10.1145/258915.258944},
 acmid = {258944},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandra:1997:DDS:258915.258945,
 author = {Chandra, Rohit and Chen, Ding-Kai and Cox, Robert and Maydan, Dror E. and Nedeljkovic, Nenad and Anderson, Jennifer M.},
 title = {Data distribution support on distributed shared memory multiprocessors},
 abstract = {Cache-coherent multiprocessors with distributed shared memory are becoming increasingly popular for parallel computing. However, obtaining high performance on these machines mquires that an application execute with good data locality. In addition to making efiective use of caches, it is often necessary to distribute data structures across the local memories of the processing nodes, thereby reducing the latency of cache misses.We have designed a set of abstractions for performing data distribution in the context of explicitly parallel programs and implemented them within the SGI MIPSpro compiler system. Our system incorporates many unique features to enhance both programmability and performance. We address the former by providing a very simple programmming model with extensive support for error detection. Regarding performance, we carefully design the user abstractions with the underlying compiler optimizations in mind, we incorporate several optimization techniques to generate efficient code for accessing distributed data, and we provide a tight integration of these techniques with other optimizations within the compiler Our initial experience suggests that the directives are easy to use and can yield substantial performance gains, in some cases by as much as a factor of 3 over the same codes without distribution.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {334--345},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258945},
 doi = {http://doi.acm.org/10.1145/258915.258945},
 acmid = {258945},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1997:DDS:258916.258945,
 author = {Chandra, Rohit and Chen, Ding-Kai and Cox, Robert and Maydan, Dror E. and Nedeljkovic, Nenad and Anderson, Jennifer M.},
 title = {Data distribution support on distributed shared memory multiprocessors},
 abstract = {Cache-coherent multiprocessors with distributed shared memory are becoming increasingly popular for parallel computing. However, obtaining high performance on these machines mquires that an application execute with good data locality. In addition to making efiective use of caches, it is often necessary to distribute data structures across the local memories of the processing nodes, thereby reducing the latency of cache misses.We have designed a set of abstractions for performing data distribution in the context of explicitly parallel programs and implemented them within the SGI MIPSpro compiler system. Our system incorporates many unique features to enhance both programmability and performance. We address the former by providing a very simple programmming model with extensive support for error detection. Regarding performance, we carefully design the user abstractions with the underlying compiler optimizations in mind, we incorporate several optimization techniques to generate efficient code for accessing distributed data, and we provide a tight integration of these techniques with other optimizations within the compiler Our initial experience suggests that the directives are easy to use and can yield substantial performance gains, in some cases by as much as a factor of 3 over the same codes without distribution.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {334--345},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258945},
 doi = {http://doi.acm.org/10.1145/258916.258945},
 acmid = {258945},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kodukula:1997:DMB:258916.258946,
 author = {Kodukula, Induprakas and Ahmed, Nawaaz and Pingali, Keshav},
 title = {Data-centric multi-level blocking},
 abstract = {We present a simple and novel framework for generating blocked codes for high-performance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our data-centric transformations permit a more direct solution to the problem of enhancing data locality than current control-centric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {346--357},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258916.258946},
 doi = {http://doi.acm.org/10.1145/258916.258946},
 acmid = {258946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kodukula:1997:DMB:258915.258946,
 author = {Kodukula, Induprakas and Ahmed, Nawaaz and Pingali, Keshav},
 title = {Data-centric multi-level blocking},
 abstract = {We present a simple and novel framework for generating blocked codes for high-performance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our data-centric transformations permit a more direct solution to the problem of enhancing data locality than current control-centric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {346--357},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258915.258946},
 doi = {http://doi.acm.org/10.1145/258915.258946},
 acmid = {258946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ernst:1997:CC:258915.258947,
 author = {Ernst, Jens and Evans, William and Fraser, Christopher W. and Proebsting, Todd A. and Lucco, Steven},
 title = {Code compression},
 abstract = {Current research in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This view has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of network and disk speeds to CPU speeds grows exponentially.For example, we have seen the CPU idle for most of the time during paging, so compressing pages can increase total performance even though the CPU must decompress or interpret the page contents. Another profile shows that many functions are called just once, so reduced paging could pay for their interpretation overhead.This paper describes:\&amp;bull; Measurements that show how code compression can save space and</i> total time in some important real-world scenarios.\&amp;bull; A compressed executable representation that is roughly the same size as gzipped x86 programs and can be interpreted without decompression. It can also be compiled to high-quality machine code at 2.5 megabytes per second on a 120MHz Pentium processor\&amp;bull; A compressed "wire" representation that must be decompressed before execution but is, for example, roughly 21\% the size of SPARC code when compressing gcc.},
 booktitle = {Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and implementation},
 series = {PLDI '97},
 year = {1997},
 isbn = {0-89791-907-6},
 location = {Las Vegas, Nevada, United States},
 pages = {358--365},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/258915.258947},
 doi = {http://doi.acm.org/10.1145/258915.258947},
 acmid = {258947},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ernst:1997:CC:258916.258947,
 author = {Ernst, Jens and Evans, William and Fraser, Christopher W. and Proebsting, Todd A. and Lucco, Steven},
 title = {Code compression},
 abstract = {Current research in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This view has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of network and disk speeds to CPU speeds grows exponentially.For example, we have seen the CPU idle for most of the time during paging, so compressing pages can increase total performance even though the CPU must decompress or interpret the page contents. Another profile shows that many functions are called just once, so reduced paging could pay for their interpretation overhead.This paper describes:\&amp;bull; Measurements that show how code compression can save space and</i> total time in some important real-world scenarios.\&amp;bull; A compressed executable representation that is roughly the same size as gzipped x86 programs and can be interpreted without decompression. It can also be compiled to high-quality machine code at 2.5 megabytes per second on a 120MHz Pentium processor\&amp;bull; A compressed "wire" representation that must be decompressed before execution but is, for example, roughly 21\% the size of SPARC code when compressing gcc.},
 journal = {SIGPLAN Not.},
 volume = {32},
 issue = {5},
 month = {May},
 year = {1997},
 issn = {0362-1340},
 pages = {358--365},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/258916.258947},
 doi = {http://doi.acm.org/10.1145/258916.258947},
 acmid = {258947},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ruttenberg:1996:SPS:249069.231385,
 author = {Ruttenberg, John and Gao, G. R. and Stoutchinin, A. and Lichtenstein, W.},
 title = {Software pipelining showdown: optimal vs. heuristic methods in a production compiler},
 abstract = {This paper is a scientific comparison of two code generation techniques with identical goals --- generation of the best possible software pipelined code for computers with instruction level parallelism. Both are variants of modulo scheduling</i>, a framework for generation of software pipelines pioneered by Rau and Glaser [RaG181], but are otherwise quite dissimilar.One technique was developed at Silicon Graphics and is used in the MIPSpro compiler. This is the production compiler for SGI's systems which are based on the MIPS R8000 processor [Hsu94]. It is essentially a branch--and--bound enumeration of possible schedules with extensive pruning. This method is heuristic because of the way it prunes and also because of the interaction between register allocation and scheduling.The second technique aims to produce optimal results by formulating the scheduling and register allocation problem as an integrated integer linear programming (ILP</i><sup>1</sup>) problem. This idea has received much recent exposure in the literature [AlGoGa95, Feautrier94, GoAlGa94a, GoAlGa94b, Eichenberger95], but to our knowledge all previous implementations have been too preliminary for detailed measurement and evaluation. In particular, we believe this to be the first published measurement of runtime performance for ILP based generation of software pipelines.A particularly valuable result of this study was evaluation of the heuristic pipelining technology in the SGI compiler. One of the motivations behind the McGill research was the hope that optimal software pipelining, while not in itself practical for use in production compilers, would be useful for their evaluation and validation. Our comparison has indeed provided a quantitative validation of the SGI compiler's pipeliner, leading us to increased confidence in both techniques.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231385},
 doi = {http://doi.acm.org/10.1145/249069.231385},
 acmid = {231385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ruttenberg:1996:SPS:231379.231385,
 author = {Ruttenberg, John and Gao, G. R. and Stoutchinin, A. and Lichtenstein, W.},
 title = {Software pipelining showdown: optimal vs. heuristic methods in a production compiler},
 abstract = {This paper is a scientific comparison of two code generation techniques with identical goals --- generation of the best possible software pipelined code for computers with instruction level parallelism. Both are variants of modulo scheduling</i>, a framework for generation of software pipelines pioneered by Rau and Glaser [RaG181], but are otherwise quite dissimilar.One technique was developed at Silicon Graphics and is used in the MIPSpro compiler. This is the production compiler for SGI's systems which are based on the MIPS R8000 processor [Hsu94]. It is essentially a branch--and--bound enumeration of possible schedules with extensive pruning. This method is heuristic because of the way it prunes and also because of the interaction between register allocation and scheduling.The second technique aims to produce optimal results by formulating the scheduling and register allocation problem as an integrated integer linear programming (ILP</i><sup>1</sup>) problem. This idea has received much recent exposure in the literature [AlGoGa95, Feautrier94, GoAlGa94a, GoAlGa94b, Eichenberger95], but to our knowledge all previous implementations have been too preliminary for detailed measurement and evaluation. In particular, we believe this to be the first published measurement of runtime performance for ILP based generation of software pipelines.A particularly valuable result of this study was evaluation of the heuristic pipelining technology in the SGI compiler. One of the motivations behind the McGill research was the hope that optimal software pipelining, while not in itself practical for use in production compilers, would be useful for their evaluation and validation. Our comparison has indeed provided a quantitative validation of the SGI compiler's pipeliner, leading us to increased confidence in both techniques.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231385},
 doi = {http://doi.acm.org/10.1145/231379.231385},
 acmid = {231385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eichenberger:1996:RMM:231379.231386,
 author = {Eichenberger, Alexandre E. and Davidson, Edward S.},
 title = {A reduced multipipeline machine description that preserves scheduling constraints},
 abstract = {High performance compilers increasingly rely on accurate modeling of the machine resources to efficiently exploit the instruction level parallelism of an application. In this paper, we propose a reduced machine description that results in faster detection of resource contentions while preserving the scheduling constraints present in the original machine description. The proposed approach reduces a machine description in an automated, error-free, and efficient fashion, Moreover, it fully supports schedulers that backtrack and process operations in arbitrary order. Reduced descriptions for the DEC Alpha 21064, MIPS R3000/R3010, and Cydra 5 result in 4 to 7 times faster detection of resource contentions and require 22 to 90\% of the memory storage used by the original machine descriptions. Precise measurement for the Cydra 5 indicates that reducing the machine description results in a 2.9 times faster contention query module.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {12--22},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231386},
 doi = {http://doi.acm.org/10.1145/231379.231386},
 acmid = {231386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eichenberger:1996:RMM:249069.231386,
 author = {Eichenberger, Alexandre E. and Davidson, Edward S.},
 title = {A reduced multipipeline machine description that preserves scheduling constraints},
 abstract = {High performance compilers increasingly rely on accurate modeling of the machine resources to efficiently exploit the instruction level parallelism of an application. In this paper, we propose a reduced machine description that results in faster detection of resource contentions while preserving the scheduling constraints present in the original machine description. The proposed approach reduces a machine description in an automated, error-free, and efficient fashion, Moreover, it fully supports schedulers that backtrack and process operations in arbitrary order. Reduced descriptions for the DEC Alpha 21064, MIPS R3000/R3010, and Cydra 5 result in 4 to 7 times faster detection of resource contentions and require 22 to 90\% of the memory storage used by the original machine descriptions. Precise measurement for the Cydra 5 indicates that reducing the machine description results in a 2.9 times faster contention query module.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {12--22},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231386},
 doi = {http://doi.acm.org/10.1145/249069.231386},
 acmid = {231386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flanagan:1996:CBW:231379.231387,
 author = {Flanagan, Cormac and Flatt, Matthew and Krishnamurthi, Shriram and Weirich, Stephanie and Felleisen, Matthias},
 title = {Catching bugs in the web of program invariants},
 abstract = {MrSpidey is a user-friendly, interactive static debugger for Scheme. A static debugger supplements the standard debugger by analyzing the program and pinpointing those program operations that may cause run-time errors such as dereferencing the null pointer or applying non-functions. The program analysis of MrSpidey computes value set descriptions for each term in the program and constructs a value flow graph connecting the set descriptions. Using the set descriptions, MrSpidey can identify and highlight potentially erroneous program operations, whose cause the programmer can then explore by selectively exposing portions of the value flow graph.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {23--32},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231387},
 doi = {http://doi.acm.org/10.1145/231379.231387},
 acmid = {231387},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flanagan:1996:CBW:249069.231387,
 author = {Flanagan, Cormac and Flatt, Matthew and Krishnamurthi, Shriram and Weirich, Stephanie and Felleisen, Matthias},
 title = {Catching bugs in the web of program invariants},
 abstract = {MrSpidey is a user-friendly, interactive static debugger for Scheme. A static debugger supplements the standard debugger by analyzing the program and pinpointing those program operations that may cause run-time errors such as dereferencing the null pointer or applying non-functions. The program analysis of MrSpidey computes value set descriptions for each term in the program and constructs a value flow graph connecting the set descriptions. Using the set descriptions, MrSpidey can identify and highlight potentially erroneous program operations, whose cause the programmer can then explore by selectively exposing portions of the value flow graph.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {23--32},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231387},
 doi = {http://doi.acm.org/10.1145/249069.231387},
 acmid = {231387},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adl-Tabatabai:1996:SDS:249069.231388,
 author = {Adl-Tabatabai, Ali-Reza and Gross, Thomas},
 title = {Source-level debugging of scalar optimized code},
 abstract = {Although compiler optimizations play a crucial role in the performance of modern computer systems, debugger technology has lagged behind in its support of optimization. Yet debugging the unoptimized translation is often impossible or futile, so handling of code optimizations in the debugger is necessary. But compiler optimizations make it difficult to provide source-level debugger functionality: Global optimizations can cause the runtime value of a variable to be inconsistent with the source-level value expected at a breakpoint; such variables are called endangered</i> variables. A debugger must detect and warn the user of endangered variables otherwise the user may draw incorrect conclusions about the program. This paper presents a new algorithm for detecting variables that are endangered due to global scalar optimization. Our approach provides more precise classifications of variables and is still simpler than past approaches. We have implemented and evaluated our techniques in the context of the cmcc optimizing C compiler. We describe the compiler extensions necessary to perform the required bookkeeping of compiler optimization. We present measurements of the effect of optimizations on a debugger's ability to present the expected values of variables to the user.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231388},
 doi = {http://doi.acm.org/10.1145/249069.231388},
 acmid = {231388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adl-Tabatabai:1996:SDS:231379.231388,
 author = {Adl-Tabatabai, Ali-Reza and Gross, Thomas},
 title = {Source-level debugging of scalar optimized code},
 abstract = {Although compiler optimizations play a crucial role in the performance of modern computer systems, debugger technology has lagged behind in its support of optimization. Yet debugging the unoptimized translation is often impossible or futile, so handling of code optimizations in the debugger is necessary. But compiler optimizations make it difficult to provide source-level debugger functionality: Global optimizations can cause the runtime value of a variable to be inconsistent with the source-level value expected at a breakpoint; such variables are called endangered</i> variables. A debugger must detect and warn the user of endangered variables otherwise the user may draw incorrect conclusions about the program. This paper presents a new algorithm for detecting variables that are endangered due to global scalar optimization. Our approach provides more precise classifications of variables and is still simpler than past approaches. We have implemented and evaluated our techniques in the context of the cmcc optimizing C compiler. We describe the compiler extensions necessary to perform the required bookkeeping of compiler optimization. We present measurements of the effect of optimizations on a debugger's ability to present the expected values of variables to the user.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231388},
 doi = {http://doi.acm.org/10.1145/231379.231388},
 acmid = {231388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Evans:1996:SDD:231379.231389,
 author = {Evans, David},
 title = {Static detection of dynamic memory errors},
 abstract = {Many important classes of bugs result from invalid assumptions about the results of functions and the values of parameters and global variables. Using traditional methods, these bugs cannot be detected efficiently at compile-time, since detailed cross-procedural analyses would be required to determine the relevant assumptions. In this work, we introduce annotations to make certain assumptions explicit at interface points. An efficient static checking tool that exploits these annotations can detect a broad class of errors including misuses of null pointers, uses of dead storage, memory leaks, and dangerous aliasing. This technique has been used successfully to fix memory management problems in a large program.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {44--53},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231389},
 doi = {http://doi.acm.org/10.1145/231379.231389},
 acmid = {231389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Evans:1996:SDD:249069.231389,
 author = {Evans, David},
 title = {Static detection of dynamic memory errors},
 abstract = {Many important classes of bugs result from invalid assumptions about the results of functions and the values of parameters and global variables. Using traditional methods, these bugs cannot be detected efficiently at compile-time, since detailed cross-procedural analyses would be required to determine the relevant assumptions. In this work, we introduce annotations to make certain assumptions explicit at interface points. An efficient static checking tool that exploits these annotations can detect a broad class of errors including misuses of null pointers, uses of dead storage, memory leaks, and dangerous aliasing. This technique has been used successfully to fix memory management problems in a large program.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {44--53},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231389},
 doi = {http://doi.acm.org/10.1145/249069.231389},
 acmid = {231389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rinard:1996:CAN:249069.231390,
 author = {Rinard, Martin C. and Diniz, Pedro C.},
 title = {Commutativity analysis: a new analysis framework for parallelizing compilers},
 abstract = {This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code.We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut N-body solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {54--67},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/249069.231390},
 doi = {http://doi.acm.org/10.1145/249069.231390},
 acmid = {231390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rinard:1996:CAN:231379.231390,
 author = {Rinard, Martin C. and Diniz, Pedro C.},
 title = {Commutativity analysis: a new analysis framework for parallelizing compilers},
 abstract = {This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code.We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut N-body solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {54--67},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/231379.231390},
 doi = {http://doi.acm.org/10.1145/231379.231390},
 acmid = {231390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chakrabarti:1996:GCA:249069.231391,
 author = {Chakrabarti, Soumen and Gupta, Manish and Choi, Jong-Deok},
 title = {Global communication analysis and optimization},
 abstract = {Reducing communication cost is crucial to achieving good performance on scalable parallel machines. This paper presents a new compiler algorithm for global analysis and optimization of communication in data-parallel programs. Our algorithm is distinct from existing approaches in that rather than handling loop-nests and array references one by one, it considers all communication in a procedure and their interactions under different placements before making a final decision on the placement of any communication. It exploits the flexibility resulting from this advanced analysis to eliminate redundancy, reduce the number of messages, and reduce contention for cache and communication buffers, all in a unified framework. In contrast, single loop-nest analysis often retains redundant communication, and more aggressive dataflow analysis on array sections can generate too many messages or cache and buffer contention. The algorithm has been implemented in the IBM pHPF compiler for High Performance Fortran. During compilation, the number of messages per processor goes down by as much as a factor of nine for some HPF programs. We present performance results for the IBM SP2 and a network of Sparc workstations (NOW) connected by a Myrinet switch. In many cases, the communication cost is reduced by a factor of two.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231391},
 doi = {http://doi.acm.org/10.1145/249069.231391},
 acmid = {231391},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chakrabarti:1996:GCA:231379.231391,
 author = {Chakrabarti, Soumen and Gupta, Manish and Choi, Jong-Deok},
 title = {Global communication analysis and optimization},
 abstract = {Reducing communication cost is crucial to achieving good performance on scalable parallel machines. This paper presents a new compiler algorithm for global analysis and optimization of communication in data-parallel programs. Our algorithm is distinct from existing approaches in that rather than handling loop-nests and array references one by one, it considers all communication in a procedure and their interactions under different placements before making a final decision on the placement of any communication. It exploits the flexibility resulting from this advanced analysis to eliminate redundancy, reduce the number of messages, and reduce contention for cache and communication buffers, all in a unified framework. In contrast, single loop-nest analysis often retains redundant communication, and more aggressive dataflow analysis on array sections can generate too many messages or cache and buffer contention. The algorithm has been implemented in the IBM pHPF compiler for High Performance Fortran. During compilation, the number of messages per processor goes down by as much as a factor of nine for some HPF programs. We present performance results for the IBM SP2 and a network of Sparc workstations (NOW) connected by a Myrinet switch. In many cases, the communication cost is reduced by a factor of two.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231391},
 doi = {http://doi.acm.org/10.1145/231379.231391},
 acmid = {231391},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Trinder:1996:GPP:249069.231392,
 author = {Trinder, P. W. and Hammond, K. and Mattson,Jr., J. S. and Partridge, A. S. and Peyton Jones, S. L.},
 title = {GUM: a portable parallel implementation of Haskell},
 abstract = {GUM is a portable, parallel implementation of the Haskell functional language. Despite sustained research interest in parallel functional programming, GUM is one of the first such systems to be made publicly available.GUM is message-based, and portability is facilitated by using the PVM communications harness that is available on many multi-processors. As a result, GUM is available for both shared-memory (Sun SPARCserver multiprocessors) and distributed-memory (networks of workstations) architectures. The high message-latency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message.Initial performance figures demonstrate absolute speedups relative to the best sequential compiler technology. To improve the performance of a parallel Haskell program GUM provides tools for monitoring and visualising the behaviour of threads and of processors during execution.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {79--88},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231392},
 doi = {http://doi.acm.org/10.1145/249069.231392},
 acmid = {231392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Trinder:1996:GPP:231379.231392,
 author = {Trinder, P. W. and Hammond, K. and Mattson,Jr., J. S. and Partridge, A. S. and Peyton Jones, S. L.},
 title = {GUM: a portable parallel implementation of Haskell},
 abstract = {GUM is a portable, parallel implementation of the Haskell functional language. Despite sustained research interest in parallel functional programming, GUM is one of the first such systems to be made publicly available.GUM is message-based, and portability is facilitated by using the PVM communications harness that is available on many multi-processors. As a result, GUM is available for both shared-memory (Sun SPARCserver multiprocessors) and distributed-memory (networks of workstations) architectures. The high message-latency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message.Initial performance figures demonstrate absolute speedups relative to the best sequential compiler technology. To improve the performance of a parallel Haskell program GUM provides tools for monitoring and visualising the behaviour of threads and of processors during execution.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {79--88},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231392},
 doi = {http://doi.acm.org/10.1145/231379.231392},
 acmid = {231392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boehm:1996:SG:231379.231394,
 author = {Boehm, Hans-J.},
 title = {Simple garbage-collector-safety},
 abstract = {A conservative garbage collector can typically be used with conventionally compiled programs written in C or C++. But two safety issues must be considered. First, the source code must not hide pointers from the garbage collector. This primarily requires stricter adherence to existing restrictions in the language definition. Second, we must ensure that the compiler will not perform transformations that invalidate this requirement.We argue that the same technique can be used to address both issues. We present an algorithm for annotating source or intermediate code to either check the validity of pointer arithmetic in the source, or to guarantee that under minimal, clearly defined assumptions about the compiler, the optimizer cannot "disguise" pointers. We discuss an implementation based on a preprocessor for the GNU C compiler (gcc), and give some measurements of program slow down.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {89--98},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231394},
 doi = {http://doi.acm.org/10.1145/231379.231394},
 acmid = {231394},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boehm:1996:SG:249069.231394,
 author = {Boehm, Hans-J.},
 title = {Simple garbage-collector-safety},
 abstract = {A conservative garbage collector can typically be used with conventionally compiled programs written in C or C++. But two safety issues must be considered. First, the source code must not hide pointers from the garbage collector. This primarily requires stricter adherence to existing restrictions in the language definition. Second, we must ensure that the compiler will not perform transformations that invalidate this requirement.We argue that the same technique can be used to address both issues. We present an algorithm for annotating source or intermediate code to either check the validity of pointer arithmetic in the source, or to guarantee that under minimal, clearly defined assumptions about the compiler, the optimizer cannot "disguise" pointers. We discuss an implementation based on a preprocessor for the GNU C compiler (gcc), and give some measurements of program slow down.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {89--98},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231394},
 doi = {http://doi.acm.org/10.1145/249069.231394},
 acmid = {231394},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bruggeman:1996:RCP:231379.231395,
 author = {Bruggeman, Carl and Waddell, Oscar and Dybvig, R. Kent},
 title = {Representing control in the presence of one-shot continuations},
 abstract = {Traditional first-class continuation mechanisms allow a captured continuation to be invoked multiple times. Many continuations, however, are invoked only once. This paper introduces one-shot</i> continuations, shows how they interact with traditional multi-shot continuations, and describes a stack-based implementation of control that handles both one-shot and multi-shot continuations. The implementation eliminates the copying overhead for one-shot continuations that is inherent in multi-shot continuations.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {99--107},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/231379.231395},
 doi = {http://doi.acm.org/10.1145/231379.231395},
 acmid = {231395},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bruggeman:1996:RCP:249069.231395,
 author = {Bruggeman, Carl and Waddell, Oscar and Dybvig, R. Kent},
 title = {Representing control in the presence of one-shot continuations},
 abstract = {Traditional first-class continuation mechanisms allow a captured continuation to be invoked multiple times. Many continuations, however, are invoked only once. This paper introduces one-shot</i> continuations, shows how they interact with traditional multi-shot continuations, and describes a stack-based implementation of control that handles both one-shot and multi-shot continuations. The implementation eliminates the copying overhead for one-shot continuations that is inherent in multi-shot continuations.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {99--107},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/249069.231395},
 doi = {http://doi.acm.org/10.1145/249069.231395},
 acmid = {231395},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burger:1996:PFN:231379.231397,
 author = {Burger, Robert G. and Dybvig, R. Kent},
 title = {Printing floating-point numbers quickly and accurately},
 abstract = {This paper presents a fast and accurate algorithm for printing floating-point numbers in both free- and fixed-format modes. In free-format mode, the algorithm generates the shortest, correctly rounded output string that converts to the same number when read back in, accommodating whatever rounding mode the reader uses. In fixed-format mode, the algorithm generates a correctly rounded output string using special # marks to denote insignificant trailing digits. For both modes, the algorithm employs a fast estimator to scale floating-point numbers efficiently.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {108--116},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/231379.231397},
 doi = {http://doi.acm.org/10.1145/231379.231397},
 acmid = {231397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {floating-point printing, run-time systems},
} 

@article{Burger:1996:PFN:249069.231397,
 author = {Burger, Robert G. and Dybvig, R. Kent},
 title = {Printing floating-point numbers quickly and accurately},
 abstract = {This paper presents a fast and accurate algorithm for printing floating-point numbers in both free- and fixed-format modes. In free-format mode, the algorithm generates the shortest, correctly rounded output string that converts to the same number when read back in, accommodating whatever rounding mode the reader uses. In fixed-format mode, the algorithm generates a correctly rounded output string using special # marks to denote insignificant trailing digits. For both modes, the algorithm employs a fast estimator to scale floating-point numbers efficiently.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {108--116},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/249069.231397},
 doi = {http://doi.acm.org/10.1145/249069.231397},
 acmid = {231397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {floating-point printing, run-time systems},
} 

@article{Dawson:1996:PPA:249069.231399,
 author = {Dawson, Steven and Ramakrishnan, C. R. and Warren, David S.},
 title = {Practical program analysis using general purpose logic programming systems\&mdash;a case study},
 abstract = {Many analysis problems can be cast in the form of evaluating minimal models of a logic program. Although such formulations are appealing due to their simplicity and declarativeness, they have not been widely used in practice because, either existing logic programming systems do not guarantee completeness, or those that do have been viewed as too inefficient for integration into a compiler. The objective of this paper is to re-examine this issue in the context of recent advances in implementation technologies of logic programming systems.We find that such declarative formulations can indeed be used in practical systems, when combined with the appropriate tool for evaluation. We use existing</i> formulations of analysis problems --- groundness analysis of logic programs, and strictness analysis of functional programs --- in this case study, and the XSB system, a table-based logic programming system, as the evaluation tool of choice. We give experimental evidence that the resultant groundness and strictness analysis systems are practical in terms of both time and space. In terms of implementation effort, the analyzers took less than 2 man-weeks (in total), to develop, optimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the entire system, including the components to read and preprocess input programs and to collect the analysis results, consists of about 500 lines of code.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {117--126},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231399},
 doi = {http://doi.acm.org/10.1145/249069.231399},
 acmid = {231399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dawson:1996:PPA:231379.231399,
 author = {Dawson, Steven and Ramakrishnan, C. R. and Warren, David S.},
 title = {Practical program analysis using general purpose logic programming systems\&mdash;a case study},
 abstract = {Many analysis problems can be cast in the form of evaluating minimal models of a logic program. Although such formulations are appealing due to their simplicity and declarativeness, they have not been widely used in practice because, either existing logic programming systems do not guarantee completeness, or those that do have been viewed as too inefficient for integration into a compiler. The objective of this paper is to re-examine this issue in the context of recent advances in implementation technologies of logic programming systems.We find that such declarative formulations can indeed be used in practical systems, when combined with the appropriate tool for evaluation. We use existing</i> formulations of analysis problems --- groundness analysis of logic programs, and strictness analysis of functional programs --- in this case study, and the XSB system, a table-based logic programming system, as the evaluation tool of choice. We give experimental evidence that the resultant groundness and strictness analysis systems are practical in terms of both time and space. In terms of implementation effort, the analyzers took less than 2 man-weeks (in total), to develop, optimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the entire system, including the components to read and preprocess input programs and to collect the analysis results, consists of about 500 lines of code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {117--126},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231399},
 doi = {http://doi.acm.org/10.1145/231379.231399},
 acmid = {231399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adl-Tabatabai:1996:ELM:231379.231402,
 author = {Adl-Tabatabai, Ali-Reza and Langdale, Geoff and Lucco, Steven and Wahbe, Robert},
 title = {Efficient and language-independent mobile programs},
 abstract = {This paper evaluates the design and implementation of Omniware: a safe, efficient, and language-independent system for executing mobile program modules. Previous approaches to implementing mobile code rely on either language semantics or abstract machine interpretation to enforce safety. In the former case, the mobile code system sacrifices universality to gain safety by dictating a particular source language or type system. In the latter case, the mobile code system sacrifices performance to gain safety through abstract machine interpretation.Omniware uses software fault isolation, a technology developed to provide safe extension code for databases and operating systems, to achieve a unique combination of language-independence and excellent performance. Software fault isolation uses only the semantics of the underlying processor to determine whether a mobile code module can corrupt its execution environment. This separation of programming language implementation from program module safety enables our mobile code system to use a radically simplified virtual machine as its basis for portability. We measured the performance of Omniware using a suite of four SPEC92 programs on the Pentium, PowerPC, Mips, and Sparc processor architectures. Including the overhead for enforcing safety on all four processors, OmniVM executed the benchmark programs within 21\% as fast as the optimized, unsafe code produced by the vendor-supplied compiler.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {127--136},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231402},
 doi = {http://doi.acm.org/10.1145/231379.231402},
 acmid = {231402},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adl-Tabatabai:1996:ELM:249069.231402,
 author = {Adl-Tabatabai, Ali-Reza and Langdale, Geoff and Lucco, Steven and Wahbe, Robert},
 title = {Efficient and language-independent mobile programs},
 abstract = {This paper evaluates the design and implementation of Omniware: a safe, efficient, and language-independent system for executing mobile program modules. Previous approaches to implementing mobile code rely on either language semantics or abstract machine interpretation to enforce safety. In the former case, the mobile code system sacrifices universality to gain safety by dictating a particular source language or type system. In the latter case, the mobile code system sacrifices performance to gain safety through abstract machine interpretation.Omniware uses software fault isolation, a technology developed to provide safe extension code for databases and operating systems, to achieve a unique combination of language-independence and excellent performance. Software fault isolation uses only the semantics of the underlying processor to determine whether a mobile code module can corrupt its execution environment. This separation of programming language implementation from program module safety enables our mobile code system to use a radically simplified virtual machine as its basis for portability. We measured the performance of Omniware using a suite of four SPEC92 programs on the Pentium, PowerPC, Mips, and Sparc processor architectures. Including the overhead for enforcing safety on all four processors, OmniVM executed the benchmark programs within 21\% as fast as the optimized, unsafe code produced by the vendor-supplied compiler.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {127--136},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231402},
 doi = {http://doi.acm.org/10.1145/249069.231402},
 acmid = {231402},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1996:OMR:231379.231407,
 author = {Lee, Peter and Leone, Mark},
 title = {Optimizing ML with run-time code generation},
 abstract = {We describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Run-time code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {137--148},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/231379.231407},
 doi = {http://doi.acm.org/10.1145/231379.231407},
 acmid = {231407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1996:OMR:249069.231407,
 author = {Lee, Peter and Leone, Mark},
 title = {Optimizing ML with run-time code generation},
 abstract = {We describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Run-time code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {137--148},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/249069.231407},
 doi = {http://doi.acm.org/10.1145/249069.231407},
 acmid = {231407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Auslander:1996:FED:249069.231409,
 author = {Auslander, Joel and Philipose, Matthai and Chambers, Craig and Eggers, Susan J. and Bershad, Brian N.},
 title = {Fast, effective dynamic compilation},
 abstract = {Dynamic compilation enables optimization based on the values of invariant data computed at run-time. Using the values of these run-time constants, a dynamic compiler can eliminate their memory loads, perform constant propagation and folding, remove branches they determine, and fully unroll loops they bound. However, the performance benefits of the more efficient, dynamically-compiled code are offset by the run-time cost of the dynamic compile. Our approach to dynamic compilation strives for both fast dynamic compilation and</i> high-quality dynamically-compiled code: the programmer annotates regions of the programs that should be compiled dynamically; a static, optimizing compiler automatically produces pre-optimized machine-code templates, using a pair of dataflow analyses that identify which variables will be constant at run-time; and a simple, dynamic compiler copies the templates, patching in the computed values of the run-time constants, to produce optimized, executable code. Our work targets general- purpose, imperative programming languages, initially C. Initial experiments applying dynamic compilation to C programs have produced speedups ranging from 1.2 to 1.8.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {149--159},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231409},
 doi = {http://doi.acm.org/10.1145/249069.231409},
 acmid = {231409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Auslander:1996:FED:231379.231409,
 author = {Auslander, Joel and Philipose, Matthai and Chambers, Craig and Eggers, Susan J. and Bershad, Brian N.},
 title = {Fast, effective dynamic compilation},
 abstract = {Dynamic compilation enables optimization based on the values of invariant data computed at run-time. Using the values of these run-time constants, a dynamic compiler can eliminate their memory loads, perform constant propagation and folding, remove branches they determine, and fully unroll loops they bound. However, the performance benefits of the more efficient, dynamically-compiled code are offset by the run-time cost of the dynamic compile. Our approach to dynamic compilation strives for both fast dynamic compilation and</i> high-quality dynamically-compiled code: the programmer annotates regions of the programs that should be compiled dynamically; a static, optimizing compiler automatically produces pre-optimized machine-code templates, using a pair of dataflow analyses that identify which variables will be constant at run-time; and a simple, dynamic compiler copies the templates, patching in the computed values of the run-time constants, to produce optimized, executable code. Our work targets general- purpose, imperative programming languages, initially C. Initial experiments applying dynamic compilation to C programs have produced speedups ranging from 1.2 to 1.8.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {149--159},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231409},
 doi = {http://doi.acm.org/10.1145/231379.231409},
 acmid = {231409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Engler:1996:VRE:231379.231411,
 author = {Engler, Dawson R.},
 title = {VCODE: a retargetable, extensible, very fast dynamic code generation system},
 abstract = {Dynamic code generation is the creation of executable code at runtime. Such "on-the-fly" code generation is a powerful technique, enabling applications to use runtime information to improve performance by up to an order of magnitude [4, 8,20, 22, 23].Unfortunately, previous general-purpose dynamic code generation systems have been either inefficient or non-portable. We present VCODE, a retargetable, extensible, very fast dynamic code generation system. An important feature of VCODE is that it generates machine code "in-place" without the use of intermediate data structures. Eliminating the need to construct and consume an intermediate representation at runtime makes VCODE both efficient and extensible. VCODE dynamically generates code at an approximate cost of six to ten instructions per generated instruction, making it over an order of magnitude faster than the most efficient general-purpose code generation system in the literature [10].Dynamic code generation is relatively well known within the compiler community. However, due in large part to the lack of a publicly available dynamic code generation system, it has remained a curiosity rather than a widely used technique. A practical contribution of this work is the free, unrestricted distribution of the VCODE system, which currently runs on the MIPS, SPARC, and Alpha architectures.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231411},
 doi = {http://doi.acm.org/10.1145/231379.231411},
 acmid = {231411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Engler:1996:VRE:249069.231411,
 author = {Engler, Dawson R.},
 title = {VCODE: a retargetable, extensible, very fast dynamic code generation system},
 abstract = {Dynamic code generation is the creation of executable code at runtime. Such "on-the-fly" code generation is a powerful technique, enabling applications to use runtime information to improve performance by up to an order of magnitude [4, 8,20, 22, 23].Unfortunately, previous general-purpose dynamic code generation systems have been either inefficient or non-portable. We present VCODE, a retargetable, extensible, very fast dynamic code generation system. An important feature of VCODE is that it generates machine code "in-place" without the use of intermediate data structures. Eliminating the need to construct and consume an intermediate representation at runtime makes VCODE both efficient and extensible. VCODE dynamically generates code at an approximate cost of six to ten instructions per generated instruction, making it over an order of magnitude faster than the most efficient general-purpose code generation system in the literature [10].Dynamic code generation is relatively well known within the compiler community. However, due in large part to the lack of a publicly available dynamic code generation system, it has remained a curiosity rather than a widely used technique. A practical contribution of this work is the free, unrestricted distribution of the VCODE system, which currently runs on the MIPS, SPARC, and Alpha architectures.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231411},
 doi = {http://doi.acm.org/10.1145/249069.231411},
 acmid = {231411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reppy:1996:SOS:249069.231412,
 author = {Reppy, John and Riecke, Jon},
 title = {Simple objects for Standard ML},
 abstract = {We propose a new approach to adding objects to Standard ML (SML) based on explicit declarations of object types, object constructors, and subtyping relationships, with a generalization of the SML case statement to a "typecase" on object types. The language, called Object ML (OML), has a type system that conservatively extends the SML type system, preserves sound static typing, and permits type inference. The type system sacrifices some of the expressiveness found in recently proposed schemes, but has the virtue of simplicity. We give examples of how features found in other object-oriented languages can be emulated in OML, discuss the formal properties of OML, and describe some implementation issues.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231412},
 doi = {http://doi.acm.org/10.1145/249069.231412},
 acmid = {231412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {functional programming, object-oriented programming, programming languages design, standard ML},
} 

@inproceedings{Reppy:1996:SOS:231379.231412,
 author = {Reppy, John and Riecke, Jon},
 title = {Simple objects for Standard ML},
 abstract = {We propose a new approach to adding objects to Standard ML (SML) based on explicit declarations of object types, object constructors, and subtyping relationships, with a generalization of the SML case statement to a "typecase" on object types. The language, called Object ML (OML), has a type system that conservatively extends the SML type system, preserves sound static typing, and permits type inference. The type system sacrifices some of the expressiveness found in recently proposed schemes, but has the virtue of simplicity. We give examples of how features found in other object-oriented languages can be emulated in OML, discuss the formal properties of OML, and describe some implementation issues.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231412},
 doi = {http://doi.acm.org/10.1145/231379.231412},
 acmid = {231412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {functional programming, object-oriented programming, programming languages design, standard ML},
} 

@inproceedings{Tarditi:1996:TTO:231379.231414,
 author = {Tarditi, D. and Morrisett, G. and Cheng, P. and Stone, C. and Harper, R. and Lee, P.},
 title = {TIL: a type-directed optimizing compiler for ML},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/231379.231414},
 doi = {http://doi.acm.org/10.1145/231379.231414},
 acmid = {231414},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tarditi:1996:TTO:249069.231414,
 author = {Tarditi, D. and Morrisett, G. and Cheng, P. and Stone, C. and Harper, R. and Lee, P.},
 title = {TIL: a type-directed optimizing compiler for ML},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/249069.231414},
 doi = {http://doi.acm.org/10.1145/249069.231414},
 acmid = {231414},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jagannathan:1996:FI:249069.231417,
 author = {Jagannathan, Suresh and Wright, Andrew},
 title = {Flow-directed inlining},
 abstract = {A flow-directed inlining</i> strategy uses information derived from control-flow analysis to specialize and inline procedures for functional and object-oriented languages. Since it uses control-flow analysis to identify candidate call sites, flow-directed inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flow-directed inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flow-directed inlining encourages modular implementations: control-flow analysis, inlining, and post-inlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {193--205},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/249069.231417},
 doi = {http://doi.acm.org/10.1145/249069.231417},
 acmid = {231417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jagannathan:1996:FI:231379.231417,
 author = {Jagannathan, Suresh and Wright, Andrew},
 title = {Flow-directed inlining},
 abstract = {A flow-directed inlining</i> strategy uses information derived from control-flow analysis to specialize and inline procedures for functional and object-oriented languages. Since it uses control-flow analysis to identify candidate call sites, flow-directed inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flow-directed inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flow-directed inlining encourages modular implementations: control-flow analysis, inlining, and post-inlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {193--205},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/231379.231417},
 doi = {http://doi.acm.org/10.1145/231379.231417},
 acmid = {231417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sperber:1996:RCP:231379.231419,
 author = {Sperber, Michael and Thiemann, Peter},
 title = {Realistic compilation by partial evaluation},
 abstract = {Two key steps in the compilation of strict functional languages are the conversion of higher-order functions to data structures (closures</i>) and the transformation to tail-recursive style. We show how to perform both steps at once by applying first-order offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C compilers. In addition, we have integrated various optimizations such as constant propagation, higher-order removal, and arity raising simply by modifying the underlying interpreter. Purely first-order methods suffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {206--214},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/231379.231419},
 doi = {http://doi.acm.org/10.1145/231379.231419},
 acmid = {231419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation of higher-order functional languages, partial evaluation, semantics-directed compiler generation},
} 

@article{Sperber:1996:RCP:249069.231419,
 author = {Sperber, Michael and Thiemann, Peter},
 title = {Realistic compilation by partial evaluation},
 abstract = {Two key steps in the compilation of strict functional languages are the conversion of higher-order functions to data structures (closures</i>) and the transformation to tail-recursive style. We show how to perform both steps at once by applying first-order offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C compilers. In addition, we have integrated various optimizations such as constant propagation, higher-order removal, and arity raising simply by modifying the underlying interpreter. Purely first-order methods suffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {206--214},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/249069.231419},
 doi = {http://doi.acm.org/10.1145/249069.231419},
 acmid = {231419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation of higher-order functional languages, partial evaluation, semantics-directed compiler generation},
} 

@inproceedings{Knoblock:1996:DS:231379.231428,
 author = {Knoblock, Todd B. and Ruf, Erik},
 title = {Data specialization},
 abstract = {Given a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231428},
 doi = {http://doi.acm.org/10.1145/231379.231428},
 acmid = {231428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Knoblock:1996:DS:249069.231428,
 author = {Knoblock, Todd B. and Ruf, Erik},
 title = {Data specialization},
 abstract = {Given a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231428},
 doi = {http://doi.acm.org/10.1145/249069.231428},
 acmid = {231428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramsey:1996:RMI:249069.231429,
 author = {Ramsey, Norman},
 title = {Relocating machine instructions by currying},
 abstract = {Relocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machine-dependent object-file format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machine-dependence.The New Jersey Machine-Code Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instruction-set specification, the toolkit generates the bit-manipulating code. Instructions are described by constructors</i>, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as "relocatable," meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machine-independent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machine-independent method of relocation.Currying rewrites a \&amp;lambda;-term into two nested \&amp;lambda;-terms. The standard implementation has the first \&amp;lambda; allocate a closure and store therein its operands and a pointer to the second \&amp;lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner \&amp;lambda;-terms---one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second \&amp;lambda; into the first makes many of the second \&amp;lambda;s identical---a handful are enough for a whole instruction set. This optimization reduces the size of machine-dependent assembly and linking code by 15--20\% for the MIPS, SPARC, and PowerPC, and by about 30\% for the Pentium. It also makes the second \&amp;lambda;s equivalent to relocating transformations named in standard object-file formats.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {226--236},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231429},
 doi = {http://doi.acm.org/10.1145/249069.231429},
 acmid = {231429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramsey:1996:RMI:231379.231429,
 author = {Ramsey, Norman},
 title = {Relocating machine instructions by currying},
 abstract = {Relocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machine-dependent object-file format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machine-dependence.The New Jersey Machine-Code Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instruction-set specification, the toolkit generates the bit-manipulating code. Instructions are described by constructors</i>, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as "relocatable," meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machine-independent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machine-independent method of relocation.Currying rewrites a \&amp;lambda;-term into two nested \&amp;lambda;-terms. The standard implementation has the first \&amp;lambda; allocate a closure and store therein its operands and a pointer to the second \&amp;lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner \&amp;lambda;-terms---one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second \&amp;lambda; into the first makes many of the second \&amp;lambda;s identical---a handful are enough for a whole instruction set. This optimization reduces the size of machine-dependent assembly and linking code by 15--20\% for the MIPS, SPARC, and PowerPC, and by about 30\% for the Pentium. It also makes the second \&amp;lambda;s equivalent to relocating transformations named in standard object-file formats.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {226--236},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231429},
 doi = {http://doi.acm.org/10.1145/231379.231429},
 acmid = {231429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandra:1996:TLS:231379.231430,
 author = {Chandra, Satish and Richards, Brad and Larus, James R.},
 title = {Teapot: language support for writing memory coherence protocols},
 abstract = {Recent shared-memory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve message-passing performance---while retaining the convenient programming model of a global address space---and to implement high-level language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domain-specific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as hand-written C code. A Teapot specification can be compiled both to an executable coherence protocol and to input for a model checking system, which permits the specification to be verified. We report our experiences coding and verifying several protocols written in Teapot, along with measurements of the overhead incurred by writing a protocol in a higher-level language.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/231379.231430},
 doi = {http://doi.acm.org/10.1145/231379.231430},
 acmid = {231430},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1996:TLS:249069.231430,
 author = {Chandra, Satish and Richards, Brad and Larus, James R.},
 title = {Teapot: language support for writing memory coherence protocols},
 abstract = {Recent shared-memory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve message-passing performance---while retaining the convenient programming model of a global address space---and to implement high-level language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domain-specific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as hand-written C code. A Teapot specification can be compiled both to an executable coherence protocol and to input for a model checking system, which permits the specification to be verified. We report our experiences coding and verifying several protocols written in Teapot, along with measurements of the overhead incurred by writing a protocol in a higher-level language.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/249069.231430},
 doi = {http://doi.acm.org/10.1145/249069.231430},
 acmid = {231430},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bailey:1996:TCD:231379.231431,
 author = {Bailey, Mark W. and Davidson, Jack W.},
 title = {Target-sensitive construction of diagnostic programs for procedure calling sequence generators},
 abstract = {Building compilers that generate correct code is difficult. In this paper we present a compiler testing technique that closes the gap between actual compiler implementations and correct compilers. Using formal specifications of procedure calling conventions, we have built a target-sensitive test suite generator that builds test cases for a specific aspect of compiler code generators the procedure calling sequence generator. By exercising compilers with these target-specific test suites, our automated testing tool has exposed bugs in every compiler tested. These compilers include ones that have been in heavy use for many years. The detected bugs cause more than 14,000 test cases to fail.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/231379.231431},
 doi = {http://doi.acm.org/10.1145/231379.231431},
 acmid = {231431},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bailey:1996:TCD:249069.231431,
 author = {Bailey, Mark W. and Davidson, Jack W.},
 title = {Target-sensitive construction of diagnostic programs for procedure calling sequence generators},
 abstract = {Building compilers that generate correct code is difficult. In this paper we present a compiler testing technique that closes the gap between actual compiler implementations and correct compilers. Using formal specifications of procedure calling conventions, we have built a target-sensitive test suite generator that builds test cases for a specific aspect of compiler code generators the procedure calling sequence generator. By exercising compilers with these target-specific test suites, our automated testing tool has exposed bugs in every compiler tested. These compilers include ones that have been in heavy use for many years. The detected bugs cause more than 14,000 test cases to fail.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/249069.231431},
 doi = {http://doi.acm.org/10.1145/249069.231431},
 acmid = {231431},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Russinovich:1996:RCN:249069.231432,
 author = {Russinovich, Mark and Cogswell, Bryce},
 title = {Replay for concurrent non-deterministic shared-memory applications},
 abstract = {Replay of shared-memory program execution is desirable in many domains including cyclic debugging, fault tolerance and performance monitoring. Past approaches to repeatable execution have focused on the problem of re-executing the shared-memory access patterns in parallel programs. With the proliferation of operating system supported threads and shared memory for uniprocessor programs, there is a clear need for efficient replay of concurrent applications. The solutions for parallel systems can be performance prohibitive when applied to the uniprocessor case. We present an algorithm, called the repeatable scheduling algorithm, combining scheduling and instruction counts to provide an invariant for efficient, language independent replay of concurrent shared-memory applications. The approach is shown to have trace overheads that are independent of the amount of sharing that takes place. An implementation for cyclic debugging on Mach 3.0 is evaluated and benchmarks show typical performance overheads of around 10\%. The algorithm implemented is compared with optimal event-based tracing and shown to do better with respect to the number of events monitored or number of events logged, in most cases by several orders of magnitude.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {258--266},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/249069.231432},
 doi = {http://doi.acm.org/10.1145/249069.231432},
 acmid = {231432},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction counter, non-determinism, repeatable execution, shared memory},
} 

@inproceedings{Russinovich:1996:RCN:231379.231432,
 author = {Russinovich, Mark and Cogswell, Bryce},
 title = {Replay for concurrent non-deterministic shared-memory applications},
 abstract = {Replay of shared-memory program execution is desirable in many domains including cyclic debugging, fault tolerance and performance monitoring. Past approaches to repeatable execution have focused on the problem of re-executing the shared-memory access patterns in parallel programs. With the proliferation of operating system supported threads and shared memory for uniprocessor programs, there is a clear need for efficient replay of concurrent applications. The solutions for parallel systems can be performance prohibitive when applied to the uniprocessor case. We present an algorithm, called the repeatable scheduling algorithm, combining scheduling and instruction counts to provide an invariant for efficient, language independent replay of concurrent shared-memory applications. The approach is shown to have trace overheads that are independent of the amount of sharing that takes place. An implementation for cyclic debugging on Mach 3.0 is evaluated and benchmarks show typical performance overheads of around 10\%. The algorithm implemented is compared with optimal event-based tracing and shown to do better with respect to the number of events monitored or number of events logged, in most cases by several orders of magnitude.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {258--266},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/231379.231432},
 doi = {http://doi.acm.org/10.1145/231379.231432},
 acmid = {231432},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction counter, non-determinism, repeatable execution, shared memory},
} 

@inproceedings{Ramalingam:1996:DFF:231379.231433,
 author = {Ramalingam, G.},
 title = {Data flow frequency analysis},
 abstract = {Conventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know how often</i> or with what probability</i> a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems --- the class of finite bi-distributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of the usual meet-overall-paths quantity. We show that Kildall's result expressing the meet-over-all-paths value as a maximal-fixed-point carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {267--277},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/231379.231433},
 doi = {http://doi.acm.org/10.1145/231379.231433},
 acmid = {231433},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramalingam:1996:DFF:249069.231433,
 author = {Ramalingam, G.},
 title = {Data flow frequency analysis},
 abstract = {Conventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know how often</i> or with what probability</i> a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems --- the class of finite bi-distributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of the usual meet-overall-paths quantity. We show that Kildall's result expressing the meet-over-all-paths value as a maximal-fixed-point carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {267--277},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/249069.231433},
 doi = {http://doi.acm.org/10.1145/249069.231433},
 acmid = {231433},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sreedhar:1996:NFE:231379.231434,
 author = {Sreedhar, Vugranam C. and Gao, Guang R. and Lee, Yong-Fong},
 title = {A new framework for exhaustive and incremental data flow analysis using DJ graphs},
 abstract = {We present a new elimination-based framework for exhaustive and incremental data flow analysis using the DJ graph representation of a program. Unlike the previous approaches to elimination-based incremental data flow analysis, our approach can handle arbitrary non-structural and structural changes to program flowgraphs, including those causing irreducibility. We show how our approach is related to (iterated) dominance frontiers, and exploit this relationship to establish the complexity of our exhaustive analysis and to aid the design of our incremental analysis.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {278--290},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/231379.231434},
 doi = {http://doi.acm.org/10.1145/231379.231434},
 acmid = {231434},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sreedhar:1996:NFE:249069.231434,
 author = {Sreedhar, Vugranam C. and Gao, Guang R. and Lee, Yong-Fong},
 title = {A new framework for exhaustive and incremental data flow analysis using DJ graphs},
 abstract = {We present a new elimination-based framework for exhaustive and incremental data flow analysis using the DJ graph representation of a program. Unlike the previous approaches to elimination-based incremental data flow analysis, our approach can handle arbitrary non-structural and structural changes to program flowgraphs, including those causing irreducibility. We show how our approach is related to (iterated) dominance frontiers, and exploit this relationship to establish the complexity of our exhaustive analysis and to aid the design of our incremental analysis.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {278--290},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/249069.231434},
 doi = {http://doi.acm.org/10.1145/249069.231434},
 acmid = {231434},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bilardi:1996:FGC:249069.231435,
 author = {Bilardi, Gianfranco and Pingali, Keshav},
 title = {A framework for generalized control dependence},
 abstract = {We generalize the notion of dominance</i> by defining a generalized dominance relation with respect to a set of paths in the control flow graph G</i> = (V, E</i>). This new definition leads to a generalized notion of control dependence</i>, which includes standard control dependence</i> and weak control dependence</i> as special cases.If the set of paths underlying a generalized dominance relation satisfies some natural closure conditions, that dominance relation is tree-structured. Given this tree, the corresponding control dependence relation can be computed optimally by reduction to the Roman Chariots Problem</i>, which we have developed previously for computing standard control dependence. More precisely, given linear preprocessing time and space, we can answer the (generalized version of the) so called cd, conds, and cdequiv queries in time proportional to the output of the query.To illustrate the utility of the framework, we show how weak control dependence can be computed optimally in O</i>(|E</i>|) preprocessing space and time. This improves the O</i>(|V</i>|<sup>3</sup>) time required by the best previous algorithm for this problem.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {5},
 month = {May},
 year = {1996},
 issn = {0362-1340},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/249069.231435},
 doi = {http://doi.acm.org/10.1145/249069.231435},
 acmid = {231435},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bilardi:1996:FGC:231379.231435,
 author = {Bilardi, Gianfranco and Pingali, Keshav},
 title = {A framework for generalized control dependence},
 abstract = {We generalize the notion of dominance</i> by defining a generalized dominance relation with respect to a set of paths in the control flow graph G</i> = (V, E</i>). This new definition leads to a generalized notion of control dependence</i>, which includes standard control dependence</i> and weak control dependence</i> as special cases.If the set of paths underlying a generalized dominance relation satisfies some natural closure conditions, that dominance relation is tree-structured. Given this tree, the corresponding control dependence relation can be computed optimally by reduction to the Roman Chariots Problem</i>, which we have developed previously for computing standard control dependence. More precisely, given linear preprocessing time and space, we can answer the (generalized version of the) so called cd, conds, and cdequiv queries in time proportional to the output of the query.To illustrate the utility of the framework, we show how weak control dependence can be computed optimally in O</i>(|E</i>|) preprocessing space and time. This improves the O</i>(|V</i>|<sup>3</sup>) time required by the best previous algorithm for this problem.},
 booktitle = {Proceedings of the ACM SIGPLAN 1996 conference on Programming language design and implementation},
 series = {PLDI '96},
 year = {1996},
 isbn = {0-89791-795-2},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/231379.231435},
 doi = {http://doi.acm.org/10.1145/231379.231435},
 acmid = {231435},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilson:1995:ECP:223428.207111,
 author = {Wilson, Robert P. and Lam, Monica S.},
 title = {Efficient context-sensitive pointer analysis for C programs},
 abstract = {This paper proposes an efficient technique for context-sensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful\&mdash;a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207111},
 doi = {http://doi.acm.org/10.1145/223428.207111},
 acmid = {207111},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wilson:1995:ECP:207110.207111,
 author = {Wilson, Robert P. and Lam, Monica S.},
 title = {Efficient context-sensitive pointer analysis for C programs},
 abstract = {This paper proposes an efficient technique for context-sensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful\&mdash;a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207111},
 doi = {http://doi.acm.org/10.1145/207110.207111},
 acmid = {207111},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ruf:1995:CAA:223428.207112,
 author = {Ruf, Erik},
 title = {Context-insensitive alias analysis reconsidered},
 abstract = {Recent work on alias analysis in the presence of pointers has concentrated on context-sensitive interprocedural analyses, which treat multiple calls to a single procedure independently rather than constructing a single approximation to a procedure's effect on all of its callers. While context-sensitive modeling offers the potential for greater precision by considering only realizable call-return paths, its empirical benefits have yet to be measured.This paper compares the precision of a simple, efficient, context-insensitive points-to analysis for the C programming language with that of a maximally context-sensitive version of the same analysis. We demonstrate that, for a number of pointer-intensive benchmark programs, context-insensitivity exerts little to no precision penalty. We also describe techniques for using the output of context-insensitive analysis to improve the efficiency of context-sensitive analysis without affecting precision.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223428.207112},
 doi = {http://doi.acm.org/10.1145/223428.207112},
 acmid = {207112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ruf:1995:CAA:207110.207112,
 author = {Ruf, Erik},
 title = {Context-insensitive alias analysis reconsidered},
 abstract = {Recent work on alias analysis in the presence of pointers has concentrated on context-sensitive interprocedural analyses, which treat multiple calls to a single procedure independently rather than constructing a single approximation to a procedure's effect on all of its callers. While context-sensitive modeling offers the potential for greater precision by considering only realizable call-return paths, its empirical benefits have yet to be measured.This paper compares the precision of a simple, efficient, context-insensitive points-to analysis for the C programming language with that of a maximally context-sensitive version of the same analysis. We demonstrate that, for a number of pointer-intensive benchmark programs, context-insensitivity exerts little to no precision penalty. We also describe techniques for using the output of context-insensitive analysis to improve the efficiency of context-sensitive analysis without affecting precision.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/207110.207112},
 doi = {http://doi.acm.org/10.1145/207110.207112},
 acmid = {207112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carini:1995:FIC:223428.207113,
 author = {Carini, Paul R. and Hind, Michael},
 title = {Flow-sensitive interprocedural constant propagation},
 abstract = {We present a flow-sensitive interprocedural constant propagation algorithm, which supports recursion while only performing one flow-sensitive analysis of each procedure. We present experimental results which show that this method finds substantially more constants than previous methods and is efficient in practice. We introduce new metrics for evaluating interprocedural constant propagation algorithms which measure the number of interprocedural constant values that are propagated. We use these metrics to provide further experimental results for our algorithm.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {23--31},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223428.207113},
 doi = {http://doi.acm.org/10.1145/223428.207113},
 acmid = {207113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carini:1995:FIC:207110.207113,
 author = {Carini, Paul R. and Hind, Michael},
 title = {Flow-sensitive interprocedural constant propagation},
 abstract = {We present a flow-sensitive interprocedural constant propagation algorithm, which supports recursion while only performing one flow-sensitive analysis of each procedure. We present experimental results which show that this method finds substantially more constants than previous methods and is efficient in practice. We introduce new metrics for evaluating interprocedural constant propagation algorithms which measure the number of interprocedural constant values that are propagated. We use these metrics to provide further experimental results for our algorithm.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {23--31},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/207110.207113},
 doi = {http://doi.acm.org/10.1145/207110.207113},
 acmid = {207113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pingali:1995:ADS:223428.207114,
 author = {Pingali, Keshav and Bilardi, Gianfranco},
 title = {APT: a data structure for optimal control dependence computation},
 abstract = {The control dependence relation is used extensively in restructuring compilers. This relation is usually represented using the control dependence graph; unfortunately, the size of this data structure can be quadratic in the size of the program, even for some structured programs. In this paper, we introduce a data structure called the augmented post-dominator tree (APT) which is constructed in space and time proportional to the size of the program, and which can answer control dependence queries in time proportional to the size of the output. Therefore, APT is an optimal representation of control dependence. We also show that using APT, we can compute SSA graphs, as well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, we put APT in perspective by showing that it can be viewed as a factored representation of control dependence graph in which filtered search is used to answer queries.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {32--46},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/223428.207114},
 doi = {http://doi.acm.org/10.1145/223428.207114},
 acmid = {207114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pingali:1995:ADS:207110.207114,
 author = {Pingali, Keshav and Bilardi, Gianfranco},
 title = {APT: a data structure for optimal control dependence computation},
 abstract = {The control dependence relation is used extensively in restructuring compilers. This relation is usually represented using the control dependence graph; unfortunately, the size of this data structure can be quadratic in the size of the program, even for some structured programs. In this paper, we introduce a data structure called the augmented post-dominator tree (APT) which is constructed in space and time proportional to the size of the program, and which can answer control dependence queries in time proportional to the size of the output. Therefore, APT is an optimal representation of control dependence. We also show that using APT, we can compute SSA graphs, as well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, we put APT in perspective by showing that it can be viewed as a factored representation of control dependence graph in which filtered search is used to answer queries.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {32--46},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/207110.207114},
 doi = {http://doi.acm.org/10.1145/207110.207114},
 acmid = {207114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tu:1995:EBP:223428.207115,
 author = {Tu, Peng and Padua, David},
 title = {Efficient building and placing of gating functions},
 abstract = {In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at \&oslash;-nodes. The gating functions specify the control dependences for each reaching definition at a \&oslash;-node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with \&oslash;-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a \&oslash;-node, but also a gating path expression which defines a gating function for the \&oslash;-node.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223428.207115},
 doi = {http://doi.acm.org/10.1145/223428.207115},
 acmid = {207115},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tu:1995:EBP:207110.207115,
 author = {Tu, Peng and Padua, David},
 title = {Efficient building and placing of gating functions},
 abstract = {In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at \&oslash;-nodes. The gating functions specify the control dependences for each reaching definition at a \&oslash;-node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with \&oslash;-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a \&oslash;-node, but also a gating path expression which defines a gating function for the \&oslash;-node.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/207110.207115},
 doi = {http://doi.acm.org/10.1145/207110.207115},
 acmid = {207115},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mueller:1995:ACB:207110.207116,
 author = {Mueller, Frank and Whalley, David B.},
 title = {Avoiding conditional branches by code replication},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {56--66},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/207110.207116},
 doi = {http://doi.acm.org/10.1145/207110.207116},
 acmid = {207116},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mueller:1995:ACB:223428.207116,
 author = {Mueller, Frank and Whalley, David B.},
 title = {Avoiding conditional branches by code replication},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {56--66},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223428.207116},
 doi = {http://doi.acm.org/10.1145/223428.207116},
 acmid = {207116},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Patterson:1995:ASB:223428.207117,
 author = {Patterson, Jason R. C.},
 title = {Accurate static branch prediction by value range propagation},
 abstract = {The ability to predict at compile time the likelihood of a particular branch being taken provides valuable information for several optimizations, including global instruction scheduling, code layout, function inlining, interprocedural register allocation and many high level optimizations. Previous attempts at static branch prediction have either used simple heuristics, which can be quite inaccurate, or put the burden onto the programmer by using execution profiling data or source code hints.This paper presents a new approach to static branch prediction called value range propagation. This method tracks the weighted value ranges of variables through a program, much like constant propagation. These value ranges may be either numeric of symbolic in nature.    Branch prediction is then performed by simply consulting the value range of the appropriate variable. Heuristics are used as a fallback for cases where the value range of the variable cannot be determined statically. In the process, value range propagationsubsumes both constant propagation and copy propagation.Experimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques. The value range propagation method can be implemented over any ``factored" dataflow representation with a static single assignment property (such as SSA form or a dependence flow graph where the variables have been renamed to achieve single assignment). Experimental results indicate that   the technique maintains the linear runtime behavior of constant propagation experienced in practice.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207117},
 doi = {http://doi.acm.org/10.1145/223428.207117},
 acmid = {207117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Patterson:1995:ASB:207110.207117,
 author = {Patterson, Jason R. C.},
 title = {Accurate static branch prediction by value range propagation},
 abstract = {The ability to predict at compile time the likelihood of a particular branch being taken provides valuable information for several optimizations, including global instruction scheduling, code layout, function inlining, interprocedural register allocation and many high level optimizations. Previous attempts at static branch prediction have either used simple heuristics, which can be quite inaccurate, or put the burden onto the programmer by using execution profiling data or source code hints.This paper presents a new approach to static branch prediction called value range propagation. This method tracks the weighted value ranges of variables through a program, much like constant propagation. These value ranges may be either numeric of symbolic in nature.    Branch prediction is then performed by simply consulting the value range of the appropriate variable. Heuristics are used as a fallback for cases where the value range of the variable cannot be determined statically. In the process, value range propagationsubsumes both constant propagation and copy propagation.Experimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques. The value range propagation method can be implemented over any ``factored" dataflow representation with a static single assignment property (such as SSA form or a dependence flow graph where the variables have been renamed to achieve single assignment). Experimental results indicate that   the technique maintains the linear runtime behavior of constant propagation experienced in practice.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207117},
 doi = {http://doi.acm.org/10.1145/207110.207117},
 acmid = {207117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Calder:1995:CSB:223428.207118,
 author = {Calder, Brad and Grunwald, Dirk and Lindsay, Donald and Martin, James and Mozer, Michael and Zorn, Benjamin},
 title = {Corpus-based static branch prediction},
 abstract = {Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this paper, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction, evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this paper, we use a neural network to map static features associated with each  branch to the probability that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique, it is effective across a range of programming languages and programming styles, and it does not rely on the use of expert-defined heuristics. In this paper, we describe the application of ESP to the problem of branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20\%, as compared with the 25\% miss rate obtained using the best existing  program-based   heuristics.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {79--92},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/223428.207118},
 doi = {http://doi.acm.org/10.1145/223428.207118},
 acmid = {207118},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Calder:1995:CSB:207110.207118,
 author = {Calder, Brad and Grunwald, Dirk and Lindsay, Donald and Martin, James and Mozer, Michael and Zorn, Benjamin},
 title = {Corpus-based static branch prediction},
 abstract = {Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this paper, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction, evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this paper, we use a neural network to map static features associated with each  branch to the probability that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique, it is effective across a range of programming languages and programming styles, and it does not rely on the use of expert-defined heuristics. In this paper, we describe the application of ESP to the problem of branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20\%, as compared with the 25\% miss rate obtained using the best existing  program-based   heuristics.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {79--92},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/207110.207118},
 doi = {http://doi.acm.org/10.1145/207110.207118},
 acmid = {207118},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dean:1995:SSO:223428.207119,
 author = {Dean, Jeffrey and Chambers, Craig and Grove, David},
 title = {Selective specialization for object-oriented languages},
 abstract = {Dynamic dispatching is a major source of run-time overhead in object-oriented languages, due both to the direct cost of method lookup and to the indirect effect of preventing other optimizations. To reduce this overhead, optimizing compilers for object-oriented languages analyze the classes of objects stored in program variables, with the goal of bounding the possible classes of message receivers enough so that the compiler can uniquely determine the target of a message send at compile time and replace the message send with a direct procedure call. Specialization is one important technique for improving the precision of this static class information: by compiling multiple versions of a method, each applicable to a subset of the possible argument classes of the  method, more precise static information about the classes of the method's arguments is obtained. Previous specialization strategies have not been selective about where this technique is applied, and therefore tended to significantly increase compile time and code space usage, particularly for large applications. In this paper, we present a more general framework for specialization in object-oriented languages and describe a goal directed specialization algorithm that makes selective decisions to apply specialization to those cases where it provides the highest benefit. Our results show that our algorithm improves the performance of a group of sizeable programs by 65\% to 275\% while increasing  compiled code space requirements by only 4\% to 10\%. Moreover, when compared to the previous  state-of-the-art specialization scheme, our algorithm improves performance by 11\% to 67\% while simultaneously reducing code space requirements by 65\% to 73\%.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {93--102},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223428.207119},
 doi = {http://doi.acm.org/10.1145/223428.207119},
 acmid = {207119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dean:1995:SSO:207110.207119,
 author = {Dean, Jeffrey and Chambers, Craig and Grove, David},
 title = {Selective specialization for object-oriented languages},
 abstract = {Dynamic dispatching is a major source of run-time overhead in object-oriented languages, due both to the direct cost of method lookup and to the indirect effect of preventing other optimizations. To reduce this overhead, optimizing compilers for object-oriented languages analyze the classes of objects stored in program variables, with the goal of bounding the possible classes of message receivers enough so that the compiler can uniquely determine the target of a message send at compile time and replace the message send with a direct procedure call. Specialization is one important technique for improving the precision of this static class information: by compiling multiple versions of a method, each applicable to a subset of the possible argument classes of the  method, more precise static information about the classes of the method's arguments is obtained. Previous specialization strategies have not been selective about where this technique is applied, and therefore tended to significantly increase compile time and code space usage, particularly for large applications. In this paper, we present a more general framework for specialization in object-oriented languages and describe a goal directed specialization algorithm that makes selective decisions to apply specialization to those cases where it provides the highest benefit. Our results show that our algorithm improves the performance of a group of sizeable programs by 65\% to 275\% while increasing  compiled code space requirements by only 4\% to 10\%. Moreover, when compared to the previous  state-of-the-art specialization scheme, our algorithm improves performance by 11\% to 67\% while simultaneously reducing code space requirements by 65\% to 73\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {93--102},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/207110.207119},
 doi = {http://doi.acm.org/10.1145/207110.207119},
 acmid = {207119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fernandez:1995:SEL:207110.207121,
 author = {Fern\'{a}ndez, Mary F.},
 title = {Simple and effective link-time optimization of Modula-3 programs},
 abstract = {Modula-3 supports development of modular programs by separating an object's interface from its implementation. This separation induces a runtime overhead in the implementation of objects, because it prevents the compiler from having complete information about a program's type hierarchy. This overhead can be reduced at link time, when the entire type hierarchy becomes available. We describe opportunities for link-time optimization of Modula-3, present two link-time optimizations that reduce the runtime costs of Modula-3's opaque types and methods, and show how link-time optimization could provide C++ which the benefits of opaques types at no additional runtime cost.Our optimization techniques are implemented in mld, a retargetable linker for the MIPS, SPARC, and Intel 486, mld links a machine-independent intermediate code that is suitable for link-time optimization and code generation. Linking intermediate code simplifies implementation of the optimizations and makes it possible to evaluate them on a wide range of architectures. mld's optimizations are effective: they reduce the total number of instructions executed by up to 14\% and convert as many as 79\% of indirect calls to direct calls.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {103--115},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/207110.207121},
 doi = {http://doi.acm.org/10.1145/207110.207121},
 acmid = {207121},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fernandez:1995:SEL:223428.207121,
 author = {Fern\'{a}ndez, Mary F.},
 title = {Simple and effective link-time optimization of Modula-3 programs},
 abstract = {Modula-3 supports development of modular programs by separating an object's interface from its implementation. This separation induces a runtime overhead in the implementation of objects, because it prevents the compiler from having complete information about a program's type hierarchy. This overhead can be reduced at link time, when the entire type hierarchy becomes available. We describe opportunities for link-time optimization of Modula-3, present two link-time optimizations that reduce the runtime costs of Modula-3's opaque types and methods, and show how link-time optimization could provide C++ which the benefits of opaques types at no additional runtime cost.Our optimization techniques are implemented in mld, a retargetable linker for the MIPS, SPARC, and Intel 486, mld links a machine-independent intermediate code that is suitable for link-time optimization and code generation. Linking intermediate code simplifies implementation of the optimizations and makes it possible to evaluate them on a wide range of architectures. mld's optimizations are effective: they reduce the total number of instructions executed by up to 14\% and convert as many as 79\% of indirect calls to direct calls.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {103--115},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223428.207121},
 doi = {http://doi.acm.org/10.1145/223428.207121},
 acmid = {207121},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shao:1995:TCS:223428.207123,
 author = {Shao, Zhong and Appel, Andrew W.},
 title = {A type-based compiler for standard ML},
 abstract = {Compile-time type information should be valuable in efficient compilation of statically typed functional languages such as Standard ML. But how should type-directed compilation work in real compilers, and how much performance gain will type-based optimizations yield? In order to support more efficient data representations and gain more experience about type-directed compilation, we have implemented a new type-based middle end and back end for the Standard ML of New Jersey compiler. We describe the basic design of the new compiler, identify a number of practical issues, and then compare the performance of our new compiler with the old non-type-based compiler. Our measurement shows that a combination of several simple type-based optimizations reduces heap allocation by 36\%; and  improves the already-efficient code generated by the old non-type-based compiler by about 19\% on a DECstation 500.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {116--129},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/223428.207123},
 doi = {http://doi.acm.org/10.1145/223428.207123},
 acmid = {207123},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shao:1995:TCS:207110.207123,
 author = {Shao, Zhong and Appel, Andrew W.},
 title = {A type-based compiler for standard ML},
 abstract = {Compile-time type information should be valuable in efficient compilation of statically typed functional languages such as Standard ML. But how should type-directed compilation work in real compilers, and how much performance gain will type-based optimizations yield? In order to support more efficient data representations and gain more experience about type-directed compilation, we have implemented a new type-based middle end and back end for the Standard ML of New Jersey compiler. We describe the basic design of the new compiler, identify a number of practical issues, and then compare the performance of our new compiler with the old non-type-based compiler. Our measurement shows that a combination of several simple type-based optimizations reduces heap allocation by 36\%; and  improves the already-efficient code generated by the old non-type-based compiler by about 19\% on a DECstation 500.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {116--129},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/207110.207123},
 doi = {http://doi.acm.org/10.1145/207110.207123},
 acmid = {207123},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burger:1995:RAU:223428.207125,
 author = {Burger, Robert G. and Waddell, Oscar and Dybvig, R. Kent},
 title = {Register allocation using lazy saves, eager restores, and greedy shuffling},
 abstract = {This paper presents a fast and effective linear intraprocedural register allocation strategy that optimizes register usage across procedure calls. It capitalizes on our observation that while procedures that do not contain calls (syntactic leaf routines) account for under one third of all procedure activations, procedures that actually make no calls (effective leaf routines) account for over two thirds of all procedure activations. Well-suited for both caller-and calle-save registers, our strategy employs a ``lazy" save mechanism that avoids saves for all effective leaf routines, an ``eager" restore mechanism that reduces the effect of memory latency, and a ``greedy" register shuffling algorithm that does a remarkbly  good job  of minimizing the need for temporaries in setting up procedure calls.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {130--138},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223428.207125},
 doi = {http://doi.acm.org/10.1145/223428.207125},
 acmid = {207125},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burger:1995:RAU:207110.207125,
 author = {Burger, Robert G. and Waddell, Oscar and Dybvig, R. Kent},
 title = {Register allocation using lazy saves, eager restores, and greedy shuffling},
 abstract = {This paper presents a fast and effective linear intraprocedural register allocation strategy that optimizes register usage across procedure calls. It capitalizes on our observation that while procedures that do not contain calls (syntactic leaf routines) account for under one third of all procedure activations, procedures that actually make no calls (effective leaf routines) account for over two thirds of all procedure activations. Well-suited for both caller-and calle-save registers, our strategy employs a ``lazy" save mechanism that avoids saves for all effective leaf routines, an ``eager" restore mechanism that reduces the effect of memory latency, and a ``greedy" register shuffling algorithm that does a remarkbly  good job  of minimizing the need for temporaries in setting up procedure calls.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {130--138},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/207110.207125},
 doi = {http://doi.acm.org/10.1145/207110.207125},
 acmid = {207125},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Altman:1995:SMS:223428.207128,
 author = {Altman, Erik R. and Govindarajan, R. and Gao, Guang R.},
 title = {Scheduling and mapping: software pipelining in the presence of structural hazards},
 abstract = {Recently, software pipelining methods based on an ILP (Integer Linear Programming) framework have been successfully applied to derive rate-optimal schedules for architectures involving clean pipelines - pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how, under a unified ILP framework, to simultaneously represent resource constraints for unclean pipelines, and the assignment or mapping of operations from a loop to those pipelines. In this paper we provide a framework which does exactly this, and in addition constructs rate-optimal software pipelined schedules. The proposed formulation and a solution method have been implemented and tested on a set of 1006 loops taken from various scientific and integer benchmark suites. The formulation found a rate-optimal schedule for 75\% of the loops, and required a median time of only 2 seconds per loop on a Sparc 10/30.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {139--150},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207128},
 doi = {http://doi.acm.org/10.1145/223428.207128},
 acmid = {207128},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Altman:1995:SMS:207110.207128,
 author = {Altman, Erik R. and Govindarajan, R. and Gao, Guang R.},
 title = {Scheduling and mapping: software pipelining in the presence of structural hazards},
 abstract = {Recently, software pipelining methods based on an ILP (Integer Linear Programming) framework have been successfully applied to derive rate-optimal schedules for architectures involving clean pipelines - pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how, under a unified ILP framework, to simultaneously represent resource constraints for unclean pipelines, and the assignment or mapping of operations from a loop to those pipelines. In this paper we provide a framework which does exactly this, and in addition constructs rate-optimal software pipelined schedules. The proposed formulation and a solution method have been implemented and tested on a set of 1006 loops taken from various scientific and integer benchmark suites. The formulation found a rate-optimal schedule for 75\% of the loops, and required a median time of only 2 seconds per loop on a Sparc 10/30.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {139--150},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207128},
 doi = {http://doi.acm.org/10.1145/207110.207128},
 acmid = {207128},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lo:1995:IBS:223428.207132,
 author = {Lo, Jack L. and Eggers, Susan J.},
 title = {Improving balanced scheduling with compiler optimizations that increase instruction-level parallelism},
 abstract = {Traditional list schedulers order instructions based on an optimistic  estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on non-blocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instruction-level parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively.Since its success depends on the amount of instruction-level parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiflow compiler, we simulated a non-blocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207132},
 doi = {http://doi.acm.org/10.1145/223428.207132},
 acmid = {207132},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lo:1995:IBS:207110.207132,
 author = {Lo, Jack L. and Eggers, Susan J.},
 title = {Improving balanced scheduling with compiler optimizations that increase instruction-level parallelism},
 abstract = {Traditional list schedulers order instructions based on an optimistic  estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on non-blocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instruction-level parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively.Since its success depends on the amount of instruction-level parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiflow compiler, we simulated a non-blocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207132},
 doi = {http://doi.acm.org/10.1145/207110.207132},
 acmid = {207132},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Amagbegnon:1995:IDS:207110.207134,
 author = {Amagb\'{e}gnon, Pascalin and Besnard, Lo\"{\i}c and Le Guernic, Paul},
 title = {Implementation of the data-flow synchronous language SIGNAL},
 abstract = {This paper presents the techniques used for the compilation of the data-flow, synchronous language SIGNAL. The key feature of the compiler is that it performs formal calculus on systems of boolean equations. The originality of the implementation of the compiler lies in the use of a tree structure to solve the equations.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {163--173},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/207110.207134},
 doi = {http://doi.acm.org/10.1145/207110.207134},
 acmid = {207134},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Amagbegnon:1995:IDS:223428.207134,
 author = {Amagb\'{e}gnon, Pascalin and Besnard, Lo\"{\i}c and Le Guernic, Paul},
 title = {Implementation of the data-flow synchronous language SIGNAL},
 abstract = {This paper presents the techniques used for the compilation of the data-flow, synchronous language SIGNAL. The key feature of the compiler is that it performs formal calculus on systems of boolean equations. The originality of the implementation of the compiler lies in the use of a tree structure to solve the equations.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {163--173},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223428.207134},
 doi = {http://doi.acm.org/10.1145/223428.207134},
 acmid = {207134},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aiken:1995:BSM:223428.207137,
 author = {Aiken, Alexander and F\"{a}hndrich, Manuel and Levien, Raph},
 title = {Better static memory management: improving region-based analysis of higher-order languages},
 abstract = {Static memory management replaces runtime garbage collection with compile-time annotations that make all memory allocation and deallocation explicit in a program. We improve upon the Tofte/Talpin region-based scheme for compile-time memory management[TT94]. In the Tofte/Talpin approach, all values, including closures, are stored in regions. Region lifetimes coincide with lexical scope, thus forming a runtime stack of regions and eliminating the need for garbage collection. We relax the requirement that region lifetimes be lexical. Rather, regions are allocated late and deallocated as early as possible by explicit memory operations. The placement of allocation and deallocation annotations is determined by solving a system of constraints that expresses all possible annotations. Experiments show that our approach reduces memory requirements significantly, in some cases asymptotically.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207137},
 doi = {http://doi.acm.org/10.1145/223428.207137},
 acmid = {207137},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aiken:1995:BSM:207110.207137,
 author = {Aiken, Alexander and F\"{a}hndrich, Manuel and Levien, Raph},
 title = {Better static memory management: improving region-based analysis of higher-order languages},
 abstract = {Static memory management replaces runtime garbage collection with compile-time annotations that make all memory allocation and deallocation explicit in a program. We improve upon the Tofte/Talpin region-based scheme for compile-time memory management[TT94]. In the Tofte/Talpin approach, all values, including closures, are stored in regions. Region lifetimes coincide with lexical scope, thus forming a runtime stack of regions and eliminating the need for garbage collection. We relax the requirement that region lifetimes be lexical. Rather, regions are allocated late and deallocated as early as possible by explicit memory operations. The placement of allocation and deallocation annotations is determined by solving a system of constraints that expresses all possible annotations. Experiments show that our approach reduces memory requirements significantly, in some cases asymptotically.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207137},
 doi = {http://doi.acm.org/10.1145/207110.207137},
 acmid = {207137},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Liao:1995:SAD:223428.207139,
 author = {Liao, Stan and Devadas, Srinivas and Keutzer, Kurt and Tjiang, Steve and Wang, Albert},
 title = {Storage assignment to decrease code size},
 abstract = {DSP architectures typically provide indirect addressing modes with auto-increment and decrement. In addition, indexing mode is not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into auto-increment and auto-decrement modes improves the size of the generated code.In this paper we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given. Our experimental results indicate an improvement of 3.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {186--195},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223428.207139},
 doi = {http://doi.acm.org/10.1145/223428.207139},
 acmid = {207139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Liao:1995:SAD:207110.207139,
 author = {Liao, Stan and Devadas, Srinivas and Keutzer, Kurt and Tjiang, Steve and Wang, Albert},
 title = {Storage assignment to decrease code size},
 abstract = {DSP architectures typically provide indirect addressing modes with auto-increment and decrement. In addition, indexing mode is not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into auto-increment and auto-decrement modes improves the size of the generated code.In this paper we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given. Our experimental results indicate an improvement of 3.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {186--195},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/207110.207139},
 doi = {http://doi.acm.org/10.1145/207110.207139},
 acmid = {207139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krishnamurthy:1995:OPP:207110.207142,
 author = {Krishnamurthy, Arvind and Yelick, Katherine},
 title = {Optimizing parallel programs with explicit synchronization},
 abstract = {We present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, based on work by Shasha and Snir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from post-wait synchronization, barriers, and locks.We demonstrate the use of this analysis by optimizing remote access on distributed memory machines. The optimizations include message pipelining, to allow multiple outstanding remote memory operations, conversion of two-way to one-way communication, and elimination of communication through data re-use. The performance improvements are as high as 20-35\% for programs running on a CM-5 multiprocessor using the Split-C language as a global address layer.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {196--204},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/207110.207142},
 doi = {http://doi.acm.org/10.1145/207110.207142},
 acmid = {207142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krishnamurthy:1995:OPP:223428.207142,
 author = {Krishnamurthy, Arvind and Yelick, Katherine},
 title = {Optimizing parallel programs with explicit synchronization},
 abstract = {We present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, based on work by Shasha and Snir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from post-wait synchronization, barriers, and locks.We demonstrate the use of this analysis by optimizing remote access on distributed memory machines. The optimizations include message pipelining, to allow multiple outstanding remote memory operations, conversion of two-way to one-way communication, and elimination of communication through data re-use. The performance improvements are as high as 20-35\% for programs running on a CM-5 multiprocessor using the Split-C language as a global address layer.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {196--204},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223428.207142},
 doi = {http://doi.acm.org/10.1145/223428.207142},
 acmid = {207142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cierniak:1995:UDC:223428.207145,
 author = {Cierniak, Micha\l and Li, Wei},
 title = {Unifying data and control transformations for distributed shared-memory machines},
 abstract = {We present a unified approach to locality optimization that employs both data and control transformations. Data transformations include changing the array layout in memory. Control transformations involve changing the execution order of programs. We have developed new techniques for compiler optimizations for distributed shared-memory machines, although the same techniques can be used for sequential machines with a memory hierarchy.Our compiler optimizations are based on an algebraic representation of data mappings and a new data locality model. We present a pure data transformation algorithm and an algorithm unifying data and control transformations. While there has been much work on control transformations, the opportunities for data transformations have been largely neglected. In fact, data transformations have the advantage of being applicable to programs that cannot be optimized with control transformations. The unified algorithm, which performs data and control transformations simultaneously, offers improvement over optimizations obtained by applying data and control transformations separately.The experimental results using a set of applications on a parallel machine show that the new optimizations improve performance significantly. These results are further analyzed using locality metrics with instrumentation and simulation.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {205--217},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223428.207145},
 doi = {http://doi.acm.org/10.1145/223428.207145},
 acmid = {207145},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cierniak:1995:UDC:207110.207145,
 author = {Cierniak, Micha\l and Li, Wei},
 title = {Unifying data and control transformations for distributed shared-memory machines},
 abstract = {We present a unified approach to locality optimization that employs both data and control transformations. Data transformations include changing the array layout in memory. Control transformations involve changing the execution order of programs. We have developed new techniques for compiler optimizations for distributed shared-memory machines, although the same techniques can be used for sequential machines with a memory hierarchy.Our compiler optimizations are based on an algebraic representation of data mappings and a new data locality model. We present a pure data transformation algorithm and an algorithm unifying data and control transformations. While there has been much work on control transformations, the opportunities for data transformations have been largely neglected. In fact, data transformations have the advantage of being applicable to programs that cannot be optimized with control transformations. The unified algorithm, which performs data and control transformations simultaneously, offers improvement over optimizations obtained by applying data and control transformations separately.The experimental results using a set of applications on a parallel machine show that the new optimizations improve performance significantly. These results are further analyzed using locality metrics with instrumentation and simulation.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {205--217},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/207110.207145},
 doi = {http://doi.acm.org/10.1145/207110.207145},
 acmid = {207145},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rauchwerger:1995:LTS:223428.207148,
 author = {Rauchwerger, Lawrence and Padua, David},
 title = {The LRPD test: speculative run-time parallelization of loops with privatization and reduction parallelization},
 abstract = {Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall, and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is re-executed serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching; it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {218--232},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/223428.207148},
 doi = {http://doi.acm.org/10.1145/223428.207148},
 acmid = {207148},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rauchwerger:1995:LTS:207110.207148,
 author = {Rauchwerger, Lawrence and Padua, David},
 title = {The LRPD test: speculative run-time parallelization of loops with privatization and reduction parallelization},
 abstract = {Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall, and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is re-executed serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching; it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {218--232},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/207110.207148},
 doi = {http://doi.acm.org/10.1145/207110.207148},
 acmid = {207148},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Knoop:1995:PAM:223428.207150,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {The power of assignment motion},
 abstract = {Assignment motion (AM) and expression motion (EM) are the basis of powerful and at the first sight incomparable techniques for removing partially redundant code from a program. Whereas AM aims at the elimination of complete assignments, a transformation which is always desirable, the more flexible EM requires temporaries to remove partial redundancies. Based on the observation that a simple program transformation enhances AM to subsume EM, we develop an algorithm that for the first time captures all second order effects between AM and EM transformations. Under usual structural restrictions, the worst case time complexity of our algorithm is essentially quadratic, a fact which explains the promising experience with our implementation.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {233--245},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223428.207150},
 doi = {http://doi.acm.org/10.1145/223428.207150},
 acmid = {207150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assignment motion, bit-vector data flow analyses, code motion, data flow analysis, partially redundant assignment and expression elimination, program optimization},
} 

@inproceedings{Knoop:1995:PAM:207110.207150,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {The power of assignment motion},
 abstract = {Assignment motion (AM) and expression motion (EM) are the basis of powerful and at the first sight incomparable techniques for removing partially redundant code from a program. Whereas AM aims at the elimination of complete assignments, a transformation which is always desirable, the more flexible EM requires temporaries to remove partial redundancies. Based on the observation that a simple program transformation enhances AM to subsume EM, we develop an algorithm that for the first time captures all second order effects between AM and EM transformations. Under usual structural restrictions, the worst case time complexity of our algorithm is essentially quadratic, a fact which explains the promising experience with our implementation.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {233--245},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/207110.207150},
 doi = {http://doi.acm.org/10.1145/207110.207150},
 acmid = {207150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assignment motion, bit-vector data flow analyses, code motion, data flow analysis, partially redundant assignment and expression elimination, program optimization},
} 

@article{Click:1995:GCM:223428.207154,
 author = {Click, Cliff},
 title = {Global code motion/global value numbering},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207154},
 doi = {http://doi.acm.org/10.1145/223428.207154},
 acmid = {207154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Click:1995:GCM:207110.207154,
 author = {Click, Cliff},
 title = {Global code motion/global value numbering},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207154},
 doi = {http://doi.acm.org/10.1145/207110.207154},
 acmid = {207154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1995:IPR:207110.207157,
 author = {Agrawal, Gagan and Saltz, Joel and Das, Raja},
 title = {Interprocedural partial redundancy elimination and its application to distributed memory compilation},
 abstract = {Partial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207157},
 doi = {http://doi.acm.org/10.1145/207110.207157},
 acmid = {207157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1995:IPR:223428.207157,
 author = {Agrawal, Gagan and Saltz, Joel and Das, Raja},
 title = {Interprocedural partial redundancy elimination and its application to distributed memory compilation},
 abstract = {Partial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207157},
 doi = {http://doi.acm.org/10.1145/223428.207157},
 acmid = {207157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kolte:1995:ERA:207110.207160,
 author = {Kolte, Priyadarshan and Wolfe, Michael},
 title = {Elimination of redundant array subscript range checks},
 abstract = {This paper presents a compiler optimization algorithm to reduce the run time overhead of array subscript range checks in programs without compromising safety. The algorithm is based on partial redundancy elimination and it incorporates previously developed algorithms for range check optimization. We implemented the algorithm in our research compiler, Nascent, and conducted experiments on a suite of 10 benchmark programs to obtain four results: (1) the execution overhead of naive range checking is high enough to merit optimization, (2) there are substantial differences between various optimizations, (3) loop-based optimizations that hoist checks out of loops are effective in eliminating about 98\% of the range checks, and (4) more sophisticated analysis and optimization algorithms produce very marginal benefits.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {270--278},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/207110.207160},
 doi = {http://doi.acm.org/10.1145/207110.207160},
 acmid = {207160},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kolte:1995:ERA:223428.207160,
 author = {Kolte, Priyadarshan and Wolfe, Michael},
 title = {Elimination of redundant array subscript range checks},
 abstract = {This paper presents a compiler optimization algorithm to reduce the run time overhead of array subscript range checks in programs without compromising safety. The algorithm is based on partial redundancy elimination and it incorporates previously developed algorithms for range check optimization. We implemented the algorithm in our research compiler, Nascent, and conducted experiments on a suite of 10 benchmark programs to obtain four results: (1) the execution overhead of naive range checking is high enough to merit optimization, (2) there are substantial differences between various optimizations, (3) loop-based optimizations that hoist checks out of loops are effective in eliminating about 98\% of the range checks, and (4) more sophisticated analysis and optimization algorithms produce very marginal benefits.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {270--278},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223428.207160},
 doi = {http://doi.acm.org/10.1145/223428.207160},
 acmid = {207160},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coleman:1995:TSS:207110.207162,
 author = {Coleman, Stephanie and McKinley, Kathryn S.},
 title = {Tile size selection using cache organization and data layout},
 abstract = {When dense matrix computations are too large to fit in cache, previous research proposes tiling to reduce or eliminate capacity misses. This paper presents a new algorithm for choosing problem-size dependent tile sizes based on the cache size and cache line size for a direct-mapped cache. The algorithm eliminates both capacity and self-interference misses and reduces cross-interference misses. We measured simulated miss rates and execution times for our algorithm and two others on a variety of problem sizes and cache organizations. At higher set associativity, our algorithm does not always achieve the best performance. However on direct-mapped caches, our algorithm improves simulated miss rates and measured execution times when compared with previous work.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/207110.207162},
 doi = {http://doi.acm.org/10.1145/207110.207162},
 acmid = {207162},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Coleman:1995:TSS:223428.207162,
 author = {Coleman, Stephanie and McKinley, Kathryn S.},
 title = {Tile size selection using cache organization and data layout},
 abstract = {When dense matrix computations are too large to fit in cache, previous research proposes tiling to reduce or eliminate capacity misses. This paper presents a new algorithm for choosing problem-size dependent tile sizes based on the cache size and cache line size for a direct-mapped cache. The algorithm eliminates both capacity and self-interference misses and reduces cross-interference misses. We measured simulated miss rates and execution times for our algorithm and two others on a variety of problem sizes and cache organizations. At higher set associativity, our algorithm does not always achieve the best performance. However on direct-mapped caches, our algorithm improves simulated miss rates and measured execution times when compared with previous work.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223428.207162},
 doi = {http://doi.acm.org/10.1145/223428.207162},
 acmid = {207162},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Larus:1995:EME:223428.207163,
 author = {Larus, James R. and Schnarr, Eric},
 title = {EEL: machine-independent executable editing},
 abstract = {EEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and time-consuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine- and system-independent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223428.207163},
 doi = {http://doi.acm.org/10.1145/223428.207163},
 acmid = {207163},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Larus:1995:EME:207110.207163,
 author = {Larus, James R. and Schnarr, Eric},
 title = {EEL: machine-independent executable editing},
 abstract = {EEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and time-consuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine- and system-independent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/207110.207163},
 doi = {http://doi.acm.org/10.1145/207110.207163},
 acmid = {207163},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barrett:1995:GCU:223428.207164,
 author = {Barrett, David A. and Zorn, Benjamin G.},
 title = {Garbage collection using a dynamic threatening boundary},
 abstract = {Generational techniques have been very successful in reducing the impact of garbage collection algorithms upon the performance of programs. However, all generational algorithms occasionally promote objects that later become garbage, resulting in an accumulation of garbage in older generations. Reclaiming this tenured garbage without resorting to collecting the entire heap is a difficult problem. In this paper, we describe a mechanism that extends existing generational collection algorithms by allowing them to reclaim tenured garbage more effectively. In particular, our dynamic threatening boundary mechanism divides memory into two spaces, one for shortlived, and another for long-lived objects. Unlike previous work, our collection mechanism can  dynamically adjust the boundary between these two spaces either forward or backward in time, essentially allowing data to become untenured. We describe an implementation of the dynamic threatening boundary mechanism and quantify its associated costs. We also describe a policy for setting the threatening boundary and evaluate its performance relative to existing generational collection algorithms. Our results show that a policy that uses the dynamic threatening boundary mechanism is effective at reclaiming tenured garbage.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {301--314},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/223428.207164},
 doi = {http://doi.acm.org/10.1145/223428.207164},
 acmid = {207164},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barrett:1995:GCU:207110.207164,
 author = {Barrett, David A. and Zorn, Benjamin G.},
 title = {Garbage collection using a dynamic threatening boundary},
 abstract = {Generational techniques have been very successful in reducing the impact of garbage collection algorithms upon the performance of programs. However, all generational algorithms occasionally promote objects that later become garbage, resulting in an accumulation of garbage in older generations. Reclaiming this tenured garbage without resorting to collecting the entire heap is a difficult problem. In this paper, we describe a mechanism that extends existing generational collection algorithms by allowing them to reclaim tenured garbage more effectively. In particular, our dynamic threatening boundary mechanism divides memory into two spaces, one for shortlived, and another for long-lived objects. Unlike previous work, our collection mechanism can  dynamically adjust the boundary between these two spaces either forward or backward in time, essentially allowing data to become untenured. We describe an implementation of the dynamic threatening boundary mechanism and quantify its associated costs. We also describe a policy for setting the threatening boundary and evaluate its performance relative to existing generational collection algorithms. Our results show that a policy that uses the dynamic threatening boundary mechanism is effective at reclaiming tenured garbage.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {301--314},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/207110.207164},
 doi = {http://doi.acm.org/10.1145/207110.207164},
 acmid = {207164},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ertl:1995:SCI:207110.207165,
 author = {Ertl, M. Anton},
 title = {Stack caching for interpreters},
 abstract = {An interpreter can spend a significant part of its execution time on accessing arguments of virtual machine instructions. This paper explores two methods to reduce this overhead for virtual stack machines by caching top-of-stack values in (real machine) registers. The dynamic method is based on having, for every possible state of the cache, one specialized version of the whole interpreter; the execution of an instruction usually changes the state of the cache and the next instruction is executed in the version corresponding to the new state. In the static method a state machine that keeps track of the cache state is added to the compiler. Common instructions exist in specialized versions for several states, but it is not necessary to have a version of every instruction for every cache state. Stack manipulation instructions are optimized away.},
 booktitle = {Proceedings of the ACM SIGPLAN 1995 conference on Programming language design and implementation},
 series = {PLDI '95},
 year = {1995},
 isbn = {0-89791-697-2},
 location = {La Jolla, California, United States},
 pages = {315--327},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/207110.207165},
 doi = {http://doi.acm.org/10.1145/207110.207165},
 acmid = {207165},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ertl:1995:SCI:223428.207165,
 author = {Ertl, M. Anton},
 title = {Stack caching for interpreters},
 abstract = {An interpreter can spend a significant part of its execution time on accessing arguments of virtual machine instructions. This paper explores two methods to reduce this overhead for virtual stack machines by caching top-of-stack values in (real machine) registers. The dynamic method is based on having, for every possible state of the cache, one specialized version of the whole interpreter; the execution of an instruction usually changes the state of the cache and the next instruction is executed in the version corresponding to the new state. In the static method a state machine that keeps track of the cache state is added to the compiler. Common instructions exist in specialized versions for several states, but it is not necessary to have a version of every instruction for every cache state. Stack manipulation instructions are optimized away.},
 journal = {SIGPLAN Not.},
 volume = {30},
 issue = {6},
 month = {June},
 year = {1995},
 issn = {0362-1340},
 pages = {315--327},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223428.207165},
 doi = {http://doi.acm.org/10.1145/223428.207165},
 acmid = {207165},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sabry:1994:CUD:773473.178244,
 author = {Sabry, Amr and Felleisen, Matthias},
 title = {Is continuation-passing useful for data flow analysis?},
 abstract = {The widespread use of the continuation-passing style (CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a common belief that the transformation has a positive effect on the analysis of programs. Investigations by Nielson [13] and Burn/Filho [5,6] support, to some degree, this belief with theoretical results. However, they do not pinpoint the source of increased abstract information and do not explain the observation of many people that continuation-passing confuses some conventional data flow analyses.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178244},
 doi = {http://doi.acm.org/10.1145/773473.178244},
 acmid = {178244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sabry:1994:CUD:178243.178244,
 author = {Sabry, Amr and Felleisen, Matthias},
 title = {Is continuation-passing useful for data flow analysis?},
 abstract = {The widespread use of the continuation-passing style (CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a common belief that the transformation has a positive effect on the analysis of programs. Investigations by Nielson [13] and Burn/Filho [5,6] support, to some degree, this belief with theoretical results. However, they do not pinpoint the source of increased abstract information and do not explain the observation of many people that continuation-passing confuses some conventional data flow analyses.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178244},
 doi = {http://doi.acm.org/10.1145/178243.178244},
 acmid = {178244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Appel:1994:SCS:178243.178245,
 author = {Appel, Andrew W. and MacQueen, David B.},
 title = {Separate compilation for Standard ML},
 abstract = {Languages that support abstraction and modular structure, such as Standard ML, Modula, Ada, and (more or less) C++, may have deeply nested dependency hierarchies among source files. In ML the problem is particularly severe because ML's powerful parameterized module (functor) facility entails dependencies among implementation modules, not just among interfaces.To efficiently compile individual modules in such languages, it is useful (in ML, necessary) to infer, digest, and cache the static environment resulting from the compilation of each module.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/178243.178245},
 doi = {http://doi.acm.org/10.1145/178243.178245},
 acmid = {178245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:1994:SCS:773473.178245,
 author = {Appel, Andrew W. and MacQueen, David B.},
 title = {Separate compilation for Standard ML},
 abstract = {Languages that support abstraction and modular structure, such as Standard ML, Modula, Ada, and (more or less) C++, may have deeply nested dependency hierarchies among source files. In ML the problem is particularly severe because ML's powerful parameterized module (functor) facility entails dependencies among implementation modules, not just among interfaces.To efficiently compile individual modules in such languages, it is useful (in ML, necessary) to infer, digest, and cache the static environment resulting from the compilation of each module.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/773473.178245},
 doi = {http://doi.acm.org/10.1145/773473.178245},
 acmid = {178245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Launchbury:1994:LFS:178243.178246,
 author = {Launchbury, John and Peyton Jones, Simon L.},
 title = {Lazy functional state threads},
 abstract = {Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a non-strict, purely-functional language.The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank-2 polymorphic type.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {24--35},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178246},
 doi = {http://doi.acm.org/10.1145/178243.178246},
 acmid = {178246},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Launchbury:1994:LFS:773473.178246,
 author = {Launchbury, John and Peyton Jones, Simon L.},
 title = {Lazy functional state threads},
 abstract = {Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a non-strict, purely-functional language.The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank-2 polymorphic type.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {24--35},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178246},
 doi = {http://doi.acm.org/10.1145/773473.178246},
 acmid = {178246},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ebcioglu:1994:VCT:773473.178247,
 author = {Ebcioglu, Kemal and Groves, Randy D. and Kim, Ki-Chang and Silberman, Gabriel M. and Ziv, Isaac},
 title = {VLIW compilation techniques in a superscalar environment},
 abstract = {We describe techniques for converting the intermediate code representation of a given program, as generated by a modern compiler, to another representation which produces the same run-time results, but can run faster on a superscalar machine. The algorithms, based on novel parallelization techniques for Very Long Instruction Word (VLIW) architectures, find and place together independently executable operations that may be far apart in the original code. i.e., they may be separated by many conditional branches or belong to different iterations of a loop. As a result, the functional units in the superscalar are presented with more work that can proceed in parallel, thus achieving higher performance than the approach of using hardware instruction dispatch techniques alone.While general scheduling techniques improve performance by removing idle pipeline cycles, to further improve performance on a superscalar with only a few functional units requires a reduction in the pathlength. We have designed a set of new algorithms for reducing pathlength and removing stalls due to branches, namely speculative load-store motion out of loops, unspeculation, limited combining, basic block expansion, and prolog tailoring. These algorithms were implemented in a prototype version of the IBM RS/6000 xlc compiler and have shown significant improvement in SPEC integer benchmarks on the IBM POWER machines.Also, we describe a new technique to obtain profiling information with low overhead, and some applications of profiling directed feedback, including scheduling heuristics, code reordering and branch reversal.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {36--48},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/773473.178247},
 doi = {http://doi.acm.org/10.1145/773473.178247},
 acmid = {178247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLIW, compiler optimizations, global scheduling, profiling directed feedback, software pipelining, superscalars},
} 

@inproceedings{Ebcioglu:1994:VCT:178243.178247,
 author = {Ebcioglu, Kemal and Groves, Randy D. and Kim, Ki-Chang and Silberman, Gabriel M. and Ziv, Isaac},
 title = {VLIW compilation techniques in a superscalar environment},
 abstract = {We describe techniques for converting the intermediate code representation of a given program, as generated by a modern compiler, to another representation which produces the same run-time results, but can run faster on a superscalar machine. The algorithms, based on novel parallelization techniques for Very Long Instruction Word (VLIW) architectures, find and place together independently executable operations that may be far apart in the original code. i.e., they may be separated by many conditional branches or belong to different iterations of a loop. As a result, the functional units in the superscalar are presented with more work that can proceed in parallel, thus achieving higher performance than the approach of using hardware instruction dispatch techniques alone.While general scheduling techniques improve performance by removing idle pipeline cycles, to further improve performance on a superscalar with only a few functional units requires a reduction in the pathlength. We have designed a set of new algorithms for reducing pathlength and removing stalls due to branches, namely speculative load-store motion out of loops, unspeculation, limited combining, basic block expansion, and prolog tailoring. These algorithms were implemented in a prototype version of the IBM RS/6000 xlc compiler and have shown significant improvement in SPEC integer benchmarks on the IBM POWER machines.Also, we describe a new technique to obtain profiling information with low overhead, and some applications of profiling directed feedback, including scheduling heuristics, code reordering and branch reversal.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {36--48},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/178243.178247},
 doi = {http://doi.acm.org/10.1145/178243.178247},
 acmid = {178247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLIW, compiler optimizations, global scheduling, profiling directed feedback, software pipelining, superscalars},
} 

@article{Srivastava:1994:LOA:773473.178248,
 author = {Srivastava, Amitabh and Wall, David W.},
 title = {Link-time optimization of address calculation on a 64-bit architecture},
 abstract = {Compilers for new machines with 64-bit addresses must generate code that works when the memory used by the program is large. Procedures and global variables are accessed indirectly via global address tables, and calling conventions include code to establish the addressability of the appropriate tables. In the common case of a program that does not require a lot of memory, all of this can be simplified considerably, with a corresponding reduction in program size and execution time.We have used our link-time code modification system OM to perform program transformations related to global address use on the Alpha AXP. Though simple, many of these arewhole-program optimizations that can be done only when we can see the entire  program at once, so link-time is an ideal occasion to perform them.This paper describes the optimizations performed and shows their effects on program size and performance. Relatively modest transformations, possible without moving code, improve the performance of SPEC benchmarks by an average of 1.5\%. More ambitious transformations, requiring an understanding of program structure that is thorough but not difficult at link-time, can do even better, reducing program size by 10\% or more, and improving performance by an average of 3.8\%.Even a program compiled monolithically with interprocedural optimization can benefit nearly as much from this technique, if it contains statically-linked pre-compiled library code. When the benchmark sources were compiled in this way, we  were still able to improve their performance by 1.35\% with the modest transformations and 3.4\% with the ambitious transformations.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178248},
 doi = {http://doi.acm.org/10.1145/773473.178248},
 acmid = {178248},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Srivastava:1994:LOA:178243.178248,
 author = {Srivastava, Amitabh and Wall, David W.},
 title = {Link-time optimization of address calculation on a 64-bit architecture},
 abstract = {Compilers for new machines with 64-bit addresses must generate code that works when the memory used by the program is large. Procedures and global variables are accessed indirectly via global address tables, and calling conventions include code to establish the addressability of the appropriate tables. In the common case of a program that does not require a lot of memory, all of this can be simplified considerably, with a corresponding reduction in program size and execution time.We have used our link-time code modification system OM to perform program transformations related to global address use on the Alpha AXP. Though simple, many of these arewhole-program optimizations that can be done only when we can see the entire  program at once, so link-time is an ideal occasion to perform them.This paper describes the optimizations performed and shows their effects on program size and performance. Relatively modest transformations, possible without moving code, improve the performance of SPEC benchmarks by an average of 1.5\%. More ambitious transformations, requiring an understanding of program structure that is thorough but not difficult at link-time, can do even better, reducing program size by 10\% or more, and improving performance by an average of 3.8\%.Even a program compiled monolithically with interprocedural optimization can benefit nearly as much from this technique, if it contains statically-linked pre-compiled library code. When the benchmark sources were compiled in this way, we  were still able to improve their performance by 1.35\% with the modest transformations and 3.4\% with the ambitious transformations.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178248},
 doi = {http://doi.acm.org/10.1145/178243.178248},
 acmid = {178248},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Granlund:1994:DII:178243.178249,
 author = {Granlund, Torbj\"{o}rn and Montgomery, Peter L.},
 title = {Division by invariant integers using multiplication},
 abstract = {
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178249},
 doi = {http://doi.acm.org/10.1145/178243.178249},
 acmid = {178249},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Granlund:1994:DII:773473.178249,
 author = {Granlund, Torbj\"{o}rn and Montgomery, Peter L.},
 title = {Division by invariant integers using multiplication},
 abstract = {
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178249},
 doi = {http://doi.acm.org/10.1145/773473.178249},
 acmid = {178249},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:1994:PCP:178243.178250,
 author = {Wang, Ko-Yang},
 title = {Precise compile-time performance prediction for superscalar-based computers},
 abstract = {Optimizing compilers (particularly parallel compilers) are constrained by their ability to predict performance consequences of the transformations they apply. Many factors, such as unknowns in control structures, dynamic behavior of programs, and complexity of the underlying hardware, make it very difficult for compilers to estimate the performance of the transformations accurately and efficiently. In this paper, we present a performance prediction framework that combines several innovative approaches to solve this problem. First, the framework employs a detailed, architecture-specific, but portable, cost model that can be used to estimate the cost of straight line code efficiently. Second, aggregated costs of loops and conditional statements are computed and represented symbolically. This avoids unnecessary, premature guesses and preserves the precision of the prediction. Third, symbolic comparison allows compilers to choose the best transformation dynamically and systematically. Some methodologies for applying the framework to optimizing parallel compilers to support automatic, performance-guided program restructuring are discussed.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178250},
 doi = {http://doi.acm.org/10.1145/178243.178250},
 acmid = {178250},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:1994:PCP:773473.178250,
 author = {Wang, Ko-Yang},
 title = {Precise compile-time performance prediction for superscalar-based computers},
 abstract = {Optimizing compilers (particularly parallel compilers) are constrained by their ability to predict performance consequences of the transformations they apply. Many factors, such as unknowns in control structures, dynamic behavior of programs, and complexity of the underlying hardware, make it very difficult for compilers to estimate the performance of the transformations accurately and efficiently. In this paper, we present a performance prediction framework that combines several innovative approaches to solve this problem. First, the framework employs a detailed, architecture-specific, but portable, cost model that can be used to estimate the cost of straight line code efficiently. Second, aggregated costs of loops and conditional statements are computed and represented symbolically. This avoids unnecessary, premature guesses and preserves the precision of the prediction. Third, symbolic comparison allows compilers to choose the best transformation dynamically and systematically. Some methodologies for applying the framework to optimizing parallel compilers to support automatic, performance-guided program restructuring are discussed.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178250},
 doi = {http://doi.acm.org/10.1145/773473.178250},
 acmid = {178250},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wagner:1994:ASE:178243.178251,
 author = {Wagner, Tim A. and Maverick, Vance and Graham, Susan L. and Harrison, Michael A.},
 title = {Accurate static estimators for program optimization},
 abstract = {Determining the relative execution frequency of program regions is essential for many important optimization techniques, including register allocation, function inlining, and instruction scheduling. Estimates derived from profiling with sample inputs are generally regarded as the most accurate source of this information; static (compile-time) estimates are considered to be distinctly inferior. If static estimates were shown to be competitive, however, their convenience would outweigh minor gains from profiling, and they would provide a sound basis for optimization when profiling is impossible.We use quantitative metrics to compare estimates from static analysis to those derived from profiles. For C programs, simple techniques for predicting branches and loop counts suffice  to estimate intraprocedural frequency patterns with high accuracy. To determine inter-procedural estimates successfully, we combine function-level information with a Markov model of control flow over the call graph to produce arc and basic block frequency estimates for the entire program.For a suite of 14 programs, including the C programs from the SPEC92 benchmark suite, we demonstrate that static estimates are competitive with those derived from profiles. Using simple heuristics, we can determine the most frequently executed blocks in each function with 81\% accuracy. With the Markov model, we identify 80\% of the frequently called functions. Combining the two techniques, we identify 76\% of the most frequently executed call sites.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178251},
 doi = {http://doi.acm.org/10.1145/178243.178251},
 acmid = {178251},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wagner:1994:ASE:773473.178251,
 author = {Wagner, Tim A. and Maverick, Vance and Graham, Susan L. and Harrison, Michael A.},
 title = {Accurate static estimators for program optimization},
 abstract = {Determining the relative execution frequency of program regions is essential for many important optimization techniques, including register allocation, function inlining, and instruction scheduling. Estimates derived from profiling with sample inputs are generally regarded as the most accurate source of this information; static (compile-time) estimates are considered to be distinctly inferior. If static estimates were shown to be competitive, however, their convenience would outweigh minor gains from profiling, and they would provide a sound basis for optimization when profiling is impossible.We use quantitative metrics to compare estimates from static analysis to those derived from profiles. For C programs, simple techniques for predicting branches and loop counts suffice  to estimate intraprocedural frequency patterns with high accuracy. To determine inter-procedural estimates successfully, we combine function-level information with a Markov model of control flow over the call graph to produce arc and basic block frequency estimates for the entire program.For a suite of 14 programs, including the C programs from the SPEC92 benchmark suite, we demonstrate that static estimates are competitive with those derived from profiles. Using simple heuristics, we can determine the most frequently executed blocks in each function with 81\% accuracy. With the Markov model, we identify 80\% of the frequently called functions. Combining the two techniques, we identify 76\% of the most frequently executed call sites.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178251},
 doi = {http://doi.acm.org/10.1145/773473.178251},
 acmid = {178251},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krall:1994:ISB:773473.178252,
 author = {Krall, Andreas},
 title = {Improving semi-static branch prediction by code replication},
 abstract = {Speculative execution on superscalar processors demands substantially better branch prediction than what has been previously available. In this paper we present code replication techniques that improve the accuracy of semi-static branch prediction to a level comparable to dynamic branch prediction schemes. Our technique uses profiling to collect information about the correlation between different branches and about the correlation between the subsequent outcomes of a single branch. Using this information and code replication the outcome of branches is represented in the program state. Our experiments have shown that the misprediction rate can almost be halved while the code size is increased by one third.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/773473.178252},
 doi = {http://doi.acm.org/10.1145/773473.178252},
 acmid = {178252},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krall:1994:ISB:178243.178252,
 author = {Krall, Andreas},
 title = {Improving semi-static branch prediction by code replication},
 abstract = {Speculative execution on superscalar processors demands substantially better branch prediction than what has been previously available. In this paper we present code replication techniques that improve the accuracy of semi-static branch prediction to a level comparable to dynamic branch prediction schemes. Our technique uses profiling to collect information about the correlation between different branches and about the correlation between the subsequent outcomes of a single branch. Using this information and code replication the outcome of branches is represented in the program state. Our experiments have shown that the misprediction rate can almost be halved while the code size is increased by one third.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/178243.178252},
 doi = {http://doi.acm.org/10.1145/178243.178252},
 acmid = {178252},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{von Hanxleden:1994:GBC:178243.178253,
 author = {von Hanxleden, Reinhard and Kennedy, Ken},
 title = {GIVE-N-TAKE\&mdash;a balanced code placement framework},
 abstract = {GIVE-N-TAKE is a code placement framework which uses a general producer-consumer concept. An advantage of GIVE-N-TAKE over existing partial redundancy elimination techniques is its concept of production regions, instead of single locations, which can be beneficial for general latency hiding. GIVE-N-TAKE guaranteed balanced production, that is, each production will be started and stopped once. The framework can also take advantage of production coming ``for free," as induced by side effects, without disturbing balance. GIVE-N-TAKE can place production either before or after consumption, and it also provides the option to hoist code out of potentially zero-trip loop (nest) constructs. GIVE-N-TAKE uses a fast elimination method based on  Tarjan intervals, with a complexity linear in the program size in most cases.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/178243.178253},
 doi = {http://doi.acm.org/10.1145/178243.178253},
 acmid = {178253},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{von Hanxleden:1994:GBC:773473.178253,
 author = {von Hanxleden, Reinhard and Kennedy, Ken},
 title = {GIVE-N-TAKE\&mdash;a balanced code placement framework},
 abstract = {GIVE-N-TAKE is a code placement framework which uses a general producer-consumer concept. An advantage of GIVE-N-TAKE over existing partial redundancy elimination techniques is its concept of production regions, instead of single locations, which can be beneficial for general latency hiding. GIVE-N-TAKE guaranteed balanced production, that is, each production will be started and stopped once. The framework can also take advantage of production coming ``for free," as induced by side effects, without disturbing balance. GIVE-N-TAKE can place production either before or after consumption, and it also provides the option to hoist code out of potentially zero-trip loop (nest) constructs. GIVE-N-TAKE uses a fast elimination method based on  Tarjan intervals, with a complexity linear in the program size in most cases.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/773473.178253},
 doi = {http://doi.acm.org/10.1145/773473.178253},
 acmid = {178253},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pugh:1994:CSP:773473.178254,
 author = {Pugh, William},
 title = {Counting solutions to Presburger formulas: how and why},
 abstract = {We describe methods that are able to count the number of integer solutions to selected free variables of a Presburger formula, or sum a polynomial over all integer solutions of selected free variables of a Presburger formula. This answer is given symbolically, in terms of symbolic constants (the remaining free variables in the Presburger formula).
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {121--134},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/773473.178254},
 doi = {http://doi.acm.org/10.1145/773473.178254},
 acmid = {178254},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pugh:1994:CSP:178243.178254,
 author = {Pugh, William},
 title = {Counting solutions to Presburger formulas: how and why},
 abstract = {We describe methods that are able to count the number of integer solutions to selected free variables of a Presburger formula, or sum a polynomial over all integer solutions of selected free variables of a Presburger formula. This answer is given symbolically, in terms of symbolic constants (the remaining free variables in the Presburger formula).
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {121--134},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/178243.178254},
 doi = {http://doi.acm.org/10.1145/178243.178254},
 acmid = {178254},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fisher:1994:PCS:773473.178255,
 author = {Fisher, Allan L. and Ghuloum, Anwar M.},
 title = {Parallelizing complex scans and reductions},
 abstract = {We present a method for automatically extracting parallel prefix programs from sequential loops, even in the presence of complicated conditional statements. Rather than searching for associative operators in the loop body directly, the method rests on the observation that functional composition itself is associative. Accordingly, we model the loop body as a multivalued function of multiple parameters, and look for a closed-form representation of arbitrary compositions of loop body instances. Careful analysis of conditionals allows this search to succeed in cases where existing automatic methods fail. The method has been implemented and used to generate code for the iWarp parallel computer.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {135--146},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178255},
 doi = {http://doi.acm.org/10.1145/773473.178255},
 acmid = {178255},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fisher:1994:PCS:178243.178255,
 author = {Fisher, Allan L. and Ghuloum, Anwar M.},
 title = {Parallelizing complex scans and reductions},
 abstract = {We present a method for automatically extracting parallel prefix programs from sequential loops, even in the presence of complicated conditional statements. Rather than searching for associative operators in the loop body directly, the method rests on the observation that functional composition itself is associative. Accordingly, we model the loop body as a multivalued function of multiple parameters, and look for a closed-form representation of arbitrary compositions of loop body instances. Careful analysis of conditionals allows this search to succeed in cases where existing automatic methods fail. The method has been implemented and used to generate code for the iWarp parallel computer.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {135--146},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178255},
 doi = {http://doi.acm.org/10.1145/178243.178255},
 acmid = {178255},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Knoop:1994:PDC:178243.178256,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {Partial dead code elimination},
 abstract = {A new aggressive algorithm for the elimination of partially dead code is presented, i.e., of code which is only dead on some program paths. Besides being more powerful than the usual approaches to dead code elimination, this algorithm is optimal in the following sense: partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impairing some program executions.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {147--158},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178256},
 doi = {http://doi.acm.org/10.1145/178243.178256},
 acmid = {178256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assignment motion, bit-vector data flow analyses, code motion, data flow analysis, dead code elimination, partial redundancy elimination, program optimization},
} 

@article{Knoop:1994:PDC:773473.178256,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {Partial dead code elimination},
 abstract = {A new aggressive algorithm for the elimination of partially dead code is presented, i.e., of code which is only dead on some program paths. Besides being more powerful than the usual approaches to dead code elimination, this algorithm is optimal in the following sense: partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impairing some program executions.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {147--158},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178256},
 doi = {http://doi.acm.org/10.1145/773473.178256},
 acmid = {178256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assignment motion, bit-vector data flow analyses, code motion, data flow analysis, dead code elimination, partial redundancy elimination, program optimization},
} 

@article{Briggs:1994:EPR:773473.178257,
 author = {Briggs, Preston and Cooper, Keith D.},
 title = {Effective partial redundancy elimination},
 abstract = {Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178257},
 doi = {http://doi.acm.org/10.1145/773473.178257},
 acmid = {178257},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briggs:1994:EPR:178243.178257,
 author = {Briggs, Preston and Cooper, Keith D.},
 title = {Effective partial redundancy elimination},
 abstract = {Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178257},
 doi = {http://doi.acm.org/10.1145/178243.178257},
 acmid = {178257},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1994:PST:773473.178258,
 author = {Johnson, Richard and Pearson, David and Pingali, Keshav},
 title = {The program structure tree: computing control regions in linear time},
 abstract = {In this paper, we describe the program structure tree (PST), a hierarchical representation of program structure based on single entry single exit (SESE) regions of the control flow graph. We give a linear-time algorithm for finding SESE regions and for building the PST of arbitrary control flow graphs (including irreducible ones). Next, we establish a connection between SESE regions and control dependence equivalence classes, and show how to use the algorithm to find control regions in linear time. Finally, we discuss some applications of the PST. Many control flow algorithms, such as construction of Static Single Assignment form, can be speeded up by applying the algorithms in a divide-and-conquer style to each SESE region on its own. The PST is also used to speed up data flow  analysis by exploiting ``sparsity". Experimental results from the Perfect Club and SPEC89 benchmarks confirm that the PST approach finds and exploits program structure.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {171--185},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/773473.178258},
 doi = {http://doi.acm.org/10.1145/773473.178258},
 acmid = {178258},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Johnson:1994:PST:178243.178258,
 author = {Johnson, Richard and Pearson, David and Pingali, Keshav},
 title = {The program structure tree: computing control regions in linear time},
 abstract = {In this paper, we describe the program structure tree (PST), a hierarchical representation of program structure based on single entry single exit (SESE) regions of the control flow graph. We give a linear-time algorithm for finding SESE regions and for building the PST of arbitrary control flow graphs (including irreducible ones). Next, we establish a connection between SESE regions and control dependence equivalence classes, and show how to use the algorithm to find control regions in linear time. Finally, we discuss some applications of the PST. Many control flow algorithms, such as construction of Static Single Assignment form, can be speeded up by applying the algorithms in a divide-and-conquer style to each SESE region on its own. The PST is also used to speed up data flow  analysis by exploiting ``sparsity". Experimental results from the Perfect Club and SPEC89 benchmarks confirm that the PST approach finds and exploits program structure.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {171--185},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/178243.178258},
 doi = {http://doi.acm.org/10.1145/178243.178258},
 acmid = {178258},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Davidson:1994:MAC:178243.178259,
 author = {Davidson, Jack W. and Jinturkar, Sanjay},
 title = {Memory access coalescing: a technique for eliminating redundant memory accesses},
 abstract = {As microprocessor speeds increase, memory bandwidth is increasingly the performance bottleneck for microprocessors. This has occurred because innovation and technological improvements in processor design have outpaced advances in memory design. Most attempts at addressing this problem have involved hardware solutions. Unfortunately, these solutions do little to help the situation with respect to current microprocessors. In previous work, we developed, implemented, and evaluated an algorithm that exploited the ability of newer machines with wide-buses to load/store multiple floating-point operands in a single memory reference. This paper describes a general code improvement algorithm that transforms code to better exploit the available memory bandwidth on existing microprocessors as  well as wide-bus machines. Where possible and advantageous, the algorithm coalesces narrow memory references into wide ones. An interesting characteristic of the algorithm is that some decisions about the applicability of the transformation are made at run time. This dynamic analysis significantly increases the probability of the transformation being applied. The code improvement transformation was implemented and added to the repertoire of code improvements of an existing retargetable optimizing back end. Using three current architectures as evaluation platforms, the effectiveness of the transformation was measured on a set of compute- and memory-intensive programs. Interestingly, the effectiveness of the transformation varied significantly with respect to the instruction-set   architecture of the tested platform. For one of the tested architectures, improvements in execution speed ranging from 5 to 40 percent were observed. For another, the improvements in execution speed ranged from 5 to 20 percent, while for yet another, the transformation resulted in slower code for all programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {186--195},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/178243.178259},
 doi = {http://doi.acm.org/10.1145/178243.178259},
 acmid = {178259},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Davidson:1994:MAC:773473.178259,
 author = {Davidson, Jack W. and Jinturkar, Sanjay},
 title = {Memory access coalescing: a technique for eliminating redundant memory accesses},
 abstract = {As microprocessor speeds increase, memory bandwidth is increasingly the performance bottleneck for microprocessors. This has occurred because innovation and technological improvements in processor design have outpaced advances in memory design. Most attempts at addressing this problem have involved hardware solutions. Unfortunately, these solutions do little to help the situation with respect to current microprocessors. In previous work, we developed, implemented, and evaluated an algorithm that exploited the ability of newer machines with wide-buses to load/store multiple floating-point operands in a single memory reference. This paper describes a general code improvement algorithm that transforms code to better exploit the available memory bandwidth on existing microprocessors as  well as wide-bus machines. Where possible and advantageous, the algorithm coalesces narrow memory references into wide ones. An interesting characteristic of the algorithm is that some decisions about the applicability of the transformation are made at run time. This dynamic analysis significantly increases the probability of the transformation being applied. The code improvement transformation was implemented and added to the repertoire of code improvements of an existing retargetable optimizing back end. Using three current architectures as evaluation platforms, the effectiveness of the transformation was measured on a set of compute- and memory-intensive programs. Interestingly, the effectiveness of the transformation varied significantly with respect to the instruction-set   architecture of the tested platform. For one of the tested architectures, improvements in execution speed ranging from 5 to 40 percent were observed. For another, the improvements in execution speed ranged from 5 to 20 percent, while for yet another, the transformation resulted in slower code for all programs.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {186--195},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/773473.178259},
 doi = {http://doi.acm.org/10.1145/773473.178259},
 acmid = {178259},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Srivastava:1994:ASB:773473.178260,
 author = {Srivastava, Amitabh and Eustace, Alan},
 title = {ATOM: a system for building customized program analysis tools},
 abstract = {ATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all code-instrumenting tools; this is the difficult and time-consuming part. The user simply defines the tool-specific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {196--205},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/773473.178260},
 doi = {http://doi.acm.org/10.1145/773473.178260},
 acmid = {178260},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Srivastava:1994:ASB:178243.178260,
 author = {Srivastava, Amitabh and Eustace, Alan},
 title = {ATOM: a system for building customized program analysis tools},
 abstract = {ATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all code-instrumenting tools; this is the difficult and time-consuming part. The user simply defines the tool-specific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {196--205},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/178243.178260},
 doi = {http://doi.acm.org/10.1145/178243.178260},
 acmid = {178260},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reinhold:1994:CPG:178243.178261,
 author = {Reinhold, Mark B.},
 title = {Cache performance of garbage-collected programs},
 abstract = {As processor speeds continue to improve relative to main-memory access times, cache performance is becoming an increasingly important component of program performance. Prior work on the cache performance of garbage-collected programs either argues or assumes that conventional garbage-collection methods will yield poor performance, and has therefore concentrated on new collection algorithms designed specifically to improve cache-level reference locality.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {206--217},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178261},
 doi = {http://doi.acm.org/10.1145/178243.178261},
 acmid = {178261},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reinhold:1994:CPG:773473.178261,
 author = {Reinhold, Mark B.},
 title = {Cache performance of garbage-collected programs},
 abstract = {As processor speeds continue to improve relative to main-memory access times, cache performance is becoming an increasingly important component of program performance. Prior work on the cache performance of garbage-collected programs either argues or assumes that conventional garbage-collection methods will yield poor performance, and has therefore concentrated on new collection algorithms designed specifically to improve cache-level reference locality.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {206--217},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178261},
 doi = {http://doi.acm.org/10.1145/773473.178261},
 acmid = {178261},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hummel:1994:GDD:773473.178262,
 author = {Hummel, Joseph and Hendren, Laurie J. and Nicolau, Alexandru},
 title = {A general data dependence test for dynamic, pointer-based data structures},
 abstract = {Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {218--229},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178262},
 doi = {http://doi.acm.org/10.1145/773473.178262},
 acmid = {178262},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hummel:1994:GDD:178243.178262,
 author = {Hummel, Joseph and Hendren, Laurie J. and Nicolau, Alexandru},
 title = {A general data dependence test for dynamic, pointer-based data structures},
 abstract = {Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {218--229},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178262},
 doi = {http://doi.acm.org/10.1145/178243.178262},
 acmid = {178262},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Deutsch:1994:IMA:773473.178263,
 author = {Deutsch, Alain},
 title = {Interprocedural may-alias analysis for pointers: beyond <italic>k</italic>-limiting},
 abstract = {Existing methods for alias analysis of recursive pointer data structures are based on two approximation techniques: k-limiting, and store-based (or equivalently location or region-based) approximations, which blur distinction between elements of recursive data structures. Although notable progress in inter-procedural alias analysis has been recently accomplished, very little progress in the precision of analysis of recursive pointer data structures has been seen since the inception of these approximation techniques by Jones and Muchnick a decade ago. As a result, optimizing, verifying and parallelizing programs with pointers has remained difficult.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {230--241},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178263},
 doi = {http://doi.acm.org/10.1145/773473.178263},
 acmid = {178263},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Deutsch:1994:IMA:178243.178263,
 author = {Deutsch, Alain},
 title = {Interprocedural may-alias analysis for pointers: beyond <italic>k</italic>-limiting},
 abstract = {Existing methods for alias analysis of recursive pointer data structures are based on two approximation techniques: k-limiting, and store-based (or equivalently location or region-based) approximations, which blur distinction between elements of recursive data structures. Although notable progress in inter-procedural alias analysis has been recently accomplished, very little progress in the precision of analysis of recursive pointer data structures has been seen since the inception of these approximation techniques by Jones and Muchnick a decade ago. As a result, optimizing, verifying and parallelizing programs with pointers has remained difficult.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {230--241},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178263},
 doi = {http://doi.acm.org/10.1145/178243.178263},
 acmid = {178263},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Emami:1994:CIP:178243.178264,
 author = {Emami, Maryam and Ghiya, Rakesh and Hendren, Laurie J.},
 title = {Context-sensitive interprocedural points-to analysis in the presence of function pointers},
 abstract = {This paper reports on the design, implementation, and empirical results of a new method for dealing with the aliasing problem in C. The method is based on approximating the points-to relationships between accessible stack locations, and can be used to generate alias pairs, or used directly for other analyses and transformations.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {242--256},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/178243.178264},
 doi = {http://doi.acm.org/10.1145/178243.178264},
 acmid = {178264},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Emami:1994:CIP:773473.178264,
 author = {Emami, Maryam and Ghiya, Rakesh and Hendren, Laurie J.},
 title = {Context-sensitive interprocedural points-to analysis in the presence of function pointers},
 abstract = {This paper reports on the design, implementation, and empirical results of a new method for dealing with the aliasing problem in C. The method is based on approximating the points-to relationships between accessible stack locations, and can be used to generate alias pairs, or used directly for other analyses and transformations.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {242--256},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/773473.178264},
 doi = {http://doi.acm.org/10.1145/773473.178264},
 acmid = {178264},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kurlander:1994:ZRS:178243.178420,
 author = {Kurlander, Steven M. and Fischer, Charles N.},
 title = {Zero-cost range splitting},
 abstract = {This paper presents a new optimization technique that uses empty delay slots to improve code scheduling. We are able to split live ranges for free, by inserting spill code into empty delay slots. Splitting a live range can reduce interferences with other live ranges and can sometimes free registers. Live ranges no longer interfering with the split live range can sometimes make use of the extra register.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {257--265},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/178243.178420},
 doi = {http://doi.acm.org/10.1145/178243.178420},
 acmid = {178420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kurlander:1994:ZRS:773473.178420,
 author = {Kurlander, Steven M. and Fischer, Charles N.},
 title = {Zero-cost range splitting},
 abstract = {This paper presents a new optimization technique that uses empty delay slots to improve code scheduling. We are able to split live ranges for free, by inserting spill code into empty delay slots. Splitting a live range can reduce interferences with other live ranges and can sometimes free registers. Live ranges no longer interfering with the split live range can sometimes make use of the extra register.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {257--265},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/773473.178420},
 doi = {http://doi.acm.org/10.1145/773473.178420},
 acmid = {178420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Norris:1994:RAO:178243.178427,
 author = {Norris, Cindy and Pollock, Lori L.},
 title = {Register allocation over the program dependence graph},
 abstract = {This paper describes RAP, a Register Allocator that allocates registers over the Program Dependence Graph (PDG) representation of a program in a hierarchical manner. The PDG program representation has been used successfully for scalar optimizations, the detection and improvement of parallelism for vector machines, multiple processor machines, and machines that exhibit instruction level parallelism, as well as debugging, the integration of different versions of a program, and translation of imperative programs for data flow machines. By basing register allocation on the PDG, the register allocation phase may be more easily integrated and intertwined with other optimization analyses and transformations. In addition, the advantages of a hierarchical approach to global register allocation can be attained without constructing an additional structure used solely for register allocation. Our experimental results have shown that on average, code allocated registers via RAP executed 2.7\% faster than code allocated registers via a standard global register allocator.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {266--277},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178427},
 doi = {http://doi.acm.org/10.1145/178243.178427},
 acmid = {178427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Norris:1994:RAO:773473.178427,
 author = {Norris, Cindy and Pollock, Lori L.},
 title = {Register allocation over the program dependence graph},
 abstract = {This paper describes RAP, a Register Allocator that allocates registers over the Program Dependence Graph (PDG) representation of a program in a hierarchical manner. The PDG program representation has been used successfully for scalar optimizations, the detection and improvement of parallelism for vector machines, multiple processor machines, and machines that exhibit instruction level parallelism, as well as debugging, the integration of different versions of a program, and translation of imperative programs for data flow machines. By basing register allocation on the PDG, the register allocation phase may be more easily integrated and intertwined with other optimization analyses and transformations. In addition, the advantages of a hierarchical approach to global register allocation can be attained without constructing an additional structure used solely for register allocation. Our experimental results have shown that on average, code allocated registers via RAP executed 2.7\% faster than code allocated registers via a standard global register allocator.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {266--277},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178427},
 doi = {http://doi.acm.org/10.1145/773473.178427},
 acmid = {178427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wismuller:1994:DGO:773473.178430,
 author = {Wism\"{u}ller, Roland},
 title = {Debugging of globally optimized programs using data flow analysis},
 abstract = {Advanced processor and machine architectures need optimizing compilers to be efficiently programmed in high level languages. Therefore the need for source level debuggers that can handle optimized programs is rising. One difficulty in debugging optimized code arises from the problem to determine the values of source code variables. To ensure correct debugger behaviour with optimized programs, the debugger not only has to determine the variable's storage location or associated register. It must also verify that the variable is current, i.e. the value determined from that location is really the value that the variable would have in unoptimized code. We will deduce requirements on algorithms for currentness determination and present an algorithm meeting this requirements that is more general than previous work. We will also give first experiences with an implementation. To our knowledge this is the first implementation of a currentness determination algorithm for globally optimized code.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178430},
 doi = {http://doi.acm.org/10.1145/773473.178430},
 acmid = {178430},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wismuller:1994:DGO:178243.178430,
 author = {Wism\"{u}ller, Roland},
 title = {Debugging of globally optimized programs using data flow analysis},
 abstract = {Advanced processor and machine architectures need optimizing compilers to be efficiently programmed in high level languages. Therefore the need for source level debuggers that can handle optimized programs is rising. One difficulty in debugging optimized code arises from the problem to determine the values of source code variables. To ensure correct debugger behaviour with optimized programs, the debugger not only has to determine the variable's storage location or associated register. It must also verify that the variable is current, i.e. the value determined from that location is really the value that the variable would have in unoptimized code. We will deduce requirements on algorithms for currentness determination and present an algorithm meeting this requirements that is more general than previous work. We will also give first experiences with an implementation. To our knowledge this is the first implementation of a currentness determination algorithm for globally optimized code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178430},
 doi = {http://doi.acm.org/10.1145/178243.178430},
 acmid = {178430},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Austin:1994:EDP:773473.178446,
 author = {Austin, Todd M. and Breach, Scott E. and Sohi, Gurindar S.},
 title = {Efficient detection of all pointer and array access errors},
 abstract = {We present a pointer and array access checking technique that provides complete error coverage through a simple set of program transformations. Our technique, based on an extended safe pointer representation, has a number of novel aspects. Foremost, it is the first technique that detects all spatial and temporal access errors. Its use is not limited by the expressiveness of the language; that is, it can be applied successfully to compiled or interpreted languages with subscripted and mutable pointers, local references, and explicit and typeless dynamic storage management, e.g., C. Because it is a source level transformation, it is amenable to both compile- and run-time optimization. Finally, its performance, even without compile-time optimization, is quite good. We implemented a prototype translator for the C language and analyzed the checking overheads of six non-trivial, pointer intensive programs. Execution overheads range from 130\% to 540\%; with text and data size overheads typically below 100\%.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {290--301},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178446},
 doi = {http://doi.acm.org/10.1145/773473.178446},
 acmid = {178446},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Austin:1994:EDP:178243.178446,
 author = {Austin, Todd M. and Breach, Scott E. and Sohi, Gurindar S.},
 title = {Efficient detection of all pointer and array access errors},
 abstract = {We present a pointer and array access checking technique that provides complete error coverage through a simple set of program transformations. Our technique, based on an extended safe pointer representation, has a number of novel aspects. Foremost, it is the first technique that detects all spatial and temporal access errors. Its use is not limited by the expressiveness of the language; that is, it can be applied successfully to compiled or interpreted languages with subscripted and mutable pointers, local references, and explicit and typeless dynamic storage management, e.g., C. Because it is a source level transformation, it is amenable to both compile- and run-time optimization. Finally, its performance, even without compile-time optimization, is quite good. We implemented a prototype translator for the C language and analyzed the checking overheads of six non-trivial, pointer intensive programs. Execution overheads range from 130\% to 540\%; with text and data size overheads typically below 100\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {290--301},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178446},
 doi = {http://doi.acm.org/10.1145/178243.178446},
 acmid = {178446},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1994:SPJ:773473.178456,
 author = {Agrawal, Hiralal},
 title = {On slicing programs with jump statements},
 abstract = {Program slices have potential uses in many software engineering applications. Traditional slicing algorithms, however, do not work correctly on programs that contain explicit jump statements. Two similar algorithms were proposed recently to alleviate this problem. Both require the flowgraph and the program dependence graph of the program to be modified. In this paper, we propose an alternative algorithm that leaves these graphs intact and uses a separate graph to store the additional required information. We also show that this algorithm permits an extremely efficient, conservative adaptation for use with programs that contain only ``structured" jump statements.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {302--312},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/773473.178456},
 doi = {http://doi.acm.org/10.1145/773473.178456},
 acmid = {178456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1994:SPJ:178243.178456,
 author = {Agrawal, Hiralal},
 title = {On slicing programs with jump statements},
 abstract = {Program slices have potential uses in many software engineering applications. Traditional slicing algorithms, however, do not work correctly on programs that contain explicit jump statements. Two similar algorithms were proposed recently to alleviate this problem. Both require the flowgraph and the program dependence graph of the program to be modified. In this paper, we propose an alternative algorithm that leaves these graphs intact and uses a separate graph to store the additional required information. We also show that this algorithm permits an extremely efficient, conservative adaptation for use with programs that contain only ``structured" jump statements.},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {302--312},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/178243.178456},
 doi = {http://doi.acm.org/10.1145/178243.178456},
 acmid = {178456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Netzer:1994:OTI:178243.178477,
 author = {Netzer, Robert H. B. and Weaver, Mark H.},
 title = {Optimal tracing and incremental reexecution for debugging long-running programs},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {313--325},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/178243.178477},
 doi = {http://doi.acm.org/10.1145/178243.178477},
 acmid = {178477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Netzer:1994:OTI:773473.178477,
 author = {Netzer, Robert H. B. and Weaver, Mark H.},
 title = {Optimal tracing and incremental reexecution for debugging long-running programs},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {313--325},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/773473.178477},
 doi = {http://doi.acm.org/10.1145/773473.178477},
 acmid = {178477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Holzle:1994:ODC:773473.178478,
 author = {H\"{o}lzle, Urs and Ungar, David},
 title = {Optimizing dynamically-dispatched calls with run-time type feedback},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {326--336},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/773473.178478},
 doi = {http://doi.acm.org/10.1145/773473.178478},
 acmid = {178478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Holzle:1994:ODC:178243.178478,
 author = {H\"{o}lzle, Urs and Ungar, David},
 title = {Optimizing dynamically-dispatched calls with run-time type feedback},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {326--336},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/178243.178478},
 doi = {http://doi.acm.org/10.1145/178243.178478},
 acmid = {178478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Van Hentenryck:1994:TAP:773473.178479,
 author = {Van Hentenryck, P. and Cortesi, A. and Le Charlier, B.},
 title = {Type analysis of Prolog using type graphs},
 abstract = {Type analysis of Prolog is of primary importance for high-performance compilers, since type information may lead to better indexing and to sophisticated specializations of unification and built-in predicates to name a few. However, these optimizations often require a sophisticated type inference system capable of inferring disjunctive and recursive types and hence expensive in computation time.
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178479},
 doi = {http://doi.acm.org/10.1145/773473.178479},
 acmid = {178479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Van Hentenryck:1994:TAP:178243.178479,
 author = {Van Hentenryck, P. and Cortesi, A. and Le Charlier, B.},
 title = {Type analysis of Prolog using type graphs},
 abstract = {Type analysis of Prolog is of primary importance for high-performance compilers, since type information may lead to better indexing and to sophisticated specializations of unification and built-in predicates to name a few. However, these optimizations often require a sophisticated type inference system capable of inferring disjunctive and recursive types and hence expensive in computation time.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178479},
 doi = {http://doi.acm.org/10.1145/178243.178479},
 acmid = {178479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Van Hentenryck:1994:BWT:773473.178488,
 author = {Van Hentenryck, Pascal and Ramachandran, Viswanath},
 title = {Backtracking without trailing in CLP (R<sub><i>Lin</i></sub>)},
 abstract = {
},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {6},
 month = {June},
 year = {1994},
 issn = {0362-1340},
 pages = {349--360},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/773473.178488},
 doi = {http://doi.acm.org/10.1145/773473.178488},
 acmid = {178488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Van Hentenryck:1994:BWT:178243.178488,
 author = {Van Hentenryck, Pascal and Ramachandran, Viswanath},
 title = {Backtracking without trailing in CLP (R<sub><i>Lin</i></sub>)},
 abstract = {
},
 booktitle = {Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
 series = {PLDI '94},
 year = {1994},
 isbn = {0-89791-662-X},
 location = {Orlando, Florida, United States},
 pages = {349--360},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/178243.178488},
 doi = {http://doi.acm.org/10.1145/178243.178488},
 acmid = {178488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wahbe:1993:PDB:155090.155091,
 author = {Wahbe, Robert and Lucco, Steven and Graham, Susan L.},
 title = {Practical data breakpoints: design and implementation},
 abstract = {A data breakpoint associates debugging actions with programmer-specified conditions on the memory state of an executing program. Data breakpoints provide a means for discovering program bugs that are tedious or impossible to isolate using control breakpoints alone. In practice, programmers rarely use data breakpoints, because they are either unimplemented or prohibitively slow in available debugging software. In this paper, we present the design and implementation of a practical data breakpoint facility.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155090.155091},
 doi = {http://doi.acm.org/10.1145/155090.155091},
 acmid = {155091},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wahbe:1993:PDB:173262.155091,
 author = {Wahbe, Robert and Lucco, Steven and Graham, Susan L.},
 title = {Practical data breakpoints: design and implementation},
 abstract = {A data breakpoint associates debugging actions with programmer-specified conditions on the memory state of an executing program. Data breakpoints provide a means for discovering program bugs that are tedious or impossible to isolate using control breakpoints alone. In practice, programmers rarely use data breakpoints, because they are either unimplemented or prohibitively slow in available debugging software. In this paper, we present the design and implementation of a practical data breakpoint facility.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/173262.155091},
 doi = {http://doi.acm.org/10.1145/173262.155091},
 acmid = {155091},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adl-Tabatabai:1993:DRE:155090.155092,
 author = {Adl-Tabatabai, Ali-Reza and Gross, Thomas},
 title = {Detection and recovery of endangered variables caused by instruction scheduling},
 abstract = {Instruction scheduling re-orders and interleaves instruction sequences from different source statements. This impacts the task of a symbolic debugger, which attempts to present the user a picture of program execution that matches the source program. At a breakpoint B, if the value in the run-time location of a variable V may not correspond to the value the user expects V to have, then this variable is endangered at B. This paper describes an approach to detecting and recovering endangered variables caused by instruction scheduling. We measure the effects of instruction scheduling on a symbolic debugger's ability to recover source values at a breakpoint. This paper reports measurements for three C programs from the SPEC suite and a collection of programs from the Numerical Recipes, which have been compiled with a variant of a commercial C compiler.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {13--25},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/155090.155092},
 doi = {http://doi.acm.org/10.1145/155090.155092},
 acmid = {155092},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adl-Tabatabai:1993:DRE:173262.155092,
 author = {Adl-Tabatabai, Ali-Reza and Gross, Thomas},
 title = {Detection and recovery of endangered variables caused by instruction scheduling},
 abstract = {Instruction scheduling re-orders and interleaves instruction sequences from different source statements. This impacts the task of a symbolic debugger, which attempts to present the user a picture of program execution that matches the source program. At a breakpoint B, if the value in the run-time location of a variable V may not correspond to the value the user expects V to have, then this variable is endangered at B. This paper describes an approach to detecting and recovering endangered variables caused by instruction scheduling. We measure the effects of instruction scheduling on a symbolic debugger's ability to recover source values at a breakpoint. This paper reports measurements for three C programs from the SPEC suite and a collection of programs from the Numerical Recipes, which have been compiled with a variant of a commercial C compiler.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {13--25},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/173262.155092},
 doi = {http://doi.acm.org/10.1145/173262.155092},
 acmid = {155092},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boyd:1993:IAO:155090.155093,
 author = {Boyd, Mickey R. and Whalley, David B.},
 title = {Isolation and analysis of optimization errors},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155093},
 doi = {http://doi.acm.org/10.1145/155090.155093},
 acmid = {155093},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boyd:1993:IAO:173262.155093,
 author = {Boyd, Mickey R. and Whalley, David B.},
 title = {Isolation and analysis of optimization errors},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155093},
 doi = {http://doi.acm.org/10.1145/173262.155093},
 acmid = {155093},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cytron:1993:EAM:155090.155094,
 author = {Cytron, Ron and Gershbein, Reid},
 title = {Efficient accommodation of may-alias information in SSA form},
 abstract = {We present an algorithm for incrementally including may-alias information into Static Single Assignment form by computing a sequence of increasingly precise (and correspondingly larger) partial SSA forms. Our experiments show significant speedup of our method over exhaustive use of may-alias information, as optimization problems converge well before most may-aliases are needed.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {36--45},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155094},
 doi = {http://doi.acm.org/10.1145/155090.155094},
 acmid = {155094},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cytron:1993:EAM:173262.155094,
 author = {Cytron, Ron and Gershbein, Reid},
 title = {Efficient accommodation of may-alias information in SSA form},
 abstract = {We present an algorithm for incrementally including may-alias information into Static Single Assignment form by computing a sequence of increasingly precise (and correspondingly larger) partial SSA forms. Our experiments show significant speedup of our method over exhaustive use of may-alias information, as optimization problems converge well before most may-aliases are needed.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {36--45},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155094},
 doi = {http://doi.acm.org/10.1145/173262.155094},
 acmid = {155094},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bourdoncle:1993:ADH:155090.155095,
 author = {Bourdoncle, Fran\c{c}ois},
 title = {Abstract debugging of higher-order imperative languages},
 abstract = {Abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. We present an abstract interpretation-based method, called abstract debugging, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. We show how invariant assertions and intermittent assertions, such as termination, can be used to formally debug programs. Finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the Syntox system that enables the abstract debugging of the Pascal language by the determination of the range of the scalar variables of programs.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {46--55},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155095},
 doi = {http://doi.acm.org/10.1145/155090.155095},
 acmid = {155095},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bourdoncle:1993:ADH:173262.155095,
 author = {Bourdoncle, Fran\c{c}ois},
 title = {Abstract debugging of higher-order imperative languages},
 abstract = {Abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. We present an abstract interpretation-based method, called abstract debugging, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. We show how invariant assertions and intermittent assertions, such as termination, can be used to formally debug programs. Finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the Syntox system that enables the abstract debugging of the Pascal language by the determination of the range of the scalar variables of programs.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {46--55},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155095},
 doi = {http://doi.acm.org/10.1145/173262.155095},
 acmid = {155095},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Landi:1993:IMS:155090.155096,
 author = {Landi, William and Ryder, Barbara G. and Zhang, Sean},
 title = {Interprocedural modification side effect analysis with pointer aliasing},
 abstract = {We present a new interprocedural modification side effects algorithm for C programs, that can discern side effects through general-purpose pointer usage. Ours is the first complete design and implementation of such an algorithm. Preliminary performance findings support the practicality of the technique, which is based on our previous approximation algorithm for pointer aliases [LR92]. Each indirect store through a pointer variable is found, on average, to correspond to a store into 1.2 locations. This indicates that our program-point-specific pointer aliasing information is quite precise when used to determine the effects of these stores.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {56--67},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155090.155096},
 doi = {http://doi.acm.org/10.1145/155090.155096},
 acmid = {155096},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Landi:1993:IMS:173262.155096,
 author = {Landi, William and Ryder, Barbara G. and Zhang, Sean},
 title = {Interprocedural modification side effect analysis with pointer aliasing},
 abstract = {We present a new interprocedural modification side effects algorithm for C programs, that can discern side effects through general-purpose pointer usage. Ours is the first complete design and implementation of such an algorithm. Preliminary performance findings support the practicality of the technique, which is based on our previous approximation algorithm for pointer aliases [LR92]. Each indirect store through a pointer variable is found, on average, to correspond to a store into 1.2 locations. This indicates that our program-point-specific pointer aliasing information is quite precise when used to determine the effects of these stores.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {56--67},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/173262.155096},
 doi = {http://doi.acm.org/10.1145/173262.155096},
 acmid = {155096},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Duesterwald:1993:PDF:155090.155097,
 author = {Duesterwald, Evelyn and Gupta, Rajiv and Soffa, Mary Lou},
 title = {A practical data flow framework for array reference analysis and its use in optimizations},
 abstract = {Data flow analysis techniques have traditionally been restricted to the analysis of scalar variables. This retriction, however, imposes a limitation on the kinds of optimizations that can be performed in loops containing array references. We present a data flow framework for array reference analysis that provides the information needed in various optimizations targeted at sequential or fine-grained parallel architectures. The framework extends the traditional scalar framework by incorporating iteration distance values into the analysis to qualify the computed data flow solution during the fixed point iteration. Analyses phrased in this framework are capable of discovering recurrent access patterns among array references that evolve during the execution of a loop. Applications of our  framework are discussed for register allocation, load/store optimizations, and controlled loop unrolling.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {68--77},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155097},
 doi = {http://doi.acm.org/10.1145/155090.155097},
 acmid = {155097},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Duesterwald:1993:PDF:173262.155097,
 author = {Duesterwald, Evelyn and Gupta, Rajiv and Soffa, Mary Lou},
 title = {A practical data flow framework for array reference analysis and its use in optimizations},
 abstract = {Data flow analysis techniques have traditionally been restricted to the analysis of scalar variables. This retriction, however, imposes a limitation on the kinds of optimizations that can be performed in loops containing array references. We present a data flow framework for array reference analysis that provides the information needed in various optimizations targeted at sequential or fine-grained parallel architectures. The framework extends the traditional scalar framework by incorporating iteration distance values into the analysis to qualify the computed data flow solution during the fixed point iteration. Analyses phrased in this framework are capable of discovering recurrent access patterns among array references that evolve during the execution of a loop. Applications of our  framework are discussed for register allocation, load/store optimizations, and controlled loop unrolling.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {68--77},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155097},
 doi = {http://doi.acm.org/10.1145/173262.155097},
 acmid = {155097},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1993:DPA:173262.155098,
 author = {Johnson, Richard and Pingali, Keshav},
 title = {Dependence-based program analysis},
 abstract = {Program analysis and optimization can be speeded up through the use of the dependence flow graph (DFG), a representation of program dependences which generalizes def-use chains and static single assignment (SSA) form. In this paper, we give a simple graph-theoretic description of the DFG and show how the DFG for a program can be constructed in O(EV) time. We then show how forward and backward dataflow analyses can be performed efficiently on the DFG, using constant propagation and elimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow equations in the DFG. Our construction algorithm is of independent interest because it can be used to construct a program's control dependence graph in O(E) time and its SSA representation in O(EV) time, which are improvements over existing algorithms.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/173262.155098},
 doi = {http://doi.acm.org/10.1145/173262.155098},
 acmid = {155098},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Johnson:1993:DPA:155090.155098,
 author = {Johnson, Richard and Pingali, Keshav},
 title = {Dependence-based program analysis},
 abstract = {Program analysis and optimization can be speeded up through the use of the dependence flow graph (DFG), a representation of program dependences which generalizes def-use chains and static single assignment (SSA) form. In this paper, we give a simple graph-theoretic description of the DFG and show how the DFG for a program can be constructed in O(EV) time. We then show how forward and backward dataflow analyses can be performed efficiently on the DFG, using constant propagation and elimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow equations in the DFG. Our construction algorithm is of independent interest because it can be used to construct a program's control dependence graph in O(E) time and its SSA representation in O(EV) time, which are improvements over existing algorithms.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155090.155098},
 doi = {http://doi.acm.org/10.1145/155090.155098},
 acmid = {155098},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Grove:1993:ICP:155090.155099,
 author = {Grove, Dan and Torczon, Linda},
 title = {Interprocedural constant propagation: a study of jump function implementation},
 abstract = {An implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a jump function. While Callahan et al. propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the pass-through parameter jump function, as the most cost-effective in practice.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155099},
 doi = {http://doi.acm.org/10.1145/155090.155099},
 acmid = {155099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Grove:1993:ICP:173262.155099,
 author = {Grove, Dan and Torczon, Linda},
 title = {Interprocedural constant propagation: a study of jump function implementation},
 abstract = {An implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a jump function. While Callahan et al. propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the pass-through parameter jump function, as the most cost-effective in practice.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155099},
 doi = {http://doi.acm.org/10.1145/173262.155099},
 acmid = {155099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Graham:1993:OIP:173262.155100,
 author = {Graham, Susan L. and Lucco, Steven and Sharp, Oliver},
 title = {Orchestrating interactions among parallel computations},
 abstract = {Many parallel programs contain multiple sub-computations, each with distinct communication and load balancing requirements. The traditional approach to compiling such programs is to impose a processor synchronization barrier between sub-computations, optimizing each as a separate entity. This paper develops a methodology for managing the interactions among sub-computations, avoiding strict synchronization where concurrent or pipelined relationships are possible.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {100--111},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/173262.155100},
 doi = {http://doi.acm.org/10.1145/173262.155100},
 acmid = {155100},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Graham:1993:OIP:155090.155100,
 author = {Graham, Susan L. and Lucco, Steven and Sharp, Oliver},
 title = {Orchestrating interactions among parallel computations},
 abstract = {Many parallel programs contain multiple sub-computations, each with distinct communication and load balancing requirements. The traditional approach to compiling such programs is to impose a processor synchronization barrier between sub-computations, optimizing each as a separate entity. This paper develops a methodology for managing the interactions among sub-computations, avoiding strict synchronization where concurrent or pipelined relationships are possible.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {100--111},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155090.155100},
 doi = {http://doi.acm.org/10.1145/155090.155100},
 acmid = {155100},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1993:GOP:155090.155101,
 author = {Anderson, Jennifer M. and Lam, Monica S.},
 title = {Global optimizations for parallelism and locality on scalable parallel machines},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {112--125},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/155090.155101},
 doi = {http://doi.acm.org/10.1145/155090.155101},
 acmid = {155101},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1993:GOP:173262.155101,
 author = {Anderson, Jennifer M. and Lam, Monica S.},
 title = {Global optimizations for parallelism and locality on scalable parallel machines},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {112--125},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/173262.155101},
 doi = {http://doi.acm.org/10.1145/173262.155101},
 acmid = {155101},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Amarasinghe:1993:COC:173262.155102,
 author = {Amarasinghe, Saman P. and Lam, Monica S.},
 title = {Communication optimization and code generation for distributed memory machines},
 abstract = {This paper presents several algorithms to solve code generation and optimization problems specific to machines with distributed address spaces. Given a description of how the computation is to be partitioned across the processors in a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each processor. Our compiler generated the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor, and translates global data addresses to local addresses.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {126--138},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/173262.155102},
 doi = {http://doi.acm.org/10.1145/173262.155102},
 acmid = {155102},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Amarasinghe:1993:COC:155090.155102,
 author = {Amarasinghe, Saman P. and Lam, Monica S.},
 title = {Communication optimization and code generation for distributed memory machines},
 abstract = {This paper presents several algorithms to solve code generation and optimization problems specific to machines with distributed address spaces. Given a description of how the computation is to be partitioned across the processors in a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each processor. Our compiler generated the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor, and translates global data addresses to local addresses.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {126--138},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/155090.155102},
 doi = {http://doi.acm.org/10.1145/155090.155102},
 acmid = {155102},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adams:1993:FDR:155090.155103,
 author = {Adams, Norman and Curtis, Pavel and Spreitzer, Mike},
 title = {First-class data-type representations in SCHEMEXEROX},
 abstract = {In most programming language implementations, the compiler has detailed knowledge of the representations of and operations on primitive data typed and data-type constructors. In SCHEMEXEROX, this knowledge is almost entirely external to the compiler, in ordinary, procedural user code. The primitive representations and operations are embodied in first-class ``representation types" that are constructed and implemented in an abstract and high-level fashion. Despite this abstractness, a few generally-useful optimizing transformations are sufficient to allow the SCHEMEXEROX compiler to generate efficient code for the primitive operations, essentially as good as could be achieved using more contorted, traditional techniques.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {139--146},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/155090.155103},
 doi = {http://doi.acm.org/10.1145/155090.155103},
 acmid = {155103},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adams:1993:FDR:173262.155103,
 author = {Adams, Norman and Curtis, Pavel and Spreitzer, Mike},
 title = {First-class data-type representations in SCHEMEXEROX},
 abstract = {In most programming language implementations, the compiler has detailed knowledge of the representations of and operations on primitive data typed and data-type constructors. In SCHEMEXEROX, this knowledge is almost entirely external to the compiler, in ordinary, procedural user code. The primitive representations and operations are embodied in first-class ``representation types" that are constructed and implemented in an abstract and high-level fashion. Despite this abstractness, a few generally-useful optimizing transformations are sufficient to allow the SCHEMEXEROX compiler to generate efficient code for the primitive operations, essentially as good as could be achieved using more contorted, traditional techniques.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {139--146},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/173262.155103},
 doi = {http://doi.acm.org/10.1145/173262.155103},
 acmid = {155103},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sitaram:1993:HC:173262.155104,
 author = {Sitaram, Dorai},
 title = {Handling control},
 abstract = {Non-local control transfer and exception handling have a long tradition in higher-order programming languages such as Common Lisp, Scheme and ML. However, each language stops short of providing a full and complementary approach\&mdash;control handling is provided only if the corresponding control operator is first-order. In this work, we describe handlers in a higher-order control setting. We invoke our earlier theoretical result that all denotational models of control languages invariably include capabilities that handle control. These capabilities, when incorporated into the language, form an elegant and powerful higher-order generalization of the first-order exception-handling mechanism.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {147--155},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/173262.155104},
 doi = {http://doi.acm.org/10.1145/173262.155104},
 acmid = {155104},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sitaram:1993:HC:155090.155104,
 author = {Sitaram, Dorai},
 title = {Handling control},
 abstract = {Non-local control transfer and exception handling have a long tradition in higher-order programming languages such as Common Lisp, Scheme and ML. However, each language stops short of providing a full and complementary approach\&mdash;control handling is provided only if the corresponding control operator is first-order. In this work, we describe handlers in a higher-order control setting. We invoke our earlier theoretical result that all denotational models of control languages invariably include capabilities that handle control. These capabilities, when incorporated into the language, form an elegant and powerful higher-order generalization of the first-order exception-handling mechanism.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {147--155},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/155090.155104},
 doi = {http://doi.acm.org/10.1145/155090.155104},
 acmid = {155104},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weise:1993:PSM:155090.155105,
 author = {Weise, Daniel and Crew, Roger},
 title = {Programmable syntax macros},
 abstract = {Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP (the C preprocessor), syntax macros operate on Abstract Syntax Trees (ASTs). Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, difficult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems by having the macro language be a  minimal extension of the programming language, by introducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro definition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {156--165},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155105},
 doi = {http://doi.acm.org/10.1145/155090.155105},
 acmid = {155105},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weise:1993:PSM:173262.155105,
 author = {Weise, Daniel and Crew, Roger},
 title = {Programmable syntax macros},
 abstract = {Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP (the C preprocessor), syntax macros operate on Abstract Syntax Trees (ASTs). Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, difficult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems by having the macro language be a  minimal extension of the programming language, by introducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro definition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {156--165},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155105},
 doi = {http://doi.acm.org/10.1145/173262.155105},
 acmid = {155105},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hong:1993:CRP:155090.155106,
 author = {Hong, Seongsoo and Gerber, Richard},
 title = {Compiling real-time programs into schedulable code},
 abstract = {We present a programming language with first-class timing constructs, whose semantics is based on time-constrained relationships between observable events. Since a system specification postulates timing relationships between events, realizing the specification in a program becomes a more straightforward process.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {166--176},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/155090.155106},
 doi = {http://doi.acm.org/10.1145/155090.155106},
 acmid = {155106},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hong:1993:CRP:173262.155106,
 author = {Hong, Seongsoo and Gerber, Richard},
 title = {Compiling real-time programs into schedulable code},
 abstract = {We present a programming language with first-class timing constructs, whose semantics is based on time-constrained relationships between observable events. Since a system specification postulates timing relationships between events, realizing the specification in a program becomes a more straightforward process.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {166--176},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/173262.155106},
 doi = {http://doi.acm.org/10.1145/173262.155106},
 acmid = {155106},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Grunwald:1993:ICL:155090.155107,
 author = {Grunwald, Dirk and Zorn, Benjamin and Henderson, Robert},
 title = {Improving the cache locality of memory allocation},
 abstract = {The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace-driven simualtion of five large allocation-intensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequential-fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be space-efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155107},
 doi = {http://doi.acm.org/10.1145/155090.155107},
 acmid = {155107},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Grunwald:1993:ICL:173262.155107,
 author = {Grunwald, Dirk and Zorn, Benjamin and Henderson, Robert},
 title = {Improving the cache locality of memory allocation},
 abstract = {The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace-driven simualtion of five large allocation-intensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequential-fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be space-efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155107},
 doi = {http://doi.acm.org/10.1145/173262.155107},
 acmid = {155107},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barrett:1993:ULP:173262.155108,
 author = {Barrett, David A. and Zorn, Benjamin G.},
 title = {Using lifetime predictors to improve memory allocation performance},
 abstract = {Dynamic storage allocation is used heavily in many application areas including interpreters, simulators, optimizers, and translators. We describe research that can improve all aspects of the performance of dynamic storage allocation by predicting the lifetimes of short-lived objects when they are allocated. Using five significant, allocation-intensive C programs, we show that a great fraction of all bytes allocated are short-lived (\&gt; 90\% in all cases). Furthermore, we describe an algorithm for liftetime prediction that accurately predicts the lifetimes of 42\&ndash;99\% of all objects allocated. We describe and simulate a storage allocator that takes adavantage of lifetime prediction of short-lived objects and show that it can significantly improve a program's memory overhead and reference locality, and even, at times, improve CPU performance as well.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {187--196},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155108},
 doi = {http://doi.acm.org/10.1145/173262.155108},
 acmid = {155108},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barrett:1993:ULP:155090.155108,
 author = {Barrett, David A. and Zorn, Benjamin G.},
 title = {Using lifetime predictors to improve memory allocation performance},
 abstract = {Dynamic storage allocation is used heavily in many application areas including interpreters, simulators, optimizers, and translators. We describe research that can improve all aspects of the performance of dynamic storage allocation by predicting the lifetimes of short-lived objects when they are allocated. Using five significant, allocation-intensive C programs, we show that a great fraction of all bytes allocated are short-lived (\&gt; 90\% in all cases). Furthermore, we describe an algorithm for liftetime prediction that accurately predicts the lifetimes of 42\&ndash;99\% of all objects allocated. We describe and simulate a storage allocator that takes adavantage of lifetime prediction of short-lived objects and show that it can significantly improve a program's memory overhead and reference locality, and even, at times, improve CPU performance as well.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {187--196},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155108},
 doi = {http://doi.acm.org/10.1145/155090.155108},
 acmid = {155108},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boehm:1993:SEC:155090.155109,
 author = {Boehm, Hans-Juergen},
 title = {Space efficient conservative garbage collection},
 abstract = {We call a garbage collector conservative if it has only partial information about the location of pointers, and is thus forced to treat arbitrary bit patterns as though they might be pointers, in at least some cases. We show that some very inexpensive, but previously unused techniques can have dramatic impact on the effectiveness of conservative garbage collectors in reclaiming memory. Our most significant observation is that static data that appears to point to the heap should not result in misidentified references to the heap. The garbage collector has enough information to allocate around such references. We also observe that programming style has a significant impact on the amount of spuriously retained storage, typically even if the collector is not terribly conservative. Some fairly common C and C++ programming style significantly decrease the effectiveness of any garbage collector. These observations suffice to explain some of the different assessments of conservative collection that have appeared in the literature.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155109},
 doi = {http://doi.acm.org/10.1145/155090.155109},
 acmid = {155109},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boehm:1993:SEC:173262.155109,
 author = {Boehm, Hans-Juergen},
 title = {Space efficient conservative garbage collection},
 abstract = {We call a garbage collector conservative if it has only partial information about the location of pointers, and is thus forced to treat arbitrary bit patterns as though they might be pointers, in at least some cases. We show that some very inexpensive, but previously unused techniques can have dramatic impact on the effectiveness of conservative garbage collectors in reclaiming memory. Our most significant observation is that static data that appears to point to the heap should not result in misidentified references to the heap. The garbage collector has enough information to allocate around such references. We also observe that programming style has a significant impact on the amount of spuriously retained storage, typically even if the collector is not terribly conservative. Some fairly common C and C++ programming style significantly decrease the effectiveness of any garbage collector. These observations suffice to explain some of the different assessments of conservative collection that have appeared in the literature.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155109},
 doi = {http://doi.acm.org/10.1145/173262.155109},
 acmid = {155109},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dybvig:1993:GGG:173262.155110,
 author = {Dybvig, R. Kent and Bruggeman, Carl and Eby, David},
 title = {Guardians in a generation-based garbage collector},
 abstract = {This paper describes a new language feature that allows dynamically allocated objects to be saved from deallocation by an automatic storage management system so that clean-up or other actions can be performed using the data stored within the objects. The program has full control over the timing of clean-up actions, which eliminates several potential problems and often  eliminates the need for critical sections in code that interacts with clean-up actions. Our implementation is ``generation-friendly" in the sense that the additional overhead within a generation-based garbage collector is proportional to the work already done there, and the overhead within the mutator is proportional to the number of clean-up actions actually performed.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155110},
 doi = {http://doi.acm.org/10.1145/173262.155110},
 acmid = {155110},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dybvig:1993:GGG:155090.155110,
 author = {Dybvig, R. Kent and Bruggeman, Carl and Eby, David},
 title = {Guardians in a generation-based garbage collector},
 abstract = {This paper describes a new language feature that allows dynamically allocated objects to be saved from deallocation by an automatic storage management system so that clean-up or other actions can be performed using the data stored within the objects. The program has full control over the timing of clean-up actions, which eliminates several potential problems and often  eliminates the need for critical sections in code that interacts with clean-up actions. Our implementation is ``generation-friendly" in the sense that the additional overhead within a generation-based garbage collector is proportional to the work already done there, and the overhead within the mutator is proportional to the number of clean-up actions actually performed.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155110},
 doi = {http://doi.acm.org/10.1145/155090.155110},
 acmid = {155110},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nettles:1993:RRG:173262.155111,
 author = {Nettles, Scott and O'Toole, James},
 title = {Real-time replication garbage collection},
 abstract = {We have implemented the first copying garbage collector that permits continuous unimpeded mutator access to the original objects during copying. The garbage collector incrementally replicates all accessible objects and uses a mutation log to bring the replicas up-to-date with changes made by the mutator. An experimental implementation demonstrates that the costs of using our algorithm are small and that bounded pause times of 50 milliseconds can be readily achieved.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {217--226},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155111},
 doi = {http://doi.acm.org/10.1145/173262.155111},
 acmid = {155111},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent collection, copying garbage collection, incremental collection, real-time garbage collection, replication},
} 

@inproceedings{Nettles:1993:RRG:155090.155111,
 author = {Nettles, Scott and O'Toole, James},
 title = {Real-time replication garbage collection},
 abstract = {We have implemented the first copying garbage collector that permits continuous unimpeded mutator access to the original objects during copying. The garbage collector incrementally replicates all accessible objects and uses a mutation log to bring the replicas up-to-date with changes made by the mutator. An experimental implementation demonstrates that the costs of using our algorithm are small and that bounded pause times of 50 milliseconds can be readily achieved.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {217--226},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155111},
 doi = {http://doi.acm.org/10.1145/155090.155111},
 acmid = {155111},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent collection, copying garbage collection, incremental collection, real-time garbage collection, replication},
} 

@article{Peterson:1993:ITC:173262.155112,
 author = {Peterson, John and Jones, Mark},
 title = {Implementing type classes},
 abstract = {We describe the implementation of a type checker for the functional programming language Haskell that supports the use of type classes. This extends the type system of ML to support overloading (ad-hoc polymorphism) and can be used to implement features such as equality types and numeric overloading in a simple and general way.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {227--236},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155112},
 doi = {http://doi.acm.org/10.1145/173262.155112},
 acmid = {155112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peterson:1993:ITC:155090.155112,
 author = {Peterson, John and Jones, Mark},
 title = {Implementing type classes},
 abstract = {We describe the implementation of a type checker for the functional programming language Haskell that supports the use of type classes. This extends the type system of ML to support overloading (ad-hoc polymorphism) and can be used to implement features such as equality types and numeric overloading in a simple and general way.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {227--236},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155112},
 doi = {http://doi.acm.org/10.1145/155090.155112},
 acmid = {155112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flanagan:1993:ECC:173262.155113,
 author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
 title = {The essence of compiling with continuations},
 abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the ``continuation"). Since the nai\&uml;ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.
},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {237--247},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/173262.155113},
 doi = {http://doi.acm.org/10.1145/173262.155113},
 acmid = {155113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flanagan:1993:ECC:155090.155113,
 author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
 title = {The essence of compiling with continuations},
 abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the ``continuation"). Since the nai\&uml;ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {237--247},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/155090.155113},
 doi = {http://doi.acm.org/10.1145/155090.155113},
 acmid = {155113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pinter:1993:RAI:173262.155114,
 author = {Pinter, Shlomit S.},
 title = {Register allocation with instruction scheduling},
 abstract = {We present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the parallel interference graph, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {248--257},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155114},
 doi = {http://doi.acm.org/10.1145/173262.155114},
 acmid = {155114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pinter:1993:RAI:155090.155114,
 author = {Pinter, Shlomit S.},
 title = {Register allocation with instruction scheduling},
 abstract = {We present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the parallel interference graph, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {248--257},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155114},
 doi = {http://doi.acm.org/10.1145/155090.155114},
 acmid = {155114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huff:1993:LMS:173262.155115,
 author = {Huff, Richard A.},
 title = {Lifetime-sensitive modulo scheduling},
 abstract = {This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time. This novel bidirectional slack-scheduling method has been implemented in a FORTRAN compiler and tested on many scientific benchmarks. The empirical results\&mdash;when measured against an absolute lower bound on execution time, and against a novel schedule-independent absolute lower bound on register pressure\&mdash;indicate near-optimal performance.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {258--267},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155115},
 doi = {http://doi.acm.org/10.1145/173262.155115},
 acmid = {155115},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Huff:1993:LMS:155090.155115,
 author = {Huff, Richard A.},
 title = {Lifetime-sensitive modulo scheduling},
 abstract = {This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time. This novel bidirectional slack-scheduling method has been implemented in a FORTRAN compiler and tested on many scientific benchmarks. The empirical results\&mdash;when measured against an absolute lower bound on execution time, and against a novel schedule-independent absolute lower bound on register pressure\&mdash;indicate near-optimal performance.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {258--267},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155115},
 doi = {http://doi.acm.org/10.1145/155090.155115},
 acmid = {155115},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kolte:1993:LRA:173262.155116,
 author = {Kolte, Priyadarshan and Harrold, Mary Jean},
 title = {Load/store range analysis for global register allocation},
 abstract = {Live range splitting techniques improve global register allocation by splitting the live ranges of variables into segments that are individually allocated registers. Load/store range analysis is a new technique for live range splitting that is based on reaching definition and live variable analyses. Our analysis localizes the profits and the register requirements of every access to every variable to provide a fine granularity of candidates for register allocation. Experiments on a suite of C and FORTRAN benchmark programs show that a graph coloring register allocator operating on load/store ranges often provides better allocations than the same allocator operating on live ranges. Experimental results also show that the computational cost of using load/store ranges for register allocation is moderately more than the cost of using live ranges.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155116},
 doi = {http://doi.acm.org/10.1145/173262.155116},
 acmid = {155116},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kolte:1993:LRA:155090.155116,
 author = {Kolte, Priyadarshan and Harrold, Mary Jean},
 title = {Load/store range analysis for global register allocation},
 abstract = {Live range splitting techniques improve global register allocation by splitting the live ranges of variables into segments that are individually allocated registers. Load/store range analysis is a new technique for live range splitting that is based on reaching definition and live variable analyses. Our analysis localizes the profits and the register requirements of every access to every variable to provide a fine granularity of candidates for register allocation. Experiments on a suite of C and FORTRAN benchmark programs show that a graph coloring register allocator operating on load/store ranges often provides better allocations than the same allocator operating on live ranges. Experimental results also show that the computational cost of using load/store ranges for register allocation is moderately more than the cost of using live ranges.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155116},
 doi = {http://doi.acm.org/10.1145/155090.155116},
 acmid = {155116},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kerns:1993:BSI:173262.155117,
 author = {Kerns, Daniel R. and Eggers, Susan J.},
 title = {Balanced scheduling: instruction scheduling when memory latency is uncertain},
 abstract = {Traditional list schedulers order instructions based on an optimistic estimate of the load delay imposed by the implementation. Therefore they cannot respond to variations in load latencies (due to cache hits or misses, congestion in the memory interconnect, etc.) and cannot easily be applied across different implementations. We have developed an alternative algorithm, known as balanced scheduling, that schedule instructions based on an estimate of the amount of instruction level parallelism in the program. Since scheduling decisions are program-rather than machine-based, balanced scheduling is unaffected by implementation changes. Since it is based on the amount of instruction level parallelism that a program can support, it can respond better to variations in load latencies. Performance improvements over a traditional list scheduler on a Fortran workload and simulating several different machine types (cache-based workstations, large parallel machines with a multipath interconnect and a combination, all with non-blocking processors) are quite good, averaging between 3\% and 18\%.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/173262.155117},
 doi = {http://doi.acm.org/10.1145/173262.155117},
 acmid = {155117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kerns:1993:BSI:155090.155117,
 author = {Kerns, Daniel R. and Eggers, Susan J.},
 title = {Balanced scheduling: instruction scheduling when memory latency is uncertain},
 abstract = {Traditional list schedulers order instructions based on an optimistic estimate of the load delay imposed by the implementation. Therefore they cannot respond to variations in load latencies (due to cache hits or misses, congestion in the memory interconnect, etc.) and cannot easily be applied across different implementations. We have developed an alternative algorithm, known as balanced scheduling, that schedule instructions based on an estimate of the amount of instruction level parallelism in the program. Since scheduling decisions are program-rather than machine-based, balanced scheduling is unaffected by implementation changes. Since it is based on the amount of instruction level parallelism that a program can support, it can respond better to variations in load latencies. Performance improvements over a traditional list scheduler on a Fortran workload and simulating several different machine types (cache-based workstations, large parallel machines with a multipath interconnect and a combination, all with non-blocking processors) are quite good, averaging between 3\% and 18\%.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {278--289},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155090.155117},
 doi = {http://doi.acm.org/10.1145/155090.155117},
 acmid = {155117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Warter:1993:RI:155090.155118,
 author = {Warter, Nancy J. and Mahlke, Scott A. and Hwu, Wen-Mei W. and Rau, B. Ramakrishna},
 title = {Reverse If-Conversion},
 abstract = {In this paper we present a set of isomorphic control transformations that allow the compiler to apply local scheduling techniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global scheduling are eliminated. This approach relies on a new technique, Reverse If-Conversion (RIC), that transforms scheduled If-Converted code back to the control flow graph representation. This paper presents the predicate internal representation, the algorithms for RIC, and the correctness of RIC. In addition, the scheduling issues are addressed and an application to software pipelining is presented.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {290--299},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/155090.155118},
 doi = {http://doi.acm.org/10.1145/155090.155118},
 acmid = {155118},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Warter:1993:RI:173262.155118,
 author = {Warter, Nancy J. and Mahlke, Scott A. and Hwu, Wen-Mei W. and Rau, B. Ramakrishna},
 title = {Reverse If-Conversion},
 abstract = {In this paper we present a set of isomorphic control transformations that allow the compiler to apply local scheduling techniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global scheduling are eliminated. This approach relies on a new technique, Reverse If-Conversion (RIC), that transforms scheduled If-Converted code back to the control flow graph representation. This paper presents the predicate internal representation, the algorithms for RIC, and the correctness of RIC. In addition, the scheduling issues are addressed and an application to software pipelining is presented.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {290--299},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/173262.155118},
 doi = {http://doi.acm.org/10.1145/173262.155118},
 acmid = {155118},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ball:1993:BPF:173262.155119,
 author = {Ball, Thomas and Larus, James R.},
 title = {Branch prediction for free},
 abstract = {Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.},
 journal = {SIGPLAN Not.},
 volume = {28},
 issue = {6},
 month = {June},
 year = {1993},
 issn = {0362-1340},
 pages = {300--313},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/173262.155119},
 doi = {http://doi.acm.org/10.1145/173262.155119},
 acmid = {155119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ball:1993:BPF:155090.155119,
 author = {Ball, Thomas and Larus, James R.},
 title = {Branch prediction for free},
 abstract = {Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.},
 booktitle = {Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation},
 series = {PLDI '93},
 year = {1993},
 isbn = {0-89791-598-4},
 location = {Albuquerque, New Mexico, United States},
 pages = {300--313},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/155090.155119},
 doi = {http://doi.acm.org/10.1145/155090.155119},
 acmid = {155119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brooks:1992:NAD:143103.143108,
 author = {Brooks, Gary and Hansen, Gilbert J. and Simmons, Steve},
 title = {A new approach to debugging optimized code},
 abstract = {Debugging optimized code is a desirable capability not provided by most current debuggers. Users are forced to debug the unoptimized code when a bug occurs in the optimized version. Current research offers partial solutions for a small class of optimizations, but not a unified approach that handles a wide range of optimizations, such as the sophisticated optimizations performed by supercomputer compilers.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143103.143108},
 doi = {http://doi.acm.org/10.1145/143103.143108},
 acmid = {143108},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brooks:1992:NAD:143095.143108,
 author = {Brooks, Gary and Hansen, Gilbert J. and Simmons, Steve},
 title = {A new approach to debugging optimized code},
 abstract = {Debugging optimized code is a desirable capability not provided by most current debuggers. Users are forced to debug the unoptimized code when a bug occurs in the optimized version. Current research offers partial solutions for a small class of optimizations, but not a unified approach that handles a wide range of optimizations, such as the sophisticated optimizations performed by supercomputer compilers.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143095.143108},
 doi = {http://doi.acm.org/10.1145/143095.143108},
 acmid = {143108},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sosic:1992:DTP:143103.143110,
 author = {Sosi\v{c}, Rok},
 title = {Dynascope: a tool for program directing},
 abstract = {This paper introduces program directing, a new way of program interaction. Directing enables one program, the director, to monitor and to control another program, the executor. One important application of program directing is human interaction with complex computer simulations.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143110},
 doi = {http://doi.acm.org/10.1145/143103.143110},
 acmid = {143110},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sosic:1992:DTP:143095.143110,
 author = {Sosi\v{c}, Rok},
 title = {Dynascope: a tool for program directing},
 abstract = {This paper introduces program directing, a new way of program interaction. Directing enables one program, the director, to monitor and to control another program, the executor. One important application of program directing is human interaction with complex computer simulations.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143110},
 doi = {http://doi.acm.org/10.1145/143095.143110},
 acmid = {143110},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramsey:1992:RD:143103.143112,
 author = {Ramsey, Norman and Hanson, David R.},
 title = {A retargetable debugger},
 abstract = {We are developing techniques for building retargetable debuggers. Our prototype, 1db, debugs C programs compiled for the MIPS R3000, Motorola 68020, SPARC, and VAX architectures. It can use a network to connect to faulty processes and can do cross-architecture debugging. 1db's total code size is about 16,000 lines, but it needs only 250\&ndash;550 lines of machine-dependent code for each target.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {22--31},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143112},
 doi = {http://doi.acm.org/10.1145/143103.143112},
 acmid = {143112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramsey:1992:RD:143095.143112,
 author = {Ramsey, Norman and Hanson, David R.},
 title = {A retargetable debugger},
 abstract = {We are developing techniques for building retargetable debuggers. Our prototype, 1db, debugs C programs compiled for the MIPS R3000, Motorola 68020, SPARC, and VAX architectures. It can use a network to connect to faulty processes and can do cross-architecture debugging. 1db's total code size is about 16,000 lines, but it needs only 250\&ndash;550 lines of machine-dependent code for each target.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {22--31},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143112},
 doi = {http://doi.acm.org/10.1145/143095.143112},
 acmid = {143112},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Holzle:1992:DOC:143103.143114,
 author = {H\"{o}lzle, Urs and Chambers, Craig and Ungar, David},
 title = {Debugging optimized code with dynamic deoptimization},
 abstract = {SELF's debugging system provides complete source-level debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically deoptimizing code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {32--43},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143114},
 doi = {http://doi.acm.org/10.1145/143103.143114},
 acmid = {143114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Holzle:1992:DOC:143095.143114,
 author = {H\"{o}lzle, Urs and Chambers, Craig and Ungar, David},
 title = {Debugging optimized code with dynamic deoptimization},
 abstract = {SELF's debugging system provides complete source-level debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically deoptimizing code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {32--43},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143114},
 doi = {http://doi.acm.org/10.1145/143095.143114},
 acmid = {143114},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ogata:1992:DIH:143103.143117,
 author = {Ogata, Kazuhiro and Kurihara, Satoshi and Inari, Mikio and Doi, Norihisa},
 title = {The design and implementation of HoME},
 abstract = {HoME is a version of Smalltalk which can be efficiently executed on a multiprocessor and can be executed in parallel by combining a Smalltalk process with a Mach thread and executing the process on the thread. HoME is nearly the same as ordinary Smalltalk except that multiple processes may execute in parallel. Thus, almost all applications running on ordinary Smalltalk can be executed on HoME without changes in their code.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143103.143117},
 doi = {http://doi.acm.org/10.1145/143103.143117},
 acmid = {143117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ogata:1992:DIH:143095.143117,
 author = {Ogata, Kazuhiro and Kurihara, Satoshi and Inari, Mikio and Doi, Norihisa},
 title = {The design and implementation of HoME},
 abstract = {HoME is a version of Smalltalk which can be efficiently executed on a multiprocessor and can be executed in parallel by combining a Smalltalk process with a Mach thread and executing the process on the thread. HoME is nearly the same as ordinary Smalltalk except that multiple processes may execute in parallel. Thus, almost all applications running on ordinary Smalltalk can be executed on HoME without changes in their code.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143095.143117},
 doi = {http://doi.acm.org/10.1145/143095.143117},
 acmid = {143117},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jagannathan:1992:CSC:143103.143119,
 author = {Jagannathan, Suresh and Philbin, Jim},
 title = {A customizable substrate for concurrent languages},
 abstract = {We describe an approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) programming languages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads of control and virtual processors as first-class objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {55--81},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/143103.143119},
 doi = {http://doi.acm.org/10.1145/143103.143119},
 acmid = {143119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jagannathan:1992:CSC:143095.143119,
 author = {Jagannathan, Suresh and Philbin, Jim},
 title = {A customizable substrate for concurrent languages},
 abstract = {We describe an approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) programming languages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads of control and virtual processors as first-class objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {55--81},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/143095.143119},
 doi = {http://doi.acm.org/10.1145/143095.143119},
 acmid = {143119},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wortman:1992:CCM:143095.120025,
 author = {Wortman, David B. and Junkin, Michael D.},
 title = {A concurrent compiler for Modula-2+},
 abstract = {In this paper we describe a collection of techniques for the design and implementation of concurrent compilers. We begin by describing a technique for dividing a source program into many streams so that each stream can be compiled concurrently. We discuss several compiler design issues unique to concurrent compilers including source program partitioning, symbol table management, compiler task scheduling and information flow constraints. The application of our techniques is illustrated by a complete design for a concurrent Modula-2+ compiler. After describing the structure of this compiler's performance that demonstrates that significant improvements in compilation time can be achieved through the use of concurrency.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {68--81},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/143095.120025},
 doi = {http://doi.acm.org/10.1145/143095.120025},
 acmid = {120025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wortman:1992:CCM:143103.120025,
 author = {Wortman, David B. and Junkin, Michael D.},
 title = {A concurrent compiler for Modula-2+},
 abstract = {In this paper we describe a collection of techniques for the design and implementation of concurrent compilers. We begin by describing a technique for dividing a source program into many streams so that each stream can be compiled concurrently. We discuss several compiler design issues unique to concurrent compilers including source program partitioning, symbol table management, compiler task scheduling and information flow constraints. The application of our techniques is illustrated by a complete design for a concurrent Modula-2+ compiler. After describing the structure of this compiler's performance that demonstrates that significant improvements in compilation time can be achieved through the use of concurrency.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {68--81},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/143103.120025},
 doi = {http://doi.acm.org/10.1145/143103.120025},
 acmid = {120025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tjiang:1992:STB:143095.143120,
 author = {Tjiang, Steven W. K. and Hennessy, John L.},
 title = {Sharlit\&mdash;a tool for building optimizers},
 abstract = {A complex and time-consuming function of a modern compiler is global optimization. Unlike other functions of a compiler such as parsing and code generation which examine only one statement or one basic block at a time, optimizers are much larger in scope, examining and changing large portions of a program all at once. The larger scope means optimizers must perform many program transformations. Each of these transformations makes its own particular demands on the internal representation of programs; each can interact with and depend on the others in different ways. This makes optimizers large and complex.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {82--93},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143120},
 doi = {http://doi.acm.org/10.1145/143095.143120},
 acmid = {143120},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tjiang:1992:STB:143103.143120,
 author = {Tjiang, Steven W. K. and Hennessy, John L.},
 title = {Sharlit\&mdash;a tool for building optimizers},
 abstract = {A complex and time-consuming function of a modern compiler is global optimization. Unlike other functions of a compiler such as parsing and code generation which examine only one statement or one basic block at a time, optimizers are much larger in scope, examining and changing large portions of a program all at once. The larger scope means optimizers must perform many program transformations. Each of these transformations makes its own particular demands on the internal representation of programs; each can interact with and depend on the others in different ways. This makes optimizers large and complex.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {82--93},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143120},
 doi = {http://doi.acm.org/10.1145/143103.143120},
 acmid = {143120},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1992:PFC:143103.143122,
 author = {Chen, Marina and Cowie, James},
 title = {Prototyping Fortran-90 compilers for massively parallel machines},
 abstract = {Massively parallel architectures, and the languages used to program them, are among both the most difficult and the most rapidly-changing subjects for compilation. This has created a demand for new compiler prototyping technologies that allow novel style of compilation and optimization to be tested in a reasonable amount of time.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {94--105},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143122},
 doi = {http://doi.acm.org/10.1145/143103.143122},
 acmid = {143122},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1992:PFC:143095.143122,
 author = {Chen, Marina and Cowie, James},
 title = {Prototyping Fortran-90 compilers for massively parallel machines},
 abstract = {Massively parallel architectures, and the languages used to program them, are among both the most difficult and the most rapidly-changing subjects for compilation. This has created a demand for new compiler prototyping technologies that allow novel style of compilation and optimization to be tested in a reasonable amount of time.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {94--105},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143122},
 doi = {http://doi.acm.org/10.1145/143095.143122},
 acmid = {143122},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tan:1992:CDA:143103.143123,
 author = {Tan, Jichang and Lin, I-Peng},
 title = {Compiling dataflow analysis of logic programs},
 abstract = {Abstract interpretation is a technique extensively used for global dataflow analyses of logic programs. Existing implementations of abstract interpretation are slow due to interpretive or transforming overhead and the inefficiency in manipulation of global information. Since abstract interpretation mimics standard interpretation, it is a promising alternative to compile abstract interpretation into the framework of the WAM (Warren Abstract Machine) for better performance. In this paper, we show how this approach can be effectively implemented in a low-cost manner. To evaluate the possible benefits of this approach, a prototype dataflow analyzer has been implemented for inference of mode, type and variable aliasing information of logic programs. For a subset of benchmark programs in [15], it significantly improves the performance by a factor of over 150 on the average.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {106--115},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143123},
 doi = {http://doi.acm.org/10.1145/143103.143123},
 acmid = {143123},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tan:1992:CDA:143095.143123,
 author = {Tan, Jichang and Lin, I-Peng},
 title = {Compiling dataflow analysis of logic programs},
 abstract = {Abstract interpretation is a technique extensively used for global dataflow analyses of logic programs. Existing implementations of abstract interpretation are slow due to interpretive or transforming overhead and the inefficiency in manipulation of global information. Since abstract interpretation mimics standard interpretation, it is a promising alternative to compile abstract interpretation into the framework of the WAM (Warren Abstract Machine) for better performance. In this paper, we show how this approach can be effectively implemented in a low-cost manner. To evaluate the possible benefits of this approach, a prototype dataflow analyzer has been implemented for inference of mode, type and variable aliasing information of logic programs. For a subset of benchmark programs in [15], it significantly improves the performance by a factor of over 150 on the average.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {106--115},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143123},
 doi = {http://doi.acm.org/10.1145/143095.143123},
 acmid = {143123},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Park:1992:EAL:143103.143125,
 author = {Park, Young Gil and Goldberg, Benjamin},
 title = {Escape analysis on lists},
 abstract = {Higher order functional programs constantly allocate objects dynamically. These objects are typically cons cells, closures, and records and are generally allocated in the heap and reclaimed later by some garbage collection process. This paper describes a compile time analysis, called escape analysis, for determining the lifetime of dynamically created objects in higher order functional programs, and describes optimizations that can be performed, based on the analysis, to improve storage allocation and reclamation of such objects. In particular, our analysis can be applied to programs manipulating lists, in which case optimizations can be performed to allow cons cells in spines of lists to be either reclaimed immediately or reused without incurring any garbage collection overhead. In  a previous paper on escape analysis [10], we had left open the problem of performing escape analysis on lists.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143125},
 doi = {http://doi.acm.org/10.1145/143103.143125},
 acmid = {143125},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Park:1992:EAL:143095.143125,
 author = {Park, Young Gil and Goldberg, Benjamin},
 title = {Escape analysis on lists},
 abstract = {Higher order functional programs constantly allocate objects dynamically. These objects are typically cons cells, closures, and records and are generally allocated in the heap and reclaimed later by some garbage collection process. This paper describes a compile time analysis, called escape analysis, for determining the lifetime of dynamically created objects in higher order functional programs, and describes optimizations that can be performed, based on the analysis, to improve storage allocation and reclamation of such objects. In particular, our analysis can be applied to programs manipulating lists, in which case optimizations can be performed to allow cons cells in spines of lists to be either reclaimed immediately or reused without incurring any garbage collection overhead. In  a previous paper on escape analysis [10], we had left open the problem of performing escape analysis on lists.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143125},
 doi = {http://doi.acm.org/10.1145/143095.143125},
 acmid = {143125},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jaffar:1992:AMC:143095.143127,
 author = {Jaffar, Joxan and Stuckey, Peter J. and Michaylov, Spiro and Yap, Roland H. C.},
 title = {An abstract machine for CLP(<scp>R</scp>)},
 abstract = {An abstract machine is described for the CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programming language. It is intended as a first step toward enabling CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programs to be executed with efficiency approaching that of conventional languages. The core Constraint Logic Arithmetic Machine (CLAM) extends the Warren Abstract Machine (WAM) for compiling Prolog with facilities for handling real arithmetic constraints. The full CLAM includes facilities for taking advantage of information obtained from global program analysis.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143127},
 doi = {http://doi.acm.org/10.1145/143095.143127},
 acmid = {143127},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jaffar:1992:AMC:143103.143127,
 author = {Jaffar, Joxan and Stuckey, Peter J. and Michaylov, Spiro and Yap, Roland H. C.},
 title = {An abstract machine for CLP(<scp>R</scp>)},
 abstract = {An abstract machine is described for the CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programming language. It is intended as a first step toward enabling CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programs to be executed with efficiency approaching that of conventional languages. The core Constraint Logic Arithmetic Machine (CLAM) extends the Warren Abstract Machine (WAM) for compiling Prolog with facilities for handling real arithmetic constraints. The full CLAM includes facilities for taking advantage of information obtained from global program analysis.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143127},
 doi = {http://doi.acm.org/10.1145/143103.143127},
 acmid = {143127},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pugh:1992:EFD:143103.143129,
 author = {Pugh, William and Wonnacott, David},
 title = {Eliminating false data dependences using the Omega test},
 abstract = {Array data dependence analysis methods currently in use generate false dependences that can prevent useful program transformations. These false dependences arise because the questions asked are conservative approximations to the questions we really should be asking. Unfortunately, the questions we really should be asking go beyond integer programming and require decision procedures for a sublcass of Presburger formulas. In this paper, we describe how to extend the Omega test so that it can answer these queries and allow us to eliminate these false data dependences. We have implemented the techniques described here and believe they are suitable for use in production compilers.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143129},
 doi = {http://doi.acm.org/10.1145/143103.143129},
 acmid = {143129},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pugh:1992:EFD:143095.143129,
 author = {Pugh, William and Wonnacott, David},
 title = {Eliminating false data dependences using the Omega test},
 abstract = {Array data dependence analysis methods currently in use generate false dependences that can prevent useful program transformations. These false dependences arise because the questions asked are conservative approximations to the questions we really should be asking. Unfortunately, the questions we really should be asking go beyond integer programming and require decision procedures for a sublcass of Presburger formulas. In this paper, we describe how to extend the Omega test so that it can answer these queries and allow us to eliminate these false data dependences. We have implemented the techniques described here and believe they are suitable for use in production compilers.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143129},
 doi = {http://doi.acm.org/10.1145/143095.143129},
 acmid = {143129},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Maslov:1992:DEW:143103.143130,
 author = {Maslov, Vadim},
 title = {Delinearization: an efficient way to break multiloop dependence equations},
 abstract = {Exact and efficient data dependence testing is a key to success of loop-parallelizing compiler for computationally intensive programs. A number of algorithms has been created to test array references contained in parameter loops for dependence but most of them are unable to answer the following question correctly: Are references C(i<subscrpt>1</subscrpt> + 10j<subscrpt>1</subscrpt>) and C(i<subscrpt>2</subscrpt> + 5), 0 \&le; i<subscrpt>1</subscrpt>, i<subscrpt>2</subscrpt> \&le; 4, 0 \&le; j<subscrpt>1</subscrpt>,j<subscrpt>2</subscrpt> \&le; 9 independent? The technique introduced in this paper recognizes that  i<subscrpt>1</subscrpt>, i<subscrpt>2</subscrpt> and j<subscrpt>1</subscrpt>, j<subscrpt>2</subscrpt> make different order contributions to the subscript index, and breaks dependence equation i<subscrpt>1</subscrpt> + 10j<subscrpt>1</subscrpt> = i<subscrpt>2</subscrpt> + 10j<subscrpt>2</subscrpt> + 5 into two equations i<subscrpt>1</subscrpt> = i<subscrpt>2</subscrpt> and 10j<subscrpt>1</subscrpt> = 10j<subscrpt>2</subscrpt> which then can be solved independently. Since resulting equations contain less variables it is less expensive to solve them. We call this technique  delinearization because it is reverse of the linearization much discussed in the literature.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {152--161},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143130},
 doi = {http://doi.acm.org/10.1145/143103.143130},
 acmid = {143130},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Maslov:1992:DEW:143095.143130,
 author = {Maslov, Vadim},
 title = {Delinearization: an efficient way to break multiloop dependence equations},
 abstract = {Exact and efficient data dependence testing is a key to success of loop-parallelizing compiler for computationally intensive programs. A number of algorithms has been created to test array references contained in parameter loops for dependence but most of them are unable to answer the following question correctly: Are references C(i<subscrpt>1</subscrpt> + 10j<subscrpt>1</subscrpt>) and C(i<subscrpt>2</subscrpt> + 5), 0 \&le; i<subscrpt>1</subscrpt>, i<subscrpt>2</subscrpt> \&le; 4, 0 \&le; j<subscrpt>1</subscrpt>,j<subscrpt>2</subscrpt> \&le; 9 independent? The technique introduced in this paper recognizes that  i<subscrpt>1</subscrpt>, i<subscrpt>2</subscrpt> and j<subscrpt>1</subscrpt>, j<subscrpt>2</subscrpt> make different order contributions to the subscript index, and breaks dependence equation i<subscrpt>1</subscrpt> + 10j<subscrpt>1</subscrpt> = i<subscrpt>2</subscrpt> + 10j<subscrpt>2</subscrpt> + 5 into two equations i<subscrpt>1</subscrpt> = i<subscrpt>2</subscrpt> and 10j<subscrpt>1</subscrpt> = 10j<subscrpt>2</subscrpt> which then can be solved independently. Since resulting equations contain less variables it is less expensive to solve them. We call this technique  delinearization because it is reverse of the linearization much discussed in the literature.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {152--161},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143130},
 doi = {http://doi.acm.org/10.1145/143095.143130},
 acmid = {143130},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolfe:1992:BIV:143095.143131,
 author = {Wolfe, Michael},
 title = {Beyond induction variables},
 abstract = {Induction variable detection is usually closely tied to the strength reduction optimization. This paper studies induction variable analysis from a different perspective, that of finding induction variables for data dependence analysis. While classical induction variable analysis techniques have been used successfully up to now, we have found a simple algorithm based on the Static Single Assignment form of a program that finds all linear induction variables in a loop. Moreover, this algorithm is easily extended to find induction variables in multiple nested loops, to find nonlinear induction variables, and to classify other integer scalar assignments in loops, such as monotonic, periodic and wrap-around variables. Some of these other variables are now classified using ad hoc pattern recognition, while others are not analyzed by current compilers. Giving a unified approach improves the speed of compilers and allows a more general classification scheme. We also show how to use these variables in data dependence testing.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {162--174},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143095.143131},
 doi = {http://doi.acm.org/10.1145/143095.143131},
 acmid = {143131},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolfe:1992:BIV:143103.143131,
 author = {Wolfe, Michael},
 title = {Beyond induction variables},
 abstract = {Induction variable detection is usually closely tied to the strength reduction optimization. This paper studies induction variable analysis from a different perspective, that of finding induction variables for data dependence analysis. While classical induction variable analysis techniques have been used successfully up to now, we have found a simple algorithm based on the Static Single Assignment form of a program that finds all linear induction variables in a loop. Moreover, this algorithm is easily extended to find induction variables in multiple nested loops, to find nonlinear induction variables, and to classify other integer scalar assignments in loops, such as monotonic, periodic and wrap-around variables. Some of these other variables are now classified using ad hoc pattern recognition, while others are not analyzed by current compilers. Giving a unified approach improves the speed of compilers and allows a more general classification scheme. We also show how to use these variables in data dependence testing.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {162--174},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143103.143131},
 doi = {http://doi.acm.org/10.1145/143103.143131},
 acmid = {143131},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sarkar:1992:GFI:143103.143132,
 author = {Sarkar, Vivek and Thekkath, Radhika},
 title = {A general framework for iteration-reordering loop transformations},
 abstract = {This paper describes a general framework for representing iteration-reordering transformations. These transformations can be both matrix-based and non-matrix-based. Transformations are defined by rules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. The framework is extensible, and can be used to represent any iteration-reordering transformation. Mapping rules for several common transformations are included in the paper.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {175--187},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143103.143132},
 doi = {http://doi.acm.org/10.1145/143103.143132},
 acmid = {143132},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sarkar:1992:GFI:143095.143132,
 author = {Sarkar, Vivek and Thekkath, Radhika},
 title = {A general framework for iteration-reordering loop transformations},
 abstract = {This paper describes a general framework for representing iteration-reordering transformations. These transformations can be both matrix-based and non-matrix-based. Transformations are defined by rules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. The framework is extensible, and can be used to represent any iteration-reordering transformation. Mapping rules for several common transformations are included in the paper.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {175--187},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143095.143132},
 doi = {http://doi.acm.org/10.1145/143095.143132},
 acmid = {143132},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{v. Hanxleden:1992:RSC:143095.143133,
 author = {v. Hanxleden, Reinhard and Kennedy, Ken},
 title = {Relaxing SIMD control flow constraints using loop transformations},
 abstract = {Many loop nests in scientific codes contain a parallelizable outer loop but have an inner loop for which the number of iterations varies between different iterations of the outer loop. When running this kind of loop nest on a SIMD machine, the SIMD-inherent restriction to single program counter common to all processors will cause a performance degradation relative to comparable MIMD implementations. This problem is not due to limited parallelism or bad load balance, it is merely a problem of control flow.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {188--199},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143133},
 doi = {http://doi.acm.org/10.1145/143095.143133},
 acmid = {143133},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{v. Hanxleden:1992:RSC:143103.143133,
 author = {v. Hanxleden, Reinhard and Kennedy, Ken},
 title = {Relaxing SIMD control flow constraints using loop transformations},
 abstract = {Many loop nests in scientific codes contain a parallelizable outer loop but have an inner loop for which the number of iterations varies between different iterations of the outer loop. When running this kind of loop nest on a SIMD machine, the SIMD-inherent restriction to single program counter common to all processors will cause a performance degradation relative to comparable MIMD implementations. This problem is not due to limited parallelism or bad load balance, it is merely a problem of control flow.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {188--199},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143133},
 doi = {http://doi.acm.org/10.1145/143103.143133},
 acmid = {143133},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lucco:1992:DSM:143095.143134,
 author = {Lucco, Steven},
 title = {A dynamic scheduling method for irregular parallel programs},
 abstract = {This paper develops a methodology for compiling and executing irregular parallel programs. Such programs implement parallel operations whose size and work distribution depend on input data. We show a fundamental relationship between three quantities that characterize an irregular parallel computation: the total available parallelism, the optimal grain size, and the statistical variance of execution times for individual tasks. This relationship yields a dynamic scheduling algorithm that substantially reduces the overhead of executing irregular parallel operations.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143134},
 doi = {http://doi.acm.org/10.1145/143095.143134},
 acmid = {143134},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lucco:1992:DSM:143103.143134,
 author = {Lucco, Steven},
 title = {A dynamic scheduling method for irregular parallel programs},
 abstract = {This paper develops a methodology for compiling and executing irregular parallel programs. Such programs implement parallel operations whose size and work distribution depend on input data. We show a fundamental relationship between three quantities that characterize an irregular parallel computation: the total available parallelism, the optimal grain size, and the statistical variance of execution times for individual tasks. This relationship yields a dynamic scheduling algorithm that substantially reduces the overhead of executing irregular parallel operations.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143134},
 doi = {http://doi.acm.org/10.1145/143103.143134},
 acmid = {143134},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dhamdhere:1992:ALP:143095.143135,
 author = {Dhamdhere, Dhananjay M. and Rosen, Barry K. and Zadeck, F. Kenneth},
 title = {How to analyze large programs efficiently and informatively},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {212--223},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143135},
 doi = {http://doi.acm.org/10.1145/143095.143135},
 acmid = {143135},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dhamdhere:1992:ALP:143103.143135,
 author = {Dhamdhere, Dhananjay M. and Rosen, Barry K. and Zadeck, F. Kenneth},
 title = {How to analyze large programs efficiently and informatively},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {212--223},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143135},
 doi = {http://doi.acm.org/10.1145/143103.143135},
 acmid = {143135},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Knoop:1992:LCM:143103.143136,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {Lazy code motion},
 abstract = {We present a bit-vector algorithm for the optimal and economical placement of computations within flow graphs, which is as efficient as standard uni-directional analyses. The point of our algorithm is the decomposition of the bi-directional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further uni-directional analysis components exclude any unnecessary code motion. This laziness of our algorithm minimizes the register pressure, which has drastic effects on the run-time behaviour of the optimized programs in practice, where an economical use of registers is essential.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143103.143136},
 doi = {http://doi.acm.org/10.1145/143103.143136},
 acmid = {143136},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Knoop:1992:LCM:143095.143136,
 author = {Knoop, Jens and R\"{u}thing, Oliver and Steffen, Bernhard},
 title = {Lazy code motion},
 abstract = {We present a bit-vector algorithm for the optimal and economical placement of computations within flow graphs, which is as efficient as standard uni-directional analyses. The point of our algorithm is the decomposition of the bi-directional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further uni-directional analysis components exclude any unnecessary code motion. This laziness of our algorithm minimizes the register pressure, which has drastic effects on the run-time behaviour of the optimized programs in practice, where an economical use of registers is essential.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143095.143136},
 doi = {http://doi.acm.org/10.1145/143095.143136},
 acmid = {143136},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Landi:1992:SAA:143095.143137,
 author = {Landi, William and Ryder, Barbara G.},
 title = {A safe approximate algorithm for interprocedural aliasing},
 abstract = {During execution, when two or more names exist for the same location at some program point, we call them aliases. In a language which allows arbitrary pointers, the problem of determining aliases at a program point is \&rgr;-space-hard [Lan92]. We present an algorithm for the Conditional May Alias problem, which can be used to safely approximate Interprocedural May Alias in the presence of pointers. This algorithm is as precise as possible in the worst case and has been implemented in a prototype analysis tool for C programs. Preliminary speed and precision results are presented.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {235--248},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/143095.143137},
 doi = {http://doi.acm.org/10.1145/143095.143137},
 acmid = {143137},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Landi:1992:SAA:143103.143137,
 author = {Landi, William and Ryder, Barbara G.},
 title = {A safe approximate algorithm for interprocedural aliasing},
 abstract = {During execution, when two or more names exist for the same location at some program point, we call them aliases. In a language which allows arbitrary pointers, the problem of determining aliases at a program point is \&rgr;-space-hard [Lan92]. We present an algorithm for the Conditional May Alias problem, which can be used to safely approximate Interprocedural May Alias in the presence of pointers. This algorithm is as precise as possible in the worst case and has been implemented in a prototype analysis tool for C programs. Preliminary speed and precision results are presented.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {235--248},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/143103.143137},
 doi = {http://doi.acm.org/10.1145/143103.143137},
 acmid = {143137},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hendren:1992:ARP:143103.143138,
 author = {Hendren, Laurie J. and Hummell, Joseph and Nicolau, Alexandru},
 title = {Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs},
 abstract = {Even though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined pointer data structures.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143138},
 doi = {http://doi.acm.org/10.1145/143103.143138},
 acmid = {143138},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hendren:1992:ARP:143095.143138,
 author = {Hendren, Laurie J. and Hummell, Joseph and Nicolau, Alexandru},
 title = {Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs},
 abstract = {Even though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined pointer data structures.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143138},
 doi = {http://doi.acm.org/10.1145/143095.143138},
 acmid = {143138},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hoover:1992:AIC:143103.143139,
 author = {Hoover, Roger},
 title = {Alphonse: incremental computation as a programming abstraction},
 abstract = {Alphonse is a program transformation system that uses dynamic dependency analysis and incremental computation techniques to automatically generate efficient dynamic implementations from simple exhaustive imperative program specifications.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143139},
 doi = {http://doi.acm.org/10.1145/143103.143139},
 acmid = {143139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hoover:1992:AIC:143095.143139,
 author = {Hoover, Roger},
 title = {Alphonse: incremental computation as a programming abstraction},
 abstract = {Alphonse is a program transformation system that uses dynamic dependency analysis and incremental computation techniques to automatically generate efficient dynamic implementations from simple exhaustive imperative program specifications.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143139},
 doi = {http://doi.acm.org/10.1145/143095.143139},
 acmid = {143139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Diwan:1992:CSG:143095.143140,
 author = {Diwan, Amer and Moss, Eliot and Hudson, Richard},
 title = {Compiler support for garbage collection in a statically typed language},
 abstract = {We consider the problem of supporting compacting garbage collection in the presence of modern compiler optimizations. Since our collector may move any heap object, it must accurately locate, follow, and update all pointers and values derived from pointers. To assist the collector, we extend the compiler to emit tables describing live pointers, and values derived from pointers, at each program location where collection may occur. Significant results include identification of a number of problems posed by optimizations, solutions to those problems, a working compiler, and experimental data concerning table sizes, table compression, and time overhead of decoding tables during collection. While gc support can affect the code produced, our sample programs show no significant changes, the table sizes are a modest fraction of the size of the optimized code, and stack tracing is a small fraction of total gc time. Since the compiler enhancements are also modest, we conclude that the approach is practical.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {273--282},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143140},
 doi = {http://doi.acm.org/10.1145/143095.143140},
 acmid = {143140},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Diwan:1992:CSG:143103.143140,
 author = {Diwan, Amer and Moss, Eliot and Hudson, Richard},
 title = {Compiler support for garbage collection in a statically typed language},
 abstract = {We consider the problem of supporting compacting garbage collection in the presence of modern compiler optimizations. Since our collector may move any heap object, it must accurately locate, follow, and update all pointers and values derived from pointers. To assist the collector, we extend the compiler to emit tables describing live pointers, and values derived from pointers, at each program location where collection may occur. Significant results include identification of a number of problems posed by optimizations, solutions to those problems, a working compiler, and experimental data concerning table sizes, table compression, and time overhead of decoding tables during collection. While gc support can affect the code produced, our sample programs show no significant changes, the table sizes are a modest fraction of the size of the optimized code, and stack tracing is a small fraction of total gc time. Since the compiler enhancements are also modest, we conclude that the approach is practical.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {273--282},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143140},
 doi = {http://doi.acm.org/10.1145/143103.143140},
 acmid = {143140},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rau:1992:RAS:143103.143141,
 author = {Rau, B. R. and Lee, M. and Tirumalai, P. P. and Schlansker, M. S.},
 title = {Register allocation for software pipelined loops},
 abstract = {Software pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {283--299},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/143103.143141},
 doi = {http://doi.acm.org/10.1145/143103.143141},
 acmid = {143141},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rau:1992:RAS:143095.143141,
 author = {Rau, B. R. and Lee, M. and Tirumalai, P. P. and Schlansker, M. S.},
 title = {Register allocation for software pipelined loops},
 abstract = {Software pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {283--299},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/143095.143141},
 doi = {http://doi.acm.org/10.1145/143095.143141},
 acmid = {143141},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Proebsting:1992:PRA:143103.143142,
 author = {Proebsting, Todd A. and Fischer, Charles N.},
 title = {Probabilistic register allocation},
 abstract = {A new global register allocation technique, probabilistic register allocation, is described. Probabilistic register allocation quantifies the costs and benefits of allocating variables to registers over live ranges so that excellent allocation choices can be made. Local allocation is done first, and then global allocation is done iteratively beginning in the most deeply nested loops. Because local allocation precedes global allocation, probabilistic allocation does not interfere with the use of well-known, high-quality local register allocation and instruction scheduling techniques.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {300--310},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143103.143142},
 doi = {http://doi.acm.org/10.1145/143103.143142},
 acmid = {143142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Proebsting:1992:PRA:143095.143142,
 author = {Proebsting, Todd A. and Fischer, Charles N.},
 title = {Probabilistic register allocation},
 abstract = {A new global register allocation technique, probabilistic register allocation, is described. Probabilistic register allocation quantifies the costs and benefits of allocating variables to registers over live ranges so that excellent allocation choices can be made. Local allocation is done first, and then global allocation is done iteratively beginning in the most deeply nested loops. Because local allocation precedes global allocation, probabilistic allocation does not interfere with the use of well-known, high-quality local register allocation and instruction scheduling techniques.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {300--310},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143095.143142},
 doi = {http://doi.acm.org/10.1145/143095.143142},
 acmid = {143142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briggs:1992:REM:143095.143143,
 author = {Briggs, Preston and Cooper, Keith D. and Torczon, Linda},
 title = {Rematerialization},
 abstract = {This paper examines a problem that arises during global register allocation \&ndash; rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {311--321},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143095.143143},
 doi = {http://doi.acm.org/10.1145/143095.143143},
 acmid = {143143},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Briggs:1992:REM:143103.143143,
 author = {Briggs, Preston and Cooper, Keith D. and Torczon, Linda},
 title = {Rematerialization},
 abstract = {This paper examines a problem that arises during global register allocation \&ndash; rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {311--321},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143103.143143},
 doi = {http://doi.acm.org/10.1145/143103.143143},
 acmid = {143143},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mueller:1992:AUJ:143103.143144,
 author = {Mueller, Frank and Whalley, David B.},
 title = {Avoiding unconditional jumps by code replication},
 abstract = {This study evaluates a global optimization technique that avoids unconditional jumps by replicating code. When implemented in the back-end of an optimizing compiler, this technique can be generalized to work on almost all instances of unconditional jumps, including those generated from conditional statements and unstructured loops. The replication method is based on the idea of finding a replacement for each unconditional jump which minimizes the growth in code size. This is achieved by choosing the shortest sequence of instructions as a replacement. Measurements taken from a variety of programs showed that not only the number of executed instructions decreased, but also that the total cache work was reduced (except for small caches) despite increases in code size. Pipelined and superscalar machines may also benefit from an increase in the average basic block size.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {322--330},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/143103.143144},
 doi = {http://doi.acm.org/10.1145/143103.143144},
 acmid = {143144},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mueller:1992:AUJ:143095.143144,
 author = {Mueller, Frank and Whalley, David B.},
 title = {Avoiding unconditional jumps by code replication},
 abstract = {This study evaluates a global optimization technique that avoids unconditional jumps by replicating code. When implemented in the back-end of an optimizing compiler, this technique can be generalized to work on almost all instances of unconditional jumps, including those generated from conditional statements and unstructured loops. The replication method is based on the idea of finding a replacement for each unconditional jump which minimizes the growth in code size. This is achieved by choosing the shortest sequence of instructions as a replacement. Measurements taken from a variety of programs showed that not only the number of executed instructions decreased, but also that the total cache work was reduced (except for small caches) despite increases in code size. Pipelined and superscalar machines may also benefit from an increase in the average basic block size.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {322--330},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/143095.143144},
 doi = {http://doi.acm.org/10.1145/143095.143144},
 acmid = {143144},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Proebsting:1992:SEB:143103.143145,
 author = {Proebsting, Todd A.},
 title = {Simple and efficient BURS table generation},
 abstract = {A simple and efficient algorithm for generating bottom-up rewrite system (BURS) tables is described. A small prototype implementation produces tables 10 to 30 times more quickly than the best current techniques. The algorithm does not require novel data structures or complicated algorithmic techniques. Previously published methods for the on-the-fly elimination of states are generalized and simplified to create a new method, triangle trimming, that is employed in the algorithm.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {331--340},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143103.143145},
 doi = {http://doi.acm.org/10.1145/143103.143145},
 acmid = {143145},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Proebsting:1992:SEB:143095.143145,
 author = {Proebsting, Todd A.},
 title = {Simple and efficient BURS table generation},
 abstract = {A simple and efficient algorithm for generating bottom-up rewrite system (BURS) tables is described. A small prototype implementation produces tables 10 to 30 times more quickly than the best current techniques. The algorithm does not require novel data structures or complicated algorithmic techniques. Previously published methods for the on-the-fly elimination of states are generalized and simplified to create a new method, triangle trimming, that is employed in the algorithm.},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {331--340},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143095.143145},
 doi = {http://doi.acm.org/10.1145/143095.143145},
 acmid = {143145},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Granlund:1992:EBU:143095.143146,
 author = {Granlund, Torbj\"{o}rn and Kenner, Richard},
 title = {Eliminating branches using a superoptimizer and the GNU C compiler},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1992 conference on Programming language design and implementation},
 series = {PLDI '92},
 year = {1992},
 isbn = {0-89791-475-9},
 location = {San Francisco, California, United States},
 pages = {341--352},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143095.143146},
 doi = {http://doi.acm.org/10.1145/143095.143146},
 acmid = {143146},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Granlund:1992:EBU:143103.143146,
 author = {Granlund, Torbj\"{o}rn and Kenner, Richard},
 title = {Eliminating branches using a superoptimizer and the GNU C compiler},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {7},
 month = {July},
 year = {1992},
 issn = {0362-1340},
 pages = {341--352},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143103.143146},
 doi = {http://doi.acm.org/10.1145/143103.143146},
 acmid = {143146},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Maydan:1991:EED:113445.113447,
 author = {Maydan, Dror E. and Hennessy, John L. and Lam, Monica S.},
 title = {Efficient and exact data dependence analysis},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/113445.113447},
 doi = {http://doi.acm.org/10.1145/113445.113447},
 acmid = {113447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Maydan:1991:EED:113446.113447,
 author = {Maydan, Dror E. and Hennessy, John L. and Lam, Monica S.},
 title = {Efficient and exact data dependence analysis},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/113446.113447},
 doi = {http://doi.acm.org/10.1145/113446.113447},
 acmid = {113447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goff:1991:PDT:113445.113448,
 author = {Goff, Gina and Kennedy, Ken and Tseng, Chau-Wen},
 title = {Practical dependence testing},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {15--29},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113448},
 doi = {http://doi.acm.org/10.1145/113445.113448},
 acmid = {113448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goff:1991:PDT:113446.113448,
 author = {Goff, Gina and Kennedy, Ken and Tseng, Chau-Wen},
 title = {Practical dependence testing},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {15--29},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113448},
 doi = {http://doi.acm.org/10.1145/113446.113448},
 acmid = {113448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolf:1991:DLO:113446.113449,
 author = {Wolf, Michael E. and Lam, Monica S.},
 title = {A data locality optimizing algorithm},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {30--44},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113449},
 doi = {http://doi.acm.org/10.1145/113446.113449},
 acmid = {113449},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolf:1991:DLO:113445.113449,
 author = {Wolf, Michael E. and Lam, Monica S.},
 title = {A data locality optimizing algorithm},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {30--44},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113449},
 doi = {http://doi.acm.org/10.1145/113445.113449},
 acmid = {113449},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Morris:1991:CPC:113446.113450,
 author = {Morris, W. G.},
 title = {CCG: a prototype coagulating code generator},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {45--58},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/113446.113450},
 doi = {http://doi.acm.org/10.1145/113446.113450},
 acmid = {113450},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Morris:1991:CPC:113445.113450,
 author = {Morris, W. G.},
 title = {CCG: a prototype coagulating code generator},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {45--58},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/113445.113450},
 doi = {http://doi.acm.org/10.1145/113445.113450},
 acmid = {113450},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1991:PPB:113446.113451,
 author = {Wall, David W.},
 title = {Predicting program behavior using real or estimated profiles},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113451},
 doi = {http://doi.acm.org/10.1145/113446.113451},
 acmid = {113451},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wall:1991:PPB:113445.113451,
 author = {Wall, David W.},
 title = {Predicting program behavior using real or estimated profiles},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113451},
 doi = {http://doi.acm.org/10.1145/113445.113451},
 acmid = {113451},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McFarling:1991:PMI:113445.113452,
 author = {McFarling, Scott},
 title = {Procedure merging with instruction caches},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {71--79},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/113445.113452},
 doi = {http://doi.acm.org/10.1145/113445.113452},
 acmid = {113452},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McFarling:1991:PMI:113446.113452,
 author = {McFarling, Scott},
 title = {Procedure merging with instruction caches},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {71--79},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/113446.113452},
 doi = {http://doi.acm.org/10.1145/113446.113452},
 acmid = {113452},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Launchbury:1991:SBA:113445.113453,
 author = {Launchbury, John},
 title = {Strictness and binding-time analyses: two for the price of one},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {80--91},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113453},
 doi = {http://doi.acm.org/10.1145/113445.113453},
 acmid = {113453},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Launchbury:1991:SBA:113446.113453,
 author = {Launchbury, John},
 title = {Strictness and binding-time analyses: two for the price of one},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {80--91},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113453},
 doi = {http://doi.acm.org/10.1145/113446.113453},
 acmid = {113453},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Consel:1991:PPE:113446.113454,
 author = {Consel, Charles and Khoo, Siau Cheng},
 title = {Parameterized partial evaluation},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {92--106},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113454},
 doi = {http://doi.acm.org/10.1145/113446.113454},
 acmid = {113454},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Consel:1991:PPE:113445.113454,
 author = {Consel, Charles and Khoo, Siau Cheng},
 title = {Parameterized partial evaluation},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {92--106},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113454},
 doi = {http://doi.acm.org/10.1145/113445.113454},
 acmid = {113454},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Venkatesh:1991:SAP:113446.113455,
 author = {Venkatesh, G. A.},
 title = {The semantic approach to program slicing},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/113446.113455},
 doi = {http://doi.acm.org/10.1145/113446.113455},
 acmid = {113455},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Venkatesh:1991:SAP:113445.113455,
 author = {Venkatesh, G. A.},
 title = {The semantic approach to program slicing},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/113445.113455},
 doi = {http://doi.acm.org/10.1145/113445.113455},
 acmid = {113455},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Whitfield:1991:AGG:113446.113456,
 author = {Whitfield, Deborah and Soffa, Mary Lou},
 title = {Automatic generation of global optimizers},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {120--129},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113446.113456},
 doi = {http://doi.acm.org/10.1145/113446.113456},
 acmid = {113456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Whitfield:1991:AGG:113445.113456,
 author = {Whitfield, Deborah and Soffa, Mary Lou},
 title = {Automatic generation of global optimizers},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {120--129},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113445.113456},
 doi = {http://doi.acm.org/10.1145/113445.113456},
 acmid = {113456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chatterjee:1991:SAI:113446.113457,
 author = {Chatterjee, Siddhartha and Blelloch, Guy E. and Fisher, Allan L.},
 title = {Size and access inference for data-parallel programs},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {130--144},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113457},
 doi = {http://doi.acm.org/10.1145/113446.113457},
 acmid = {113457},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chatterjee:1991:SAI:113445.113457,
 author = {Chatterjee, Siddhartha and Blelloch, Guy E. and Fisher, Allan L.},
 title = {Size and access inference for data-parallel programs},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {130--144},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113457},
 doi = {http://doi.acm.org/10.1145/113445.113457},
 acmid = {113457},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bromley:1991:FTG:113446.113458,
 author = {Bromley, Mark and Heller, Steven and McNerney, Tim and Steele,Jr., Guy L.},
 title = {Fortran at ten gigaflops: the connection machine convolution compiler},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113458},
 doi = {http://doi.acm.org/10.1145/113446.113458},
 acmid = {113458},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bromley:1991:FTG:113445.113458,
 author = {Bromley, Mark and Heller, Steven and McNerney, Tim and Steele,Jr., Guy L.},
 title = {Fortran at ten gigaflops: the connection machine convolution compiler},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113458},
 doi = {http://doi.acm.org/10.1145/113445.113458},
 acmid = {113458},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boehm:1991:MPG:113446.113459,
 author = {Boehm, Hans-J. and Demers, Alan J. and Shenker, Scott},
 title = {Mostly parallel garbage collection},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {157--164},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/113446.113459},
 doi = {http://doi.acm.org/10.1145/113446.113459},
 acmid = {113459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boehm:1991:MPG:113445.113459,
 author = {Boehm, Hans-J. and Demers, Alan J. and Shenker, Scott},
 title = {Mostly parallel garbage collection},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {157--164},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/113445.113459},
 doi = {http://doi.acm.org/10.1145/113445.113459},
 acmid = {113459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goldberg:1991:TGC:113445.113460,
 author = {Goldberg, Benjamin},
 title = {Tag-free garbage collection for strongly typed programming languages},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113460},
 doi = {http://doi.acm.org/10.1145/113445.113460},
 acmid = {113460},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goldberg:1991:TGC:113446.113460,
 author = {Goldberg, Benjamin},
 title = {Tag-free garbage collection for strongly typed programming languages},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113460},
 doi = {http://doi.acm.org/10.1145/113446.113460},
 acmid = {113460},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilson:1991:ELR:113446.113461,
 author = {Wilson, Paul R. and Lam, Michael S. and Moher, Thomas G.},
 title = {Effective \&ldquo;static-graph\&rdquo; reorganization to improve locality in garbage-collected systems},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {177--191},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113461},
 doi = {http://doi.acm.org/10.1145/113446.113461},
 acmid = {113461},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wilson:1991:ELR:113445.113461,
 author = {Wilson, Paul R. and Lam, Michael S. and Moher, Thomas G.},
 title = {Effective \&ldquo;static-graph\&rdquo; reorganization to improve locality in garbage-collected systems},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {177--191},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113461},
 doi = {http://doi.acm.org/10.1145/113445.113461},
 acmid = {113461},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1991:RAV:113446.113462,
 author = {Callahan, David and Koblenz, Brian},
 title = {Register allocation via hierarchical graph coloring},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113462},
 doi = {http://doi.acm.org/10.1145/113446.113462},
 acmid = {113462},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Callahan:1991:RAV:113445.113462,
 author = {Callahan, David and Koblenz, Brian},
 title = {Register allocation via hierarchical graph coloring},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113462},
 doi = {http://doi.acm.org/10.1145/113445.113462},
 acmid = {113462},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gao:1991:TPM:113445.113463,
 author = {Gao, Guang R. and Wong, Yue-Bong and Ning, Qi},
 title = {A timed Petri-net model for fine-grain loop scheduling},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {204--218},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113463},
 doi = {http://doi.acm.org/10.1145/113445.113463},
 acmid = {113463},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gao:1991:TPM:113446.113463,
 author = {Gao, Guang R. and Wong, Yue-Bong and Ning, Qi},
 title = {A timed Petri-net model for fine-grain loop scheduling},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {204--218},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113463},
 doi = {http://doi.acm.org/10.1145/113446.113463},
 acmid = {113463},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jain:1991:CSN:113446.113464,
 author = {Jain, Suneel},
 title = {Circular scheduling: a new technique to perform software pipelining},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113446.113464},
 doi = {http://doi.acm.org/10.1145/113446.113464},
 acmid = {113464},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jain:1991:CSN:113445.113464,
 author = {Jain, Suneel},
 title = {Circular scheduling: a new technique to perform software pipelining},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113445.113464},
 doi = {http://doi.acm.org/10.1145/113445.113464},
 acmid = {113464},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradlee:1991:MSR:113446.113465,
 author = {Bradlee, David G. and Henry, Robert R. and Eggers, Susan J.},
 title = {The Marion system for retargetable instruction scheduling},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113465},
 doi = {http://doi.acm.org/10.1145/113446.113465},
 acmid = {113465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bradlee:1991:MSR:113445.113465,
 author = {Bradlee, David G. and Henry, Robert R. and Eggers, Susan J.},
 title = {The Marion system for retargetable instruction scheduling},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113465},
 doi = {http://doi.acm.org/10.1145/113445.113465},
 acmid = {113465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bernstein:1991:GIS:113445.113466,
 author = {Bernstein, David and Rodeh, Michael},
 title = {Global instruction scheduling for superscalar machines},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {241--255},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113466},
 doi = {http://doi.acm.org/10.1145/113445.113466},
 acmid = {113466},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bernstein:1991:GIS:113446.113466,
 author = {Bernstein, David and Rodeh, Michael},
 title = {Global instruction scheduling for superscalar machines},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {241--255},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113466},
 doi = {http://doi.acm.org/10.1145/113446.113466},
 acmid = {113466},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Proebsting:1991:LOC:113446.113467,
 author = {Proebsting, Todd A. and Fischer, Charles N.},
 title = {Linear-ti optimal code scheduling for delayed-load architectures},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {256--267},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113446.113467},
 doi = {http://doi.acm.org/10.1145/113446.113467},
 acmid = {113467},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Proebsting:1991:LOC:113445.113467,
 author = {Proebsting, Todd A. and Fischer, Charles N.},
 title = {Linear-ti optimal code scheduling for delayed-load architectures},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {256--267},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/113445.113467},
 doi = {http://doi.acm.org/10.1145/113445.113467},
 acmid = {113467},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Freeman:1991:RTM:113446.113468,
 author = {Freeman, Tim and Pfenning, Frank},
 title = {Refinement types for ML},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113446.113468},
 doi = {http://doi.acm.org/10.1145/113446.113468},
 acmid = {113468},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Freeman:1991:RTM:113445.113468,
 author = {Freeman, Tim and Pfenning, Frank},
 title = {Refinement types for ML},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113445.113468},
 doi = {http://doi.acm.org/10.1145/113445.113468},
 acmid = {113468},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cartwright:1991:ST:113446.113469,
 author = {Cartwright, Robert and Fagan, Mike},
 title = {Soft typing},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {278--292},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113469},
 doi = {http://doi.acm.org/10.1145/113446.113469},
 acmid = {113469},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cartwright:1991:ST:113445.113469,
 author = {Cartwright, Robert and Fagan, Mike},
 title = {Soft typing},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {278--292},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113469},
 doi = {http://doi.acm.org/10.1145/113445.113469},
 acmid = {113469},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reppy:1991:CHC:113446.113470,
 author = {Reppy, John H.},
 title = {CML: A higher concurrent language},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {293--305},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/113446.113470},
 doi = {http://doi.acm.org/10.1145/113446.113470},
 acmid = {113470},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reppy:1991:CHC:113445.113470,
 author = {Reppy, John H.},
 title = {CML: A higher concurrent language},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {293--305},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/113445.113470},
 doi = {http://doi.acm.org/10.1145/113445.113470},
 acmid = {113470},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jaffar:1991:MMH:113446.113471,
 author = {Jaffar, Joxan and Michaylov, Spiro and Yap, Roland H. C.},
 title = {A methodology for managing hard constraints in CLP systems},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {306--316},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/113446.113471},
 doi = {http://doi.acm.org/10.1145/113446.113471},
 acmid = {113471},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jaffar:1991:MMH:113445.113471,
 author = {Jaffar, Joxan and Michaylov, Spiro and Yap, Roland H. C.},
 title = {A methodology for managing hard constraints in CLP systems},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {306--316},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/113445.113471},
 doi = {http://doi.acm.org/10.1145/113445.113471},
 acmid = {113471},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fritzson:1991:GAD:113446.113472,
 author = {Fritzson, Peter and Gyimothy, Tibor and Kamkar, Mariam and Shahmehri, Nahid},
 title = {Generalized algorithmic debugging and testing},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {317--326},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113446.113472},
 doi = {http://doi.acm.org/10.1145/113446.113472},
 acmid = {113472},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fritzson:1991:GAD:113445.113472,
 author = {Fritzson, Peter and Gyimothy, Tibor and Kamkar, Mariam and Shahmehri, Nahid},
 title = {Generalized algorithmic debugging and testing},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {317--326},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/113445.113472},
 doi = {http://doi.acm.org/10.1145/113445.113472},
 acmid = {113472},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bertot:1991:ODS:113446.113473,
 author = {Bertot, Yves},
 title = {Occurrences in debugger specifications},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {327--337},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/113446.113473},
 doi = {http://doi.acm.org/10.1145/113446.113473},
 acmid = {113473},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bertot:1991:ODS:113445.113473,
 author = {Bertot, Yves},
 title = {Occurrences in debugger specifications},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {327--337},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/113445.113473},
 doi = {http://doi.acm.org/10.1145/113445.113473},
 acmid = {113473},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kishon:1991:MSF:113445.113474,
 author = {Kishon, Amir and Hudak, Paul and Consel, Charles},
 title = {Monitoring semantics: a formal framework for specifying, implementing, and reasoning about execution monitors},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
 series = {PLDI '91},
 year = {1991},
 isbn = {0-89791-428-7},
 location = {Toronto, Ontario, Canada},
 pages = {338--352},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113445.113474},
 doi = {http://doi.acm.org/10.1145/113445.113474},
 acmid = {113474},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kishon:1991:MSF:113446.113474,
 author = {Kishon, Amir and Hudak, Paul and Consel, Charles},
 title = {Monitoring semantics: a formal framework for specifying, implementing, and reasoning about execution monitors},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {6},
 month = {May},
 year = {1991},
 issn = {0362-1340},
 pages = {338--352},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/113446.113474},
 doi = {http://doi.acm.org/10.1145/113446.113474},
 acmid = {113474},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wendt:1990:FCG:93548.93549,
 author = {Wendt, Alan L.},
 title = {Fast code generation using automatically-generated decision trees},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {9--15},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93548.93549},
 doi = {http://doi.acm.org/10.1145/93548.93549},
 acmid = {93549},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wendt:1990:FCG:93542.93549,
 author = {Wendt, Alan L.},
 title = {Fast code generation using automatically-generated decision trees},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {9--15},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93542.93549},
 doi = {http://doi.acm.org/10.1145/93542.93549},
 acmid = {93549},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pettis:1990:PGC:93542.93550,
 author = {Pettis, Karl and Hansen, Robert C.},
 title = {Profile guided code positioning},
 abstract = {This paper presents the results of our investigation of code positioning techniques using execution profile data as input into the compilation process. The primary objective of the positioning is to reduce the overhead of the instruction memory hierarchy.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {16--27},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93542.93550},
 doi = {http://doi.acm.org/10.1145/93542.93550},
 acmid = {93550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pettis:1990:PGC:93548.93550,
 author = {Pettis, Karl and Hansen, Robert C.},
 title = {Profile guided code positioning},
 abstract = {This paper presents the results of our investigation of code positioning techniques using execution profile data as input into the compilation process. The primary objective of the positioning is to reduce the overhead of the instruction memory hierarchy.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {16--27},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93548.93550},
 doi = {http://doi.acm.org/10.1145/93548.93550},
 acmid = {93550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Santhanam:1990:RAA:93542.93551,
 author = {Santhanam, Vatsa and Odnert, Daryl},
 title = {Register allocation across procedure and module boundaries},
 abstract = {This paper describes a method for compiling programs using interprocedural register allocation. A strategy for handling programs built from multiple modules is presented, as well as algorithms for global variable promotion and register spill code motion. These algorithms attempt to address some of the shortcomings of previous interprocedural register allocation strategies. Results are given for an implementation on a single register file RISC-based architecture.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {28--39},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93542.93551},
 doi = {http://doi.acm.org/10.1145/93542.93551},
 acmid = {93551},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Santhanam:1990:RAA:93548.93551,
 author = {Santhanam, Vatsa and Odnert, Daryl},
 title = {Register allocation across procedure and module boundaries},
 abstract = {This paper describes a method for compiling programs using interprocedural register allocation. A strategy for handling programs built from multiple modules is presented, as well as algorithms for global variable promotion and register spill code motion. These algorithms attempt to address some of the shortcomings of previous interprocedural register allocation strategies. Results are given for an implementation on a single register file RISC-based architecture.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {28--39},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93548.93551},
 doi = {http://doi.acm.org/10.1145/93548.93551},
 acmid = {93551},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nickerson:1990:GCR:93542.93552,
 author = {Nickerson, Brian R.},
 title = {Graph coloring register allocation for processors with multi-register operands},
 abstract = {Though graph coloring algorithms have been shown to work well when applied to register allocation problems, the technique has not been generalized for processor architectures in which some instructions refer to individual operands that are comprised of multiple registers. This paper presents a suitable generalization.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93542.93552},
 doi = {http://doi.acm.org/10.1145/93542.93552},
 acmid = {93552},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nickerson:1990:GCR:93548.93552,
 author = {Nickerson, Brian R.},
 title = {Graph coloring register allocation for processors with multi-register operands},
 abstract = {Though graph coloring algorithms have been shown to work well when applied to register allocation problems, the technique has not been generalized for processor architectures in which some instructions refer to individual operands that are comprised of multiple registers. This paper presents a suitable generalization.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93548.93552},
 doi = {http://doi.acm.org/10.1145/93548.93552},
 acmid = {93552},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Callahan:1990:IRA:93542.93553,
 author = {Callahan, David and Carr, Steve and Kennedy, Ken},
 title = {Improving register allocation for subscripted variables},
 abstract = {Most conventional compilers fail to allocate array elements to registers because standard data-flow analysis treats arrays like scalars, making it impossible to analyze the definitions and uses of individual array elements. This deficiency is particularly troublesome for floating-point registers, which are most often used as temporary repositories for subscripted variables.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {53--65},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93542.93553},
 doi = {http://doi.acm.org/10.1145/93542.93553},
 acmid = {93553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1990:IRA:93548.93553,
 author = {Callahan, David and Carr, Steve and Kennedy, Ken},
 title = {Improving register allocation for subscripted variables},
 abstract = {Most conventional compilers fail to allocate array elements to registers because standard data-flow analysis treats arrays like scalars, making it impossible to analyze the definitions and uses of individual array elements. This deficiency is particularly troublesome for floating-point registers, which are most often used as temporary repositories for subscripted variables.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {53--65},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93548.93553},
 doi = {http://doi.acm.org/10.1145/93548.93553},
 acmid = {93553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hieb:1990:RCP:93542.93554,
 author = {Hieb, R. and Dybvig, R. Kent and Bruggeman, Carl},
 title = {Representing control in the presence of first-class continuations},
 abstract = {Languages such as Scheme and Smalltalk that provide continuations as first-class data objects present a challenge to efficient implementation. Allocating activation records in a heap has proven unsatisfactory because of increased frame linkage costs, increased garbage collection overhead, and decreased locality of reference. However, simply allocating activation records on a stack and copying them when a continuation is created results in unbounded copying overhead. This paper describes a new approach based on stack allocation that does not require the stack to be copied when a continuation is created and that allows us to place a small upper bound on the amount copied when a continuation is reinstated. This new approach is faster than the naive stack allocation approach, and it does not suffer from the problems associated with unbounded copying. For continuation-intensive programs, our approach is at worst a constant factor slower than the heap allocation approach, and for typical programs, it is significantly faster. An important additional benefit is that recovery from stack overflow is handled gracefully and efficiently.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93542.93554},
 doi = {http://doi.acm.org/10.1145/93542.93554},
 acmid = {93554},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hieb:1990:RCP:93548.93554,
 author = {Hieb, R. and Dybvig, R. Kent and Bruggeman, Carl},
 title = {Representing control in the presence of first-class continuations},
 abstract = {Languages such as Scheme and Smalltalk that provide continuations as first-class data objects present a challenge to efficient implementation. Allocating activation records in a heap has proven unsatisfactory because of increased frame linkage costs, increased garbage collection overhead, and decreased locality of reference. However, simply allocating activation records on a stack and copying them when a continuation is created results in unbounded copying overhead. This paper describes a new approach based on stack allocation that does not require the stack to be copied when a continuation is created and that allows us to place a small upper bound on the amount copied when a continuation is reinstated. This new approach is faster than the naive stack allocation approach, and it does not suffer from the problems associated with unbounded copying. For continuation-intensive programs, our approach is at worst a constant factor slower than the heap allocation approach, and for typical programs, it is significantly faster. An important additional benefit is that recovery from stack overflow is handled gracefully and efficiently.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93548.93554},
 doi = {http://doi.acm.org/10.1145/93548.93554},
 acmid = {93554},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kessler:1990:FBD:93548.93555,
 author = {Kessler, Peter B.},
 title = {Fast breakpoints: design and implementation},
 abstract = {We have designed and implemented a fast breakpoint facility. Breakpoints are usually thought of as a feature of an interactive debugger, in which case the breakpoints need not be particularly fast. In our environment breakpoints are often used for non-interactive information gathering; for example, procedure call count and statement execution count profiling [Swinehart, et al.]. When used non-interactively, breakpoints should be as fast as possible, so as to perturb the execution of the program as little as possible. Even in interactive debuggers, a conditional breakpoint facility would benefit from breakpoints that could transfer to the evaluation of the condition rapidly, and continue expeditiously if the condition were not satisfied. Such conditional breakpoints could be used to check assertions, etc. Program advising could also make use of fast breakpoints [Teitelman]. Examples of advising include tracing, timing, and even animation, all of which should be part of an advanced programming environment.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {78--84},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93548.93555},
 doi = {http://doi.acm.org/10.1145/93548.93555},
 acmid = {93555},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kessler:1990:FBD:93542.93555,
 author = {Kessler, Peter B.},
 title = {Fast breakpoints: design and implementation},
 abstract = {We have designed and implemented a fast breakpoint facility. Breakpoints are usually thought of as a feature of an interactive debugger, in which case the breakpoints need not be particularly fast. In our environment breakpoints are often used for non-interactive information gathering; for example, procedure call count and statement execution count profiling [Swinehart, et al.]. When used non-interactively, breakpoints should be as fast as possible, so as to perturb the execution of the program as little as possible. Even in interactive debuggers, a conditional breakpoint facility would benefit from breakpoints that could transfer to the evaluation of the condition rapidly, and continue expeditiously if the condition were not satisfied. Such conditional breakpoints could be used to check assertions, etc. Program advising could also make use of fast breakpoints [Teitelman]. Examples of advising include tracing, timing, and even animation, all of which should be part of an advanced programming environment.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {78--84},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93542.93555},
 doi = {http://doi.acm.org/10.1145/93542.93555},
 acmid = {93555},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pugh:1990:TRL:93542.93556,
 author = {Pugh, William and Weddell, Grant},
 title = {Two-directional record layout for multiple inheritance},
 abstract = {Much recent work in polymorphic programming languages allows subtyping and multiple inheritance for records. In such systems, we would like to extract a field from a record with the same efficiency as if we were not making use of subtyping and multiple inheritance. Methods currently used make field extraction 3-5 times slower, which can produce a significant overall performance slowdown.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {85--91},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93542.93556},
 doi = {http://doi.acm.org/10.1145/93542.93556},
 acmid = {93556},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pugh:1990:TRL:93548.93556,
 author = {Pugh, William and Weddell, Grant},
 title = {Two-directional record layout for multiple inheritance},
 abstract = {Much recent work in polymorphic programming languages allows subtyping and multiple inheritance for records. In such systems, we would like to extract a field from a record with the same efficiency as if we were not making use of subtyping and multiple inheritance. Methods currently used make field extraction 3-5 times slower, which can produce a significant overall performance slowdown.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {85--91},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/93548.93556},
 doi = {http://doi.acm.org/10.1145/93548.93556},
 acmid = {93556},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Clinger:1990:RFP:93542.93557,
 author = {Clinger, William D.},
 title = {How to read floating point numbers accurately},
 abstract = {Consider the problem of converting decimal scientific notation for a number into the best binary floating point approximation to that number, for some fixed precision. This problem cannot be solved using arithmetic of any fixed precision. Hence the IEEE Standard for Binary Floating-Point Arithmetic does not require the result of such a conversion to be the best approximation.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {92--101},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93542.93557},
 doi = {http://doi.acm.org/10.1145/93542.93557},
 acmid = {93557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clinger:1990:RFP:93548.93557,
 author = {Clinger, William D.},
 title = {How to read floating point numbers accurately},
 abstract = {Consider the problem of converting decimal scientific notation for a number into the best binary floating point approximation to that number, for some fixed precision. This problem cannot be solved using arithmetic of any fixed precision. Hence the IEEE Standard for Binary Floating-Point Arithmetic does not require the result of such a conversion to be the best approximation.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {92--101},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93548.93557},
 doi = {http://doi.acm.org/10.1145/93548.93557},
 acmid = {93557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1990:OPO:93542.93558,
 author = {Lee,Jr., Vernon A. and Boehm, Hans-J.},
 title = {Optimizing programs over the constructive reals},
 abstract = {The constructive reals provide programmers with a useful mechanism for prototyping numerical programs, and for experimenting with numerical algorithms. Unfortunately, the performance of current implementations is inadequate for some potential applications. In particular, these implementations tend to be space inefficient, in that they essentially require a complete computation history to be maintained.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {102--111},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93542.93558},
 doi = {http://doi.acm.org/10.1145/93542.93558},
 acmid = {93558},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1990:OPO:93548.93558,
 author = {Lee,Jr., Vernon A. and Boehm, Hans-J.},
 title = {Optimizing programs over the constructive reals},
 abstract = {The constructive reals provide programmers with a useful mechanism for prototyping numerical programs, and for experimenting with numerical algorithms. Unfortunately, the performance of current implementations is inadequate for some potential applications. In particular, these implementations tend to be space inefficient, in that they essentially require a complete computation history to be maintained.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {102--111},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93548.93558},
 doi = {http://doi.acm.org/10.1145/93548.93558},
 acmid = {93558},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Steele:1990:PFN:93548.93559,
 author = {Steele,Jr., Guy L. and White, Jon L.},
 title = {How to print floating-point numbers accurately},
 abstract = {We present algorithms for accurately converting floating-point numbers to decimal representation. The key idea is to carry along with the computation an explicit representation of the required rounding accuracy.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {112--126},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93559},
 doi = {http://doi.acm.org/10.1145/93548.93559},
 acmid = {93559},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Steele:1990:PFN:93542.93559,
 author = {Steele,Jr., Guy L. and White, Jon L.},
 title = {How to print floating-point numbers accurately},
 abstract = {We present algorithms for accurately converting floating-point numbers to decimal representation. The key idea is to carry along with the computation an explicit representation of the required rounding accuracy.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {112--126},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93559},
 doi = {http://doi.acm.org/10.1145/93542.93559},
 acmid = {93559},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cormack:1990:TPI:93548.93560,
 author = {Cormack, Gordon V. and Wright, Andrew K.},
 title = {Type-dependent parameter inference},
 abstract = {An algorithm is presented to infer the type and operation parameters of polymorphic functions. Operation parameters are named and typed at the function definition, but are selected from the set of overloaded definitions available wherever the function is used. These parameters are always implicit, implying that the complexity of using a function does not increase with the generality of its type.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {127--136},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93548.93560},
 doi = {http://doi.acm.org/10.1145/93548.93560},
 acmid = {93560},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cormack:1990:TPI:93542.93560,
 author = {Cormack, Gordon V. and Wright, Andrew K.},
 title = {Type-dependent parameter inference},
 abstract = {An algorithm is presented to infer the type and operation parameters of polymorphic functions. Operation parameters are named and typed at the function definition, but are selected from the set of overloaded definitions available wherever the function is used. These parameters are always implicit, implying that the complexity of using a function does not increase with the generality of its type.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {127--136},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/93542.93560},
 doi = {http://doi.acm.org/10.1145/93542.93560},
 acmid = {93560},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1990:CHA:93548.93561,
 author = {Anderson, Steven and Hudak, Paul},
 title = {Compilation of Haskell array comprehensions for scientific computing},
 abstract = {Monolithic approaches to functional language arrays, such as Haskell array comprehensions, define elements all at once, at the time the array is created, instead of incrementally. Although monolithic arrays are elegant, a naive implementation can be very inefficient. For example, if a compiler does not know whether an element has zero or many definitions, it must compile runtime tests. If a compiler does not know inter-element data dependencies, it must resort to pessimistic strategies such as compiling elements as thunks, or making unnecessary copies when updating an array.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {137--149},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93548.93561},
 doi = {http://doi.acm.org/10.1145/93548.93561},
 acmid = {93561},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1990:CHA:93542.93561,
 author = {Anderson, Steven and Hudak, Paul},
 title = {Compilation of Haskell array comprehensions for scientific computing},
 abstract = {Monolithic approaches to functional language arrays, such as Haskell array comprehensions, define elements all at once, at the time the array is created, instead of incrementally. Although monolithic arrays are elegant, a naive implementation can be very inefficient. For example, if a compiler does not know whether an element has zero or many definitions, it must compile runtime tests. If a compiler does not know inter-element data dependencies, it must resort to pessimistic strategies such as compiling elements as thunks, or making unnecessary copies when updating an array.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {137--149},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93542.93561},
 doi = {http://doi.acm.org/10.1145/93542.93561},
 acmid = {93561},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chambers:1990:ITA:93542.93562,
 author = {Chambers, Craig and Ungar, David},
 title = {Interactive type analysis and extended message splitting; optimizing dynamically-typed object-oriented programs},
 abstract = {Object-oriented languages have suffered from poor performance caused by frequent and slow dynamically-bound procedure calls. The best way to speed up a procedure call is to compile it out, but dynamic binding of object-oriented procedure calls without static receiver type information precludes inlining. Iterative type analysis and extended message splitting are new compilation techniques that extract much of the necessary type information and make it possible to hoist run-time type tests out of loops.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {150--164},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93562},
 doi = {http://doi.acm.org/10.1145/93542.93562},
 acmid = {93562},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chambers:1990:ITA:93548.93562,
 author = {Chambers, Craig and Ungar, David},
 title = {Interactive type analysis and extended message splitting; optimizing dynamically-typed object-oriented programs},
 abstract = {Object-oriented languages have suffered from poor performance caused by frequent and slow dynamically-bound procedure calls. The best way to speed up a procedure call is to compile it out, but dynamic binding of object-oriented procedure calls without static receiver type information precludes inlining. Iterative type analysis and extended message splitting are new compilation techniques that extract much of the necessary type information and make it possible to hoist run-time type tests out of loops.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {150--164},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93562},
 doi = {http://doi.acm.org/10.1145/93548.93562},
 acmid = {93562},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jacobs:1990:TDS:93548.93563,
 author = {Jacobs, Dean},
 title = {Type declarations as subtype constraints in logic programming},
 abstract = {This paper presents a type system for logic programs that supports parametric polymorphism and subtypes. This system follows most knowledge representation and object-oriented schemes in that subtyping is name-based, i.e., \&tgr;<subscrpt>1</subscrpt> is considered to be a subtype of \&tgr;<subscrpt>2</subscrpt> iff it is declared as such. We take this as a fundamental principle in the sense that type declarations have the form of subtype constraints. Types are assigned meaning by viewing such constraints as Horn clauses that, together with a few basic axioms, define a subtype predicate. This technique provides a (least) model for types and, at the same time, a sound and complete proof system for deriving subtypes. Using this proof system, we define well-typedness conditions which ensure that a logic program/query respects a set of predicate types. We prove that these conditions are consistent in the sense that every atom of every resolvent produced during the execution of a well-typed program is consistent with its type.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {165--173},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/93548.93563},
 doi = {http://doi.acm.org/10.1145/93548.93563},
 acmid = {93563},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jacobs:1990:TDS:93542.93563,
 author = {Jacobs, Dean},
 title = {Type declarations as subtype constraints in logic programming},
 abstract = {This paper presents a type system for logic programs that supports parametric polymorphism and subtypes. This system follows most knowledge representation and object-oriented schemes in that subtyping is name-based, i.e., \&tgr;<subscrpt>1</subscrpt> is considered to be a subtype of \&tgr;<subscrpt>2</subscrpt> iff it is declared as such. We take this as a fundamental principle in the sense that type declarations have the form of subtype constraints. Types are assigned meaning by viewing such constraints as Horn clauses that, together with a few basic axioms, define a subtype predicate. This technique provides a (least) model for types and, at the same time, a sound and complete proof system for deriving subtypes. Using this proof system, we define well-typedness conditions which ensure that a logic program/query respects a set of predicate types. We prove that these conditions are consistent in the sense that every atom of every resolvent produced during the execution of a well-typed program is consistent with its type.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {165--173},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/93542.93563},
 doi = {http://doi.acm.org/10.1145/93542.93563},
 acmid = {93563},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Debray:1990:TGA:93542.93564,
 author = {Debray, Saumya K. and Lin, Nai-Wei and Hermnegildo, Manuel},
 title = {Task granularity analysis in logic programs},
 abstract = {While logic programming languages offer a great deal of scope for parallelism, there is usually some overhead associated with the execution of goals in parallel because of the work involved in task creation and scheduling. In practice, therefore, the ``granularity" of a goal, i.e. an estimate of the work available under it, should be taken into account when deciding whether or not to execute a goal concurrently as a separate task. This paper describes a method for estimating the granularity of a goal at compile time. The runtime overhead associated with our approach is usually quite small, and the performance improvements resulting from the incorporation of grainsize control can be quite good. This is shown by means of experimental results.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {174--188},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93564},
 doi = {http://doi.acm.org/10.1145/93542.93564},
 acmid = {93564},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Debray:1990:TGA:93548.93564,
 author = {Debray, Saumya K. and Lin, Nai-Wei and Hermnegildo, Manuel},
 title = {Task granularity analysis in logic programs},
 abstract = {While logic programming languages offer a great deal of scope for parallelism, there is usually some overhead associated with the execution of goals in parallel because of the work involved in task creation and scheduling. In practice, therefore, the ``granularity" of a goal, i.e. an estimate of the work available under it, should be taken into account when deciding whether or not to execute a goal concurrently as a separate task. This paper describes a method for estimating the granularity of a goal at compile time. The runtime overhead associated with our approach is usually quite small, and the performance improvements resulting from the incorporation of grainsize control can be quite good. This is shown by means of experimental results.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {174--188},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93564},
 doi = {http://doi.acm.org/10.1145/93548.93564},
 acmid = {93564},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitsolides:1990:GRC:93542.93565,
 author = {Mitsolides, Thanasis and Harrison, Malcolm},
 title = {Generators and the replicator control structure in the parallel environment of ALLOY},
 abstract = {The need for searching a space of solutions appears often. Many problems, such as iteration over a dynamically created domain, can be expressed most naturally using a generate-and-process style. Serial programming languages typically support solutions of these problems by providing some form of generators or backtracking.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {189--196},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/93542.93565},
 doi = {http://doi.acm.org/10.1145/93542.93565},
 acmid = {93565},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitsolides:1990:GRC:93548.93565,
 author = {Mitsolides, Thanasis and Harrison, Malcolm},
 title = {Generators and the replicator control structure in the parallel environment of ALLOY},
 abstract = {The need for searching a space of solutions appears often. Many problems, such as iteration over a dynamically created domain, can be expressed most naturally using a generate-and-process style. Serial programming languages typically support solutions of these problems by providing some form of generators or backtracking.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {189--196},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/93548.93565},
 doi = {http://doi.acm.org/10.1145/93548.93565},
 acmid = {93565},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Teitelbaum:1990:HAG:93542.93567,
 author = {Teitelbaum, Tim and Chapman, Richard},
 title = {Higher-order attribute grammars and editing environments},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {197--208},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93542.93567},
 doi = {http://doi.acm.org/10.1145/93542.93567},
 acmid = {93567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Teitelbaum:1990:HAG:93548.93567,
 author = {Teitelbaum, Tim and Chapman, Richard},
 title = {Higher-order attribute grammars and editing environments},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {197--208},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93548.93567},
 doi = {http://doi.acm.org/10.1145/93548.93567},
 acmid = {93567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jourdan:1990:DIE:93542.93568,
 author = {Jourdan, Martin and Parigot, Didier and Juli\'{e}, Catherine and Durin, Olivier and Bellec, Carole Le},
 title = {Design, implementation and evaluation of the FNC-2 attribute grammar system},
 abstract = {FNC-2 is a new attribute grammar processing system aiming at expressive power, efficiency, ease of use and versatility. Its development at INRIA started in 1986, and a first running prototype is available since early 1989. Its most important features are: efficient exhaustive and incremental visit-sequence-based evaluation of strongly (absolutely) non-circular AGs; extensive space optimizations; a specially-designed AG-description language, with provisions for true modularity; portability and versatility of the generated evaluators; complete environment for application development. This paper briefly describes the design and implementation of FNC-2 and its peripherals. Then preliminary experience with the system is reported.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {209--222},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/93542.93568},
 doi = {http://doi.acm.org/10.1145/93542.93568},
 acmid = {93568},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jourdan:1990:DIE:93548.93568,
 author = {Jourdan, Martin and Parigot, Didier and Juli\'{e}, Catherine and Durin, Olivier and Bellec, Carole Le},
 title = {Design, implementation and evaluation of the FNC-2 attribute grammar system},
 abstract = {FNC-2 is a new attribute grammar processing system aiming at expressive power, efficiency, ease of use and versatility. Its development at INRIA started in 1986, and a first running prototype is available since early 1989. Its most important features are: efficient exhaustive and incremental visit-sequence-based evaluation of strongly (absolutely) non-circular AGs; extensive space optimizations; a specially-designed AG-description language, with provisions for true modularity; portability and versatility of the generated evaluators; complete environment for application development. This paper briefly describes the design and implementation of FNC-2 and its peripherals. Then preliminary experience with the system is reported.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {209--222},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/93548.93568},
 doi = {http://doi.acm.org/10.1145/93548.93568},
 acmid = {93568},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Henry:1990:UWI:93548.93571,
 author = {Henry, Robert R. and Whaley, Kenneth M. and Forstall, Bruce},
 title = {The University of Washington illustrating compiler},
 abstract = {The University of Washington illustrating compiler (UWPI) automatically illustrates the data structures used in simple programs written in a subset of Pascal<supscrpt>2</supscrpt>. A UWPI user submits a program to UWPI, and can then watch a graphical display show time varying illustrations of the data structures and program source code. UWPI uses the information latent in the program to determine how to illustrate the program. UWPI infers the abstract data types directly from the declarations and operations used in the source program, and then lays out the illustration in a natural way by instantiating well-known layouts for the abstracts types. UWPI solves program illustration using compile-time pattern matching and type inferencing to link anticipated execution events to display events, rather than relying on user assistance or specialized programming techniques. UWPI has been used to automatically illustrate didactic sorting and searching examples, and can be used to help teach basic data structures, or to help when debugging programs.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {223--233},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93548.93571},
 doi = {http://doi.acm.org/10.1145/93548.93571},
 acmid = {93571},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Henry:1990:UWI:93542.93571,
 author = {Henry, Robert R. and Whaley, Kenneth M. and Forstall, Bruce},
 title = {The University of Washington illustrating compiler},
 abstract = {The University of Washington illustrating compiler (UWPI) automatically illustrates the data structures used in simple programs written in a subset of Pascal<supscrpt>2</supscrpt>. A UWPI user submits a program to UWPI, and can then watch a graphical display show time varying illustrations of the data structures and program source code. UWPI uses the information latent in the program to determine how to illustrate the program. UWPI infers the abstract data types directly from the declarations and operations used in the source program, and then lays out the illustration in a natural way by instantiating well-known layouts for the abstracts types. UWPI solves program illustration using compile-time pattern matching and type inferencing to link anticipated execution events to display events, rather than relying on user assistance or specialized programming techniques. UWPI has been used to automatically illustrate didactic sorting and searching examples, and can be used to help teach basic data structures, or to help when debugging programs.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {223--233},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93542.93571},
 doi = {http://doi.acm.org/10.1145/93542.93571},
 acmid = {93571},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Horwitz:1990:IST:93542.93574,
 author = {Horwitz, Susan},
 title = {Identifying the semantic and textual differences between two versions of a program},
 abstract = {Text-based file comparators (e.g., the Unix utility diff), are very general tools that can be applied to arbitrary files. However, using such tools to compare programs can be unsatisfactory because their only notion of change is based on program text rather than program behavior. This paper describes a technique for comparing two versions of a program, determining which program components represents changes, and classifying each changed component as representing either a semantic or a textual change.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {234--245},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93542.93574},
 doi = {http://doi.acm.org/10.1145/93542.93574},
 acmid = {93574},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Horwitz:1990:IST:93548.93574,
 author = {Horwitz, Susan},
 title = {Identifying the semantic and textual differences between two versions of a program},
 abstract = {Text-based file comparators (e.g., the Unix utility diff), are very general tools that can be applied to arbitrary files. However, using such tools to compare programs can be unsatisfactory because their only notion of change is based on program text rather than program behavior. This paper describes a technique for comparing two versions of a program, determining which program components represents changes, and classifying each changed component as representing either a semantic or a textual change.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {234--245},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/93548.93574},
 doi = {http://doi.acm.org/10.1145/93548.93574},
 acmid = {93574},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1990:DPS:93542.93576,
 author = {Agrawal, Hiralal and Horgan, Joseph R.},
 title = {Dynamic program slicing},
 abstract = {Program slices are useful in debugging, testing, maintenance, and understanding of programs. The conventional notion of a program slice, the static slice, is the set of all statements that might affect the value of a given variable occurrence. In this paper, we investigate the concept of the dynamic slice consisting of all statements that actually affect the value of a variable occurrence for a given program input. The sensitivity of dynamic slicing to particular program inputs makes it more useful in program debugging and testing than static slicing. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. The Dynamic Dependence Graph may be unbounded in length; therefore, we introduce the economical concept of a Reduced Dynamic Dependence Graph, which is proportional in size to the number of dynamic slices arising during the program execution.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {246--256},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93542.93576},
 doi = {http://doi.acm.org/10.1145/93542.93576},
 acmid = {93576},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1990:DPS:93548.93576,
 author = {Agrawal, Hiralal and Horgan, Joseph R.},
 title = {Dynamic program slicing},
 abstract = {Program slices are useful in debugging, testing, maintenance, and understanding of programs. The conventional notion of a program slice, the static slice, is the set of all statements that might affect the value of a given variable occurrence. In this paper, we investigate the concept of the dynamic slice consisting of all statements that actually affect the value of a variable occurrence for a given program input. The sensitivity of dynamic slicing to particular program inputs makes it more useful in program debugging and testing than static slicing. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. The Dynamic Dependence Graph may be unbounded in length; therefore, we introduce the economical concept of a Reduced Dynamic Dependence Graph, which is proportional in size to the number of dynamic slices arising during the program execution.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {246--256},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93548.93576},
 doi = {http://doi.acm.org/10.1145/93548.93576},
 acmid = {93576},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ottenstein:1990:PDW:93542.93578,
 author = {Ottenstein, Karl J. and Ballance, Robert A. and MacCabe, Arthur B.},
 title = {The program dependence web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages},
 abstract = {The Program Dependence Web (PDW) is a program representation that can be directly interpreted using control-, data-, or demand-driven models of execution. A PDW combines a single-assignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {257--271},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93578},
 doi = {http://doi.acm.org/10.1145/93542.93578},
 acmid = {93578},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ottenstein:1990:PDW:93548.93578,
 author = {Ottenstein, Karl J. and Ballance, Robert A. and MacCabe, Arthur B.},
 title = {The program dependence web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages},
 abstract = {The Program Dependence Web (PDW) is a program representation that can be directly interpreted using control-, data-, or demand-driven models of execution. A PDW combines a single-assignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {257--271},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93578},
 doi = {http://doi.acm.org/10.1145/93548.93578},
 acmid = {93578},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1990:FLO:93548.93581,
 author = {Gupta, Rajiv},
 title = {A fresh look at optimizing array bound checking},
 abstract = {This paper describes techniques for optimizing range checks performed to detect array bound violations. In addition to the elimination of range checks, the optimizations discussed in this paper also reduce the overhead due to range checks that cannot be eliminated by compile-time analysis. The optimizations reduce the program execution time and the object code size through elimination of redundant checks, propagation of checks out of loops, and combination of multiple checks into a single check. A minimal control flow graph (MCFG) is constructed using which the minimal amount of data flow information required for range check optimizations is computed. The range check optimizations are performed using the MCFG rather the CFG for the entire program. This allows the global range check optimizations to be performed efficiently since the MCFG is significantly smaller than the CFG. Any array bound violation that is detected by a program with all range checks included, will also be detected by the program after range check optimization and vice versa. Even though the above optimizations may appear to be similar to traditional code optimizations, similar reduction in the number of range checks executed can not be achieved by a traditional code optimizer. Experimental results indicate that the number of range checks performed in executing a program is greatly reduced using the above techniques.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93548.93581},
 doi = {http://doi.acm.org/10.1145/93548.93581},
 acmid = {93581},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1990:FLO:93542.93581,
 author = {Gupta, Rajiv},
 title = {A fresh look at optimizing array bound checking},
 abstract = {This paper describes techniques for optimizing range checks performed to detect array bound violations. In addition to the elimination of range checks, the optimizations discussed in this paper also reduce the overhead due to range checks that cannot be eliminated by compile-time analysis. The optimizations reduce the program execution time and the object code size through elimination of redundant checks, propagation of checks out of loops, and combination of multiple checks into a single check. A minimal control flow graph (MCFG) is constructed using which the minimal amount of data flow information required for range check optimizations is computed. The range check optimizations are performed using the MCFG rather the CFG for the entire program. This allows the global range check optimizations to be performed efficiently since the MCFG is significantly smaller than the CFG. Any array bound violation that is detected by a program with all range checks included, will also be detected by the program after range check optimization and vice versa. Even though the above optimizations may appear to be similar to traditional code optimizations, similar reduction in the number of range checks executed can not be achieved by a traditional code optimizer. Experimental results indicate that the number of range checks performed in executing a program is greatly reduced using the above techniques.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93542.93581},
 doi = {http://doi.acm.org/10.1145/93542.93581},
 acmid = {93581},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ammarguellat:1990:ARI:93548.93583,
 author = {Ammarguellat, Zahira and Harrison,III, W. L.},
 title = {Automatic recognition of induction variables and recurrence relations by abstract interpretation},
 abstract = {The recognition of recurrence relations is important in several ways to the compilation of programs. Induction variables, the simplest form of recurrence, are pivotal in loop optimizations and dependence testing. Many recurrence relations, although expressed sequentially by the programmer, lend themselves to efficient vector or parallel computation. Despite the importance of recurrences, vectorizing and parallelizing compilers to date have recognized them only in an ad-hoc fashion. In this paper we put forth a systematic method for recognizing recurrence relations automatically. Our method has two parts. First, abstract interpretation [CC77, CC79] is used to construct a map that associates each variable assigned in a loop with a symbolic form (expression) of its value. Second, the elements of this map are matched with patterns that describe recurrence relations. The scheme is easily extensible by the addition of templates, and is able to recognize nested recurrences by the propagation of the closed forms of recurrences from inner loops. We present some applications of this method and a proof of its correctness.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {283--295},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93548.93583},
 doi = {http://doi.acm.org/10.1145/93548.93583},
 acmid = {93583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ammarguellat:1990:ARI:93542.93583,
 author = {Ammarguellat, Zahira and Harrison,III, W. L.},
 title = {Automatic recognition of induction variables and recurrence relations by abstract interpretation},
 abstract = {The recognition of recurrence relations is important in several ways to the compilation of programs. Induction variables, the simplest form of recurrence, are pivotal in loop optimizations and dependence testing. Many recurrence relations, although expressed sequentially by the programmer, lend themselves to efficient vector or parallel computation. Despite the importance of recurrences, vectorizing and parallelizing compilers to date have recognized them only in an ad-hoc fashion. In this paper we put forth a systematic method for recognizing recurrence relations automatically. Our method has two parts. First, abstract interpretation [CC77, CC79] is used to construct a map that associates each variable assigned in a loop with a symbolic form (expression) of its value. Second, the elements of this map are matched with patterns that describe recurrence relations. The scheme is easily extensible by the addition of templates, and is able to recognize nested recurrences by the propagation of the closed forms of recurrences from inner loops. We present some applications of this method and a proof of its correctness.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {283--295},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/93542.93583},
 doi = {http://doi.acm.org/10.1145/93542.93583},
 acmid = {93583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chase:1990:APS:93542.93585,
 author = {Chase, David R. and Wegman, Mark and Zadeck, F. Kenneth},
 title = {Analysis of pointers and structures},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {296--310},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93585},
 doi = {http://doi.acm.org/10.1145/93542.93585},
 acmid = {93585},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chase:1990:APS:93548.93585,
 author = {Chase, David R. and Wegman, Mark and Zadeck, F. Kenneth},
 title = {Analysis of pointers and structures},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {296--310},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93585},
 doi = {http://doi.acm.org/10.1145/93548.93585},
 acmid = {93585},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tseng:1990:CPL:93542.93587,
 author = {Tseng, Ping-Sheng},
 title = {Compiling programs for a linear systolic array},
 abstract = {This paper describes an AL compiler for the Warp systolic array. AL is a programming language in which the user programs a systolic array as if it were a sequential computer and relies on the compiler to generate parallel code. This paper introduces the notion of data relations in compiling programs for systolic arrays. Unlike dependence relations among statements of a program, data relations define compatibility relations among data objects of a program. The AL compiler uses data relations to compute data compatibility classes, determine data distribution, and distribute loop iterations. The AL compiler can generate efficient parallel code almost identical to what the user would have written by hand. For example, the AL compiler generates parallel code for the LINPACK LU decomposition (SGEFA) and QR decomposition (SQRDC) routines with a nearly 8-fold speedup on the 10-cell Warp array for matrices of size 180 \&times; 180.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {311--321},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93542.93587},
 doi = {http://doi.acm.org/10.1145/93542.93587},
 acmid = {93587},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tseng:1990:CPL:93548.93587,
 author = {Tseng, Ping-Sheng},
 title = {Compiling programs for a linear systolic array},
 abstract = {This paper describes an AL compiler for the Warp systolic array. AL is a programming language in which the user programs a systolic array as if it were a sequential computer and relies on the compiler to generate parallel code. This paper introduces the notion of data relations in compiling programs for systolic arrays. Unlike dependence relations among statements of a program, data relations define compatibility relations among data objects of a program. The AL compiler uses data relations to compute data compatibility classes, determine data distribution, and distribute loop iterations. The AL compiler can generate efficient parallel code almost identical to what the user would have written by hand. For example, the AL compiler generates parallel code for the LINPACK LU decomposition (SGEFA) and QR decomposition (SQRDC) routines with a nearly 8-fold speedup on the 10-cell Warp array for matrices of size 180 \&times; 180.
},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {311--321},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/93548.93587},
 doi = {http://doi.acm.org/10.1145/93548.93587},
 acmid = {93587},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sarkar:1990:IRF:93542.93590,
 author = {Sarkar, V.},
 title = {Instruction reordering for fork-join parallelism},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {322--336},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93590},
 doi = {http://doi.acm.org/10.1145/93542.93590},
 acmid = {93590},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sarkar:1990:IRF:93548.93590,
 author = {Sarkar, V.},
 title = {Instruction reordering for fork-join parallelism},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {322--336},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93590},
 doi = {http://doi.acm.org/10.1145/93548.93590},
 acmid = {93590},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cytron:1990:CRC:93542.93592,
 author = {Cytron, Ron and Ferrante, Jeanne and Sarkar, V.},
 title = {Compact representations for control dependence},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
 series = {PLDI '90},
 year = {1990},
 isbn = {0-89791-364-7},
 location = {White Plains, New York, United States},
 pages = {337--351},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93542.93592},
 doi = {http://doi.acm.org/10.1145/93542.93592},
 acmid = {93592},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cytron:1990:CRC:93548.93592,
 author = {Cytron, Ron and Ferrante, Jeanne and Sarkar, V.},
 title = {Compact representations for control dependence},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {25},
 issue = {6},
 month = {June},
 year = {1990},
 issn = {0362-1340},
 pages = {337--351},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/93548.93592},
 doi = {http://doi.acm.org/10.1145/93548.93592},
 acmid = {93592},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Venkatesh:1989:FCE:73141.74819,
 author = {Venkatesh, G. A.},
 title = {A framework for construction and evaluation of high-level specifications for program analysis techniques},
 abstract = {Abstract interpretation introduced the notion of formal specification of program analyses. Denotational frameworks are convenient for reasoning about such specifications. However, implementation considerations make denotational specifications complex and hard to develop. We present a framework that facilitates the construction and understanding of denotational specifications for program analysis techniques. The framework is exemplified by specifications for program analysis techniques from the literature and from our own research. This approach allows program analysis techniques to be incorporated into automatically generated program synthesizers by including their specifications with the language definition.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/73141.74819},
 doi = {http://doi.acm.org/10.1145/73141.74819},
 acmid = {74819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Venkatesh:1989:FCE:74818.74819,
 author = {Venkatesh, G. A.},
 title = {A framework for construction and evaluation of high-level specifications for program analysis techniques},
 abstract = {Abstract interpretation introduced the notion of formal specification of program analyses. Denotational frameworks are convenient for reasoning about such specifications. However, implementation considerations make denotational specifications complex and hard to develop. We present a framework that facilitates the construction and understanding of denotational specifications for program analysis techniques. The framework is exemplified by specifications for program analysis techniques from the literature and from our own research. This approach allows program analysis techniques to be incorporated into automatically generated program synthesizers by including their specifications with the language definition.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/74818.74819},
 doi = {http://doi.acm.org/10.1145/74818.74819},
 acmid = {74819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cartwright:1989:SPD:74818.74820,
 author = {Cartwright, Robert and Felleisen, Mattias},
 title = {The semantics of program dependence},
 abstract = {Optimizing and parallelizing compilers for procedural languages rely on various forms of program dependence graphs (pdgs) to express the essential control and data dependencies among atomic program operations. In this paper, we provide a semantic justification for this practice by deriving two different forms of program dependence graph \&mdash; the output pdg and the def-order pdg\&mdash;and their semantic definitions from non-strict generalizations of the denotational semantics of the programming language. In the process, we demonstrate that both the output pdg and the def-order pdg (with minor technical modifications) are conventional data-flow programs. In addition, we show that the semantics of the def-order pdg dominates the semantics of the output pdg and that both of these semantics dominate\&mdash;rather than preserve\&mdash;the semantics of sequential execution.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {13--27},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74820},
 doi = {http://doi.acm.org/10.1145/74818.74820},
 acmid = {74820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cartwright:1989:SPD:73141.74820,
 author = {Cartwright, Robert and Felleisen, Mattias},
 title = {The semantics of program dependence},
 abstract = {Optimizing and parallelizing compilers for procedural languages rely on various forms of program dependence graphs (pdgs) to express the essential control and data dependencies among atomic program operations. In this paper, we provide a semantic justification for this practice by deriving two different forms of program dependence graph \&mdash; the output pdg and the def-order pdg\&mdash;and their semantic definitions from non-strict generalizations of the denotational semantics of the programming language. In the process, we demonstrate that both the output pdg and the def-order pdg (with minor technical modifications) are conventional data-flow programs. In addition, we show that the semantics of the def-order pdg dominates the semantics of the output pdg and that both of these semantics dominate\&mdash;rather than preserve\&mdash;the semantics of sequential execution.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {13--27},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74820},
 doi = {http://doi.acm.org/10.1145/73141.74820},
 acmid = {74820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Horwitz:1989:DAP:73141.74821,
 author = {Horwitz, S. and Pfeiffer, P. and Reps, T.},
 title = {Dependence analysis for pointer variables},
 abstract = {Our concern is how to determine data dependencies between program constructs in programming languages with pointer variables. We are particularly interested in computing data dependencies for languages that manipulate heap-allocated storage, such as Lisp and Pascal. We have defined a family of algorithms that compute safe approximations to the flow, output, and anti-dependencies of a program written in such a language. Our algorithms account for destructive updates to fields of a structure and thus are not limited to the cases where all structures are trees or acyclic graphs; they are applicable to programs that build cyclic structures.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {28--40},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/73141.74821},
 doi = {http://doi.acm.org/10.1145/73141.74821},
 acmid = {74821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Horwitz:1989:DAP:74818.74821,
 author = {Horwitz, S. and Pfeiffer, P. and Reps, T.},
 title = {Dependence analysis for pointer variables},
 abstract = {Our concern is how to determine data dependencies between program constructs in programming languages with pointer variables. We are particularly interested in computing data dependencies for languages that manipulate heap-allocated storage, such as Lisp and Pascal. We have defined a family of algorithms that compute safe approximations to the flow, output, and anti-dependencies of a program written in such a language. Our algorithms account for destructive updates to fields of a structure and thus are not limited to the cases where all structures are trees or acyclic graphs; they are applicable to programs that build cyclic structures.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {28--40},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74818.74821},
 doi = {http://doi.acm.org/10.1145/74818.74821},
 acmid = {74821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balasundaram:1989:TSD:73141.74822,
 author = {Balasundaram, V. and Kennedy, K.},
 title = {A technique for summarizing data access and its use in parallelism enhancing transformations},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {41--53},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/73141.74822},
 doi = {http://doi.acm.org/10.1145/73141.74822},
 acmid = {74822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Balasundaram:1989:TSD:74818.74822,
 author = {Balasundaram, V. and Kennedy, K.},
 title = {A technique for summarizing data access and its use in parallelism enhancing transformations},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {41--53},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74818.74822},
 doi = {http://doi.acm.org/10.1145/74818.74822},
 acmid = {74822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cytron:1989:AGD:73141.74823,
 author = {Cytron, R. and Hind, M. and Hsieh, W.},
 title = {Automatic generation of DAG parallelism},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {54--68},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74823},
 doi = {http://doi.acm.org/10.1145/73141.74823},
 acmid = {74823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cytron:1989:AGD:74818.74823,
 author = {Cytron, R. and Hind, M. and Hsieh, W.},
 title = {Automatic generation of DAG parallelism},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {54--68},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74823},
 doi = {http://doi.acm.org/10.1145/74818.74823},
 acmid = {74823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rogers:1989:PDT:73141.74824,
 author = {Rogers, A. and Pingali, K.},
 title = {Process decomposition through locality of reference},
 abstract = {In the context of sequential computers, it is common practice to exploit temporal locality of reference through devices such as caches and virtual memory. In the context of multiprocessors, we believe that it is equally important to exploit spatial locality of reference. We are developing a system which, given a sequential program and its domain decomposition, performs process decomposition so as to enhance spatial locality of reference. We describe an application of this method - generating code from shared-memory programs for the (distributed memory) Intel iPSC/2.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {69--80},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/73141.74824},
 doi = {http://doi.acm.org/10.1145/73141.74824},
 acmid = {74824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rogers:1989:PDT:74818.74824,
 author = {Rogers, A. and Pingali, K.},
 title = {Process decomposition through locality of reference},
 abstract = {In the context of sequential computers, it is common practice to exploit temporal locality of reference through devices such as caches and virtual memory. In the context of multiprocessors, we believe that it is equally important to exploit spatial locality of reference. We are developing a system which, given a sequential program and its domain decomposition, performs process decomposition so as to enhance spatial locality of reference. We describe an application of this method - generating code from shared-memory programs for the (distributed memory) Intel iPSC/2.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {69--80},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/74818.74824},
 doi = {http://doi.acm.org/10.1145/74818.74824},
 acmid = {74824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kranz:1989:MHP:73141.74825,
 author = {Kranz, D. A. and Halstead,Jr., R. H. and Mohr, E.},
 title = {Mul-T: a high-performance parallel Lisp},
 abstract = {Mul-T is a parallel Lisp system, based on Multilisp's future construct, that has been developed to run on an Encore Multimax multiprocessor. Mul-T is an extended version of the Yale T system and uses the T system's ORBIT compiler to achieve ``production quality" performance on stock hardware \&mdash; about 100 times faster than Multilisp. Mul-T shows that futures can be implemented cheaply enough to be useful in a production-quality system. Mul-T is fully operational, including a user interface that supports managing groups of parallel tasks.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {81--90},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/73141.74825},
 doi = {http://doi.acm.org/10.1145/73141.74825},
 acmid = {74825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kranz:1989:MHP:74818.74825,
 author = {Kranz, D. A. and Halstead,Jr., R. H. and Mohr, E.},
 title = {Mul-T: a high-performance parallel Lisp},
 abstract = {Mul-T is a parallel Lisp system, based on Multilisp's future construct, that has been developed to run on an Encore Multimax multiprocessor. Mul-T is an extended version of the Yale T system and uses the T system's ORBIT compiler to achieve ``production quality" performance on stock hardware \&mdash; about 100 times faster than Multilisp. Mul-T shows that futures can be implemented cheaply enough to be useful in a production-quality system. Mul-T is fully operational, including a user interface that supports managing groups of parallel tasks.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {81--90},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/74818.74825},
 doi = {http://doi.acm.org/10.1145/74818.74825},
 acmid = {74825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gross:1989:PCP:74818.74826,
 author = {Gross, T. and Sobel, A. and Zolg, M.},
 title = {Parallel compilation for a parallel machine},
 abstract = {An application for a parallel computer with multiple, independent processors often includes different programs (functions) for the individual processors; compilation of such functions can proceed independently. We implemented a compiler that exploits this parallelism by partitioning the input program for parallel translation. The host system for the parallel compiler is an Ethernet-based network of workstations, and different functions of the application program are compiled in parallel on different workstations. For typical programs in our environment, we observe a speedup ranging from 3 to 6 using not more than 9 processors. The paper includes detailed measurements for this parallel compiler; we report the system overhead, implementation overhead, as well as the speedup obtained when compared with sequential compilation.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/74818.74826},
 doi = {http://doi.acm.org/10.1145/74818.74826},
 acmid = {74826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gross:1989:PCP:73141.74826,
 author = {Gross, T. and Sobel, A. and Zolg, M.},
 title = {Parallel compilation for a parallel machine},
 abstract = {An application for a parallel computer with multiple, independent processors often includes different programs (functions) for the individual processors; compilation of such functions can proceed independently. We implemented a compiler that exploits this parallelism by partitioning the input program for parallel translation. The host system for the parallel compiler is an Ethernet-based network of workstations, and different functions of the application program are compiled in parallel on different workstations. For typical programs in our environment, we observe a speedup ranging from 3 to 6 using not more than 9 processors. The paper includes detailed measurements for this parallel compiler; we report the system overhead, implementation overhead, as well as the speedup obtained when compared with sequential compilation.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/73141.74826},
 doi = {http://doi.acm.org/10.1145/73141.74826},
 acmid = {74826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Horwat:1989:ECP:74818.74827,
 author = {Horwat, W. and Chien, A. A. and Dally, W. J.},
 title = {Experience with CST: programming and implementation},
 abstract = {CST is a programming language based on Smalltalk-80<supscrpt>2</supscrpt> that supports concurrency using locks, asynchronous messages, and distributed objects. In this paper, we describe CST: the language and its implementation. Example programs and initial programming experience with CST are described. Our implementation of CST generates native code for the J-machine, a fine-grained concurrent computer. Some compiler optimizations developed in conjunction with that implementation are also described.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {101--109},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/74818.74827},
 doi = {http://doi.acm.org/10.1145/74818.74827},
 acmid = {74827},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Horwat:1989:ECP:73141.74827,
 author = {Horwat, W. and Chien, A. A. and Dally, W. J.},
 title = {Experience with CST: programming and implementation},
 abstract = {CST is a programming language based on Smalltalk-80<supscrpt>2</supscrpt> that supports concurrency using locks, asynchronous messages, and distributed objects. In this paper, we describe CST: the language and its implementation. Example programs and initial programming experience with CST are described. Our implementation of CST generates native code for the J-machine, a fine-grained concurrent computer. Some compiler optimizations developed in conjunction with that implementation are also described.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {101--109},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/73141.74827},
 doi = {http://doi.acm.org/10.1145/73141.74827},
 acmid = {74827},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Koopman:1989:FLC:73141.74828,
 author = {Koopman,Jr., P. J. and Lee, P.},
 title = {A fresh look at combinator graph reduction},
 abstract = {We present a new abstract machine for graph reduction called TIGRE. Benchmark results show that TIGRE's execution speed compares quite favorably with previous combinator-graph reduction techniques on similar hardware. Furthermore, the mapping of TIGRE onto conventional hardware is simple and efficient. Mainframe implementations of TIGRE provide performance levels exceeding those previously available on custom graph reduction hardware.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {110--119},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/73141.74828},
 doi = {http://doi.acm.org/10.1145/73141.74828},
 acmid = {74828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Koopman:1989:FLC:74818.74828,
 author = {Koopman,Jr., P. J. and Lee, P.},
 title = {A fresh look at combinator graph reduction},
 abstract = {We present a new abstract machine for graph reduction called TIGRE. Benchmark results show that TIGRE's execution speed compares quite favorably with previous combinator-graph reduction techniques on similar hardware. Furthermore, the mapping of TIGRE onto conventional hardware is simple and efficient. Mainframe implementations of TIGRE provide performance levels exceeding those previously available on custom graph reduction hardware.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {110--119},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/74818.74828},
 doi = {http://doi.acm.org/10.1145/74818.74828},
 acmid = {74828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Farrow:1989:VCB:73141.74829,
 author = {Farrow, R. and Stanculescu, A. G.},
 title = {A VHDL compiler based on attribute grammar methodology},
 abstract = {This paper presents aspects of a compiler for a new hardware description language (VHDL) written using attribute grammar techniques. VHDL is introduced, along with the new compiler challenges brought by a language that extends an Ada subset for the purpose of describing hardware. Attribute grammar programming solutions are presented for some of the language challenges.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {120--130},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/73141.74829},
 doi = {http://doi.acm.org/10.1145/73141.74829},
 acmid = {74829},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Farrow:1989:VCB:74818.74829,
 author = {Farrow, R. and Stanculescu, A. G.},
 title = {A VHDL compiler based on attribute grammar methodology},
 abstract = {This paper presents aspects of a compiler for a new hardware description language (VHDL) written using attribute grammar techniques. VHDL is introduced, along with the new compiler challenges brought by a language that extends an Ada subset for the purpose of describing hardware. Attribute grammar programming solutions are presented for some of the language challenges.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {120--130},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/74818.74829},
 doi = {http://doi.acm.org/10.1145/74818.74829},
 acmid = {74829},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vogt:1989:HOA:74818.74830,
 author = {Vogt, H. H. and Swierstra, S. D. and Kuiper, M. F.},
 title = {Higher order attribute grammars},
 abstract = {A new kind of attribute grammars, called higher order attribute grammars, is defined. In higher order attribute grammars the structure tree can be expanded as a result of attribute computation. A structure tree may be stored in an attribute. The term higher order is used because of the analogy with higher order functions, where a function can be the result or parameter of another function. A relatively simple method, using OAGs, is described to derive an evaluation order on the defining attribute occurrences which comprises all possible direct and indirect attribute dependencies. As in OAGs, visit-sequences are computed from which an efficient algorithm for attribute evaluation can be derived.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {131--145},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74830},
 doi = {http://doi.acm.org/10.1145/74818.74830},
 acmid = {74830},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vogt:1989:HOA:73141.74830,
 author = {Vogt, H. H. and Swierstra, S. D. and Kuiper, M. F.},
 title = {Higher order attribute grammars},
 abstract = {A new kind of attribute grammars, called higher order attribute grammars, is defined. In higher order attribute grammars the structure tree can be expanded as a result of attribute computation. A structure tree may be stored in an attribute. The term higher order is used because of the analogy with higher order functions, where a function can be the result or parameter of another function. A relatively simple method, using OAGs, is described to derive an evaluation order on the defining attribute occurrences which comprises all possible direct and indirect attribute dependencies. As in OAGs, visit-sequences are computed from which an efficient algorithm for attribute evaluation can be derived.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {131--145},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74830},
 doi = {http://doi.acm.org/10.1145/73141.74830},
 acmid = {74830},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chambers:1989:COC:74818.74831,
 author = {Chambers, C. and Ungar, D.},
 title = {Customization: optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language},
 abstract = {Dynamically-typed object-oriented languages please programmers, but their lack of static type information penalizes performance. Our new implementation techniques extract static type information from declaration-free programs. Our system compiles several copies of a given procedure, each customized for one receiver type, so that the type of the receiver is bound at compile time. The compiler predicts types that are statically unknown but likely, and inserts run-time type tests to verify its predictions. It splits calls, compiling a copy on each control path, optimized to the specific types on that path. Coupling these new techniques with compile-time message lookup, aggressive procedure inlining, and traditional optimizations has doubled the performance of dynamically-typed object-oriented languages.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {146--160},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74831},
 doi = {http://doi.acm.org/10.1145/74818.74831},
 acmid = {74831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chambers:1989:COC:73141.74831,
 author = {Chambers, C. and Ungar, D.},
 title = {Customization: optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language},
 abstract = {Dynamically-typed object-oriented languages please programmers, but their lack of static type information penalizes performance. Our new implementation techniques extract static type information from declaration-free programs. Our system compiles several copies of a given procedure, each customized for one receiver type, so that the type of the receiver is bound at compile time. The compiler predicts types that are statically unknown but likely, and inserts run-time type tests to verify its predictions. It splits calls, compiling a copy on each control path, optimized to the specific types on that path. Coupling these new techniques with compile-time message lookup, aggressive procedure inlining, and traditional optimizations has doubled the performance of dynamically-typed object-oriented languages.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {146--160},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74831},
 doi = {http://doi.acm.org/10.1145/73141.74831},
 acmid = {74831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cormack:1989:LSP:74818.74832,
 author = {Cormack, G. V.},
 title = {An LR substring parser for noncorrecting syntax error recovery},
 abstract = {For a context-free grammar G, a construction is given to produce an LR parser that recognizes any substring of the language generated by G. The construction yields a conflict-free (deterministic) parser for the bounded context class of grammars (Floyd, 1964). The same construction yields either a left-to-right or right-to-left substring parser, as required to implement Non-correcting Syntax Error Recovery as proposed by Richter (1985). Experience in constructing a substring parser for Pascal is described.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {161--169},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/74818.74832},
 doi = {http://doi.acm.org/10.1145/74818.74832},
 acmid = {74832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cormack:1989:LSP:73141.74832,
 author = {Cormack, G. V.},
 title = {An LR substring parser for noncorrecting syntax error recovery},
 abstract = {For a context-free grammar G, a construction is given to produce an LR parser that recognizes any substring of the language generated by G. The construction yields a conflict-free (deterministic) parser for the bounded context class of grammars (Floyd, 1964). The same construction yields either a left-to-right or right-to-left substring parser, as required to implement Non-correcting Syntax Error Recovery as proposed by Richter (1985). Experience in constructing a substring parser for Pascal is described.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {161--169},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/73141.74832},
 doi = {http://doi.acm.org/10.1145/73141.74832},
 acmid = {74832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Salomon:1989:SNP:73141.74833,
 author = {Salomon, D. J. and Cormack, G. V.},
 title = {Scannerless NSLR(1) parsing of programming languages},
 abstract = {The disadvantages of traditional two-phase parsing (a scanner phase preprocessing input for a parser phase) are discussed. We present metalanguage enhancements for context-free grammars that allow the syntax of programming languages to be completely described in a single grammar. The enhancements consist of two new grammar rules, the exclusion rule, and the adjacency-restriction rule. We also present parser construction techniques for building parsers from these enhanced grammars, that eliminate the need for a scanner phase.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {170--178},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/73141.74833},
 doi = {http://doi.acm.org/10.1145/73141.74833},
 acmid = {74833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salomon:1989:SNP:74818.74833,
 author = {Salomon, D. J. and Cormack, G. V.},
 title = {Scannerless NSLR(1) parsing of programming languages},
 abstract = {The disadvantages of traditional two-phase parsing (a scanner phase preprocessing input for a parser phase) are discussed. We present metalanguage enhancements for context-free grammars that allow the syntax of programming languages to be completely described in a single grammar. The enhancements consist of two new grammar rules, the exclusion rule, and the adjacency-restriction rule. We also present parser construction techniques for building parsers from these enhanced grammars, that eliminate the need for a scanner phase.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {170--178},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/74818.74833},
 doi = {http://doi.acm.org/10.1145/74818.74833},
 acmid = {74833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heering:1989:IGP:74818.74834,
 author = {Heering, J. and Klint, P. and Rekers, J.},
 title = {Incremental generation of parsers},
 abstract = {An LR-based parser generator for arbitrary context-free grammars is described, which generates parsers by need and processes grammar modifications by updating already existing parsers. We motivate the need for these techniques in the context of interactive language definition environments, present all required algorithms, and give measurements comparing their performance with that of conventional techniques.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {179--191},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74818.74834},
 doi = {http://doi.acm.org/10.1145/74818.74834},
 acmid = {74834},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heering:1989:IGP:73141.74834,
 author = {Heering, J. and Klint, P. and Rekers, J.},
 title = {Incremental generation of parsers},
 abstract = {An LR-based parser generator for arbitrary context-free grammars is described, which generates parsers by need and processes grammar modifications by updating already existing parsers. We motivate the need for these techniques in the context of interactive language definition environments, present all required algorithms, and give measurements comparing their performance with that of conventional techniques.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {179--191},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/73141.74834},
 doi = {http://doi.acm.org/10.1145/73141.74834},
 acmid = {74834},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boehm:1989:TIP:73141.74835,
 author = {Boehm, H.-J.},
 title = {Type inference in the presence of type abstraction},
 abstract = {A number of recent programming language designs incorporate a type checking system based on the Girard-Reynolds polymorphic \&lgr;-calculus. This allows the construction of general purpose, reusable software without sacrificing compile-time type checking. A major factor constraining the implementation of these languages is the difficulty of automatically inferring the lengthy type information that is otherwise required if full use is made of these languages. There is no known algorithm to solve any natural and fully general formulation of this ``type inference" problem. One very reasonable formulation of the problem is known to be undecidable.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {192--206},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74835},
 doi = {http://doi.acm.org/10.1145/73141.74835},
 acmid = {74835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boehm:1989:TIP:74818.74835,
 author = {Boehm, H.-J.},
 title = {Type inference in the presence of type abstraction},
 abstract = {A number of recent programming language designs incorporate a type checking system based on the Girard-Reynolds polymorphic \&lgr;-calculus. This allows the construction of general purpose, reusable software without sacrificing compile-time type checking. A major factor constraining the implementation of these languages is the difficulty of automatically inferring the lengthy type information that is otherwise required if full use is made of these languages. There is no known algorithm to solve any natural and fully general formulation of this ``type inference" problem. One very reasonable formulation of the problem is known to be undecidable.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {192--206},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74835},
 doi = {http://doi.acm.org/10.1145/74818.74835},
 acmid = {74835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{O'Toole:1989:TRF:73141.74836,
 author = {O'Toole,Jr., J. W. and Gifford, D. K.},
 title = {Type reconstruction with first-class polymorphic values},
 abstract = {We present the first type reconstruction system which combines the implicit typing of ML with the full power of the explicitly typed second-order polymorphic lambda calculus. The system will accept ML-style programs, explicitly typed programs, and programs that use explicit types for all first-class polymorphic values. We accomplish this flexibility by providing both generic and explicitly-quantified polymorphic types, as well as operators which convert between these two forms of polymorphism. This type reconstruction system is an integral part of the FX-89 programming language. We present a type reconstruction algorithm for the system. The type reconstruction algorithm is proven sound and complete with respect to the formal typing rules.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {207--217},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/73141.74836},
 doi = {http://doi.acm.org/10.1145/73141.74836},
 acmid = {74836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{O'Toole:1989:TRF:74818.74836,
 author = {O'Toole,Jr., J. W. and Gifford, D. K.},
 title = {Type reconstruction with first-class polymorphic values},
 abstract = {We present the first type reconstruction system which combines the implicit typing of ML with the full power of the explicitly typed second-order polymorphic lambda calculus. The system will accept ML-style programs, explicitly typed programs, and programs that use explicit types for all first-class polymorphic values. We accomplish this flexibility by providing both generic and explicitly-quantified polymorphic types, as well as operators which convert between these two forms of polymorphism. This type reconstruction system is an integral part of the FX-89 programming language. We present a type reconstruction algorithm for the system. The type reconstruction algorithm is proven sound and complete with respect to the formal typing rules.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {207--217},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/74818.74836},
 doi = {http://doi.acm.org/10.1145/74818.74836},
 acmid = {74836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jouvelot:1989:RCC:74818.74837,
 author = {Jouvelot, P. and Gifford, D. K.},
 title = {Reasoning about continuations with control effects},
 abstract = {We present a new static analysis method for first-class continuations that uses an effect system to classify the control domain behavior of expressions in a typed polymorphic language. We introduce two new control effects, goto and comefrom, that describe the control flow properties of expressions. An expression that does not have a goto effect is said to be continuation following because it will always call its passed return continuation. An expression that does not have a comefrom effect is said to be continuation discarding because it will never preserve its return continuation for later use. Unobservable control effects can be masked by the effect system. Control effect soundness theorems guarantee that the effects computed statically by the effect system are a conservative approximation of the dynamic behavior of an expression.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {218--226},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/74818.74837},
 doi = {http://doi.acm.org/10.1145/74818.74837},
 acmid = {74837},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jouvelot:1989:RCC:73141.74837,
 author = {Jouvelot, P. and Gifford, D. K.},
 title = {Reasoning about continuations with control effects},
 abstract = {We present a new static analysis method for first-class continuations that uses an effect system to classify the control domain behavior of expressions in a typed polymorphic language. We introduce two new control effects, goto and comefrom, that describe the control flow properties of expressions. An expression that does not have a goto effect is said to be continuation following because it will always call its passed return continuation. An expression that does not have a comefrom effect is said to be continuation discarding because it will never preserve its return continuation for later use. Unobservable control effects can be masked by the effect system. Control effect soundness theorems guarantee that the effects computed statically by the effect system are a conservative approximation of the dynamic behavior of an expression.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {218--226},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/73141.74837},
 doi = {http://doi.acm.org/10.1145/73141.74837},
 acmid = {74837},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Emmelmann:1989:BGE:73141.74838,
 author = {Emmelmann, H. and Schr\"{o}er, F.-W. and Landwehr, Rudolf},
 title = {BEG: a generator for efficient back ends},
 abstract = {This paper describes a system that generates compiler back ends from a strictly declarative specification of the code generation process. The generated back ends use tree pattern matching for code selection. Two methods for register allocation supporting a wide range of target architectures are provided. A general bottom-up pattern matching method avoids problems that occurred with previous systems using LR-parsing. The performance of compilers using generated back ends is comparable to very fast production compilers. Some figures are given about the results of using the system to generate the back end of a Modula-2 compiler.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {227--237},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/73141.74838},
 doi = {http://doi.acm.org/10.1145/73141.74838},
 acmid = {74838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Emmelmann:1989:BGE:74818.74838,
 author = {Emmelmann, H. and Schr\"{o}er, F.-W. and Landwehr, Rudolf},
 title = {BEG: a generator for efficient back ends},
 abstract = {This paper describes a system that generates compiler back ends from a strictly declarative specification of the code generation process. The generated back ends use tree pattern matching for code selection. Two methods for register allocation supporting a wide range of target architectures are provided. A general bottom-up pattern matching method avoids problems that occurred with previous systems using LR-parsing. The performance of compilers using generated back ends is comparable to very fast production compilers. Some figures are given about the results of using the system to generate the back end of a Modula-2 compiler.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {227--237},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/74818.74838},
 doi = {http://doi.acm.org/10.1145/74818.74838},
 acmid = {74838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fraser:1989:LWC:74818.74839,
 author = {Fraser, C. W.},
 title = {A language for writing code generators},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {238--245},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/74818.74839},
 doi = {http://doi.acm.org/10.1145/74818.74839},
 acmid = {74839},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fraser:1989:LWC:73141.74839,
 author = {Fraser, C. W.},
 title = {A language for writing code generators},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {238--245},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/73141.74839},
 doi = {http://doi.acm.org/10.1145/73141.74839},
 acmid = {74839},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chang:1989:IFE:73141.74840,
 author = {Chang, P. P. and Hwu, W.-W.},
 title = {Inline function expansion for compiling C programs},
 abstract = {Inline function expansion replaces a function call with the function body. With automatic inline function expansion, programs can be constructed with many small functions to handle complexity and then rely on the compilation to eliminate most of the function calls. Therefore, inline expansion serves a tool for satisfying two conflicting goals: minizing the complexity of the program development and minimizing the function call overhead of program execution. A simple inline expansion procedure is presented which uses profile information to address three critical issues: code expansion, stack expansion, and unavailable function bodies. Experiments show that a large percentage of function calls/returns (about 59\%) can be eliminated with a modest code expansion cost (about 17\%) for twelve UNIX<supscrpt>*</supscrpt> programs.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/73141.74840},
 doi = {http://doi.acm.org/10.1145/73141.74840},
 acmid = {74840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chang:1989:IFE:74818.74840,
 author = {Chang, P. P. and Hwu, W.-W.},
 title = {Inline function expansion for compiling C programs},
 abstract = {Inline function expansion replaces a function call with the function body. With automatic inline function expansion, programs can be constructed with many small functions to handle complexity and then rely on the compilation to eliminate most of the function calls. Therefore, inline expansion serves a tool for satisfying two conflicting goals: minizing the complexity of the program development and minimizing the function call overhead of program execution. A simple inline expansion procedure is presented which uses profile information to address three critical issues: code expansion, stack expansion, and unavailable function bodies. Experiments show that a large percentage of function calls/returns (about 59\%) can be eliminated with a modest code expansion cost (about 17\%) for twelve UNIX<supscrpt>*</supscrpt> programs.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/74818.74840},
 doi = {http://doi.acm.org/10.1145/74818.74840},
 acmid = {74840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bernstein:1989:SCM:74818.74841,
 author = {Bernstein, D. and Golumbic, M. and Mansour, y. and Pinter, R. and Goldin, D. and Krawczyk, H. and Nahshon, I.},
 title = {Spill code minimization techniques for optimizing compliers},
 abstract = {Global register allocation and spilling is commonly performed by solving a graph coloring problem. In this paper we present a new coherent set of heuristic methods for reducing the amount of spill code generated. This results in more efficient (and shorter) compiled code. Our approach has been compared to both standard and priority-based coloring algorithms, universally outperforming them.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {258--263},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/74818.74841},
 doi = {http://doi.acm.org/10.1145/74818.74841},
 acmid = {74841},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bernstein:1989:SCM:73141.74841,
 author = {Bernstein, D. and Golumbic, M. and Mansour, y. and Pinter, R. and Goldin, D. and Krawczyk, H. and Nahshon, I.},
 title = {Spill code minimization techniques for optimizing compliers},
 abstract = {Global register allocation and spilling is commonly performed by solving a graph coloring problem. In this paper we present a new coherent set of heuristic methods for reducing the amount of spill code generated. This results in more efficient (and shorter) compiled code. Our approach has been compared to both standard and priority-based coloring algorithms, universally outperforming them.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {258--263},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/73141.74841},
 doi = {http://doi.acm.org/10.1145/73141.74841},
 acmid = {74841},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1989:RAV:74818.74842,
 author = {Gupta, R. and Soffa, M. L. and Steele, T.},
 title = {Register allocation via clique separators},
 abstract = {Although graph coloring is widely recognized as an effective technique for global register allocation, the overhead can be quite high, not only in execution time but also in memory, as the size of the interference graph needed in coloring can become quite large. In this paper, we present an algorithm based upon a result by R. Tarjan regarding the colorability of graphs which are decomposable using clique separators, that improves on the overhead of coloring. The algorithm first partitions program code into code segments using the notion of clique separators. The interference graphs for the code partitions are next constructed one at a time and colored independently. The colorings for the partitions are combined to obtain a register allocation for the program code. The technique presented is both efficient in space and time because the graph for only a single code segment needs to be constructed and colored at any given point in time. The partitioning of a graph using clique separators increases the likelihood of obtaining a coloring without spilling and hence an efficient allocation of registers for the program. For straight line code an optimal allocation for the entire program code can be obtained from optimal allocations for individual code segments. In the presence of branches, optimal allocation along one execution path and a near optimal allocation along alternative paths can be potentially obtained. Since the algorithm is highly efficient, it eliminates the need for a local register allocation phase.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {264--274},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/74818.74842},
 doi = {http://doi.acm.org/10.1145/74818.74842},
 acmid = {74842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1989:RAV:73141.74842,
 author = {Gupta, R. and Soffa, M. L. and Steele, T.},
 title = {Register allocation via clique separators},
 abstract = {Although graph coloring is widely recognized as an effective technique for global register allocation, the overhead can be quite high, not only in execution time but also in memory, as the size of the interference graph needed in coloring can become quite large. In this paper, we present an algorithm based upon a result by R. Tarjan regarding the colorability of graphs which are decomposable using clique separators, that improves on the overhead of coloring. The algorithm first partitions program code into code segments using the notion of clique separators. The interference graphs for the code partitions are next constructed one at a time and colored independently. The colorings for the partitions are combined to obtain a register allocation for the program code. The technique presented is both efficient in space and time because the graph for only a single code segment needs to be constructed and colored at any given point in time. The partitioning of a graph using clique separators increases the likelihood of obtaining a coloring without spilling and hence an efficient allocation of registers for the program. For straight line code an optimal allocation for the entire program code can be obtained from optimal allocations for individual code segments. In the presence of branches, optimal allocation along one execution path and a near optimal allocation along alternative paths can be potentially obtained. Since the algorithm is highly efficient, it eliminates the need for a local register allocation phase.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {264--274},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/73141.74842},
 doi = {http://doi.acm.org/10.1145/73141.74842},
 acmid = {74842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Briggs:1989:CHR:74818.74843,
 author = {Briggs, P. and Cooper, K. D. and Kennedy, K. and Torczon, L.},
 title = {Coloring  heuristics for register allocation},
 abstract = {We describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compile-time and implementation requirements. We present experimental data to compare the two methods.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {275--284},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/74818.74843},
 doi = {http://doi.acm.org/10.1145/74818.74843},
 acmid = {74843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briggs:1989:CHR:73141.74843,
 author = {Briggs, P. and Cooper, K. D. and Kennedy, K. and Torczon, L.},
 title = {Coloring  heuristics for register allocation},
 abstract = {We describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compile-time and implementation requirements. We present experimental data to compare the two methods.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {275--284},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/73141.74843},
 doi = {http://doi.acm.org/10.1145/73141.74843},
 acmid = {74843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schonberg:1989:ODA:74818.74844,
 author = {Schonberg, D.},
 title = {On-the-fly detection of access anomalies},
 abstract = {Access anomalies are a common class of bugs in shared-memory parallel programs. An access anomaly occurs when two concurrent execution threads both write (or one thread reads and the other writes) the same shared memory location without coordination. Approaches to the detection of access anomalies include static analysis, post-mortem trace analysis, and on-the-fly monitoring.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {285--297},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74818.74844},
 doi = {http://doi.acm.org/10.1145/74818.74844},
 acmid = {74844},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schonberg:1989:ODA:73141.74844,
 author = {Schonberg, D.},
 title = {On-the-fly detection of access anomalies},
 abstract = {Access anomalies are a common class of bugs in shared-memory parallel programs. An access anomaly occurs when two concurrent execution threads both write (or one thread reads and the other writes) the same shared memory location without coordination. Approaches to the detection of access anomalies include static analysis, post-mortem trace analysis, and on-the-fly monitoring.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {285--297},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/73141.74844},
 doi = {http://doi.acm.org/10.1145/73141.74844},
 acmid = {74844},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sarkar:1989:DAP:74818.74845,
 author = {Sarkar, V.},
 title = {Determining average program execution times and their variance},
 abstract = {This paper presents a general framework for determining average program execution times and their variance, based on the program's interval structure and control dependence graph. Average execution times and variance values are computed using frequency information from an optimized counter-based execution profile of the program.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {298--312},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/74818.74845},
 doi = {http://doi.acm.org/10.1145/74818.74845},
 acmid = {74845},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sarkar:1989:DAP:73141.74845,
 author = {Sarkar, V.},
 title = {Determining average program execution times and their variance},
 abstract = {This paper presents a general framework for determining average program execution times and their variance, based on the program's interval structure and control dependence graph. Average execution times and variance values are computed using frequency information from an optimized counter-based execution profile of the program.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {298--312},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/73141.74845},
 doi = {http://doi.acm.org/10.1145/73141.74845},
 acmid = {74845},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goldberg:1989:GRC:73141.74846,
 author = {Goldberg, B.},
 title = {Generational reference counting: a reduced-communication distributed storage reclamation scheme},
 abstract = {This paper describes generational reference counting, a new distributed storage reclamation scheme for loosely-coupled multiprocessors. It has a significantly lower communication overhead than distributed versions of conventional reference counting. Although generational reference counting has greater computational and space requirements than ordinary reference counting, it may provide a significant saving in overall execution time on machines in which message passing is expensive.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {313--321},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/73141.74846},
 doi = {http://doi.acm.org/10.1145/73141.74846},
 acmid = {74846},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goldberg:1989:GRC:74818.74846,
 author = {Goldberg, B.},
 title = {Generational reference counting: a reduced-communication distributed storage reclamation scheme},
 abstract = {This paper describes generational reference counting, a new distributed storage reclamation scheme for loosely-coupled multiprocessors. It has a significantly lower communication overhead than distributed versions of conventional reference counting. Although generational reference counting has greater computational and space requirements than ordinary reference counting, it may provide a significant saving in overall execution time on machines in which message passing is expensive.
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {313--321},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/74818.74846},
 doi = {http://doi.acm.org/10.1145/74818.74846},
 acmid = {74846},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Atkinson:1989:ECP:74818.74847,
 author = {Atkinson, R. and Demers, A. and Hauser, C. and Jacobi, C. and Kessler, P. and Weiser, M.},
 title = {Experiences creating a portable cedar},
 abstract = {Cedar is the name for both a language and an environment in use in the Computer Science Laboratory at Xerox PARC since 1980. The Cedar language is a superset of Mesa, the major additions being garbage collection and runtime types. Neither the language nor the environment was originally intended to be portable, and for many years ran only on D-machines at PARC and a few other locations in Xerox. We recently re-implemented the language to make it portable across many different architectures. Our strategy was, first, to use machine-dependent C code as an intermediate language, second, to create a language-independent layer known as the Portable Common Runtime, and third, to write a relatively large amount of Cedar-specific runtime code in a subset of Cedar itself. By treating C as an intermediate code we are able to achieve reasonably fast compilation, very good eventual machine code, and all with relatively small programmer effort. Because Cedar is a much richer language than C, there were numerous issues to resolve in performing an efficient translation and in providing reasonable debugging. These strategies will be of use to many other porters of high-level languages who may wish to use C as an assembler language without giving up either ease of debugging or high performance. We present a brief description of the Cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the Unix<supscrpt>*</supscrpt> operating system, and some measures of the performance of our ``Portable Cedar".
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {322--329},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/74818.74847},
 doi = {http://doi.acm.org/10.1145/74818.74847},
 acmid = {74847},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Atkinson:1989:ECP:73141.74847,
 author = {Atkinson, R. and Demers, A. and Hauser, C. and Jacobi, C. and Kessler, P. and Weiser, M.},
 title = {Experiences creating a portable cedar},
 abstract = {Cedar is the name for both a language and an environment in use in the Computer Science Laboratory at Xerox PARC since 1980. The Cedar language is a superset of Mesa, the major additions being garbage collection and runtime types. Neither the language nor the environment was originally intended to be portable, and for many years ran only on D-machines at PARC and a few other locations in Xerox. We recently re-implemented the language to make it portable across many different architectures. Our strategy was, first, to use machine-dependent C code as an intermediate language, second, to create a language-independent layer known as the Portable Common Runtime, and third, to write a relatively large amount of Cedar-specific runtime code in a subset of Cedar itself. By treating C as an intermediate code we are able to achieve reasonably fast compilation, very good eventual machine code, and all with relatively small programmer effort. Because Cedar is a much richer language than C, there were numerous issues to resolve in performing an efficient translation and in providing reasonable debugging. These strategies will be of use to many other porters of high-level languages who may wish to use C as an assembler language without giving up either ease of debugging or high performance. We present a brief description of the Cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the Unix<supscrpt>*</supscrpt> operating system, and some measures of the performance of our ``Portable Cedar".
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {322--329},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/73141.74847},
 doi = {http://doi.acm.org/10.1145/73141.74847},
 acmid = {74847},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilson:1989:DMP:74818.74848,
 author = {Wilson, P. R. and Moher, T. G.},
 title = {Demonic memory for process histories},
 abstract = {Demonic memory is a form of reconstructive memory for process histories. As a process executes, its states are regularly checkpointed, generating a history of the process at low time resolution. Following the initial generation, any prior state of the process can be reconstructed by starting from a checkpointed state and re-executing the process up through the desired state, thereby exploiting the redundancy between the states of a process and the description of that process (i.e., a computer program).
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {330--343},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/74818.74848},
 doi = {http://doi.acm.org/10.1145/74818.74848},
 acmid = {74848},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wilson:1989:DMP:73141.74848,
 author = {Wilson, P. R. and Moher, T. G.},
 title = {Demonic memory for process histories},
 abstract = {Demonic memory is a form of reconstructive memory for process histories. As a process executes, its states are regularly checkpointed, generating a history of the process at low time resolution. Following the initial generation, any prior state of the process can be reconstructed by starting from a checkpointed state and re-executing the process up through the desired state, thereby exploiting the redundancy between the states of a process and the description of that process (i.e., a computer program).
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {330--343},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/73141.74848},
 doi = {http://doi.acm.org/10.1145/73141.74848},
 acmid = {74848},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chi:1989:UMR:73141.74849,
 author = {Chi, C.-H. and Dietz, H.},
 title = {Unified management of registers and cache using liveness and cache bypass},
 abstract = {In current computer memory system hierarchy, registers and cache are both used to bridge the reference delay gap between the fast processor(s) and the slow main memory. While registers are managed by the compiler using program flow analysis, cache is mainly controlled by hardware without any program understanding. Due to the lack of coordination in managing these two memory structures, significant loss of system performance results because:
},
 booktitle = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming language design and implementation},
 series = {PLDI '89},
 year = {1989},
 isbn = {0-89791-306-X},
 location = {Portland, Oregon, United States},
 pages = {344--353},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/73141.74849},
 doi = {http://doi.acm.org/10.1145/73141.74849},
 acmid = {74849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chi:1989:UMR:74818.74849,
 author = {Chi, C.-H. and Dietz, H.},
 title = {Unified management of registers and cache using liveness and cache bypass},
 abstract = {In current computer memory system hierarchy, registers and cache are both used to bridge the reference delay gap between the fast processor(s) and the slow main memory. While registers are managed by the compiler using program flow analysis, cache is mainly controlled by hardware without any program understanding. Due to the lack of coordination in managing these two memory structures, significant loss of system performance results because:
},
 journal = {SIGPLAN Not.},
 volume = {24},
 issue = {7},
 month = {June},
 year = {1989},
 issn = {0362-1340},
 pages = {344--353},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/74818.74849},
 doi = {http://doi.acm.org/10.1145/74818.74849},
 acmid = {74849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chase:1988:SCS:53990.53991,
 author = {Chase, D. R.},
 title = {Safety consideration for storage allocation optimizations},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.53991},
 doi = {http://doi.acm.org/10.1145/53990.53991},
 acmid = {53991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chase:1988:SCS:960116.53991,
 author = {Chase, D. R.},
 title = {Safety consideration for storage allocation optimizations},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.53991},
 doi = {http://doi.acm.org/10.1145/960116.53991},
 acmid = {53991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Appel:1988:RCC:53990.53992,
 author = {Appel, A. W. and Ellis, J. R. and Li, K.},
 title = {Real-time concurrent collection on stock multiprocessors},
 abstract = {We've designed and implemented a copying garbage-collection algorithm that is efficient, real-time, concurrent, runs on commercial uniprocessors and shared-memory multiprocessors, and requires no change to compilers. The algorithm uses standard virtual-memory hardware to detect references to ``from space" objects and to synchronize the collector and mutator threads. We've implemented and measured a prototype running on SRC's 5-processor Firefly. It will be straightforward to merge our techniques with generational collection. An incremental, non-concurrent version could be implemented easily on many versions of Unix.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.53992},
 doi = {http://doi.acm.org/10.1145/53990.53992},
 acmid = {53992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:1988:RCC:960116.53992,
 author = {Appel, A. W. and Ellis, J. R. and Li, K.},
 title = {Real-time concurrent collection on stock multiprocessors},
 abstract = {We've designed and implemented a copying garbage-collection algorithm that is efficient, real-time, concurrent, runs on commercial uniprocessors and shared-memory multiprocessors, and requires no change to compilers. The algorithm uses standard virtual-memory hardware to detect references to ``from space" objects and to synchronize the collector and mutator threads. We've implemented and measured a prototype running on SRC's 5-processor Firefly. It will be straightforward to merge our techniques with generational collection. An incremental, non-concurrent version could be implemented easily on many versions of Unix.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.53992},
 doi = {http://doi.acm.org/10.1145/960116.53992},
 acmid = {53992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Larus:1988:DCS:53990.53993,
 author = {Larus, J. R. and Hilfinger, P. N.},
 title = {Detecting conflicts between structure accesses},
 abstract = {Two references to a record structure conflict if they access the same field and at least one modifies the location. Because structures can be connected by pointers, deciding if two statements conflict requires knowledge of the possible aliases for the locations that they access.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {24--31},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/53990.53993},
 doi = {http://doi.acm.org/10.1145/53990.53993},
 acmid = {53993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Larus:1988:DCS:960116.53993,
 author = {Larus, J. R. and Hilfinger, P. N.},
 title = {Detecting conflicts between structure accesses},
 abstract = {Two references to a record structure conflict if they access the same field and at least one modifies the location. Because structures can be connected by pointers, deciding if two statements conflict requires knowledge of the possible aliases for the locations that they access.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {24--31},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960116.53993},
 doi = {http://doi.acm.org/10.1145/960116.53993},
 acmid = {53993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Horwitz:1988:ISU:960116.53994,
 author = {Horwitz, S. and Reps, T. and Binkley, D.},
 title = {Interprocedural slicing using dependence graphs},
 abstract = {A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing \&mdash; generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/960116.53994},
 doi = {http://doi.acm.org/10.1145/960116.53994},
 acmid = {53994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Horwitz:1988:ISU:53990.53994,
 author = {Horwitz, S. and Reps, T. and Binkley, D.},
 title = {Interprocedural slicing using dependence graphs},
 abstract = {A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing \&mdash; generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/53990.53994},
 doi = {http://doi.acm.org/10.1145/53990.53994},
 acmid = {53994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Callahan:1988:PSG:53990.53995,
 author = {Callahan, D.},
 title = {The program summary graph and flow-sensitive interprocedual data flow analysis},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.53995},
 doi = {http://doi.acm.org/10.1145/53990.53995},
 acmid = {53995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1988:PSG:960116.53995,
 author = {Callahan, D.},
 title = {The program summary graph and flow-sensitive interprocedual data flow analysis},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.53995},
 doi = {http://doi.acm.org/10.1145/960116.53995},
 acmid = {53995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cooper:1988:ISA:53990.53996,
 author = {Cooper, K. D. and Kennedy, K.},
 title = {Interprocedural side-effect analysis in linear time},
 abstract = {We present a new method for solving Banning's alias-free flow-insensitive side-effect analysis problem. The algorithm employs a new data structure, called the binding multi-graph, along with depth-first search to achieve a running time that is linear in the size of the call multi-graph of the program. This method can be extended to produce fast algorithms for data-flow problems with more complex lattice structures.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {57--66},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.53996},
 doi = {http://doi.acm.org/10.1145/53990.53996},
 acmid = {53996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooper:1988:ISA:960116.53996,
 author = {Cooper, K. D. and Kennedy, K.},
 title = {Interprocedural side-effect analysis in linear time},
 abstract = {We present a new method for solving Banning's alias-free flow-insensitive side-effect analysis problem. The algorithm employs a new data structure, called the binding multi-graph, along with depth-first search to achieve a running time that is linear in the size of the call multi-graph of the program. This method can be extended to produce fast algorithms for data-flow problems with more complex lattice structures.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {57--66},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.53996},
 doi = {http://doi.acm.org/10.1145/960116.53996},
 acmid = {53996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wall:1988:RWV:53990.53997,
 author = {Wall, D. W.},
 title = {Register windows vs. register allocation},
 abstract = {A large register set can be exploited by keeping variables and constants in registers instead of in memory. Hardware register windows and compile-time or link-time global register allocation are ways to do this. A measure of the effectiveness of any of these register management schemes is how thoroughly they remove loads and stores. This measure must also count extra loads and stores executed because of window overflow or conflicts between procedures.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/53990.53997},
 doi = {http://doi.acm.org/10.1145/53990.53997},
 acmid = {53997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1988:RWV:960116.53997,
 author = {Wall, D. W.},
 title = {Register windows vs. register allocation},
 abstract = {A large register set can be exploited by keeping variables and constants in registers instead of in memory. Hardware register windows and compile-time or link-time global register allocation are ways to do this. A measure of the effectiveness of any of these register management schemes is how thoroughly they remove loads and stores. This measure must also count extra loads and stores executed because of window overflow or conflicts between procedures.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/960116.53997},
 doi = {http://doi.acm.org/10.1145/960116.53997},
 acmid = {53997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fraser:1988:AGF:53990.53998,
 author = {Fraser, C. W. and Wendt, A. L.},
 title = {Automatic generation of fast optimizing code generators},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {79--84},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/53990.53998},
 doi = {http://doi.acm.org/10.1145/53990.53998},
 acmid = {53998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fraser:1988:AGF:960116.53998,
 author = {Fraser, C. W. and Wendt, A. L.},
 title = {Automatic generation of fast optimizing code generators},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {79--84},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/960116.53998},
 doi = {http://doi.acm.org/10.1145/960116.53998},
 acmid = {53998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chow:1988:MRU:53990.53999,
 author = {Chow, F. C.},
 title = {Minimizing register usage penalty at procedure calls},
 abstract = {Inter-procedural register allocation can minimize the register usage penalty at procedure calls by reducing the saving and restoring of registers at procedure boundaries. A one-pass inter-procedural register allocation scheme based on processing the procedures in a depth-first traversal of the call graph is presented. This scheme can be overlaid on top of intra-procedural register allocation via a simple extension to the priority-based coloring algorithm. Using two different usage conventions for the registers, the scheme can distribute register saves/restores throughout the call graph even in the presence of recursion, indirect calls or separate compilation. A natural and efficient way to pass parameters emerges from this scheme. A separate technique uses data flow analysis to optimize the placement of the save/restore code for registers within individual procedures. The techniques described have been implemented in a production compiler suite. Measurements of the effects of these techniques on a set of practical programs are presented and the results analysed.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {85--94},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.53999},
 doi = {http://doi.acm.org/10.1145/53990.53999},
 acmid = {53999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chow:1988:MRU:960116.53999,
 author = {Chow, F. C.},
 title = {Minimizing register usage penalty at procedure calls},
 abstract = {Inter-procedural register allocation can minimize the register usage penalty at procedure calls by reducing the saving and restoring of registers at procedure boundaries. A one-pass inter-procedural register allocation scheme based on processing the procedures in a depth-first traversal of the call graph is presented. This scheme can be overlaid on top of intra-procedural register allocation via a simple extension to the priority-based coloring algorithm. Using two different usage conventions for the registers, the scheme can distribute register saves/restores throughout the call graph even in the presence of recursion, indirect calls or separate compilation. A natural and efficient way to pass parameters emerges from this scheme. A separate technique uses data flow analysis to optimize the placement of the save/restore code for registers within individual procedures. The techniques described have been implemented in a production compiler suite. Measurements of the effects of these techniques on a set of practical programs are presented and the results analysed.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {85--94},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.53999},
 doi = {http://doi.acm.org/10.1145/960116.53999},
 acmid = {53999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Keutzer:1988:AHC:53990.54000,
 author = {Keutzer, K. and Wolf, W.},
 title = {Anatomy of a hardware compiler},
 abstract = {Programming-language compilers generate code targeted to machines with fixed architectures, either parallel or serial. Compiler techniques can also be used to generate the hardware on which these programming languages are executed. In this paper we demonstrate that many compilation techniques developed for programming languages are applicable to compilation of register-transfer hardware designs. Our approach uses a typical syntax-directed translation \&rarr; global optimization \&rarr; local optimization \&rarr; code generation \&rarr; peephole optimization method. In this paper we will describe ways in which we have both followed and diverged from traditional compiler approaches to these problems and compare our approach to other compiler oriented approaches to hardware compilation.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {95--104},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54000},
 doi = {http://doi.acm.org/10.1145/53990.54000},
 acmid = {54000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Keutzer:1988:AHC:960116.54000,
 author = {Keutzer, K. and Wolf, W.},
 title = {Anatomy of a hardware compiler},
 abstract = {Programming-language compilers generate code targeted to machines with fixed architectures, either parallel or serial. Compiler techniques can also be used to generate the hardware on which these programming languages are executed. In this paper we demonstrate that many compilation techniques developed for programming languages are applicable to compilation of register-transfer hardware designs. Our approach uses a typical syntax-directed translation \&rarr; global optimization \&rarr; local optimization \&rarr; code generation \&rarr; peephole optimization method. In this paper we will describe ways in which we have both followed and diverged from traditional compiler approaches to these problems and compare our approach to other compiler oriented approaches to hardware compilation.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {95--104},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54000},
 doi = {http://doi.acm.org/10.1145/960116.54000},
 acmid = {54000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Andrews:1988:DIU:53990.54001,
 author = {Andrews, K. and Henry, R. R. and Yamamoto, W. K.},
 title = {Design and implementation of the UW Illustrated compiler},
 abstract = {We have implemented an illustrated compiler for a simple block structured language. The compiler graphically displays its control and data structures, and so gives its viewers an intuitive understanding of compiler organization and operation. The illustrations were planned by hand and display information naturally and concisely.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54001},
 doi = {http://doi.acm.org/10.1145/53990.54001},
 acmid = {54001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Andrews:1988:DIU:960116.54001,
 author = {Andrews, K. and Henry, R. R. and Yamamoto, W. K.},
 title = {Design and implementation of the UW Illustrated compiler},
 abstract = {We have implemented an illustrated compiler for a simple block structured language. The compiler graphically displays its control and data structures, and so gives its viewers an intuitive understanding of compiler organization and operation. The illustrations were planned by hand and display information naturally and concisely.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54001},
 doi = {http://doi.acm.org/10.1145/960116.54001},
 acmid = {54001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yellin:1988:ILI:53990.54002,
 author = {Yellin, D. and Strom, R.},
 title = {INC: a language for incremental computations},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54002},
 doi = {http://doi.acm.org/10.1145/53990.54002},
 acmid = {54002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yellin:1988:ILI:960116.54002,
 author = {Yellin, D. and Strom, R.},
 title = {INC: a language for incremental computations},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54002},
 doi = {http://doi.acm.org/10.1145/960116.54002},
 acmid = {54002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coutant:1988:DPA:53990.54003,
 author = {Coutant, D. S. and Meloy, S. and Ruscetta, M.},
 title = {DOC: a practical approach to source-level debugging of globally optimized code},
 abstract = {As optimizing compilers become more sophisticated, the problem of debugging the source code of an application becomes more difficult. In order to investigate this problem, we implemented DOC, a prototype solution for Debugging Optimized Code. DOC is a modification of the existing C compiler and source-level symbolic debugger for the HP9000 Series 800. This paper describes our experiences in this effort. We show in an actual implementation that source-level debugging of globally optimized code is viable.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54003},
 doi = {http://doi.acm.org/10.1145/53990.54003},
 acmid = {54003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Coutant:1988:DPA:960116.54003,
 author = {Coutant, D. S. and Meloy, S. and Ruscetta, M.},
 title = {DOC: a practical approach to source-level debugging of globally optimized code},
 abstract = {As optimizing compilers become more sophisticated, the problem of debugging the source code of an application becomes more difficult. In order to investigate this problem, we implemented DOC, a prototype solution for Debugging Optimized Code. DOC is a modification of the existing C compiler and source-level symbolic debugger for the HP9000 Series 800. This paper describes our experiences in this effort. We show in an actual implementation that source-level debugging of globally optimized code is viable.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54003},
 doi = {http://doi.acm.org/10.1145/960116.54003},
 acmid = {54003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Miller:1988:MED:53990.54004,
 author = {Miller, B. P. and Choi, Jong-Deok},
 title = {A mechanism for efficient debugging of parallel programs},
 abstract = {This paper addresses the design and implementation of an integrated debugging system for parallel programs running on shared memory multi-processors (SMMP). We describe the use of flowback analysis to provide information on causal relationships between events in a program's execution without re-executing the program for debugging. We introduce a mechanism called incremental tracing that, by using semantic analyses of the debugged program, makes the flowback analysis practical with only a small amount of trace generated during execution. We extend flowback analysis to apply to parallel programs and describe a method to detect race conditions in the interactions of the co-operating processes.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54004},
 doi = {http://doi.acm.org/10.1145/53990.54004},
 acmid = {54004},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Miller:1988:MED:960116.54004,
 author = {Miller, B. P. and Choi, Jong-Deok},
 title = {A mechanism for efficient debugging of parallel programs},
 abstract = {This paper addresses the design and implementation of an integrated debugging system for parallel programs running on shared memory multi-processors (SMMP). We describe the use of flowback analysis to provide information on causal relationships between events in a program's execution without re-executing the program for debugging. We introduce a mechanism called incremental tracing that, by using semantic analyses of the debugged program, makes the flowback analysis practical with only a small amount of trace generated during execution. We extend flowback analysis to apply to parallel programs and describe a method to detect race conditions in the interactions of the co-operating processes.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54004},
 doi = {http://doi.acm.org/10.1145/960116.54004},
 acmid = {54004},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stone:1988:DCP:960116.54005,
 author = {Stone, J. M.},
 title = {Debugging concurrent processes: a case study},
 abstract = {We present a case study that illustrates a method of debugging concurrent processes in a parallel programming environment. It uses a new approach called speculative replay to reconstruct the behavior of a program from the histories of its individual processes. Known time dependencies between events in different processes are used to divide the histories into dependence blocks. A graphical representation called a concurrency map displays possibilities for concurrency among processes. The replay technique preserves the known dependencies and compares the process histories generated during replay with those that were logged during the original program execution. If a process generates a replay history that does not match its original history, replay backs up. An alternative ordering of events is created and tested to see if it produces process histories that match the original histories. Successively more controlled replay sequences are generated, by introducing additional dependencies. We describe ongoing work on tools that will control replay without reconstructing the entire space of possible event orderings.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {145--153},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960116.54005},
 doi = {http://doi.acm.org/10.1145/960116.54005},
 acmid = {54005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stone:1988:DCP:53990.54005,
 author = {Stone, J. M.},
 title = {Debugging concurrent processes: a case study},
 abstract = {We present a case study that illustrates a method of debugging concurrent processes in a parallel programming environment. It uses a new approach called speculative replay to reconstruct the behavior of a program from the histories of its individual processes. Known time dependencies between events in different processes are used to divide the histories into dependence blocks. A graphical representation called a concurrency map displays possibilities for concurrency among processes. The replay technique preserves the known dependencies and compares the process histories generated during replay with those that were logged during the original program execution. If a process generates a replay history that does not match its original history, replay backs up. An alternative ordering of events is created and tested to see if it produces process histories that match the original histories. Successively more controlled replay sequences are generated, by introducing additional dependencies. We describe ongoing work on tools that will control replay without reconstructing the entire space of possible event orderings.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {145--153},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/53990.54005},
 doi = {http://doi.acm.org/10.1145/53990.54005},
 acmid = {54005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jain:1988:EAD:960116.54006,
 author = {Jain, S. and Thompson, C.},
 title = {An efficient approach to data flow analysis in a multiple pass global optimizer},
 abstract = {Data flow analysis is a time-consuming part of the optimization process. As transformations are made in a multiple pass global optimizer, the data flow information must be updated to reflect these changes. Various approaches have been used, including complete recalculation as well as partial recalculation over the affected area. The approach presented here has been designed for maximum efficiency. Data flow information is completely calculated only once, using an interval analysis method which is slightly faster than a purely iterative approach, and which allows partial recomputation when appropriate. A minimal set of data flow information is computed, keeping the computation and update cost low. Following each set of transformations, the data flow information is updated based on knowledge of the effect of each change. This approach saves considerable time over complete recalculation, and proper ordering of the various optimizations minimizes the amount of update required.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54006},
 doi = {http://doi.acm.org/10.1145/960116.54006},
 acmid = {54006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jain:1988:EAD:53990.54006,
 author = {Jain, S. and Thompson, C.},
 title = {An efficient approach to data flow analysis in a multiple pass global optimizer},
 abstract = {Data flow analysis is a time-consuming part of the optimization process. As transformations are made in a multiple pass global optimizer, the data flow information must be updated to reflect these changes. Various approaches have been used, including complete recalculation as well as partial recalculation over the affected area. The approach presented here has been designed for maximum efficiency. Data flow information is completely calculated only once, using an interval analysis method which is slightly faster than a purely iterative approach, and which allows partial recomputation when appropriate. A minimal set of data flow information is computed, keeping the computation and update cost low. Following each set of transformations, the data flow information is updated based on knowledge of the effect of each change. This approach saves considerable time over complete recalculation, and proper ordering of the various optimizations minimizes the amount of update required.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54006},
 doi = {http://doi.acm.org/10.1145/53990.54006},
 acmid = {54006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shivers:1988:CFA:960116.54007,
 author = {Shivers, O.},
 title = {Control flow analysis in scheme},
 abstract = {Traditional flow analysis techniques, such as the ones typically employed by optimizing Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique \&mdash; control flow analysis \&mdash; which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/960116.54007},
 doi = {http://doi.acm.org/10.1145/960116.54007},
 acmid = {54007},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shivers:1988:CFA:53990.54007,
 author = {Shivers, O.},
 title = {Control flow analysis in scheme},
 abstract = {Traditional flow analysis techniques, such as the ones typically employed by optimizing Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique \&mdash; control flow analysis \&mdash; which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/53990.54007},
 doi = {http://doi.acm.org/10.1145/53990.54007},
 acmid = {54007},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schwarz:1988:OAD:960116.54008,
 author = {Schwarz, B. and Kirchg\"{a}ssner, W. and Landwehr, R.},
 title = {An optimizer for Ada - design, experiences and results},
 abstract = {In this paper we describe the design of a global machine independent low level optimizer for the Karlsruhe Ada Compiler. We give a short overview on the optimizations and data structures used in the optimizer as well as some experiences with the optimizer. Detailed measurements are provided for a collection of benchmarks. The average improvement of code speed is 40\%.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54008},
 doi = {http://doi.acm.org/10.1145/960116.54008},
 acmid = {54008},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schwarz:1988:OAD:53990.54008,
 author = {Schwarz, B. and Kirchg\"{a}ssner, W. and Landwehr, R.},
 title = {An optimizer for Ada - design, experiences and results},
 abstract = {In this paper we describe the design of a global machine independent low level optimizer for the Karlsruhe Ada Compiler. We give a short overview on the optimizations and data structures used in the optimizer as well as some experiences with the optimizer. Detailed measurements are provided for a collection of benchmarks. The average improvement of code speed is 40\%.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54008},
 doi = {http://doi.acm.org/10.1145/53990.54008},
 acmid = {54008},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ballance:1988:GAI:960116.54009,
 author = {Ballance, R. A. and Butcher, J. and Graham, S. L.},
 title = {Grammatical abstraction and incremental syntax analysis in a language-based editor},
 abstract = {Processors for programming languages and other formal languages typically use a concrete syntax to describe the user's view of a program and an abstract syntax to represent language structures internally. Grammatical abstraction is defined as a relationship between two context-free grammars. It formalizes the notion of one syntax being ``more abstract" than another. Two variants of abstraction are presented. Weak grammatical abstraction supports (i) the construction during LR parsing of an internal representation that is closely related to the abstract syntax and (ii) incremental LR parsing using that internal representation as its base. Strong grammatical abstraction tightens the correspondence so that top-down construction of incrementally-parsable internal representations is possible. These results arise from an investigation into language-based editing systems, but apply to any program that transforms a linguistic object from a representation in its concrete syntax to a representation in its abstract syntax or vice versa.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {185--198},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/960116.54009},
 doi = {http://doi.acm.org/10.1145/960116.54009},
 acmid = {54009},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ballance:1988:GAI:53990.54009,
 author = {Ballance, R. A. and Butcher, J. and Graham, S. L.},
 title = {Grammatical abstraction and incremental syntax analysis in a language-based editor},
 abstract = {Processors for programming languages and other formal languages typically use a concrete syntax to describe the user's view of a program and an abstract syntax to represent language structures internally. Grammatical abstraction is defined as a relationship between two context-free grammars. It formalizes the notion of one syntax being ``more abstract" than another. Two variants of abstraction are presented. Weak grammatical abstraction supports (i) the construction during LR parsing of an internal representation that is closely related to the abstract syntax and (ii) incremental LR parsing using that internal representation as its base. Strong grammatical abstraction tightens the correspondence so that top-down construction of incrementally-parsable internal representations is possible. These results arise from an investigation into language-based editing systems, but apply to any program that transforms a linguistic object from a representation in its concrete syntax to a representation in its abstract syntax or vice versa.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {185--198},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/53990.54009},
 doi = {http://doi.acm.org/10.1145/53990.54009},
 acmid = {54009},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pfenning:1988:HAS:960116.54010,
 author = {Pfenning, F. and Elliot, C.},
 title = {Higher-order abstract syntax},
 abstract = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {199--208},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54010},
 doi = {http://doi.acm.org/10.1145/960116.54010},
 acmid = {54010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pfenning:1988:HAS:53990.54010,
 author = {Pfenning, F. and Elliot, C.},
 title = {Higher-order abstract syntax},
 abstract = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {199--208},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54010},
 doi = {http://doi.acm.org/10.1145/53990.54010},
 acmid = {54010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Walz:1988:IEG:960116.54011,
 author = {Walz, J. A. and Johnson, G. F.},
 title = {Incremental evaluation for a general class of circular attribute grammars},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {209--221},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/960116.54011},
 doi = {http://doi.acm.org/10.1145/960116.54011},
 acmid = {54011},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Walz:1988:IEG:53990.54011,
 author = {Walz, J. A. and Johnson, G. F.},
 title = {Incremental evaluation for a general class of circular attribute grammars},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {209--221},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/53990.54011},
 doi = {http://doi.acm.org/10.1145/53990.54011},
 acmid = {54011},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pleban:1988:AGR:53990.54012,
 author = {Pleban, U. F. and Lee, P.},
 title = {An automatically generated, realistic compiler for imperative programming language},
 abstract = {We describe the automatic generation of a complete, realistic compiler from formal specifications of the syntax and semantics of Sol/C, a nontrivial imperative language ``sort of like C." The compiler exhibits a three pass structure, is efficient, and produces object programs whose performance characteristics compare favorably with those produced by commercially available compilers. To our knowledge, this is the first time that this has been accomplished.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/53990.54012},
 doi = {http://doi.acm.org/10.1145/53990.54012},
 acmid = {54012},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pleban:1988:AGR:960116.54012,
 author = {Pleban, U. F. and Lee, P.},
 title = {An automatically generated, realistic compiler for imperative programming language},
 abstract = {We describe the automatic generation of a complete, realistic compiler from formal specifications of the syntax and semantics of Sol/C, a nontrivial imperative language ``sort of like C." The compiler exhibits a three pass structure, is efficient, and produces object programs whose performance characteristics compare favorably with those produced by commercially available compilers. To our knowledge, this is the first time that this has been accomplished.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/960116.54012},
 doi = {http://doi.acm.org/10.1145/960116.54012},
 acmid = {54012},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Seshadri:1988:SAC:53990.54013,
 author = {Seshadri, V. and Weber, S. and Wortman, D. B. and Yu, C. P. and Small, I.},
 title = {Semantic analysis in a concurrent compiler},
 abstract = {Traditional compilers are usually sequential programs that serially process source programs through lexical analysis, syntax analysis, semantic analysis and code generation. The availability of multiprocessor computers has made it feasible to consider alternatives to this serial compilation process. The authors are currently engaged in a project to devise ways of structuring compilers so that they can take advantage of modern multiprocessor hardware. This paper is about the most difficult aspect of concurrent compilation: concurrent semantic analysis.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {233--240},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/53990.54013},
 doi = {http://doi.acm.org/10.1145/53990.54013},
 acmid = {54013},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Seshadri:1988:SAC:960116.54013,
 author = {Seshadri, V. and Weber, S. and Wortman, D. B. and Yu, C. P. and Small, I.},
 title = {Semantic analysis in a concurrent compiler},
 abstract = {Traditional compilers are usually sequential programs that serially process source programs through lexical analysis, syntax analysis, semantic analysis and code generation. The availability of multiprocessor computers has made it feasible to consider alternatives to this serial compilation process. The authors are currently engaged in a project to devise ways of structuring compilers so that they can take advantage of modern multiprocessor hardware. This paper is about the most difficult aspect of concurrent compilation: concurrent semantic analysis.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {233--240},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960116.54013},
 doi = {http://doi.acm.org/10.1145/960116.54013},
 acmid = {54013},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Allen:1988:CCV:53990.54014,
 author = {Allen, R. and Johnson, S.},
 title = {Compiling C for vectorization, parallelization, and inline expansion},
 abstract = {Practical implementations of real languages are often an excellent way of testing the applicability of theoretical principles. Many stresses and strains arise from fitting practicalities, such as performance and standard compatibility, to theoretical models and methods. These stresses and strains are valuable sources of new research and insight, as well as an oft-needed check on the egos of theoreticians.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {241--249},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/53990.54014},
 doi = {http://doi.acm.org/10.1145/53990.54014},
 acmid = {54014},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Allen:1988:CCV:960116.54014,
 author = {Allen, R. and Johnson, S.},
 title = {Compiling C for vectorization, parallelization, and inline expansion},
 abstract = {Practical implementations of real languages are often an excellent way of testing the applicability of theoretical principles. Many stresses and strains arise from fitting practicalities, such as performance and standard compatibility, to theoretical models and methods. These stresses and strains are valuable sources of new research and insight, as well as an oft-needed check on the egos of theoreticians.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {241--249},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960116.54014},
 doi = {http://doi.acm.org/10.1145/960116.54014},
 acmid = {54014},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reppy:1988:SOF:53990.54015,
 author = {Reppy, J. H.},
 title = {Synchronous operations as first-class values},
 abstract = {Synchronous message passing via channels is an interprocess communication (IPC) mechanism found in several concurrent languages, such as CSP, occam, and Amber. Such languages provide a powerful selective I/O operation, which plays a vital role in managing communication with multiple processes. Because the channel IPC mechanism is ``operation-oriented," only procedural abstraction techniques can be used in structuring the communication/synchronization aspects of a system. This has the unfortunate effect of restricting the use of selective I/O, which in turn limits the communication structure. We propose a new, ``value-oriented" approach to channel-based synchronization. We make synchronous operations first-class values, called events, in much the same way that functions are first-class values in functional programming languages. Our approach allows the use of data abstraction techniques for structuring IPC. We have incorporated events into PML, a concurrent functional programming language, and have implemented run-time support for them as part of the Pegasus system.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {250--259},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54015},
 doi = {http://doi.acm.org/10.1145/53990.54015},
 acmid = {54015},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reppy:1988:SOF:960116.54015,
 author = {Reppy, J. H.},
 title = {Synchronous operations as first-class values},
 abstract = {Synchronous message passing via channels is an interprocess communication (IPC) mechanism found in several concurrent languages, such as CSP, occam, and Amber. Such languages provide a powerful selective I/O operation, which plays a vital role in managing communication with multiple processes. Because the channel IPC mechanism is ``operation-oriented," only procedural abstraction techniques can be used in structuring the communication/synchronization aspects of a system. This has the unfortunate effect of restricting the use of selective I/O, which in turn limits the communication structure. We propose a new, ``value-oriented" approach to channel-based synchronization. We make synchronous operations first-class values, called events, in much the same way that functions are first-class values in functional programming languages. Our approach allows the use of data abstraction techniques for structuring IPC. We have incorporated events into PML, a concurrent functional programming language, and have implemented run-time support for them as part of the Pegasus system.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {250--259},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54015},
 doi = {http://doi.acm.org/10.1145/960116.54015},
 acmid = {54015},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Liskov:1988:PLS:53990.54016,
 author = {Liskov, B. and Shrira, L.},
 title = {Promises: linguistic support for efficient asynchronous procedure calls in distributed systems},
 abstract = {This paper deals with the integration of an efficient asynchronous remote procedure call mechanism into a programming language. It describes a new data type called a promise that was designed to support asynchronous calls. Promises allow a caller to run in parallel with a call and to pick up the results of the call, including any exceptions it raises, in a convenient and type-safe manner. The paper also discusses efficient composition of sequences of asynchronous calls to different locations in a network.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {260--267},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/53990.54016},
 doi = {http://doi.acm.org/10.1145/53990.54016},
 acmid = {54016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Liskov:1988:PLS:960116.54016,
 author = {Liskov, B. and Shrira, L.},
 title = {Promises: linguistic support for efficient asynchronous procedure calls in distributed systems},
 abstract = {This paper deals with the integration of an efficient asynchronous remote procedure call mechanism into a programming language. It describes a new data type called a promise that was designed to support asynchronous calls. Promises allow a caller to run in parallel with a call and to pick up the results of the call, including any exceptions it raises, in a convenient and type-safe manner. The paper also discusses efficient composition of sequences of asynchronous calls to different locations in a network.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {260--267},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960116.54016},
 doi = {http://doi.acm.org/10.1145/960116.54016},
 acmid = {54016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pallas:1988:MSC:53990.54017,
 author = {Pallas, J. and Ungar, D.},
 title = {Multiprocessor Smalltalk: a case study of a multiprocessor-based programming environment},
 abstract = {We have adapted an interactive programming system (Smalltalk) to a multiprocessor (the Firefly). The task was not as difficult as might be expected, thanks to the application of three basic strategies: serialization, replication, and reorganization. Serialization of access to resources disallows concurrent access. Replication provides multiple instances of resources when they cannot or should not be serialized. Reorganization allows us to restructure part of the system when the other two strategies cannot be applied.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54017},
 doi = {http://doi.acm.org/10.1145/53990.54017},
 acmid = {54017},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pallas:1988:MSC:960116.54017,
 author = {Pallas, J. and Ungar, D.},
 title = {Multiprocessor Smalltalk: a case study of a multiprocessor-based programming environment},
 abstract = {We have adapted an interactive programming system (Smalltalk) to a multiprocessor (the Firefly). The task was not as difficult as might be expected, thanks to the application of three basic strategies: serialization, replication, and reorganization. Serialization of access to resources disallows concurrent access. Replication provides multiple instances of resources when they cannot or should not be serialized. Reorganization allows us to restructure part of the system when the other two strategies cannot be applied.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {268--277},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54017},
 doi = {http://doi.acm.org/10.1145/960116.54017},
 acmid = {54017},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rose:1988:RTH:960116.54018,
 author = {Rose, J. R.},
 title = {Refined types: highly differentiated type systems and their use in the design of intermediate languages},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {278--287},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54018},
 doi = {http://doi.acm.org/10.1145/960116.54018},
 acmid = {54018},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rose:1988:RTH:53990.54018,
 author = {Rose, J. R.},
 title = {Refined types: highly differentiated type systems and their use in the design of intermediate languages},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {278--287},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54018},
 doi = {http://doi.acm.org/10.1145/53990.54018},
 acmid = {54018},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weiner:1988:PCP:53990.54019,
 author = {Weiner, J. L. and Ramakrishman, S.},
 title = {A piggy-back compiler for Prolog},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {288--296},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/53990.54019},
 doi = {http://doi.acm.org/10.1145/53990.54019},
 acmid = {54019},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weiner:1988:PCP:960116.54019,
 author = {Weiner, J. L. and Ramakrishman, S.},
 title = {A piggy-back compiler for Prolog},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {288--296},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960116.54019},
 doi = {http://doi.acm.org/10.1145/960116.54019},
 acmid = {54019},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Debray:1988:UTL:960116.54020,
 author = {Debray, S. K.},
 title = {Unfold/fold transformations and loop optimization of logic programs},
 abstract = {Programs typically spend much of their execution time in loops. This makes the generation of efficient code for loops essential for good performance. Loop optimization of logic programming languages is complicated by the fact that such languages lack the iterative constructs of traditional languages, and instead use recursion to express loops. In this paper, we examine the application of unfold/fold transformations to three kinds of loop optimization for logic programming languages: recursion removal, loop fusion and code motion out of loops. We describe simple unfold/fold transformation sequences for these optimizations that can be automated relatively easily. In the process, we show that the properties of unification and logical variables can sometimes be used to generalize, from traditional languages, the conditions under which these optimizations may be carried out. Our experience suggests that such source-level transformations may be used as an effective tool for the optimization of logic programs.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/960116.54020},
 doi = {http://doi.acm.org/10.1145/960116.54020},
 acmid = {54020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Debray:1988:UTL:53990.54020,
 author = {Debray, S. K.},
 title = {Unfold/fold transformations and loop optimization of logic programs},
 abstract = {Programs typically spend much of their execution time in loops. This makes the generation of efficient code for loops essential for good performance. Loop optimization of logic programming languages is complicated by the fact that such languages lack the iterative constructs of traditional languages, and instead use recursion to express loops. In this paper, we examine the application of unfold/fold transformations to three kinds of loop optimization for logic programming languages: recursion removal, loop fusion and code motion out of loops. We describe simple unfold/fold transformation sequences for these optimizations that can be automated relatively easily. In the process, we show that the properties of unification and logical variables can sometimes be used to generalize, from traditional languages, the conditions under which these optimizations may be carried out. Our experience suggests that such source-level transformations may be used as an effective tool for the optimization of logic programs.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/53990.54020},
 doi = {http://doi.acm.org/10.1145/53990.54020},
 acmid = {54020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aiken:1988:OLP:53990.54021,
 author = {Aiken, A. and Nicolau, A.},
 title = {Optimal loop parallelization},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {308--317},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54021},
 doi = {http://doi.acm.org/10.1145/53990.54021},
 acmid = {54021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aiken:1988:OLP:960116.54021,
 author = {Aiken, A. and Nicolau, A.},
 title = {Optimal loop parallelization},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {308--317},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54021},
 doi = {http://doi.acm.org/10.1145/960116.54021},
 acmid = {54021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lam:1988:SPE:960116.54022,
 author = {Lam, M.},
 title = {Software pipelining: an effective scheduling technique for VLIW machines},
 abstract = {This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {318--328},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/960116.54022},
 doi = {http://doi.acm.org/10.1145/960116.54022},
 acmid = {54022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lam:1988:SPE:53990.54022,
 author = {Lam, M.},
 title = {Software pipelining: an effective scheduling technique for VLIW machines},
 abstract = {This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {318--328},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/53990.54022},
 doi = {http://doi.acm.org/10.1145/53990.54022},
 acmid = {54022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Benitez:1988:PGO:960116.54023,
 author = {Benitez, M. E. and Davidson, J. W.},
 title = {A portable global optimizer and linker},
 abstract = {To reduce complexity and simplify their implementation, most compilers are organized as a set of passes or phases. Each phase performs a particular piece of the compilation process. In an optimizing compiler, the assignment of function and order of application of the phases is a critical part of the design. A particularly difficult problem is the arrangement of the code generation and optimization phases so as to avoid phase ordering problems caused by the interaction of the phases. In this paper, we discuss the implementation of a compiler/linker that has been designed to avoid these problems. The key aspect of this design is that the synthesis phases of the compiler and the system linker share the same intermediate program representation. This results in two benefits. It permits the synthesis phases of the compiler to be performed in any order and repeatedly, thus eliminating potential phase ordering problems. Second, it permits code selection to be invoked at any point during the synthesis phases as well as at link time. The ability to perform code selection at link time presents many opportunities for additional optimizations. Measurements about the effectiveness of using this approach in a C compiler on two different machines are presented.
},
 journal = {SIGPLAN Not.},
 volume = {23},
 issue = {7},
 month = {June},
 year = {1988},
 issn = {0362-1340},
 pages = {329--338},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960116.54023},
 doi = {http://doi.acm.org/10.1145/960116.54023},
 acmid = {54023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Benitez:1988:PGO:53990.54023,
 author = {Benitez, M. E. and Davidson, J. W.},
 title = {A portable global optimizer and linker},
 abstract = {To reduce complexity and simplify their implementation, most compilers are organized as a set of passes or phases. Each phase performs a particular piece of the compilation process. In an optimizing compiler, the assignment of function and order of application of the phases is a critical part of the design. A particularly difficult problem is the arrangement of the code generation and optimization phases so as to avoid phase ordering problems caused by the interaction of the phases. In this paper, we discuss the implementation of a compiler/linker that has been designed to avoid these problems. The key aspect of this design is that the synthesis phases of the compiler and the system linker share the same intermediate program representation. This results in two benefits. It permits the synthesis phases of the compiler to be performed in any order and repeatedly, thus eliminating potential phase ordering problems. Second, it permits code selection to be invoked at any point during the synthesis phases as well as at link time. The ability to perform code selection at link time presents many opportunities for additional optimizations. Measurements about the effectiveness of using this approach in a C compiler on two different machines are presented.
},
 booktitle = {Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
 series = {PLDI '88},
 year = {1988},
 isbn = {0-89791-269-1},
 location = {Atlanta, Georgia, United States},
 pages = {329--338},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/53990.54023},
 doi = {http://doi.acm.org/10.1145/53990.54023},
 acmid = {54023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

