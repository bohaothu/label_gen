%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography on threads & multithreading
% ------------------------------------------
% Last time modified : Tue Jul 11 09:49:08 MET DST 1995
%
% Maintained by:
%
%      Torsten Amundsen
%      Department of Informatics
%      University of Oslo, Norway
%      email: torstena at ifi.uio.no 
%      http:  http://www.ifi.uio.no/~torstena
%
% 
% Note:
%
%    Any additions, correction, comments etc is appreciated
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% These are the journal names pre-defined by 'bibtex'
% @string(acmcs = "ACM Computing Surveys")
% @string(acta = "Acta Informatica")
% @string(cacm = "Communications of the ACM")
% @string(ibmjrd = "IBM Journal of Research and Development")
% @string(ibmsj = "IBM Systems Journal")
% @string(ieeese = "IEEE Transactions on Software Engineering")
% @string(ieeetc = "IEEE Transactions on Computers")
% @string(ieeetcad = "IEEE Transactions on Computer-Aided Design of Integrated Circuits")
% @string(ipl = "Information Processing Letters")
% @string(jacm = "Journal of the ACM")
% @string(jcss = "Journal of Computer and System Sciences")
% @string(scp = "Science of Computer Programming")
% @string(sicomp = "SIAM Journal on Computing")
% @string(tocs = "ACM Transactions on Computer Systems")
% @string(tods = "ACM Transactions on Database Systems")
% @string(tog = "ACM Transactions on Graphics")
% @string(toms = "ACM Transactions on Mathematical Software")
% @string(toois = "ACM Transactions on Office Information Systems")
% @string(toplas = "ACM Transactions on Programming Languages and Systems")
% @string(tcs = "Theoretical Computer Science")

% add any other names here.  If you use one of them you must
% include this file as the first .bib file

@string(aap = "Advances in Applied Probability")
@string(acmosr = "ACM Operating Systems Review")
@string(aihp = "Annales de l'institut Henri Poincar\'{e}")
@string(am = "Advances in Mathematics")
@string(ap = "Annals of Probability")
@string(anap = "Annals of Applied Probability")
@string(bstj = "Bell System Technical Journal")
@string(cmmp = "Communications in Mathematical Physics")
@string(cmam = "Communications in Pure and Applied Mathematics")
@string(comb = "Combinatorica")
@string(cras = "Comptes Rendus de l'Acad\'emie des Sciences")
@string(cwe92 = "Proceedings of the 1992 ACM Workshop on Continuations")
@string(dam = "Discrete and Applied Mathematics")
@string(focs = "Proceedings Foundations of Computer Science")
@string(ieeecomp = "IEEE Computer")
@string(ieeemicro = "IEEE Micro")
@string(ieeepds = "IEEE Transactions on Parallel and Distributed Systems")
@string(ieeetac = "IEEE Transactions on Automatic Control")
@string(ieeeassp = "IEEE Transactions on Acoustics, Speech, and Signal Processing")
@string(ieeetcom = "IEEE Transactions on Communications")
@string(ieeetse = "IEEE Transactions on Software Engineering")
@string(ieeetit = "IEEE Transactions on Information Theory")
@string(ieeep = "Proceedings of the IEEE")
@string(jalg = "Journal of Algorithms")
@string(jap = "Journal  of Applied Probability")
@string(jct = "Journal of Combinatorial Theory")
@string(jfa = "Journal of Functional Analysis")
@string(jmaa = "Journal of Mathematical Analysis and Applications")
@string(jpal = "Journal of Parallel and Distributed Computing")
@string(jsup = "Journal of Supercomputing")
@string(jrss = "Journal of the Royal Statistical Society B")
@string(mor = "Mathematics of Operation Research")
@string(mst = "Mathematical Systems Theory")
@string(mm = "Microprocessing and Microprogramming")
@string(nn = "Neural Networks")
@string(orl = "Operations Research Letters")
@string(or = "Operations Research")
@string(parcomp = "Parallel Computing")
@string(peis = "Probability in the Engineering and Informational Sciences")
@string(prsl = "Proceedings of the Royal Society of London, series A")
@string(ptrf = "Probability Theory and Related Fields")
@string(rsa = "Random Structures and Algorithms")
@string(questa = "Queueing Systems, Theory and Applications")
@string(siamadm = "{SIAM} Journal on Algebraic and Discrete Methods")
@string(sigarch = "{ACM} {SIGARCH}")
@string(sigmetrics = "{ACM} {SIGMETRICS}")
@string(simath = "SIAM Journal on Applied Mathematics")
@string(siprob = "SIAM Journal on Theory of Probability and
		  Applications")
@string(spa = "Stochastic Processes and their Applications")
@string(spe = "Software--Practice and Experience")
@string(stoc = "Stochastics")
@string(tams = "Transactions of the AMS")
@string(zwt = "Zeitschrift fur Wahrscheinlichkeitstheorie und verw.
		  Geb.")
@string(rocquencourt = "Domaine de Voluceau, Rocquencourt B.P. 105
		  78153 Le Chesnay C\'edex France")
@string(inria = "{INRIA}")
@string(ucb = "University of California, Berkeley")

@string(proc = "Proceedings of the ")
@string(asplos = "International Conference on Architectural Support
		  for Programming Languages and Operating Systems ")
@string(asplos4 = proc # "4th " # asplos)
@string(asplos5 = proc # "5th " # asplos)
@string(asplos6 = proc # "6th " # asplos)
@string(canews = sigarch # " Computer Architecture News")
@string(compcon =  "IEEE International Conference, COMPCON")
@string(compcon89 = proc #  " 1989 " # compcon)
@string(compcon90 = proc #  " 1990 " # compcon)
@string(compcon92 = proc #  " 1992 " # compcon)
@string(compcon93 = proc #  " 1993 " # compcon)
@string(euromicro = "EUROMICRO. Symposium on Microprocessing and
		  Microprogramming.")
@string(euromicro87 = proc #  " 1987 " # euromicro)
@string(euromicro89 = proc #  " 1989 " # euromicro)
@string(euromicro91 = proc #  " 1991 " # euromicro)
@string(euromicro92 = proc #  " 1992 " # euromicro)
@string(euromicro94 = proc #  " 1994 " # euromicro)
@string(hicss = "Hawaii International Conference on System Sciences")
@string(hicss20 = proc #  " 20th " # hicss)
@string(hicss22 = proc #  " 22nd " # hicss)
@string(hicss23 = proc #  " 23th " # hicss)
@string(hicss24 = proc #  " 24th " # hicss)
@string(hicss25 = proc #  " 25th " # hicss)
@string(hicss26 = proc #  " 26th " # hicss)
@string(hicss27 = proc #  " 27th " # hicss)
@string(hicss28 = proc #  " 28th " # hicss)
@string(iccd= "International Conference on Computer Design")
@string(iccd91 = proc # "1991 " # iccd)
@string(iccd94 = proc # "1994 " # iccd)

@string(ics = "International Conference on Supercomputing")
@string(ics88 = proc # "1988 " # ics)
@string(ics90 = proc # "1990 " # ics)
@string(ics91 = proc # "1991 " # ics)
@string(ics92 = proc # "1992 " # ics)
@string(ics93 = proc # "1993 " # ics)
@string(icommcs = "International Conference on Measurement and
		  Modeling of Computer Systems")
@string(icommcs88 = proc # "1988 " # icommcs)
@string(icommcs89 = proc # "1989 " # icommcs)
@string(icommcs90 = proc # "1990 " # icommcs)
@string(icommcs91 = proc # "1991 " # icommcs)
@string(icpp = "International Conference on Parallel Processing")
@string(icpp78 = proc # "1978 " # icpp)
@string(icpp86 = proc # "1986 " # icpp)
@string(icpp88 = proc # "1988 " # icpp)
@string(icpp89 = proc # "1989 " # icpp)
@string(icpp91 = proc # "1991 " # icpp)
@string(icpp93 = proc # "1993 " # icpp)
@string(icpp95 = proc # "1995 " # icpp)
@string(isca = "Annual International Symposium on Computer
		  Architecture")
@string(isca7 = proc # "7th " # isca)
@string(isca10 = proc # "10th " # isca)
@string(isca12 = proc # "12th " # isca)
@string(isca13 = proc # "13th " # isca)
@string(isca14 = proc # "14th " # isca)
@string(isca15 = proc # "15th " # isca)
@string(isca16 = proc # "16th " # isca)
@string(isca17 = proc # "17th " # isca)
@string(isca18 = proc # "18th " # isca)
@string(isca19 = proc # "19th " # isca)
@string(isca20 = proc # "20th " # isca)
@string(isca21 = proc # "21th " # isca)
@string(isca22 = proc # "22th " # isca)
@string(perfrev = sigmetrics # " Performance Evaluation Review")
@string(icdcs = "International Conference on Distributed Computing Systems")
@string(icdcs9  = proc # "9th " # icdcs)
@string(icdcs10 = proc # "10th " # icdcs)
@string(icdcs15 = proc # "15th " # icdcs)
@string(ipps = "IEEE International Parallel Processing Symposium")
@string(ipps5 = proc # "5th " # ipps)
@string(ipps7 = proc # "7th " # ipps)
@string(ipps8 = proc # "8th " # ipps)
@string(ispdp = "IEEE Symposium on Parallel and Distributed Processing")
@string(ispdp2 = proc # "2rd " # ispdp)
@string(ispdp3 = proc # "3rd " # ispdp)
@string(ispdp4 = proc # "4th " # ispdp)
@string(ispdp5 = proc # "5th " # ispdp)
@string(isma = "Annual International Symposium on Microarchitecture")
@string(isma24 = proc # "24th " # isma # ". Micro 24")
@string(isma25 = proc # "25th " # isma # ". Micro 25")
@string(isma26 = proc # "26th " # isma # ". Micro 26")
@string(osdi = "Symposium on Operating Systems Design and
		  Implementation (OSDI) 1994")
@string(pact = "International Conference on Parallel Architectures and
		  Compilation Techniques")
@string(pact93 = proc # "1993 " # pact)
@string(pact94 = proc # "1994 " # pact)
@string(pact95 = proc # "1995 " # pact)
@string(parle = "PARLE, International conference on Parallel
		  Architectures and Languages Europe")
@string(parle91 = proc # "1991 " # parle)
@string(parle93 = proc # "1993 " # parle)
@string(parle94 = proc # "1994 " # parle)
@string(ppopp = " Symposium on Principles and Practice of Parallel
		  Programming")
@string(ppopp4 = proc # "4th" # ppopp)
@string(ppopp5 = proc # "5th" # ppopp)
@string(sosp = "ACM Symposium on Operating Systems Principle")
@string(sosp11 = proc # "11th " # sosp)
@string(sosp12 = proc # "12th " # sosp)
@string(sosp13 = proc # "13th " # sosp)
@string(sosp14 = proc # "14th " # sosp)
@string(supcomp88 = proc # "Supercomputing'88")
@string(usenix =   "USENIX Technical Conference and Exhibition")
@string(usenixs85 = proc # "Summer 1985 " # usenix)
@string(usenixw88 = proc # "Winter 1988 " # usenix)
@string(usenixs89 = proc # "Summer 1989 " # usenix)
@string(usenixw89 = proc # "Winter 1989 " # usenix)
@string(usenixs90 = proc # "Summer 1990 " # usenix)
@string(usenixw90 = proc # "Winter 1990 " # usenix)
@string(usenixw91 = proc # "Winter 1991 " # usenix)
@string(usenixw92 = proc # "Winter 1992 " # usenix)
@string(usenixs92 = proc # "Summer 1992 " # usenix)
@string(usenixw93 = proc # "Winter 1993 " # usenix)
@string(usenixw94 = proc # "Winter 1994 " # usenix)
@string(usenixs94 = proc # "Summer 1994 " # usenix)

@PhdThesis{Abeysekera92,
  author = 	 "Don C. Abeysekera",
  title = 	 "Performance of thread based distributed systems",
  school = 	 "University of Kent, Canterbury, UK, Department of
		  Computing",
  year = 	 "1992",
  url = 	 "ftp://unix.hensa.ac.uk/pub/misc/ukc.theses/comp.sci/theses/2-93.ps.Z",
  abstract =	 "The advances in the fields of digital communication
		  and workstation design have made a system of
		  workstations connected by a network a powerful
		  competitor to the traditional centralised mainframe
		  computer systems. Both processing power and
		  information can be distributed in such systems. As a
		  result they can support complex applications
		  efficiently, such as interactive multimedia
		  applications, in which information is essentially
		  distributed. However, the real-time nature of
		  multimedia applications places strict timing
		  requirements on the underlying system.  It is
		  therefore important for a distributed system to
		  support a low cost communication  paradigm and to
		  make efficient use of the  available processing
		  power of the system. This dissertation studies the
		  possible ways of achieving these two objectives and
		  thereby improving the ability to support interactive
		  and high bandwidth applications in distributed
		  systems satisfactorily. It reports detailed resource
		  cost breakdowns and statistics in order to identify
		  the expensive   and redundant elements within an
		  interaction. Based on these statistics, several
		  techniques for reducing the amount of processing in
		  the critical path of an RPC interaction were
		  designed and tested. In these techniques emphasis is
		  given to retaining the natural structure of the
		  applications in order to make distributed
		  programming easier."
}

@InProceedings{Acton92,
  author = 	 "Donald Acton and Gerald Neufeld",
  title = 	 "Controlling concurrent access to objects in the
		  Raven system",
  editor =	 "Luis Felipe Cabrera and Eric Jul",
  pages =	 "148--152",
  booktitle =	 "Proceedings of the Second International Workshop on
		  Object Orientation in Operating systems",
  year =	 "1992",
  publisher =	 "IEEE Comput. Soc. Press",
  address =	 "Dourdan, France",
  month =	 "Sep",
  url =  	 "ftp://ftp.cs.ubc.ca/pub/local/raven/iwooos92.ps.gz",
  abstract =	 "The paper presents the rationale and design of the
		  concurrency control features of Raven, an
		  object-oriented distributed and parallel programming
		  language and system. Raven's parallel constructs
		  support coarse grained parallelism and are designed
		  to permit programmers to use parallelism with
		  relative ease. To achieve this Raven provides
		  automatic concurrency control on an object's
		  instance data at method invocation time. Raven
		  allows multiple execution threads to access an
		  object strictly for reading or a single execution
		  thread to access an object for the updating of
		  instance data. Raven is operational on a variety of
		  machine architectures, including a shared memory
		  multiprocessor. Experience indicates Raven's
		  concurrency support simplifies the task of
		  converting sequential code to run in a parallel or
		  distributed environment."
}

@Article{Adeli89,
  author = 	 "Hojjat Adeli and O. Kamal",
  title = 	 "Parallel structural analysis using threads",
  journal =	 "Microcomputers in Civil Engineering",
  year =	 "1989",
  volume =	 "4",
  number =	 "2",
  pages =	 "133--146",
  month =	 "Jun",
  abstract =	 "Practical solution of the structural analysis
		  problem in a parallel processing environment is
		  investigated through the use of the notion of cheap
		  concurrency and the concept of threads. A thread is
		  a lightweight process or independent instructions
		  executing agent capable of concurrent execution with
		  other threads. Portions of a structural analysis
		  code implemented in C have been parallelized
		  employing the Encore Parallel Threads on an Encore
		  Multimax multiprocessor computer. The issues of
		  racing condition, synchronization and mapping are
		  considered and discussed. Two synchronization
		  mechanisms, semaphores and monitors, have been
		  employed and compared. Two different mapping
		  strategies have been implemented and studied.
		  Results are reported on the effect of amount and
		  frequency of shared memory access on the speed-up,
		  the overhead time required for creating threads, and
		  comparison of overall computational time performance
		  using two space truss examples."
}

@InProceedings{Adeli90,
  author = 	 "Hojjat Adeli and O. Kamal",
  title = 	 "Concurrent analysis of structures on shared memory
		  machines",
  pages =	 "338--343",
  booktitle =	 icdcs10,
  year =	 "1990",
  address =	 "Paris, France",
  month =	 "May",
  abstract =	 "A practical solution to the structural analysis
		  problem in a parallel processing environment is
		  investigated through the use of the notion of cheap
		  concurrency and the concept of thread. Portions of a
		  structural analysis code implemented in C are
		  parallelized using the encore parallel threads on an
		  encore multimax multiprocessor computer. The issues
		  of racing condition, synchronization, and mapping
		  are considered and discussed. Results are reported
		  on the effect of amount and frequency of shared
		  memory access on the speedup. A discussion of the
		  overhead time required for creating threads and a
		  comparison of the overall computational time
		  performance using two examples are presented."
}

@TechReport{Aditya95,
  author = 	 "Shail Aditya",
  title = 	 "Normalizing Strategies for Multithreaded
		  Interpretation and Compilation of Non-Strict
		  Languages",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1995",
  type =	 "Memo",
  number =	 "CSG-Memo-374",
  month =	 "May",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-374.ps",
  abstract =	 ""
}

@inproceedings{Agarwal90,
  title        = "APRIL, A Processor Architecture for Multiprocessing",
  author       = "Anant Agarwal and Beng-Hong Lim and David Kranz and
		  John Kubiatowicz",
  booktitle    = isca17,
  year         = 1990,
  month        = "May",
  pages        = "104-114",
  note         = "Published in " # canews # " Vol. 18, No.2. Also
		  as Tech report MIT/LCS/TM-450, Massachussets
		  Institute of Technology, Laboratory for Computer
		  Science, June 1991",
  note2        = "HAR",
  url =  	 "ftp://cag.lcs.mit.edu/pub/papers/isca-april.ps.Z",
  abstract     = "Processors in large-scale multiprocessors must be
		  able to tolerate large communication latencies and
		  synchronization delays. This paper describes the
		  architecture of a rapid-contex-switching processor
		  called APRIL with support for fine-grain threads and
		  synchronization. APRIL achieves high single-thread
		  performance and suppports virtual dynamic threads. A
		  commercial RISC-based implementation of APRIL and a
		  run-time software system that can switch contexts in
		  about 10 cycles is described. Measurements taken for
		  several parallel applications on an APRIL simulator
		  show that the overhead for supporting parallel tasks
		  based on futures is reduced by a factor of two over
		  a corresponding implementation on the Encore
		  Multimax. The scalability of a multiprocessor based
		  on APRIL is explored using a performance model. We
		  show that the SPARC-based implementation of APRIL
		  can achieve close to 80% processor utilization with
		  as few as three resident threads per processor in a
		  large-scale cache-based machine with an average base
		  network latency of 55 cycles."
}

@InProceedings{Agarwal91,
  author = 	 "Anant Agarwal and Geoffrey D'Souza and Kirk Johnson
		  and David Kranz and John Kubiatowicz and Kiyoshi
		  Kurihara and Beng-Hong Lim and Gino Maa and Daniel
		  Nussbaum and Mike Parkin and Donald Yeung",
  title = 	 "The {MIT} Alewife machine : A Large-Scale
		  Distributed-Memory Multiprocessor",
  booktitle =	 "Proceedings of Workshop on Scalable Shared Memory
		  Multiprocessors",
  year =	 "1991",
  publisher =	 "Kluwer Academic",
  note2 =	 "HAR",
  url =     "ftp://cag.lcs.mit.edu/pub/papers/mitalewife.ps.Z",
  abstract =	 "The Alewife multiprocessor project focuses on the
		  architecture and design of a large-scale parallel
		  machine. The machine uses a low-dimensional direct
		  interconnection network to provide scalable
		  communication bandwidth, while allowing the
		  exploitation of locality. Despite its
		  distributed-memory architecture, Alewife allows
		  efficient shared-memory programming through a
		  multilayered approach to locality management. A new
		  scalable cache-coherence scheme called LimitLESS
		  directories allows the use of caches for reducing
		  communication latency and network bandwidth
		  requirements. Alewife also employs run-time and
		  compile-time methods for partitioning and placement
		  of data and processes to enhance communication
		  locality. While the above methods attempt to
		  minimize communication latency, communication with
		  distant processors cannot be completely avoided.
		  Alewife's processor, Sparcle, is designed to
		  tolerate these latencies by rapidly switching
		  between threads of computation. This paper describes
		  the Alewife architecture and concentrates on the
		  novel hardware features of the machine including
		  LimitLESS directories and the
		  rapid-context-switching processor."
}

@article{Agarwal92,
  title        = "Performance Tradeoffs in Multithreaded Processors",
  author       = "Anant Agarwal",
  journal      = ieeepds,
  year         = "1992",
  month        = "Sep",
  volume       = "3",
  number       = "5",
  pages        = "525--539",
  note         = "Also as Tech report MIT/LCS/TR-501, Massachusets
		  Institute of Technology, Laboratory for Computer
		  Science, April 1991",
  note2        = "HAR",
  abstract     = "High network latencies in large-scale
		  multiprocessors can cause a significant drop in
		  processor utilization.  By maintaining multiple
		  process contexts in hardware and switching among
		  them in a few cycles, multithreaded processors can
		  overlap computation with memory accesses and reduce
		  processor idle time.  This paper presents an
		  analytical performance model for multithreaded
		  processors that includes cache interference, network
		  contention, context-switching overhead, and
		  data-sharing effects.  The model is validated
		  through our own simulations and by comparison with
		  previously published simulation results.  Our
		  results indicate that processors can substantially
		  benefit from multithreading, even in systems with
		  small caches.  Large caches yield close to full
		  processor utilization with as few as two to four
		  contexts, while small caches may require up to four
		  times as many contexts.  Increased network
		  contention due to multithreading has a major effect
		  on performance. The available network bandwidth and
		  the context-switching overhead limits the best
		  possible utilization"
}

@article{Agarwal93,
  title        = "Sparcle: An Evolutionary Processor Design for
		  Large-Scale Multiprocessors",
  author       = "Anant Agarwal and John Kubiatowicz and David Kranz
		  and Beng-Hong Lim and Donald Yeoung and Geoffrey
		  D'Souza and M. Parkin",
  journal      = ieeemicro,
  year         = 1993,
  month        = "Jun",
  volume       = "13",
  number       = "3",
  pages        = "48--61",
  url = 	 "ftp://cag.lcs.mit.edu/pub/papers/sparcle.ps.Z",
  note2        = "HAR",
  abstract     = "Sparcle is a processor chip developed jointly by
		  MIT, LSI Logic, and SUN Microsystems, by evolving an
		  existing RISC architecture towards a processor
		  suited for large-scale multiprocessors. Sparcle
		  supports three multiprocessor mechanisms: fast
		  context switching, fast user-level message handling,
		  and fine-grain synchronization. The Sparcle effort
		  demonstrates that RISC architectures coupled with a
		  communications and memory management unit do not
		  require major architectural changes to support
		  multiprocessing efficiently."
}

@InProceedings{Agarwal95,
  author = 	 "Anant Agarwal and Ricardo Bianchini and David
		  Chaiken and Kirk L. Johnson and David Kranz and John
		  Kubiatowicz and Beng-Jong Lim and Kenneth Mackenzie
		  and Donald Yeung",
  title = 	 "The MIT Alewife Machine: Architecture and
		  Performance",
  pages =	 "??--??",
  booktitle =	 isca22,
  year =	 "1995",
  abstract =	 "Alewife is a multiprocessor architecture that
		  supports up to 512 processing nodes connected over a
		  scalable and cost-effective mesh network at a
		  constant cost per node.  The MIT Alewife machine, a
		  prototype implementation of the architecture,
		  demonstrates that a parallel system can be both
		  scalable and programmable.  Four mechanisms combine
		  to achieve these goals: software-extended coherent
		  shared memory provides a global, linear address
		  space; integrated message passing allows compiler
		  and operating system designers to provide efficient
		  communication and synchronization; support for
		  fine-grain computation allows many processors to
		  cooperate on small problem sizes; and latency
		  tolerance mechanisms -- including block
		  multithreading and prefetching -- mask unavoidable
		  delays due to communication; Microbenchmarks,
		  together with over a dozen complete applications
		  running on the 32-node prototype, help to analyze
		  the behavior of the system.  Analysis shows that
		  integrating message passing with shared memory
		  enables a cost-efficient solution to the cache
		  coherence problem and provides a rich set of
		  programming primitives.  Block multithreading and
		  prefetching improve performance by up to 25%
		  individually, and 35% together.  Finally language
		  constructs that allow programmers to express
		  fine-grain synchronization can improve performance
		  by over a factor of two."
}

@InProceedings{Akerholt92,
 author =        "G. Akerholt and K. Hammond and Simon Peyton Jones
		  and P. Trinder",
 editor =        "R. Heldal and C. K. Holst and P. L. Wadler",
 title =         "A Parallel Functional Database on {GRIP}",
 booktitle =     "Functional Programming, Glasgow 1991: Proceedings of
		  the 1991 Workshop, Portree, GB",
 pages =         "1--24",
 series =	 "Lecture Notes in Computer Science ???",
 publisher =     "Springer-Verlag",
 address =       "Berlin, DE",
 year =          "1992",
 note =          "ISBN : 3-540-19760-5",
 abstract =      "GRIP is a shared-memory multiprocessor designed for
		  efficient parallel evaluation of functional
		  languages, using compiled graph reduction. This
		  paper investigates the feasibility of processing
		  persistent data on GRIP, and presents results
		  obtained from a pilot implementation. A database
		  implemented in a pure functional language must be
		  modified non-destructively, i.e. the original
		  database must be preserved and a new copy
		  constructed. The naive implementation provides
		  evidence for the feasibility of data processing in
		  the form of modest real-time speed-ups, and
		  acceptable real-time performance. The functional
		  database is also used to investigate the GRIP
		  architecture, compared with an idealised
		  machine. The particular features investigated are
		  the thread-creation costs and caching of GRIP's
		  distributed memory."
}

@InProceedings{Alfieri94,
  author = 	 "R. A. Alfieri",
  title = 	 "An efficient kernel-based implementation of POSIX
		  threads",
  pages =	 "59--72",
  booktitle =	 usenixs94,
  year =	 "1994",
  month =	 "Jun",
  abstract =	 "This paper describes the kernel-based implementation
		  of POSIX Threads (Pthreads) in the DG/UX operating
		  system. The implementation achieves time efficiency
		  by using a general-purpose trap mechanism, known as
		  a Kernel Function Call (KFC), that carries an order
		  of magnitude less overhead than a traditional system
		  call. On a 50 MHz Motorola MC88110, the
		  implementation can create and exit a thread (with
		  the associated context switch) in 8.1 microseconds
		  and yield to another thread in 4.0 microseconds. The
		  implementation also achieves space efficiency by
		  paging and decoupling bulky data structures. The
		  advantages of a kernel-based implementation include
		  design simplicity, less code redundancy,
		  optimization of global (interprocess) operations,
		  avoidance of inopportune preemption, and global
		  semantic flexibility. The disadvantage is a
		  monolithic design that lacks user-level
		  flexibility."
}

@InProceedings{Alkalaj91,
  author = 	 "Leon Alkalaj and Rajeendra V. Bopanna",
  title = 	 "Performance of multi-threaded execution in a
		  shared-memory multiprocessor",
  pages =	 "330--333",
  booktitle =	 ispdp3,
  year =	 "1991",
  address =	 "Dallas, TX, USA",
  month =	 "Dec",
  abstract =	 "The authors propose a general analytic model for the
		  overlapped execution of thread management operations
		  in shared-memory multiprocessors. Performance
		  measures as speedup, utilization and network
		  contention are evaluated and verified using
		  simulations. It is assumed that the execution of
		  each thread consists of sequence of single-cycle
		  instructions, interleaved with four high-level
		  thread management instructions: create, activate,
		  suspend and halt (CASH). Each processor (also called
		  the CASH processor) in the multiprocessor is
		  specialized for the efficient execution of the CASH
		  thread execution model. This is achieved by
		  overlapping the execution of all thread management
		  operations in a separate thread management unit,
		  that executes concurrently with the thread execution
		  unit. The execution time of the thread management
		  operations is reduced by increasing the thread
		  memory bandwidth, and by storing multiple thread
		  contexts in a thread cache located on the
		  processor."
}

@InProceedings{Alkalaj92,
  author = 	 "Leon Alkalaj and Rajeendra V. Boppana",
  title = 	 "An analytic model of multi-threaded execution in a
		  shared-memory multiprocessor",
  editor =	 "W. Joosen and E. Milgrom",
  pages =	 "108--122",
  booktitle =	 "Parallel Computing: From Theory to Sound Practice.
		  Proceedings of EWPC '92, the European Workshops on
		  Parallel Computing",
  year =	 "1992",
  publisher =	 "IOS Press",
  address =	 "Barcelona, Spain",
  month =	 "Mar",
  note2 =	 "HAR",
  abstract =	 "Using fine-grain threads of computation as the unit
		  of concurrency rather than coarse-grain processes
		  has been proposed as a way to increase the processor
		  utilization. This is achieved by effectively
		  overlapping long latency operations in a thread,
		  with the continuous execution of an alternate ready
		  thread. The paper proposes an analytic model that
		  describes the overlapped execution of thread
		  management operations in a multi-threaded shared
		  memory multiprocessor."
}

@InProceedings{Alverson90,
  author =       "Robert Alverson and David Callahan and Daniel
		  Cummings and Brian Koblenz and Allan Porterfield and
		  Burton Smith",
  title =        "The {Tera} Computer System",
  booktitle =    ics90,
  year =         "1990",
  month =        "Sep",
  pages =        "1--6",
  note =         "Published in " # canews # " Vol. 18, No.3",
  note2 =        "HAR",
  url =     "ftp://www.net-serve.com/tera/arch.ps.gz",
  abstract =     "Interesting architecture. 3-d mesh of pipelined
		  packet-switch nodes, e.g., 16x16x16 is 4096 nodes,
		  with 256 procs, 512 memory units, 256 I/O cache
		  units, and 256 I/O processors attached. 2816
		  remaining nodes are just switching nodes. Each
		  processor is 64-bit custom chip with up to 128
		  simultaneous threads in execution. It alternates
		  between ready threads, with a deep pipeline.
		  Inter-instruction dependencies explicitly encoded by
		  the compiler, stalling those threads until the
		  appropriate time. Each thread has a complete set of
		  registers! Memory units have 4-bit tags on each
		  word, for full/empty and trap bits. Shared memory
		  across the network: NUMA."
}

@InBook{Alverson94,
  author = 	 "Gail Alverson and Bob Alverson and David Callahan
		  and Brian Koblenz and Allan Porterfield and Burton
		  Smith",
  title = 	 "Multithreaded computer architecture : a summary of
		  the state of the art",
  chapter = 	 "Integrated support for heterogeneous parallelism",
  pages =	 "??--??",
  publisher = 	 "Kluwer Academic",
  year = 	 "1994",
  note =	 "ISBN: 0792394771",
  abstract =	 ""
}

@Unpublished{Alverson95a,
  author = 	 "Gail Alverson and Simon Kahan and Richard Korry",
  title = 	 "Processor Management in the Tera MTA Computer
		  System",
  note2 = 	 "HAR",
  year =	 "1995",
  url = 	 "http://www.tera.com/SC95/proc-mgmt/proc-mgmt.html",
  abstract =	 "This paper describes the processor scheduling issues
		  specific to the Tera MTA (Multi Threaded
		  Architecture) computer system and presents solutions
		  to classic scheduling problems. The Tera MTA
		  exploits parallelism at all levels, from
		  fine-grained instruction-level parallelism within a
		  single processor to parallel programming across
		  processors, to multiprogramming among several
		  applications simultaneously. Consequently, processor
		  scheduling occurs at many levels, and managing these
		  levels poses unique and challenging scheduling
		  concerns. We describe the processor scheduling
		  algorithms of the user level runtime and operating
		  system and describe the issues relevant to each.
		  Many of the issues encountered and solutions
		  proposed are novel, given the multithreaded,
		  multiprogrammed nature of our architecture."
}

@Unpublished{Alverson95b,
  author = 	 "Gail Alverson and Simon Kahan and Richard Korry and
		  Cathy McCann and Burton J. Smith",
  title = 	 "Scheduling on the Tera MTA",
  year =	 "1995",
  note2 = 	 "HAR",
  url = 	 "ftp://www.net-serve.com/tera/scheduling.ps.gz",
  abstract =	 ""
}

@InProceedings{Amamiya87,
  author = 	 "Makoto Amamiya",
  title = 	 "Data flow computing and parallel reduction machine",
  editor =	 "van de Riet, R.P.",
  pages =	 "127--141",
  booktitle =	 "Frontiers in Computing. Proceedings of the
		  International Conference",
  year =	 "1987",
  publisher =	 "North-Holland, Amsterdam, Netherlands",
  month =	 "Dec",
  abstract = 	 "A parallel graph reduction model and its
		  implementation in relation to a data flow computing
		  scheme is discussed. First, a parallel graph
		  reduction mechanism and its relation to data flow
		  computing are discussed. The data flow computing
		  model which introduces the by-reference concept, is
		  shown to be a natural implementation of the parallel
		  graph reduction model. Then, a practical
		  implementation of a parallel reduction machine is
		  presented. In the implementation, a cell token flow
		  model is proposed. By using the cell token flow
		  concept, a given flow program can be transformed to
		  an efficient multi-thread control flow program. An
		  overview of the machine architecture is also
		  presented."
}

@InProceedings{Amamiya88,
  author = 	 "Makoto Amamiya",
  title = 	 "Datarol processor: an ultra-multi-processing
		  architecture for massively parallel computing",
  editor =	 "M. Cosnard and M. H. Barton and M. Vanneschi",
  pages =	 "79--92",
  booktitle =	 "Parallel Processing. Proceedings of the IFIP WG 10.3
		  Working Conference",
  year =	 "1988",
  publisher =	 "North-Holland, Amsterdam, Netherlands",
  address =	 "Pisa, Italy",
  month =	 "Apr",
  abstract =	 "An ultra-multi-processing facility is needed for
		  implementing the massively parallel functional
		  language machine, in which a large amount of fine
		  grain processes is executed in a highly concurrent
		  way. This paper proposes a machine architecture for
		  the ultra-multi-processing. The machine performs
		  parallel execution along a multi-thread control
		  flow, which is called datarol. First, the datarol
		  concept is discussed in comparison with the data
		  flow model. Then, a method to extract datarol
		  program from a functional program is described
		  through a dependency analysis. And last, a datarol
		  machine architecture is described. The datarol
		  processor, which is a circular pipeline system,
		  offers a parallel execution mechanism for
		  ultra-multi-processing based on the
		  continuation-based execution control mechanism."
}

@InProceedings{Amamiya90,
  author = 	 "Makoto Amamyia",
  title = 	 "Datarol: a massively parallel architecture for
		  functional languages",
  pages =	 "726--735",
  booktitle =	 ispdp2,
  year =	 "1990",
  address =	 "Dallas, TX, USA",
  month =	 "Dec",
  abstract =	 "Proposes a parallel machine architecture which
		  incorporates an ultra-multiprocessing facility for
		  parallel execution of functional programs. The
		  machine performs parallel executions along a
		  multi-thread control flow called datarol. A datarol
		  program, instead of using a program counter, the
		  instructions to be executed next are explicitly
		  specified in the preceding instructions. The
		  explicitly specified continuation linkage enables
		  the concurrent execution of the instructions of
		  different function instances, as well as the
		  parallel execution of multi-thread control flow
		  within a function instance. Based on a
		  continuation-based execution model, the datarol
		  processor is designed to implement an efficient
		  parallel execution mechanism needed for
		  ultra-multi-processing. First, the datarol concept
		  is discussed in comparison with a dataflow model.
		  Next, the datarol machine architecture and datarol
		  processor design are described. Finally, the
		  evaluation of the datarol architecture is shown."
}

@InProceedings{Amamiya93,
  author = 	 "Makoto Amamiya",
  title = 	 "A design principle of massively parallel
		  distributed-memory multiprocessor architecture",
  editor =	 "C. K. Yuen and A. Yonezawa",
  pages =	 "102--123",
  booktitle =	 "Proceedings of a JSPS Seminar. Parallel Programming
		  Systems",
  year =	 "1993",
  publisher =	 "World Scientific, Singapore",
  address =	 "Tokyo, Japan",
  month =	 "May",
  abstract =	 "The design principle of a massively parallel
		  distributed-memory architecture is discussed, and
		  the Datarol-II architecture, which meets this
		  principle, is proposed. The Datarol-II processor
		  design is described. Datarol-II inherits its highly
		  concurrent execution mechanism from the original
		  Datarol architecture, which offers high throughput
		  computation of multiple threads due to its
		  continuation-based ultra-multiprocessing mechanism.
		  In addition to this efficient multi-thread execution
		  Dataroll-II can also perform high speed single
		  thread execution by introducing the short-cycle
		  RISC-type execution mechanism."
}

@InProceedings{Anderson89,
  author =       "Thomas E. Anderson and Edward D. Lazowska and Henry
		  M. Levy",
  title =        "The Performance Implications of Thread Management
		  Alternatives for Shared-Memory Multiprocessors",
  booktitle =    icommcs89,
  pages =        "49--60",
  month =        "May",
  year =         "1989",
  note =         "Published in " # perfrev # " Vol 17, No.1. Also
		  published in " # ieeetc # " Vol.38, No.12,
		  pp.1631--1644, Dec 89.  Also available as tech
		  report UW-CSE-88-09-04, University of Washington",
  note2    =     "HAR",
  abstract =     "Threads ({"}lightweight{"} processes) have become a
		  common element of new languages and operating
		  systems. This paper examines the performance
		  implications of several data structure and algorithm
		  alternatives for thread management in shared-memory
		  multiprocessors. Both experimental measurements and
		  analytical model projections are presented. For
		  applications with fine-grained parallelism, small
		  differences in thread management are shown to have
		  significant performance impact, often posing a
		  tradeoff between throughput and latency.
		  Per-processor data structures can be used to improve
		  throughput, and in some circumstances to avoid
		  locking, improving latency as well. The method used
		  by processors to queue for locks is also shown to
		  affect performance significantly. Normal methods of
		  critical resource waiting can substantially degrade
		  performance with moderate numbers of waiting
		  processors. We present an Ethernet-style backoff
		  algorithm that largely eliminates this effect."
}

@InProceedings{Anderson91a,
  author =       "Thomas E. Anderson and Brian N. Bershad and Edward
		  D. Lazowska and Henry M. Levy",
  title =        "Scheduler Activations: effective kernel support for the
		  user-level management of parallelism",
  booktitle =    sosp13,
  addresse =     "Asilomar, Pacific Grove, CA",
  publisher =    "ACM SIGOPS",
  month =        oct,
  year =         "1991",
  pages =        "95--109",
  note     =     "Published in " # acmosr # " Vol.25, No.5, 1991. Also
		  published " # tocs # " Vol.10 No.1, pp.53--70, Feb 92",
  note2     =    "HAR",
  abstract =     "Threads can be supported either by the operating
		  system kernel or by user-level library code in the
		  appliation address space, but neither approach has
		  been fully satisfactory. This paper addresses this
		  dilemma. First, we argue that the performance of
		  kernel threads is inherently worse than that of
		  user-level threads, rather than this being an
		  artifact of existing implementations; we thus argue
		  that managing parallelism at the user level is
		  essential to high-performance parallel computing.
		  Next, we argue that the lack of system integration
		  exhibited by user-level threads is a consequence of
		  the lack of kernel support for user-level threads
		  provided by contemporary multiporcessor operating
		  systems; we thus argue that kernel threads or
		  processes, as currently conceived, ar the wrong
		  abstraction on which to support user-level
		  management of parallelism. Finally, we describe the
		  design, imploementation, and performance of a new
		  kernel interface and user-level thread package that
		  together provide the same functionality as kernel
		  threads without compromising the performance and
		  flexibility advantages of user-level management of
		  parallelism."

}

@PhdThesis{Anderson91b,
  author =       "Thomas Edward Anderson",
  title =        "Operating System Support for High Performance
		  Multiprocessing",
  school =       "University of Washington, Department of Computer
		  Science and Engineering",
  note =         "Also as tech report UW-CSE-91-08-10",
  year =         "1991",
  abstract =     "This dissertation concerns operating system support
		  for high performance parallel applications on
		  shared-memory multiprocessors. A primary motivation
		  behind building multiprocessors is to increase a
		  system's peak processing power at relatively low
		  cost. Multiprocessors lose their advantage, however,
		  if this added processing power is not effectively
		  utilized, and it has proven difficult in practice to
		  get good performance from parallel applications. In
		  this dissertation, I argue that this is in part due
		  to poor support for high performance parallel
		  programs in traditional operating systems, and I
		  describe techniques that address some of the
		  problems that have limited multiprocessor system
		  performance. First, I examine the performance
		  implications of several data structure and algorithm
		  alternatives for thread management. I show that for
		  applications with fine-grained parallelism, small
		  differences in thread management have a significant
		  performance impact, often posing a tradeoff between
		  throughput and latency. Per-processor data
		  structures can be used to improve throughput, and in
		  some circumstances to improve atency as
		  well. Second, I show that normal methods of spin (or
		  ``busy``) waiting for critical resources can
		  substantially degrade the performance of processors
		  doing useful work. I present two methods that
		  largely eliminate this effect: an Ethernet-style
		  backoff algorithm and a novel method of explicitly
		  queueing spinning processors in software. Third, I
		  argue that traditional operating system kernels do
		  not appropriately support high performance thread
		  management, yielding either inefficient threads or
		  poor integration of threads with multiprogramming,
		  I/O, and virtual memory. I describe a new kernel
		  interface and user-level thread package that
		  together provide high performance threads that
		  behave correctly in the presence of these
		  services. Fourth, even with an efficient and
		  correctly behaving thread system, initial
		  implementations of parallel programs typically yield
		  disappointing performance. I describe a parallel
		  program tuning tool and novel metric, normalized
		  processor time, that together efficiently capture
		  what is important to parallel performance. The
		  viability and performance of these techniques have
		  been demonstrated by implementations on the Sequent
		  Symmetry and DEC Systems Research Center Firefly
		  multiprocessors."
}

@InProceedings{Anderson91c,
  author = 	 "Thomas E. Anderson and Henry M. Levy and Brian B.
		  Bershad  and Edward D. Lazowska",
  title = 	 "The interaction of architecture and operating system
		  design",
  pages =	 "108--120",
  booktitle =	 asplos4,
  year =	 "1991",
  address =	 "Santa Clara, CA, USA",
  month =	 "Apr",
  note =	 "Published in SIGPLAN Notices Vol.26, No.4, Apr 1991.
		  Also as tech report  90-08-01,  University of
		  Washington. Dept. of Computer Science",
  note2        = "HAR",
  url =     "http://http.cs.berkeley.edu/~tea/interact.ps",
  abstract =	 "Examines recent directions in computer architecture
		  and operating systems, and the implications of
		  changes in each domain for the other. The
		  requirements of three components of operating system
		  design are discussed in detail: interprocess
		  communication, virtual memory, and thread
		  management. For each component, the authors relate
		  operating system functional and performance needs to
		  the mechanisms available on commercial RISC
		  architectures such as the MIPS R2000 and R3000, Sun
		  SPARC, IBM RS6000, Motorola 88000, and Intel i860.
		  The analysis reveals a number of specific reasons
		  why the performance of operating system primitives
		  on RISCs has not scaled with integer performance. In
		  addition, the authors identify areas in which
		  architectures could better accommodate operating
		  system needs, and areas in which operating system
		  design could accommodate certain necessary
		  characteristics of cost-effective high-performance
		  microprocessors."
}

@TechReport{Ang94,
  author = 	 "Boon Seong Ang and Arvind and Derek Chiou",
  title = 	 "StarT the next generation: integrating global caches
		  and dataflow architecture",
  institution =  "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1994",
  number =	 "CSG-memo-354",
  month =	 "Feb",
  note =	 "Proceedings of the ISCA 1992 Dataflow Workshop,
		  Hamilton Island, Australia ",
  note2 =	 "HAR",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-354.ps.gz",
  abstract =	 "The implicitly parallel programming model provides
		  an attractive approach to deal with the complexity
		  of parallel programming. Implementing this model
		  efficiently, especially on stock processors, remains
		  a big challenge, partly because of the fine
		  granularity of the parallelism exploited. The
		  Monsoon[27] project was designed to address and
		  investigate support for fine-grain parallelism, and
		  has yielded very encouraging results[13]. Our
		  experience with Monsoon and *T[24, 28], a followup
		  project after Monsoon, suggests that provision for
		  global shared memory is an area where both the
		  Monsoon and *T architectures can be improved.
		  Starting with the split-phase approach used in
		  Monsoon and *T, we propose to augment global memory
		  access by including coherent global caches. The
		  rapid improvements in stock microprocessors, and the
		  high cost and effort required to develop a
		  competitive microprocessor, presents practical
		  constraints on what can be built in any experimental
		  architecture project. We propose a machine that
		  attempts to include as many of the desired features
		  as possible within the constraint of using a stock
		  microprocessor. This machine will allow us to
		  continue research into the dataflow approach to
		  parallel computing as well as provide a prototype of
		  a commercial product."
}

@Article{Appel89,
  author =       "Andrew W. Appel",
  title =        "Allocation without Locking",
  journal =      spe,
  volume =       "19",
  number =       "7",
  month =        jul,
  year =         "1989",
  pages =        "703--705",
  note =         "Also appeared as technical report, Princeton
		  University {CS-TR-182-88}, sep. 1988",
  url =     "http://www.cs.Princeton.EDU/faculty/appel/papers/182.ps",
  abstract =     "In a programming environment with both concurrency
		  and automatic garbage collection, the allocation and
		  initialization of a new record is a sensitive
		  matter. Parallel implementations usually use a
		  locking or semaphore mechanism to ensure that
		  allocation is an atomic operation. This locking
		  significantly adds to the cost of an
		  allocation. This paper shows how allocation can run
		  extremely quickly even in a multi-thread
		  environment: open-coded, without locking. Key idea
		  is that the allocation instruction sequence is
		  highly stylized, so check when doing a garbage
		  collection if a thread is suspended in the middle of
		  an allocation, and complete it on the thread's
		  behalf."
}

@InProceedings{Aral89,
  author = 	 "Ziya Aral and Ilya Gertner and Greg Schaffer and
		  Alan Langerman",
  title = 	 "Variable Weight Processes with Flexible Resources",
  pages =	 "??--??",
  booktitle =	 usenixw89,
  year =	 "1989",
  abstract =	 ""
}

@InProceedings{Aral91,
  author = 	 "Ziya Aral and Ilya Gertner and Alan Langerman and
		  Dave Mitchell",
  title = 	 "Process Control Structures for Multiprocessor",
  pages =	 "49--58",
  booktitle =	 hicss24,
  year =	 "1991",
  volume =       "1",
  abstract =	 "Describes a new approach to implementing and using a
		  'process' abstraction in multiprocessors. A new
		  paradigm is described for virtual machines which are
		  built out of finer granularity units called Resource
		  Control Blocks (RCB). A collection of RCBs define a
		  virtual machine. Each RCB is independently sharable.
		  A user has the freedom to define a set of virtual
		  machines each spanning a different collection of
		  resources. Although the new paradigm offers
		  additional flexibility, it carries no additional
		  overhead when compared to previous implementations.
		  Backwards compatible processes are scheduled and
		  created with the same efficiency. New processes that
		  carry less weight are created much more efficiently.
		  A production system has been implemented and
		  measured on Multimax, a shared-memory
		  multiprocessor."
}

@MastersThesis{Archer82,
  author = 	 "David W. Archer",
  title = 	 "MISP: A Multiple Instruction-Stream Shared-Pipeline
		  Microprocessor",
  school = 	 "University of Illinois at Urbana-Champaign, Center
		  for Reliable and High-Performance Computing",
  year = 	 "1982",
  note =	 "Also as tech report CSG-9, Center for Reliable and
		  High-Performance Computing",
  abstract =	 ""
}

@InProceedings{Armand89,
  author = 	 "Francois Armand and Frederic Herrmann and Jim
		  Lipkis and Marc Rozier",
  title = 	 "Multi-threaded Processes in {CHORUS/MiX}",
  pages =	 "1--14",
  booktitle =	 "Proceedings of Spring 1990 EUUG Conference, Munich,
		  Germany",
  year =	 "1990",
  organization = "EUUG",
  month =	 "Apr",
  note =	 "Also Chorus tech report CS-TR-89-37",
  note2        = "HAR",
  url = 	 "ftp://opera.chorus.fr/pub/chorus-reports/CS-TR-89-37.ps.Z",
  abstract =	 "Interest in concurrent programming has spurred
		  development of 'threads', or 'lightweight
		  processes', as an operating system paradigm.
		  Unix-based systems have been especially affected by
		  this trend because the smallest unit of CPU
		  scheduling in Unix, the process, is a rich and
		  expensive software entity with a private memory
		  address space. The authors examine performance
		  constraints affecting concurrent program, including
		  real-time applications, in order to understand and
		  evaluate the demand for a new scheduling model.
		  Although performance criteria differ sharply among
		  various application domains, they conclude that a
		  single thread model can provide efficient concurrent
		  execution in a general-purpose operating system.
		  They describe the design considerations behind the
		  thread-management facilities of CHORUS/MIX, a
		  Unix-compatible operating system built for
		  distributed, real-time, and parallel computing.
		  CHORUS/MIX adopts novel approaches for signal
		  handling and other Unix facilities so as to ensure a
		  smooth transition from sequential to concurrent
		  semantics in applications."
}

@TechReport{Arvind80,
  author = 	 "Arvind and Vinod Kathail and Keshav Pingali",
  title = 	 "A Data-flow architecture with Tagged-token",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1980",
  number =	 "TR-174",
  abstract =	 ""
}

@InProceedings{Arvind87,
  author = 	 "Arvind and Robert A. Iannucci",
  title = 	 "Two Fundamental Issues In Multiprocessing",
  year = 	 "1987",
  booktitle =    "DFVLR Conf. on Parallel Processing in Science and
		  Engineering",
  pages =        "61--88",
  month =	 "June",
  series =	 "Lecture Notes in Computer Science 295",
  publisher  =   "Springer-Verlag",
  note =	 "Also as Tech report MIT/LCS/TM-330, Massachusetts
		  Institute of Technology, Laboratory for Computer
		  Science. 24 pages. " # 
                 "Reproduced in ``Selected Reprints on Dataflow and
		  Reduction Architectures'' ed. S. S. Thakkar, IEEE,
		  1987, pp. 140-164. Reproduced in David J. Lilja's
		  (ed.) Architectural Alternatives for Exploiting
		  Parallelism, IEEE Press, Los Alamitos, CA, 1991,
		  page 184-.",
  note2        = "HAR",
  abstract = 	 "A general purpose multiprocessor should be scalable,
		  i.e, show higher performance when more hardware
		  resources are added to the machine. ARchitects of
		  such multiprocessors must address the loss in
		  processor efficiency due to two fundamental issues:
		  long memory latencies and waits due to
		  synchronization events. It is argued that a well
		  designed processor can overcome these losses
		  provided there is sufficient parallelism in the
		  program being executed. The detrimental effect of
		  long latency can be reduced by instruction
		  pipelining, however the restriction of a single
		  thread of computation in von Neumann processors
		  severely limits their ability to have more than a
		  few instructions in the pipeline. Furthermore,
		  techniques to reduce the memory latency tend to
		  increase the cost of task switching. The cost of
		  synchronization events in von Neumann machines makes
		  decomposing a program into very small tasks
		  counter-productive . Dataflow machines, on the other
		  hand, treat each instruction as a task, and by
		  paying a small synchronization cost for each
		  instruction executed, offer the ultimate flexibility
		  in scheduling instructions to reduce processor idle
		  time"
}

@InProceedings{Arvind88a,
  author = 	 "Arvind and David E. Culler and G.K. Maa",
  title = 	 "Assessing the benefits of fine-grain parallelism in
		  dataflow programs",
  pages =	 "60--69",
  booktitle =	 "Proceedings. Supercomputing '88",
  year =	 "1988",
  month =	 "Nov",
  note =	 "Also appears in International Journal of
		  Supercomputer Applications, Vol.2 No.3, pp.10--36,
		  1988",
  abstract =	 "A method for assessing the benefits of fine-grain
		  parallelism in actual programs is presented. The
		  method is based on parallelism profiles and speedup
		  curves derived by executing dataflow graphs on an
		  interpreter under progressively more realistic
		  assumptions about processor resources and
		  communication costs. It is shown that programs, even
		  using traditional algorithms, exhibit ample
		  parallelism when parallelism is exposed at all
		  levels. Since only dataflow graphs compiled from the
		  high-level language Id are considered, the bias
		  introduced by the language and the compiler is
		  examined. A method of estimating speedup through
		  analysis of the ideal parallelism profile is
		  developed, avoiding repeated execution of programs.
		  It is shown that the fine-grain parallelism can be
		  used to mask large, unpredictable memory latency and
		  synchronization waits in architectures using
		  dataflow instruction execution mechanisms. The
		  effects of grouping portions of dataflow programs,
		  such as function invocations or loop iterations, and
		  requiring that the operators in a group execute on a
		  single processor, are explored."
}

@InProceedings{Arvind88b,
  author = 	 "Arvind and David E. Culler and Kattamuri Ekanadham",
  title = 	 "The price of asynchronous parallelism: an analysis
		  of dataflow architectures",
  pages =	 "541--555",
  booktitle =	 "Proceedings of CONPAR 88",
  year =	 "1988",
  month =	 "Sep",
  abstract =	 "A cost analysis is presented for the dataflow model
		  of execution. Comparisons are drawn with the
		  execution costs of equivalent imperative programs on
		  traditional sequential and parallel machines. The
		  cost of program execution is measured in terms of
		  the number of instructions executed. It is argued
		  that the complexity of dataflow instructions is
		  comparable to that of traditional load/store
		  architectures and that pipelining is very effective
		  for dataflow instructions; thus, instruction counts
		  provide a meaningful comparison."
}

@TechReport{Arvind89a,
  author = 	 "Arvind and Rishiyur S. Nikhil",
  title = 	 "A Dataflow approach to general-purpose parallel
		  computing",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1989",
  number =	 "CSG Memo 302",
  month =	 "Jul",
  abstract =	 ""
}

@Article{Arvind89b,
  author = 	 "Arvind and Rishiyur S. Nikhil and Keshav K. Pingali",
  title = 	 "I-Structures: Data structures for parallel
		  computing",
  journal =	 toplas,
  year =	 "1989",
  volume =	 "11",
  number =	 "4",
  pages =	 "598--632",
  month =	 "Oct",
  abstract =	 ""
}

@Article{Arvind90a,
  author = 	 "Arvind and Rishiyur S. Nikhil",
  title = 	 "Executing a program on the MIT tagged-token dataflow
		  architecture", 
  journal =	 ieeetc,
  year =	 "1990",
  volume =	 "39",
  number =	 "3",
  pages =	 "300--318",
  month =	 "Mar",
  note =	 "Also appears in Proceedings of PARLE87. Parallel
		  Architectures and Languages Europe.pp.1--29, vol.2",
  abstract =	 "The MIT Tagged-Token Dataflow Project has an
		  unconventional, but integrated approach to
		  general-purpose high-performance parallel computing.
		  Rather than extending conventional sequential
		  languages, Id, a high-level language with
		  fine-grained parallelism and determinacy implicit in
		  its operational semantics, is used. Id programs are
		  compiled to dynamic dataflow graphs, which
		  constitute a parallel machine language. Dataflow
		  graphs are directly executed on the MIT tagged-token
		  dataglow architecture (TTDA), a multiprocessor
		  architecture. An overview of current thinking on
		  dataflow architecture is provided by describing
		  example Id programs, their compilation to dataflow
		  graphs, and their execution on the TTDA. Related
		  work and the status of the project are described."
}

@InProceedings{Arvind90b,
  author = 	 "Arvind and Stephen A. Brobst",
  title = 	 "The Evolution of Dataflow Architectures from Static
		  Dataflow to P-RISC",
  booktitle  =   "Proceedings of Workshop on Massive
		  Parallelism, Oct. 1989, Academic Press 1990",
  year = 	 "1990",
  note =	 "Also appears in International Journal of High Speed
		  Computing, Vol.5, No.2, pp.125--153, 1993. Also as Tech
		  report CSG-Memo-316,Aug.1990 Massachussets Institute
		  of Technology, Laboratory for Computer Science ",
  abstract =	 "The authors trace the evolution of dataflow
		  architectures since their origin. They identify the
		  most significant milestones in this evolution and
		  give examples of the underlying theme to these
		  important advances. They discuss potential pitfalls
		  in designing a fundamentally new architecture. They
		  examine the origins of dataflow computing with
		  static dataflow computers. They trace the evolution
		  of data structures in the context of dataflow
		  architectures. The evolutionary steps in the
		  development of dynamic dataflow architectures are
		  presented. The dataflow/von Neumann hybrid
		  architecture, P-RISC (for parallel RISC), is
		  introduced as a new phase of evolution in dataflow
		  architectures. They conclude with a prognosis of
		  dataflow architectures."
}

@Article{Arvind9?,
  author = 	 "Arvind and Kyoo-Chan Cho and Christoper Hill and R.
		  Paul Johnson and John Marshall",
  title = 	 " A Comparison of Implicitly Parallel Multithreaded
		  and Data Parallel Implementations of an Ocean Model
		  based on the Navier-Stokes Equations",
  journal =	 "Journal of Functional Programming",
  year =	 "19??",
  volume =	 "??",
  number =	 "??",
  pages =	 "??--??",
  month =	 "??",
  abstract =	 ""
}

@InProceedings{Assenmacher93,
  author =       "Holger Assenmacher and Thomas Breitbach and Peter
		  Buhler and Volker H{\"u}bsch and Reinhard Schwarz",
  editor =       "O. M. Nierstrasz",
  title =        "{PANDA} -- Supporting Distributed Programming in
		  {C}++",
  booktitle =    "Proceedings of the 7th ECOOP '93",
  series =       "LNCS 707",
  pages =        "361--383",
  publisher =    "Springer-Verlag",
  address =      "Kaiserslautern, Germany",
  month =        jul,
  year =         "1993",
  url =     "ftp://ftp.uni-kl.de/reports_uni-kl/computer_science/system_software/1993/papers/ECOOP93.ps.gz",
  abstract =     "PANDA is a run-time package based on a very small
		  operating system kernel which supports distributed
		  applications written in C++. It provides powerful
		  abstractions such as very efficient user-level
		  threads, a uniform global address space, object and
		  thread mobility, garbage collection, and persistent
		  objects. The paper discusses the design rationales
		  underlying the PANDA system. The fundamental
		  features of PANDA are surveyed, and their
		  implementation in the current prototype environment
		  is outlined."
}

@InProceedings{Aude91,
  author = 	 "J. S. Aude and A. J. O. Cruz and A. C. Pacheco and
		  A. M. Meslin and G. Bronstein and G. P. Azevedo and
		  N. R. Figueira",
  title = 	 "{MULTIPLUS}: a modular high-performance multiprocessor",
  pages =	 "45--52",
  booktitle =	 euromicro91,
  year =	 "1991",
  address =	 "Vienna, Austria",
  month =	 "Sep",
  note =	 "Published in Microprocessing {\&} Microprogramming",
  abstract =	 "The MULTIPLUS project which is currently under
		  development at NCE/UFRJ, Brazil, aims at the study
		  of parallel processing problems in MIMD
		  environments. The project includes the development
		  of a parallel shared-memory architecture and a
		  UNIX(+)-like operating system called MULPLIX. The
		  MULTIPLUS architecture uses an inverted n-cube
		  multistage network to interconnect clusters of
		  processing nodes designed around a double-bus
		  system. As a consequence, the architecture is
		  partitionable and modular. It can easily and
		  efficiently support configurations ranging from
		  workstations to powerful parallel supercomputers
		  with up to 2048 processing nodes. The MULPLIX
		  operating system provides MULTIPLUS with an
		  efficient computing environment for parallel
		  scientific applications. MULPLIX uses the concept of
		  threads, implements busy-waiting synchronization
		  primitives very efficiently and carefully considers
		  data locality and scientific processing requirements
		  in the policies adopted for memory management and
		  thread scheduling."
}

@InProceedings{Baker94,
  author = 	 "Theodore P. Baker and Frank Mueller and Viresh
		  Rustagi",
  title = 	 "Experience with a prototype of the {POSIX} 'minimal
		  realtime system profile",
  pages =	 "12--16",
  booktitle =	 "Proceedings 11th IEEE Workshop on Real-Time
		  Operating Systems and Software. RTOSS '94",
  year =	 "1994",
  address =	 "Seattle, WA, USA",
  month =	 "May",
  url = 	 "http://www.informatik.hu-berlin.de/~mueller/ftp/pub/PART/rtoss94.ps.Z",
  abstract =	 "This paper describes experience in prototyping the
		  proposed IEEE standard 'Minimal Realtime System
		  Profile', whose primary component is support for
		  real-time threads. It provides some background,
		  describes the implementation, and reports
		  preliminary performance measurements."
}

@Article{Baker95,
  author = 	 "Henry G. Baker",
  title = 	 "'Use-once' variables and linear objects-storage
		  management, reflection and multi-threading",
  journal =	 "SIGPLAN Notices",
  year =	 "1995",
  volume =	 "30",
  number =	 "1",
  pages =	 "45--52",
  month =	 "Jan",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.netcom.com/pub/hbaker/",
  abstract = 	 "Programming languages should have 'use-once'
		  variables in addition to the usual 'multiple-use'
		  variables. 'Use-once' variables are bound to linear
		  (unshared, unaliased, or singly-referenced) objects.
		  Linear objects are cheap to access and manage,
		  because they require no synchronization or tracing
		  garbage collection. Linear objects can elegantly and
		  efficiently solve otherwise difficult problems of
		  functional/mostly-functional systems-e.g., in-place
		  updating and the efficient initialization of
		  functional objects. Use-once variables are ideal for
		  directly manipulating resources reflective
		  languages. A 'use-once' variable must be dynamically
		  referenced exactly once within its scope.
		  Unreferenced use-once variables must be explicitly
		  killed, and multiply-referenced use-once variables
		  must be explicitly copied; this duplication and
		  deletion is subject to the constraint that some
		  linear data types do not support duplication and
		  deletion methods. Use-once variables are bound only
		  to linear objects, which may reference other linear
		  or non-linear objects. Non-linear objects can
		  reference other non-linear objects, but can
		  reference a linear object only in a way that ensures
		  mutual exclusion. Although implementations have long
		  had implicit use-once variables and linear objects,
		  most languages do not provide the programmer any
		  help for their utilization. For example, use-once
		  variables allow for the safe/controlled use of
		  reified language implementation objects like
		  single-use continuations. Linear objects and
		  use-once variables map elegantly into dataflow
		  models of concurrent computation, and the graphical
		  representations of dataflow models make an appealing
		  visual linear programming language."
}

@TechReport{Barbou92,
  author = 	 "Francois Barbou Des Places and Phillipe Bernadaat
		  and Michael Condict and Sylvie Empereur and Jacques
		  Febvre and David George and James Loveluck and
		  Eamonn McManus and Simon Patience and Jose Rogado
		  and Patrick Roudaud",
  title = 	 "Architecture and Benefits of a Multithreaded OSF/1
		  Server", 
  institution =  "OSF Research institute",
  year = 	 "1992",
  type =	 "Memo",
  number =	 "??",
  address =	 "Grenoble, France",
  month =	 "Apr",
  url =     "http://www.gr.osf.org:8001/general/os_papers/Svr_Benefits.92.04.21.ps",
  abstract =	 ""
}

@InProceedings{Barth91,
  author = 	 "Paul S. Barth and Rishiyur S. Nikhil and Arvind",
  title = 	 "M-Structures: Extending a parallel, non-strict,
		  functional language with state",
  pages =	 "538--568",
  booktitle =	 "Proc. 5th Conf. on functional Prog. Languages and
		  Computer Architecture",
  year =	 "1991",
  month =	 "Aug",
  series =	 "Lecture Notes in Computer Science ???",
  note =	 "Also as CSG-Memo-327, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  url =     "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-327.ps.gz",
  abstract =	 "Functional programs are widely appreciated for being
		  'declarative' and implicitly parallel. However, when
		  modeling state, both properties are compromised
		  because the state variables must be threaded through
		  most functions. Further, state updates may require
		  much copying. A solution is to introduce
		  assignments, as in ML and Scheme; however, for
		  meaningful semantics, they resort to strict,
		  sequential evaluation. The authors present an
		  alternative approach called M-structures, which are
		  imperative data structures with implicit
		  synchronization. M-structures are added to Id, a
		  parallel nonstrict functional language. They argue
		  that some programs with M-structures improve on
		  their functional counterparts in three ways: they
		  are more declarative; more parallel; and more
		  storage efficient. Two problems are studied:
		  histogramming a tree, and graph traversal, comparing
		  functional and M-structure solutions."
}

@InProceedings{Barton-Davis93,
  author =       "P. Barton-Davis and Dylan McNamee and Raj Vaswani
		  and Edward D. Lazowska",
  title =        "Adding Scheduler Activations to Mach 3.0",
  booktitle =    "Proceedings of the USENIX Mach III Symposium",
  month =        "Apr",
  pages =        "119--136",
  year =         "1993",
  note =         "Also as Tech report UW-CSE-92-08-03, University
		  of Washington, Department of Computer Science and
		  Engineering",
  url  =   "ftp://ftp.cs.washington.edu/tr/1992/08/UW-CSE-92-08-03.PS.Z",
  abstract =     "When user-level threads are built on top of
		  traditional kernel threads, they can exhibit poor
		  performance or even incorrect behavior in the face
		  of blocking kernel operations such as I/O, page
		  faults, and processor preemption. This problem can
		  be solved by building user-level threads on top of a
		  new kernel entity, the scheduler activation. The
		  goal of the effort described in this paper was to
		  implement scheduler activations in the Mach 3.0
		  operating system. This is an {"}implementation{"}
		  paper rather than a {"}concepts{"} paper: we outline
		  the design decisions made, the kernel modifications
		  required, and our additions to the CThreads thread
		  library to take advantage of the new kernel
		  structure."
}

@InProceedings{Barton88,
  author =       "J. M. Barton and J. C. Wagner",
  title =        "Beyond Threads: Resource Sharing in {UNIX}",
  booktitle =    usenixw88,
  pages =        "259--266",
  publisher =    "USENIX",
  address =      "Dallas, TX",
  year =         "1988",
}

@InProceedings{Bastiaens94a,
  author = 	 "Konrad Bastiaens and I. Lemahieu and P. Desmedt",
  title = 	 "On the use of a Multi-threaded Operating System for
		  an efficient Parallel Implementation of the ML-EM
		  Algorithm for PET Image Reconstruction",
  pages =	 "31--39",
  booktitle =	 "Proc. of the iFIP WG10.3 Working Conf.on
		  Applications in Parallel and Distributed Computing",
  year =	 "1994",
  month =	 "Apr",
  note =	 "Appears in IFIP Transactions A , Vol.A-44, 1994",
  abstract =	 "Multi-threaded operating systems were introduced in
		  the quest to reduce the overhead caused by task
		  manipulation and synchronization. An example is the
		  recently introduced Solaris 2.2 multi-threaded
		  operating system. In this paper, a parallel
		  application in the domain of positron emission
		  tomography (PET) image reconstruction is presented,
		  for which a successful use of the multi-threaded
		  approach has led to an implementation, with a nearly
		  linear speedup, of the maximum likelihood
		  expectation maximalization (ML-EM) algorithm."
}

@InProceedings{Bastiaens94b,
  author = 	 "Koenraad Bastiaens and I. Lemahieu and P. Desmedt and W.
		  Vandermeersch",
  title = 	 "An Efficient Parallel Implementation of the ML-EM
		  Algorithm for PET Image Reconstruction with a
		  Multi-Threaded Operating System",
  pages =	 "253--259",
  booktitle =	 "Proceedings of the 2nd Euromicro Workshop on
		  Parallel and Distributed Processing",
  year =	 "1994",
  abstract =	 ""
}

@InProceedings{Beckerle93,
  author = 	 "Michael J. Beckerle",
  title = 	 "Overview of the START(*T) multithreaded computer",
  pages =	 "148--156",
  booktitle =	 compcon93,
  year =	 "1993",
  note2 =	 "HAR",
  abstract =	 "*T is a scalable computer architecture designed to
		  support a broad variety of parallel programming
		  styles including those which use multithreading to
		  tolerate the increases in memory latency which occur
		  as the machine size is scaled up. The hardware uses
		  a customized RISC microprocessor with a network
		  messaging interface and synchronization unit tightly
		  coupled into the processor architecture. This
		  hardware is coupled with a high performance network
		  having a fat-tree topology with high cross-section
		  bandwidth. In addition a HIPPI I/O system provides
		  access to the world of supercomputer-class I/O
		  devices. An innovativevve software architecture is
		  layered on this hardware to produce a working system
		  which is at its foundation a UNIX-like software
		  environment. The software architecture specifies the
		  structure of parallel programs and the way that they
		  can be controlled and debugged"
}

@TechReport{Bellosa94,
  author = 	 "Frank Bellosa",
  title = 	 "Techniques for building a fast threads package on
		  NUMA architectures",
  institution =  "University of Erlangen-Nuernberg, Germany, IMMD IV",
  year = 	 "1994",
  number =	 "Report TR-I4-94-06",
  month =	 "Feb",
  url = 	 "ftp://ftp.uni-erlangen.de/pub/papers/immd4/TR/TR-I4-94-16.ps.Z",
  abstract =	 ""
}

@InProceedings{Bemmerl92,
  author = 	 "Thomas Bemmerl",
  title = 	 "{TOPSYS} for programming distributed multiprocessor
		  computing environments",
  editor =	 "P. Dewilde and J. Vandewalle",
  pages =	 "175--180",
  booktitle =	 "CompEuro 1992 Proceedings. Computer Systems and
		  Software Engineering",
  year =	 "1992",
  address =	 "Hague, Netherlands",
  month =	 "May",
  abstract =	 "An overview is given of the TOPSYS (tools for
		  parallel systems) environment as an example of an
		  integrated tool environment supporting distributed
		  multiprocessor computing environments consisting of
		  distributed multiprocessors and networks of
		  workstations. The system so far consists of the
		  following components: the MMK message passing
		  multithreaded programming model provided as a
		  programming library; the distributed and parallel
		  debugger DETOP; the performance analyzer PATOP; the
		  visualization tool VISTOP for animation of thread
		  interaction; an application transparent dynamic load
		  balancing component; and the software package
		  ALLOCCUBE for managing the space-sharing multiuser
		  access to distributed memory multiprocessors in a
		  network-based environment. The most important
		  feature of the TOPSYS environment is its location
		  transparency. This means that all the tools provide
		  their services independent of the location of
		  processes in the distributed multiprocessor. The
		  architecture and the functionality of the MMK and
		  the TOPSYS tool environment are described."
}

@InProceedings{Berbers90,
  author = 	 "Yolande Berbers and Pierre Verbaeten",
  title = 	 "Servers, processes and subprocesses: a critical
		  evaluation",
  pages =	 "118--125",
  booktitle =	 "Proceedings of the 5th Jerusalem Conference on
		  Information Technology (JCIT). Next Decade in
		  Information Technology",
  year =	 "1990",
  address =	 "Jerusalem, Israel",
  month =	 "Oct",
  abstract =	 "Distributed operating systems are often structured
		  in a server-oriented way, with some system tasks
		  being performed by server processes. An extended
		  analysis of the servers' structure is provided along
		  with its implications for the communication system.
		  Small examples, which start from simple servers and
		  become gradually more complex, illustrate the
		  problems encountered when servers are written using
		  a classical sequential process structure. Processes
		  with several threads of control that execute the
		  same code are used to overcome the problems
		  encountered. The Hermix distributed operating system
		  is used as an example to illustrate the use of such
		  a process structure."
}

@InProceedings{Bernabeu88,
  author = 	 "J. M. Bernabeu-Auban and Phillip W. Hutto and Yousef
		  M. Khalidi and Mustaque Ahamad and William F. Appelbe
		  and Partas Dasgupta and Richard J. LeBlanc",
  title = 	 "Clouds - a distributed, object-based operating system
		  architecture and kernel implementation",
  pages =	 "25--37",
  booktitle =	 "Proceedings of the Autumn 1988 EUUG Conference",
  year =	 "1988",
  publisher =	 "Eur. UNIX Syst. User Group",
  month =	 "Oct",
  abstract =	 "Clouds is a native operating system intended for
		  large, heterogeneous hardware environments
		  consisting of inter-networked workstations,
		  computer-servers and data-servers (file-servers).
		  The authors intend for Clouds and UNIX to coexist
		  cooperatively, each system benefiting from the
		  other's advantages. A new kernel for the Clouds
		  operating system called the Ra kernel has recently
		  been completed. Ra provides three primitives,
		  segments, virtual spaces, and lightweight processes
		  called isibas, which can be composed in various ways
		  to construct components of the Clouds operating
		  system. The paper describes the architecture and
		  organisation of the Ra kernel and details of its
		  implementation. They sketch the implementation of
		  Clouds services (objects, threads, distributed
		  shared memory, etc.) using Ra primitives to
		  demonstrate the versatility and power of the Ra
		  kernel. These constructions use system objects and
		  kernel classes, two novel features of Ra. Finally,
		  they discuss experiences of using an object-oriented
		  language (C++) to build a distributed, object-based
		  operating system kernel that is both portable and
		  minimal."
}

@InProceedings{Bernabeu89,
  author = 	 "J. M. Bernabeu-Auban and Phillip W. Hutto and Yousef M.
		  Khalidi and Mustaque Ahamad and William F. Appelbe
		  and Partas Dasgupta and Richard J. LeBlanc",
  title = 	 "The architecture of Ra: a kernel for Clouds",
  volume =	 "II",
  pages =	 "936-945",
  booktitle =	 hicss22,
  year =	 "1989",
  publisher =	 "IEEE Comput. Soc. Press",
  address =	 "Kailua-Kona, HI, USA",
  month =	 "Jan",
  note =	 "Also Georgia Tech Tech Report [GIT-ICS-88/25]",
  url = 	 "ftp://helios.cc.gatech.edu/pub/papers/arch.ps.Z",
  abstract =	 "Ra is a native, minimal kernel for the Clouds
		  distributed operating system. Ra is a successor to
		  the prototype Clouds kernel and reflects lessons
		  learned from the earlier implementation effort. Ra
		  supports the same object-thread model as the
		  original Clouds kernel as a special case and
		  introduces extensibility as a major goal. Ra
		  provides three primitives, namely segments, virtual
		  spaces, and lightweight processes called isibas,
		  which can be composed in various ways to construct
		  components of the Clouds operating system. The
		  architecture and organization of Ra and details of
		  its implementation are described. The authors
		  describe the implementation of several Clouds
		  components such as objects and threads using Ra
		  primitives to demonstrate the versatility and power
		  of the Ra kernel. Ra supports plug-in system objects
		  that allow system services to be introduced and
		  removed dynamically. Ra has been implemented in C++
		  and is running on Sun-3 workstations."
}

@Article{Berrington93,
  author = 	 "Neil Berrington and Peter Broadbery and David De
		  Roure and Julian Padget",
  title = 	 "{EULISP} threads: a concurrency toolbox",
  journal =	 "LISP and Symbolic Computation",
  year =	 "1993",
  volume =	 "6",
  number =	 "1--2",
  pages =	 "177-199",
  month =	 "Aug",
  url = 	 "ftp://ftp.bath.ac.uk/pub/eulisp/threads.ps.gz",
  abstract =	 "Many current high level languages have been designed
		  with support for concurrency in mind, providing
		  constructs for the programmer to build explicit
		  parallelism into a program. The EULISP threads
		  mechanism, in conjunction with locks, and a generic
		  event waiting operation provides a set of primitive
		  tools with which such concurrency abstractions can
		  be constructed. The object system (Telos) provides a
		  powerful approach to building and controlling these
		  abstractions. The authors provide a synopsis of this
		  'concurrency toolbox', and demonstrate the
		  construction of a number of established abstractions
		  using the facilities of EULISP: pcall, futures,
		  stack groups, channels, CSP and Linda"
}

@InProceedings{Bershad89,
  author =       "Brian N. Bershad and Thomas E. Anderson and Edward
		  D. Lazowska and Henry M. Levy",
  title =        "Lightweight remote procedure call",
  booktitle  =   sosp12,
  conflocation = "Litchfield Park, AZ, 3--6 December 1989",
  month =       "Dec",
  pages =        "102--13",
  year =         "1989",
  note =         "Published in " # acmosr # " Vol.23 No.5, 1989",
  note2        = "HAR",
  abstract =     "Lightweight Remote Procedure Call (LRPC) is a
		  communication facility designed and optimized for
		  communication between protection domains on the same
		  machine. In contemporary small-kernel operating
		  systems, existing RPC systems incur an unnecessarily
		  high cost when used for the type of communication
		  that predominates --- between protection domains on
		  the same machine. This cost leads system designers
		  to coalesce weakly-related subsystems into the same
		  protection domain, trading safety for
		  performance. By reducing the overhead of
		  same-machine communication, LRPC encourages both
		  safety and performance. LRPC combines the control
		  transfer and communication model of capability
		  systems with the programming semantics and
		  large-grained protection model of RPC. LRPC achieves
		  a factor of three performance improvement over more
		  traditional approaches based on independent threads
		  exchanging messages, reducing the cost of
		  same-machine communication to nearly the lower bound
		  imposed by conventional hardware. LRPC has been
		  integrated into the Taos operating system of the DEC
		  SRC Firefly multiprocessor workstation."
}

@Article{Bershad91,
  author =       "Brian N. Bershad and Thomas E. Anderson and
		  Edward. D. Lazowska and Henry M. Levy",
  title =        "User-Level Interprocess Communication for Shared
		  Memory Multiprocessors",
  journal =      tocs,
  volume =       "9",
  number =       "2",
  pages =        "175--198",
  month =        may,
  year =         "1991",
  note =         "Also as tech report 90-05-07, University of
		  Washington. Dept. of Computer Science",
  note2     =    "HAR",
  abstract =     "Interprocess communication (IPC), in particular
		  IPC oriented towards local communication (between
		  address spaces on the same machine), has become
		  central to the design of contemporary operating
		  systems, IPC has traditionally been the
		  responsibility of the kernel, but kernel-based IPC
		  has two inherent problems. First, its performance is
		  architecturally limited by the cost of invoking the
		  kernel and reallocating a processor from one address
		  space to another. Second, applications that need
		  inexpensive threads and must provide their own
		  thread management encounter functional and
		  performance problems stemming from the interaction
		  between kernel-level communication and user-level
		  thread management. On a shared memory
		  multiprocessor, these problems can be solved by
		  moving the communication facilities out of the
		  kernel and supporting them at the user level within
		  each address space. Communication performance is
		  improved since kernel invocation and processor
		  reallocation can be avoided when communicating
		  between address spaces on the same machine. These
		  observations motivated user-level remote procedure
		  call (URPC). URPC combines a fast cross-address
		  space communication protocol using shared memory
		  with lightweight threads managed at the user level.
		  This structure allows the kernel to be bypassed
		  during cross-address space communication. The
		  programmer sees threads and RPC through a
		  conventional interface, though with unconventional
		  performance."
}

@TechReport{Bic86,
  author = 	 "Lubomir Bic",
  title = 	 "A Process-Oriented Model for Efficient Execution of
		  Dataflow Programs",
  institution =  "University of California at Irvine, Department of
		  Information and Computer Science",
  year = 	 "1986",
  number =	 "UCI ICS-TR-86-23",
  month =	 "Nov",
  abstract =	 "In a dataflow program, an instruction is enabled
		  whenever all of its operands have been produced; at
		  that time, the instruction packet is eligible for
		  execution by a free processor. Compared to a von
		  Neumann computer, the major sources of overhead are
		  (1) the need for matching of token destined for the
		  same instruction, (2) routing of tokens among
		  processors, and (3) the fact that instructions are
		  scheduled for execution individually, one at a time.
		  In this paper, we present an execution model that
		  reduces much of this overhead. A dataflow program is
		  broken into sequences of instructions thatmust be
		  executed sequentially due to their data
		  dependencies. Each sequence is loaded into execution
		  memory as a whole, were it forms a very simple
		  process.A processor is then multiplexed among the
		  ready processes in its local memory. The states of
		  these processes change between running, ready, and
		  blocked, depending on the arrival of operands. The
		  main advantage is that operands produced and
		  consumed within the same sequence are store directly
		  in one memory operation, thus bypassing the token
		  matching and routing units. Consequently, when
		  executing highly sequential programs, the dataflow
		  machine ``degenerates'' to an efficient von Neumann
		  computer."
}

@TechReport{Birrell89,
  author =       "Andrew D. Birrell",
  title =        "An Introduction to Programming with Threads.",
  institution =  "Digital Equipment Corporation, Systems Research
		  Centre",
  number =       "35",
  pages =        "35 pages.",
  month =        "6 " # jan,
  year =         "1989",
  note2  =       "HAR",
 url =  "ftp://ftp.digital.com/pub/DEC/SRC/research-reports/SRC-035.ps.Z",
  abstract =     "This paper provides an introduction to writing
		  concurrent programs with {"}threads{"}. A threads
		  facility allows you to write programs with multiple
		  simultaneous points of execution, synchronizing
		  through shared memory. The paper describes the basic
		  thread and synchronization primitives, then for each
		  primitive provides a tutorial on how to use it. The
		  tutorial sections provide advice on the best ways to
		  use the primitives, give warnings about what can go
		  wrong and offer hints about how to avoid these
		  pitfalls. The paper is aimed at experienced
		  programmers who want to acquire practical expertise
		  in writing concurrent programs."
}

@MastersThesis{Blumofe92,
  author = 	 "Robert D. Blumofe",
  title = 	 "Managing Storage for Multithreaded Computations",
  school = 	 "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  month =	 "Sep",
  note =	 "82 pages. As tech report MIT/LCS/TR-552",
  note2        = "HAR",
  availabe = 	 "ftp://ftp.theory.lcs.mit.edu/pub/rdb/msthesis.ps.Z",
  abstract =	 "Multithreading has dominant paradigm in general
		  purpose MIMD parallel computation.  We consider  the
		  problem of scheduling multithreaded computations to
		  achieve linear speedup without using significantly
		  more space-per-processor than required for a
		  single-processor execution. We begin by developing a
		  formal graph-theoretic model of multithreaded
		  computation and quantifiable measures of execution
		  time and space.  Based on these measures, we exhibit
		  natural bounds to characterize efficiency with
		  respect to time and space. Our first result shows
		  that there exist multithreaded computations such
		  that no execution schedule can simultaneously
		  achieve an efficient time bound and an efficient
		  space bound. By restricting ourselves to a class of
		  computations we call strict, however, we show that
		  for any number P  of processors and any strict
		  multithreaded computation, there exists an execution
		  schedule that achieves both an efficient time bound
		  and an efficient space bound.  We give a centralized
		  algorithm to compute such schedules, and we give a
		  distributed algorithm that computes schedules with
		  bounds that are nearly as good as the centralized
		  algorithms. We also show that some nonstrictness can
		  be allowed in an otherwise strict computation in a
		  way that may improve performance, but does not
		  adversely affect the time and space bounds"
}

@InProceedings{Blumofe93a,
  author = 	 "Robert D. Blumofe and Charles E. Leiserson",
  title = 	 "Space-Efficient Scheduling of Multithreaded
		  Computations",
  pages =	 "362--371",
  booktitle =	 "Proceedings of the Twenty-Fifth Annual ACM Symposium
		  on the Theory of Computing (STOC '93)",
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  note =	 "Also submitted to SIAM Journal on Computing",
  url = 	 "ftp://ftp.theory.lcs.mit.edu/pub/rdb/stoc93.ps.Z",
  abstract =	 ""
}

@inproceedings{Blumofe93b,
  author =       "Robert D. Blumofe",
  title =        "(Hopefully) Practical Algorithms for Scheduling
		  Multithreaded Computations",
  booktitle =    "Proceedings of the 1993 MIT Student Workshop on
		  Supercomputing Technologies",
  year =         1993,
  month =        aug,
  abstract =	 ""
}

@InProceedings{Blumofe94a,
  author = 	 "Robert D. Blumofe and Charles E. Leiserson",
  title = 	 "Scheduling Multithreaded Computations by Work Stealing,",
  booktitle =	 "Proceedings of the 35th Annual Symposium on
		  Foundations of Computer Science (FOCS '94)",
  pages =	 "356-368",
  year =	 "1994",
  address =	 "Santa Fe, NM, USA",
  month =	 "Nov",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.theory.lcs.mit.edu/pub/rdb/focs94.ps.Z",
  abstract =	 "This paper studies the problem of efficiently
		  scheduling fully strict (i.e., well-structured)
		  multithreaded computations on parallel computers. A
		  popular and practical method of scheduling this kind
		  of dynamic MIMD-style computation is 'work
		  stealing,' in which processors needing work steal
		  computational threads from other processors. In this
		  paper, we give the first provably good work-stealing
		  scheduler for multithreaded computations with
		  dependencies. Specifically, our analysis shows that
		  the expected time T/sub P/ to execute a fully strict
		  computation on P processors using our work-stealing
		  scheduler is T/sub P/=O(T/sub 1//P+T/sub infinity
		  /), where T/sub 1/ is the minimum serial execution
		  time of the multithreaded computation and T/sub
		  infinity / is the minimum execution time with an
		  infinite number of processors. Moreover, the space
		  S/sub P/ required by the execution satisfies S/sub
		  P/<or=S/sub  1/P. We also show that the expected
		  total communication of the algorithm is at most
		  O(T/sub infinity /S/sub max/P), where S/sub max/ is
		  the size of the largest activation record of any
		  thread, thereby justifying the folk wisdom that
		  work-stealing schedulers are more communication
		  efficient than their work-sharing counterparts. All
		  three of these bounds are existentially optimal to
		  within a constant factor."
}

@inproceedings{Blumofe94b,
  author =       "Robert D. Blumofe and David S. Park",
  title =        "Scheduling Large-Scale Parallel Computations on
		  Networks of Workstations",
  booktitle =    "Proceedings of the Third International Symposium on
		  High-Performance Distributed Computing",
  year =         1994,
  month =        aug,
  day =          "2--5",
  address =      "San Francisco, California",
  url =     "ftp://theory.lcs.mit.edu/pub/cilk/hpdc94.ps.Z",
  pages =        "96--105",
  abstract =	 "Workstation networks are an underutilized yet
		  valuable resource for solving large-scale parallel
		  problems. In this paper, we present ``idle-initiated''
		  techniques for efficiently scheduling large-scale
		  parallel computations on workstation networks. By
		  ``idle-initiated,'' we mean that idle computers
		  actively search out work to do rather than wait for
		  work to be assigned. The idle-initiated scheduler
		  operates at both the macro and the micro levels. On
		  the macro level, a computer without work joins an
		  ongoing parallel computation as a participant. On
		  the micro level, a participant without work ``steals''
		  work from some other participant of the same
		  computation. We have implemented these scheduling
		  techniques in Phish, a portable system for running
		  dynamic parallel applications on a network of
		  workstations."
}

@InProceedings{Blumofe95,
  author = 	 "Robert D. Blumofe and Christopher F. Joerg and
		  Bradley C. Kuszmaul and Charles E. Leiserson and
		  Keith H. Randall and Yuli Zhou.",
  title = 	 "Cilk: An Efficient Multithreaded Runtime System",
  pages =	 "??--??",
  booktitle =	 ppopp5,
  year =	 "1995",
  month =	 "Jul",
  url = 	 "ftp://theory.lcs.mit.edu/pub/cilk/PPoPP95.ps.Z",
  abstract =	 ""
}

@InProceedings{Bohm93,
  author = 	 "A. P. Willem Bohm and Walid A. Najjar and Banu
		  Shankar and Lucas Roh",
  title = 	 "An evaluation of bottom-up and top-down thread
		  generation techniques",
  pages =	 "118--127",
  booktitle =	 isma26,
  year =	 "1993",
  address =	 "Austin, TX, USA",
  month =	 "Dec",
  abstract =	 "Presents a model of coarse grain dataflow execution.
		  The authors present one top down and two bottom up
		  methods for generation of multithreaded code, and
		  evaluate their effectiveness. The bottom up
		  techniques start from a fine-grain dataflow graph
		  and coalesce this into coarse-grain clusters. The
		  top down technique generates clusters directly from
		  the intermediate data dependence graph used for
		  compiler optimizations. The authors discuss the
		  relevant phases in the compilation process. They
		  compare the effectiveness of the strategies by
		  measuring the total number of clusters executed, the
		  total number of instructions executed, cluster size,
		  and number of matches per cluster. It turns out that
		  the top down method generates more efficient code,
		  and larger clusters. However the number of matches
		  per cluster is larger for the top down method, which
		  could incur higher cluster synchronization costs."
}

@PhdThesis{Bolosky93,
  author = 	 "William Joseph Bolosky",
  title = 	 "Software Coherence in Multiprocessor Memory
		  Systems.",
  school = 	 "University of Rochester, Department of Computer
		  Science",
  year = 	 "1993",
  month =	 "May",
  note = 	 "Also tech report TR 456",
  note2        = "HAR",
  availabe = 	 "ftp://ftp.cs.rochester.edu/pub/papers/systems/93.tr456.software_coherence_in_multiprocessor_memory_systems.ps.Z",
  abstract = 	 "Processors are becoming faster and multiprocessor
		  memory interconnection systems are not keeping up.
		  Therefore, it is necessary to have threads and the
		  memory they access as near one another as possible.
		  Typically, this involves putting memory or caches
		  with the processors, which gives rise to the problem
		  of coherence: if one processor writes an address,
		  any other processor reading that address must see
		  the new value. This coherence can be maintained by
		  the hardware or with software intervention. Systems
		  of both types have been built in the past; the
		  hardware-based systems tended to outperform the
		  software ones. However, the ratio of processor to
		  interconnect speed is now so high that the extra
		  overhead of the software systems may no longer be
		  significant. This dissertation explores this issue
		  both by implementing a software-maintained system
		  and by introducing and using the technique of
		  offline optimal analysis of memory reference traces.
		  It finds that in properly built systems,
		  software-maintained coherence can perform comparably
		  to or even better than hardware-maintained
		  coherence. The architectural features necessary for
		  efficient software coherence to be profitable
		  include a small page size, a fast trap mechanism,
		  and the ability to execute instructions while remote
		  memory references are outstanding"
}

@InProceedings{Boothe92,
  author = 	 "Bob Boothe and Abhiram Ranade",
  title = 	 "Improved multithreading techniques for hiding
		  communication latency in multiprocessors",
  pages =	 "214--223",
  booktitle =	 isca19,
  year =	 "1992",
  address =	 "Gold Coast, Australia",
  month =	 "May",
  note =	 "Published in " # canews # "Vol.20, No.2, May
		  1992", 
  note2        = "HAR",
  abstract =	 "Shared memory multiprocessors are considered among
		  the easiest parallel computers to program. However
		  building shared memory machines with thousands of
		  processors has proved difficult because of the
		  inevitably long memory latencies. Much previous
		  research has focused on cache coherency techniques,
		  but it remains unclear if caches can obtain
		  sufficiently high hit rates. The authors present
		  improved multithreading techniques that can easily
		  tolerate latencies of hundreds of cycles, and yet
		  only require a small number of thread per processor.
		  High performance is achieved by introducing an
		  explicit context switch instruction that can be used
		  by a simple optimizing compiler to group together
		  several shared accesses. This grouping of shared
		  accesses dramatically reduces the frequency of
		  context switches compared to simpler multithreading
		  models. The combination of their techniques achieves
		  efficiencies of 80% or higher on a broad set of
		  applications."
}

@TechReport{Boothe93a,
  author = 	 "Robert F. Boothe",
  title = 	 "Fast Accurate Simulation of Large Shared Memory
		  Multiprocessors (revised version)",
  institution =  "University of California at Berkeley",
  year = 	 "1993",
  number = 	 "UCB-CSD-93-752",
  month = 	 "??",
  note2        = "HAR",
  url =     "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-93-752",
  abstract=      "Fast computer simulation is an essential tool in the
		  design of large parallel computers.  Our Fast
		  Accurate Simulation Tool, FAST, is able to
		  accurately simulate large shared memory
		  multiprocessors and their execution of parallel
		  applications at simulation speeds that are one to
		  two orders of magnitude faster than previous
		  comparable simulators.  The key ideas involve
		  execution driven simulation techniques that modify
		  the object code of the application program being
		  studied.  This produces an augmented version of the
		  code that is directly executed and performs much of
		  of the work of the simulation. We extend the
		  previous work by introducing several new uses of
		  code augmentation. In this paper we summarize the
		  tradeoffs made in the designs of this and previous
		  simulators.  In previous simulators, these tradeoffs
		  have often led to sacrificing accuracy for faster
		  simulation However by careful selection of
		  techniques and when to apply them, we have built a
		  simulator that is both faster and more accurate than
		  previous simulation systems.  The improved accuracy
		  comes from applying code augmentation techniques at
		  a uniform low level and from having such fast
		  context switching that accuracy/performance
		  tradeoffs become unnecessary. Our simulator has a
		  modular design and has been configured in many ways.
		  It has been used to conduct numerous experiments on
		  multithreaded machine behavior, application
		  behavior, cache behavior, compiler optimization, and
		  traffic patterns.  Because of its high performance,
		  we have been able to perform simulations of larger
		  machines than would otherwise have been feasible."
}

@PhdThesis{Boothe93b,
  author = 	 "Robert F. Boothe",
  title = 	 "Evaluation of Multithreading and Caching in Large
		  Shared Memory Parallel Computers",
  school = 	 "University of California, Berkeley - Department of
		  Computer Science",
  year = 	 "1993",
  month = 	 "Jul",
  type =         "Also tech report UCB-CSD-93-766",
  note = 	 "169 pages",
  note2        = "HAR",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-93-766",
  abstract = 	 "Shared memory multiprocessors are considered among
		  the easiest parallel computers to program. However,
		  building shared memory machines with thousands of
		  processors has proven difficult. Two main problems
		  are the long latencies to shared memory and the
		  large network bandwidth required to support the
		  shared memory programming style.  In this
		  dissertation, we quantify the magnitude of these
		  problems and evaluate multithreading and caching as
		  mechanisms for solving them. Multithreading works by
		  overlapping communication with computation, and
		  caching works by filtering out a large fraction of
		  the remote accesses.  We evaluate several
		  multithreading models using simulations of eight
		  benchmark applications. On systems with
		  multithreading but without caching, we have found
		  that the best results are obtained for the
		  explicit-switch multithreading model. This model
		  provides an explicit context switch instruction that
		  allows the compiler to select the points at which
		  context switches occur. Our results suggest that a
		  200 cycle memory access latency can be tolerated
		  using multithreading levels of 10 threads or less
		  per processor. On systems with both multithreading
		  and caching, we have found that the switch-on-miss
		  multithreading is best. For this model, our results
		  suggest that a 200 cycle memory access latency can
		  be tolerated using multithreading levels of 3
		  threads or less per processor.  We show that by
		  using multithreading techniques, systems both with
		  and without caching are able to tolerate long
		  latencies and achieve execution efficiencies of 80%.
		  The difference between systems with and without
		  caching is that the systems with caching (if
		  properly configured) use an order of magnitude less
		  network bandwidth. For large machines, the network
		  cost will be a large factor in the cost of the
		  machine, and thus the cost and complexity of cache
		  coherency will be offset by the reduction in the
		  network bandwidth requirement"
}

@InProceedings{Bottazzi91,
  author = 	 "M. Bottazzi and C. Salati",
  title = 	 "Processes, threads, parallelism in real-time
		  systems",
  editor =	 "V. A. Monaci and R. Negrini",
  pages =	 "103--107",
  booktitle =	 "Proceedings. Advanced Computer Technology, Reliable
		  Systems and Applications. 5th Annual European
		  Computer Conference CompEuro '91",
  year =	 "1991",
  address =	 "Bologna, Italy",
  month =	 "May",
  abstract =	 "The genesis and the rationale of the idea of
		  multithread processes are analyzed, and the
		  following related issues are discussed: support of
		  different granularities of parallelism, explicit vs.
		  implicit intrathreads synchronization, kernel vs.
		  use support of threads, relations with the
		  object-oriented paradigm, relations with remote
		  procedure calls (RPCs) and distributed systems, and
		  relations with open systems. The focus is on the
		  application of multithread processes in real-time
		  systems and on the way parallelism can be better
		  exploited in these systems."
}

@TechReport{Brandenburg88,
  author =       "Joe Brandenburg and M. Mirashrafi and J. McNamara and
		  D. Billstrom and E. Gilbert and M. Ferguson and
		  A. Sizer and P. Alleyne",
  title =        "Advances in Lisp for the i{PSC}/2",
  institution =  "Intel Scientific Computers",
  type =         "Technical Report",
  address =      "Beaverton, OR",
  year =         "1988",
  abstract =     "A complete redesign of Common Lisp for the Intel
		  iPSC/2 sytem resulted in a new programming model
		  based on multiple threads of execution, substantial
		  performance gains exceeding all previously published
		  timings for gabriel benchmarks, and a flexible user
		  interface to manage the complexity of concurrently
		  executing Lisp environments. Although multiple
		  thread execution is not an unknown model of
		  processing, the integration of threads with existing
		  iPSC Lisp constructs provided new insight into
		  certain concurrent applications. The performance of
		  iPSC/2 Lisp is shown to be significantly faster than
		  PSL on a Cray-XMP supercomputer, for two of the
		  largest Gabriel benchmarks, iPSC/2 Lisp is shown to
		  be the highest performance Lisp system available,
		  commercially or in academia. The user interface
		  allows direction and redirection of keyboard input,
		  program output, and the handling of errors to
		  multiple screens - important for large concurrent
		  programs. Specific information is provided on iPSC/2
		  Lisp performance on the Gabriel Benchmarks; on the
		  differences between iPSC/2 Lisp and CCLISP for the
		  iPSC/2; and on the flexibility of the new user
		  interface. Examples are given for each of the new
		  constructs, user interface, and the performance."
}

@inproceedings{Brewer94,
  author =       "Eric A. Brewer and Robert Blumofe",
  title =        "{Strata}: A Multi-Layer Communications Library",
  booktitle =    "Proceedings of the 1994 MIT Student Workshop on
		  Scalable Computing",
  year =         1994,
  month =        jul,
  url =     "ftp://jukebox.lcs.mit.edu/papers/Strata-2.0B.ps",
  abstract =	 ""
}

@InProceedings{Bruening94,
  author = 	 "Ulrich Bruening and Wolfgang K. Giloi and Wolfgang
		  Schroeder-Preikschat",
  title = 	 "Latency hiding in message-passing architectures",
  editor =	 "H. J. Siegel",
  pages =	 "704--709",
  booktitle =	 ipps8,
  year =	 "1994",
  address =	 "Cancun, Mexico",
  month =	 "Apr",
  abstract =	 "The paper demonstrates the advantages of having two
		  processors in the node of a distributed memory
		  architecture, one for computation and one for
		  communication. The architecture of such a
		  dual-processor node is discussed. To exploit fully
		  the potential for parallel execution of computation
		  threads and communication threads, a novel,
		  compiler-optimized IPC mechanism allows for an
		  unbuffered no-wait send and a prefetched receive
		  without the danger of semantics violation. It is
		  shown how an optimized parallel operating system can
		  be constructed such that the application processor's
		  involvement in communication is kept to a minimum
		  while the utilization of both processors is
		  maximized. The MANNA implementation results in an
		  effective message start-up latency of only 1...4
		  microseconds. It is also shown how the
		  dual-processor node is utilized to efficiently
		  realize virtual shared memory."
}

@InProceedings{Bruggeman92,
  author = 	 "Carl Bruggeman and Kent Dybvig",
  title = 	 "A new architecture design paradigm for parallel
		  computing in Scheme",
  pages =	 "362--379",
  booktitle =	 "Parallel Symbolic Computing: Languages, Systems, and
		  Applications. US/Japan Workshop Proceedings",
  year =	 "1992",
  address =	 "Cambridge, MA, USA",
  month =	 "Oct.",
  series =       "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  abstract =	 "The authors describe an architecture design paradigm
		  that radically reassigns various system
		  responsibilities among the compiler, operating
		  system, and architecture in order to simplify the
		  design and increase the performance of parallel
		  computing systems. Implementation techniques for
		  latently typed languages like Scheme are enhanced
		  and used to support compiler-enforced memory
		  protection and compiler-controlled exception
		  handling. Hardware design complexity is greatly
		  reduced and hardware modularity is increased by not
		  only eliminating the need to implement exception
		  handling in the processor state machine, but also by
		  eliminating global control altogether. In the
		  absence of global control, techniques such as
		  pipelining and multiple contexts that exploit
		  instruction-level and thread-level parallelism can
		  be used together without the usual processor
		  complexity problems, to increase the efficiency of
		  parallel systems. Complexity is reduced and
		  efficiency is increased at the software level as
		  well. The use of compiler-enforced memory protection
		  and a single shared system-wide virtual address
		  space increases inter-thread communication
		  efficiency as well as inter-thread protection
		  resulting in threads that not only are light-weight
		  but also enjoy the protection guarantees of
		  heavy-weight threads."
}

@Article{Buehrer87,
  author = 	 "Richard Buehrer and Kattamuri Ekanadham",
  title = 	 "Incorporating dataflow ideas into von Neumann
		  processors for parallel execution",
  journal =	 ieeetc,
  year =	 "1987",
  volume =	 "C-36",
  number =	 "12",
  pages =	 "1515-1522",
  month =	 "Dec",
  note2 =	 "HAR",
  abstract =	 "The issues of memory latency, synchronization, and
		  distribution costs in multiprocessors are
		  reviewed. The approaches taken by conventional and
		  data flow architectures are contrasted in relation
		  to these issues. It is pointed out that each
		  approach fits well for a certain situation and that
		  it is possible to have a common framework in which
		  the two execution models can be mixed to suit the
		  situation. An architecture is sketched by describing
		  a few extensions to a conventional processor. While
		  existing programs run without much change, it is
		  shown that improvements could be made by using the
		  new features to tolerate memory latency and to
		  exploit more parallelism. It is shown that data flow
		  graphs can be executed on this to exploit as much
		  parallelism as a data flow architecture could. It is
		  argued that such a testbed will provide for a
		  selective translation of program segments into the
		  appropriate execution model. A fast context
		  switching hardware is presumed for this
		  architecture"
}

@Article{Buhr90,
  author = 	 "Peter A. Buhr and Richard A. Strooboscher",
  title = 	 "mu system. Providing light-weight concurrency on
		  shared-memory multiprocessor computers running
		  {UNIX}",
  journal =	 spe,
  year =	 "1990",
  volume =	 "20",
  number =	 "9",
  pages =	 "929--964",
  abstract =	 "The mu System is a library of C routines that
		  provide light-weight concurrency on uniprocessor and
		  multiprocessor computers running the Unix operating
		  system. A discussion of the run-time structure of a
		  mu System program is given which includes the
		  following concepts: coroutines, tasks, virtual
		  processors and clusters. Next the routines that
		  implement these concepts are discussed in detail.
		  Finally, some performance figures from the mu System
		  are given and discussed, followed by a comparison of
		  the mu System with other similar systems."
}

@Article{Burgess94,
  author = 	 "P. Burgess and M. J. Livesey and C. Allison",
  title = 	 "BED: A Multithreaded Kernel for Embedded Systems",
  journal =	 "Annual review in automatic programming",
  year =	 "1994",
  volume =	 "18",
  pages =	 "133--??",
  abstract =	 ""
}

@Article{Burke87,
  author = 	 "Gary R. Burke",
  title = 	 "A multiport register file chip for the CHoPP
		  supercomputer",
  journal =	 "VLSI Systems Design",
  year =	 "1987",
  pages =	 "18--22",
  month =	 "Aug",
  abstract =	 ""
}

@InProceedings{Butner84,
  author = 	 "Steven E. Butner",
  title = 	 "The circulating context multiprocessor, an
		  architecture for reliable systems",
  pages =	 "50--52",
  booktitle =	 "International symposium on mini and microcomputers
		  (ISMM)",
  year =	 "1984",
  month =	 "Jun",
  abstract =	 ""
}

@InProceedings{Butner86,
  author = 	 "Steven E. Butner and Clinton A. Staley",
  title = 	 "A {RISC} multiprocessor based on circulating
		  context",
  pages =	 "620--624",
  booktitle =	 "1986 International Phoenix Conference on Computers and
		  Communication",
  year =	 "1986",
  abstract =	 ""
}

@InProceedings{Campos92a,
  author = 	 "G. Lino de Campos",
  title = 	 "Asynchronous polycyclic architecture",
  editor =	 "L. Bouge et al",
  pages =	 "387--398",
  booktitle =	 "Parallel Processing: CONPAR 92-VAPP V. Second Joint
		  International Conference on Vector and Parallel Processing",
  year =	 "1992",
  month =	 "Sep",
  series =       "Lecture Notes in Computer Science 634",
  publisher =	 "Springer-Verlag",
  address =	 "Lyon, France",
  abstract =	 "The asynchronous polycyclic architecture (APA) is a
		  new processor design for numerically intensive
		  applications. APA resembles the VLIW architecture,
		  in that it provides independent control and
		  concurrent operation of low-level functional units
		  within the processor. The main innovations of APA
		  are the provision for multiple threads of control
		  within each processor, the clustering of functional
		  units into groups of functional units that show very
		  weak coupling with each other, decoupled
		  access/execute and eager execution. A supercomputer
		  implementing this architecture is currently being
		  designed, using commercially available parts."
}

@InProceedings{Campos92b,
  author = 	 "G. Lino de Campos",
  title = 	 "Asynchronous polycyclic architecture: an overview",
  booktitle =	 "Algorithms, Software, Architecture. Information
		  Processing 92. IFIP 12th World Computer Congress",
  year =	 "1992",
  pages =	 "518--524",
  month =	 "Sep",
  note =	 "Published in IFIP Transactions A. Vol. A-12, 1992",
  abstract =	 "The most important criterion for evaluating a
		  computer architecture is how well it matches the
		  programs it is intended to run. Performance
		  measurements have shows that traditional vector
		  architectures fare poorly in this regard, in that
		  they are ill adapted to the needs of the numeric
		  intensive applications that occur in the real world.
		  The author analyzes the reasons for this failure,
		  and uses the resulting insight to design a new
		  processor architecture, called asynchronous
		  polycyclic architecture (APA). APA resembles the
		  VLIW architecture, in that it provides independent
		  control and concurrent operation of low-level
		  functional units within the processor. The main
		  innovations of APA are the provision for multiple
		  threads of control within each processor, the
		  clustering of functional units into groups of
		  functional units that show weak coupling with each
		  other, decoupled access/execute and eager
		  execution."
}

@InProceedings{Casewell90,
  author = 	 "Deborah Casewell and David L. Black",
  title = 	 "Implementing a Mach Debugger for Multithreaded
		  Applications",
  pages =	 "25--39",
  booktitle =	 usenixw90,
  year =	 "1990",
  month =	 "Jan",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/debugger.ps",
  abstract = 	 "Multiple threads of control add new challenges to
		  the task of application debugging and require the
		  development of new debuggers to meet these
		  challenges. This paper describes the design and
		  implementation of modifications to an existing
		  debugger (gdb) for debugging multithreaded
		  applications under the Mach operating system. It
		  also describes the operating facilities that support
		  it. Although certain implementations details are
		  specific to Mach, the underlying design principles
		  are applicable to other systems that support threads
		  in a Unix compatible environment."
}

@Article{Cattaneo92,
  author = 	 "G. Cattaneo and G. di Giore and M. Ruotolo",
  title = 	 "Another {C} threads library",
  journal =	 "SIGPLAN Notices",
  year =	 "1992",
  volume =	 "27",
  number =	 "12",
  pages =	 "81--90",
  month =	 "Dec",
  note2        = "HAR",
  abstract =	 "The paper presents the main results achieved with
		  the implementation of an efficient multithread
		  library, DIA-Thread Library, built on the top of a
		  shared memory symmetric multiprocessor environment.
		  This package is intended to be portable. It consists
		  of less than two thousands lines of C code and only
		  a few lines of assembly code. The dependencies on
		  the host operating system were minimized and no
		  assumption was made on the processor configuration,
		  both on their total number and their basic
		  interprocess communication support. DIA-Thread
		  library basically provides few functions to activate
		  and manage multiple flows of control with a little
		  extra overhead. The user application may be split in
		  a virtually unlimited number of threads which will
		  carry on their own task communicating with each
		  other by means of common memory areas protected by
		  semaphores."
}

@TechReport{Chase93,
  author =       "Jeffrey Chase and Henr M. Levy and Michael Feeley
		  and Edward D. Lazowska",
  title =        "Sharing and Protection in a Single Address Space
		  Operating System",
  institution =  "University of Washington",
  number =       "UW-CSE-93-04-02",
  month =        "Apr",
  year =         "1993",
  url =    "ftp://ftp.cs.washington.edu/tr/1993/04/UW-CSE-93-04-02.PS.Z", 
  abstract =     "The appearance of 64-bit address space
		  architectures, such as the DEC Alpha, HP PA-RISC,
		  and MIPS R4000, signals a radical shift in the
		  amount of address space available to operating
		  systems and applications. This shift provides the
		  opportunity to reexamine fundamental operating
		  system structure; specifically, to change the way
		  that operating systems use address space. This paper
		  explores memory sharing and protection support in
		  Opal, a single address space operating system
		  designed for wide-address architectures. Opal
		  threads execute within protection domains in a
		  single shared virtual address space. Sharing is
		  simplified, because addresses are context
		  independent. There is no loss of protection, because
		  addressability and access are orthogonal; the right
		  to access a segment is determined by the protection
		  domain in which a thread executes. This model
		  enables beneficial code and data sharing patterns
		  that are currently prohibitive, due in part to the
		  inherent restrictions of multiple address spaces,
		  and in part to Unix programming style. We have
		  designed and implemented an Opal prototype using the
		  Mach 3.0 microkernel as a base. Our implementation
		  demonstrates how a single address space structure
		  can be supported alongside of other environments on
		  a modern microkernel operating system, using modern
		  wide-address architectures. This paper justifies the
		  Opal model and its goals for sharing and protection,
		  presents the system and its abstractions, describes
		  the prototype implementation, and reports experience
		  with integrated applications."
}

@Article{Chaudhry94,
  author = 	 "Ghulam Chaudhry and Xuechang Li",
  title = 	 "A Case for the Multithreaded Processor Architecture",
  journal =	 canews,
  year =	 "1994",
  volume =	 "22",
  number =	 "4",
  pages =	 "55--??",
  month =	 "Sep",
  note2 =	 "HAR",
  abstract =	 "As cache based shared-memory multiprocessors are
		  scaled (number of processors are increased), there
		  will be an increase in the network latency due to
		  cache coherence and synchronization delays. These
		  parameters can cause significant drop in processor
		  utilization. Once a process has remote procedure
		  call or need data from remote side, the processor
		  has to be idle or make context switch to load the
		  next process in which the CPU time is wasted. By
		  maintaining the multiple threasd in the hardwarte,
		  and switching among them, the multithreaded
		  processor can overlap the waiting for the
		  synchronization delay so that it decreases the
		  processor idle time. This correspondence describes
		  the multithreaded processor architecture, in which
		  there are a number of hardware contexts per
		  processor. It uses coarse-gran method to schedule
		  threads and two-phase waiting algorithm to
		  synchronize the waiting threads. From this
		  architecture we can study how many hardware contexts
		  are needed, so the processor utilization can be
		  increased. Moreover, we can study the effect on the
		  utilization by changing the cache miss ratio."
}

@InProceedings{Chen90,
  author = 	 "Ding-Kai Chen and Hong-Hen Su and Pen-Chung Yew",
  title = 	 "The impact of synchronization and granularity in
		  parallel systems",
  pages =	 "239--248",
  booktitle =	 isca17,
  year =	 "1990",
  note =	 "Also as Tech report 942 - University of Illinois at
		  Urbana-Champaign, Centre of Supercomputing Research
		  and Development",
  url = 	 "http://www.csrd.uiuc.edu/reports/942.ps.gz",
  abstract =	 "We study the impact of synchronization and
		  granularity on the performance of parallel systems
		  using an execution-driven simulation technique. We
		  find that even though there can be a lot of
		  parallelism at the fine grain level, synchronization
		  and scheduling strategies determine the ultimate
		  performance of the system. Loop level parallelism
		  seems to be a more appropriate level when those
		  factors are considered. We also study barrier
		  synchronization and data synchronization at the
		  loop-iteration level and found both schemes are
		  neededfor a better performance."
}

@InProceedings{Chen94,
  author = 	 "Chiun-Shiu Chen and Chien-Chaio Tseng",
  title = 	 "Integrated Support to Improve Inter-thread
		  Communication and Synchronization in a Multithreaded
		  Processor",
  pages =	 "??--??",
  booktitle =	 "Proceedings of the International Conference on
		  Parallel and Distributed Systems (ICPADS'94)",
  year =	 "1994",
  abstract =	 "This paper presents an integrated compiler, runtime
		  control, and hardware solution to improve
		  inter-thread communication and synchronization in a
		  multithreaded processor architecture. Multithreading
		  improves processor utilization by exploiting more
		  parallelism. The improvement in utilization,
		  however, is hindered by inter-thread communication
		  and synchronization problems, which incur extra
		  communication overhead and thus degrade the
		  performance of the system. In this paper, we propose
		  efficient inter-thread communication and
		  synchronization schemes based on a superscalar DLX
		  processor with multithreading functionality. The
		  compiler, runtime control, and hardware support used
		  in the schemes are discussed. Simulations are
		  presented to show the effectiveness of the proposed
		  schemes."
}

@MastersThesis{Chiou92,
  author = 	 "Derek Chiou",
  title = 	 "Frame memory management for the Monsoon Dataflow
		  Processor",
  school = 	 "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  month =	 "Sep",
  url = 	 "http://csg-www.lcs.mit.edu:8001/~derek/MS-Thesis.ps",
  abstract =	 ""
}

@InProceedings{Chiou95,
  author = 	 "Derek Chiou and Boon Seong Ang and Arvind and
		  Michael J. Beckerle and G.Andrew Boughton and Robert
		  Greiner and James E. Hicks and James C. Hoe",
  title = 	 "StarT-NG: Delivering Seemless Parallel Computing",
  booktitle =    "Proceedings of Euro-Par 95",
  pages =        "??--??",
  year = 	 "1995",
  note =	 "Also as Tech report CSG-Memo-371, Massachussets
		  Institute of Technology, Laboratory for Computer
		  Science",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-371.ps.gz",
  abstract =	 ""
}

@InProceedings{Chiue91,
  author = 	 "Tzi-cker Chiueh",
  title = 	 "Liger: A Hybrid Dataflow Architecture Exploiting
		  Data/Control Locality",
  volume =	 "1",
  pages =	 "346--350",
  booktitle =	 icpp91,
  year =	 "1991",
  publisher =	 "CRC Press",
  month =	 "Aug",
  abstract =	 ""
}

@InProceedings{Chiueh91,
  author = 	 "Tzi-cker Chiue",
  title = 	 "Multi-threaded vectorization",
  pages =	 "352--361",
  booktitle =	 isca18,
  year =	 "1991",
  abstract =	 ""
}

@InProceedings{Chong94,
  author = 	 "Yong-Kim Ching and Kai Hwang",
  title = 	 "Evaluation of Relaxed Memory Consistency Models for
		  Multithreaded Multiprocessors ",
  pages =	 "??--??",
  booktitle =	 "Proceedings of the International Conference on
		  Parallel and Distributed Systems (ICPADS'94)",
  year =	 "1994",
  abstract =	 "Stochastic timed Petri nets are developed to
		  evaluate the relative performance of distributed
		  shared memory models for scalable multithreaded
		  multiprocessors. The shared memory models evaluated
		  include the Sequential Consistency (SC), the Weak
		  Consistency (WC), the Processor Consistency (PC) and
		  the Release Consistency (RC) models. Under saturated
		  conditions, we found that multithreading contributes
		  more than 50% of the performance improvement, while
		  the improvement from memory consistency models
		  varies between 20% to 40% of the total performance
		  gain.   Our analytical results reveal the lowest
		  performance of the SC model. The PC model requires
		  to use larger write buffer and may perform even
		  lower than the SC model if a small buffer was used.
		  The performance of the WC model depends heavily on
		  the synchronization rate in user code. For a low
		  synchronization rate, the WC model performs as well
		  as the RC model. With sufficient multithreading and
		  network bandwidth, the RC model shows the best
		  performance among the four models. "
}

@InProceedings{Christopher93,
  author = 	 "Wayne A. Christopher and Steven J. Procter and Thomas
		  E. Anderson",
  title = 	 "The Nachos Instructional Operating System.",
  booktitle =    usenixw93,
  year = 	 "1993",
  pages = 	 "481--489",
  month = 	 "Jan",
  note =         "Also as Tech report UCB-CSD-93-739, University of
		  California at Berkeley, Department of Computer
		  Science 15 pages",
  url =  	 "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-93-739",
  abstract = 	 "In teaching operating systems at an undergraduate
		  level, we believe that it is important to provide a
		  project that is realistic enough to show how real
		  operating systems work, yet is simple enough that
		  the students can understand and modify it in
		  significant ways. A number of these instructional
		  systems have been created over the last two decades,
		  but recent advances in hardware and software design,
		  along with the increasing power of available
		  computational resources, have changed the basis for
		  many of the tradeoffs made by these systems. We have
		  implemented an instructional operating system,
		  called Nachos, and designed a series of assignments
		  to go with it. Our system includes CPU and device
		  simulators, and it runs as a regular UNIX process.
		  Nachos illustrates and takes advantage of modern
		  operating systems technology, such as threads and
		  remote procedure calls, recent hardware advances,
		  such as RISC's and the prevalence of memory
		  hierarchies, and modern software design techniques,
		  such as protocol layering and object-oriented
		  programming. Nachos has been used to teach
		  undergraduate operating systems classes at several
		  universities with positive results."
}

@TechReport{Chuang94,
  author = 	 "Weihaw Chuang",
  title = 	 "PVM Light Weight Process Package",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1994",
  number =	 "CSG-Memo-372",
  month =	 "Dec",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-372.ps.gz",
  abstract =	 ""
}

@Misc{Chung95,
  author =	 "Jin-Chin Chung and Chuan-Li Wu",
  title =	 "Multi-threaded microprocessor architecture utilizing
		  static interleaving",
  howpublished = "patent",
  year =	 "1995",
  month =	 "Apr",
  note =	 "Patent number 05404469, Series 7, Appl.number
		  840903, Appl.type 1, Art Unit 235",
  abstract =	 "A static interleaving technique solves the problem
		  of resource contention in a very long instruction
		  word multi-threaded microprocessor architecture. In
		  the static interleaving technique, each function
		  unit in the processor is allocated for the execution
		  of an instruction from a particular thread in a
		  fixed predetermined time slot in a repeating pattern
		  of predetermined time slots. The fixed predetermined
		  pattern of time slots represents the resource
		  constraints imposed on the hardware to resolve the
		  contention for computing resources among the
		  instruction threads. The strategy of resource
		  allocation is exposed to a parallel compiler which
		  organizes a sequence of instructions into the
		  horizontal instruction words which form each thread
		  so as to maintain the data dependencies among the
		  instructions and to take into account the fixed
		  predetermined allocation of hardware resources."
}

@TechReport{Clarke89,
  author = 	 "T. J. Clarke",
  title = 	 "General theory relating to the implementation of
		  concurrentsymbolic computation",
  institution =  "University of Cambridge, Computer Laboratory",
  year = 	 "1989",
  number =	 "174",
  month =	 "Aug",
  abstract =	 ""
}

@InProceedings{Cogswell91,
  author = 	 "Bryce Cogswell and Zary Segall",
  title = 	 "{MACS}: a predictable architecture for real time
		  systems",
  pages =	 "296--305",
  booktitle =	 "Proceedings of Twelfth Real-Time Systems Symposium",
  year =	 "1991",
  address =	 "San Antonio, TX, USA",
  month =	 "Dec",
  abstract =	 "The {MACS} (multiple active context system)
		  architecture is a high-performance, multiple
		  context, cacheless processor designed around the
		  goal of predictability. Multiple contexts provide
		  highly predictable memory and pipeline performance,
		  while at the same time allowing multiple threads to
		  execute with independent timing characteristics. The
		  design emphasis is on provable execution speed and
		  independent timing of each context. Task-level
		  parallelism is used to maintain high processor
		  throughput while individual threads execute at a
		  relatively slow, but very predictable, rate. The
		  authors investigate the suitability of such a system
		  to real time and determine how predictability varies
		  as a function of parallelism (performance), memory
		  issue latency and memory size."
}

@InProceedings{Coleman93,
  author = 	 "Nick J. Coleman",
  title = 	 "A high speed dataflow processing element and its
		  performance compared to a von Neumann mainframe",
  pages =	 "24--33",
  booktitle =	 ipps7,
  year =	 "1993",
  address =	 "Newport, CA, USA",
  month =	 "Apr",
  abstract =	 "The Event Processor / 3 is a dataflow processing
		  element designed for high performance over a range
		  of general computing tasks. Using a multithreading
		  technique, program parallelism is exploited by
		  interleaving threads onto successive pipeline
		  stages. It may also be used as an element in a
		  multiprocessor system. This paper describes the
		  philosophy and design of the machine, and presents
		  the results of detailed simulations of the
		  performance of a single processing element. This is
		  analysed into three factors: clock period, cycles
		  per instruction and instructions per program; and
		  each factor is compared with the measured
		  performance of an advanced von Neumann computer
		  running equivalent code. It is shown that the
		  dataflow processor compares favourably, given a
		  reasonable degree of parallelism in the program."
}

@TechReport{Collins90,
  author = 	 "George Collins and Jeremy Johnson and Wolfgang W.
		  Kuechlin",
  title = 	 "PARSAC-2: A Multi-Threaded System for Symbolic and
		  Algebraic Computation",
  institution =  "Ohio State University. Computer and Information
		  Science Research Center",
  year = 	 "1990",
  number =	 "OSU-CISRC-12/90-TR38",
  month =	 "Dec",
  note =	 "13 pages",
  abstract =	 "We describe the design and development of PARSAC-2,
		  a multi-threaded, parallel version of the SAC-2
		  Computer Algebra system. For parallel programming
		  and synchronization constructs, PARSAC-2 relies on
		  the S-threads environment, a version of Mach's C
		  Threads enhanced for parallel list processing. The
		  algebraic kernel is provided by SAC-2 in the form of
		  a library of C functions, which execute virtually
		  unmodified on top of S-threads. In addition, there
		  are a growing number of parallel algorithms, either
		  newly written in C or derived from existing SAC-2
		  algorithms by adding parallel calls. We describe the
		  overall design of PARSAC and the S-threads
		  environment for parallel symbolic computation, and
		  we give several basic programming examples.Our
		  system timings show that PARSAC affords a grain-size
		  of parallelism on the order of a multiplication of
		  two 8 word integers. We also discuss the
		  implementation and analysis of one high level
		  parallel algorithm for the isolation of the real
		  roots of polynomials."
}

@InProceedings{Conde89,
  author =       "Daniel S. Conde and Felix S. Hsu and Ursula
		  Sinkewicz",
  title =        "{ULTRIX} Threads",
  booktitle =    usenixs89,
  pages =        "257--268",
  publisher =    "USENIX",
  address =      "Baltimore, MD",
  year =         "1989",
  abstract =	 ""
}

@Article{Cook70,
  author = 	 "R.W. Cook and Michael J. Flynn",
  title = 	 "System design of a dynamic microprocessor",
  journal =	 ieeetc,
  year =	 "1970",
  volume =	 "19",
  number =	 "3",
  pages =	 "??--??",
  month =	 "March",
  abstract =	 ""
}

@TechReport{Cooper88,
  author = 	 "Eric C. Cooper and Richard P. Draves",
  title = 	 "{C} Threads",
  institution =  "Carnegie Mellon University, School of Computer Science",
  year = 	 "1988",
  number =	 "CMU-CS-88-154",
  month =	 "Feb",
  note2 = 	 "HAR",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/unpublished/threads.ps",
  abstract =	 "The C Threads package allows parallel programming in
		  C under the Mach operating system. The package
		  provides multiple threads of control within a single
		  shared address space, mutual exclusion locks for
		  protection of critical regions, and condition
		  variables for thread synchronization."
}

@TechReport{Cooper90,
  author =       "Eric C. Cooper and J. Gregory Morrisett",
  title =        "Adding Threads to Standard {ML}",
  institution =  "School of Computer Science, Carnegie Mellon
		  University",
  number =       "CMU-CS-90-186",
  address =      "Pittsburgh, PA",
  year =         "1990",
  url =	 "http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jgmorris/web/papers/jgmorris-mlthreads.ps",
  abstract =     "We have added multiple threads of control to the
		  Standard {ML} programming language. Stanford {ML's}
		  support for first-class functions and automatic
		  storage management influenced the design in a number
		  of ways. We demonstrate how other concurrency and
		  synchronization operations, such as cobegin/coend,
		  futures, and events, can be implemented in terms of
		  the thread interface. Finally, we describe three
		  implementations of the thread interface: a coroutine
		  version, a uniprocessor preemptive version, and a
		  multiprocessor Mach-based version."
}

@MastersThesis{Coorg94,
  author = 	 "Satyan R. Coorg",
  title = 	 "Partioning Non-strict Languages for Multi-threaded
		  Code Generation",
  school = 	 "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1994",
  month =	 "May",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/theses/coorg-sm.ps.gz",
  abstract =	 ""
}

@InProceedings{Culler90a,
  author = 	 "David E. Culler and Anurag Sah and Klaus E. Schauser
		  and Thorsten von Eicken and John Wawrzynek",
  title = 	 "Fine-grain Parallelism with Minimal Hardware
		  Support: A Compiler-Controlled Threaded Abstract Machine",
  booktitle   =  asplos4,
  year = 	 "1991",
  pages = 	 "164--175",
  note = 	 "Published in Vol.26, No.4. Apr.1991. Also as Tech
		  report UCB-CSD-90-594, University of California
		  Berkeley, Department of Computer Science",
  month = 	 "Apr",
  note2        = "HAR",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-90-594",
  url2=   "ftp://ftp.cs.berkeley.edu/ucb/TAM/asplos91.ps",
  abstract =     "In this paper, we present a relatively primitive
		  execution model for fine-grain parallelism, in which
		  all synchronization, scheduling, and storage
		  management is explicit and  under compiler control.
		  This is defined by a threaded abstract machine (TAM)
		  with a multilevel scheduling hierarchy. Considerable
		  temporal locality of logically related threads its
		  demonstrated, providing an avenue for effective
		  register use under quasi-dynamic scheduling. A
		  prototype TAM instruction set, TL0, has been
		  developed, along with a translator to a variety of
		  existing sequential and parallel machines.
		  Compilation of Id, an extended functional language
		  requiring fine-grain synchronization, under this
		  model yields performance approaching that of
		  conventional languages on current uniprocessors.
		  Measurements suggest that the net cost of
		  synchronization on conventional multiprocessors can
		  be reduced to within a small factor of that on
		  machines with elaborate hardware support, such as
		  proposed dataflow architectures.  This brings into
		  question whether tolerance to latency and
		  inexpensive synchronization require specific
		  hardware support or merely an appropriate
		  compilation strategy and program representation"
}

@Article{Culler90b,
  author = 	 "David E. Culler and Gregory M. Papadopoulos",
  title = 	 "The Explicit Token store",
  journal =	 jpal,
  year =	 "1990",
  volume =	 "10",
  number =	 "4",
  pages =	 "289--308",
  note =	 "Also as CSG-Memo-312, Massachusetts Institute of
		  Technology, Laboratory of Computer Science",
  abstract =	 "This paper presents an unusually simple approach to
		  dynamic dataflow execution, called the explicit
		  token store (ETS) architecture, and its current
		  realization in Monsoon. The essence of dynamic
		  dataflow execution is captured by a simple
		  transition on state bits associated with storage
		  local to a processor. Low-level storage management
		  is performed by the compiler in assigning nodes to
		  slots in an activation frame, rather than
		  dynamically in hardware. The processor is simple,
		  highly pipelined, and quite general. There is
		  exactly one instruction executed for each action on
		  the dataflow graph. Thus, the machine-oriented ETS
		  model provides new insight into the real cost of
		  direct execution of dataflow graphs."
}

@InProceedings{Culler92a,
  author = 	 "David E. Culler and Michael Gunter and James C. Lee",
  title = 	 "Analysis of Multithreaded Microprocessors under
		  multiprogramming",
  booktitle =    isca19,
  pages =        "438--???",
  year = 	 "1992",
  note = 	 "Published in " # canews # " Vol.20, No.2, May
		  1992. Also Tech report UCB-CSD-92-687, University of
		  California Berkeley, Department of Computer
		  Science",
  note2        = "HAR",
  month = 	 "May",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-92-687",
  abstract =     "Multithreading has been proposed as a means of
		  tolerating long memory latencies in multiprocessor
		  systems. Fundamentally, it allows multiple
		  concurrent subsystems (cpu, network, and memory) to
		  be utilized simultaneously. This is advantageous on
		  uniprocessor systems as well, since the processor is
		  utilized while the memory system services misses.
		  We examine multithreading on high-performance
		  uniprocessors as a means of achieving better
		  cost/performance on multiple processes. Processor
		  utilization and cache behavior are studied both
		  analytically and through simulation of timesharing
		  and multithreading using interleaved reference
		  traces. Multithreading is advantageous when one has
		  large on-chip caches (32-kilobytes), associativity
		  of two, and a memory system need support only one or
		  two outstanding misses. The increase in processor
		  real-estate to support multithreading is modest,
		  given the size of the cache and floating-point
		  units.  A surprising observation is that miss ratios
		  may be lower with multithreading than with
		  timesharing under a steady-state load. This occurs
		  becasue switch-on-miss multithreading introduces
		  unfair thread scheduling, giving more CPU cycles to
		  processes with better cache behavior "
}

@InProceedings{Culler92b,
  author = 	 "David E. Culler and Klaus E. Schauser and Thorsten
		  von Eicken",
  title = 	 "Two fundamental limits on dataflow multiprocessing",
  BookTitle =    pact93,
  year = 	 "1993",
  month =	 "Jan",
  address =      "Orlando, FL",
  note =	 "Also as Tech report UCB/CSD 92/716, University of
		  California at Bekerley, Computer Science Division",
  note2        = "HAR",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-92-716",
  url2=   "ftp://ftp.cs.berkeley.edu/ucb/TAM/ppopp93.ps",
  abstract =	 "This paper examines the argument for dataflow
		  architectures in 'Two Fundamental Issues in
		  Multiprocessing [5].' We observe two key problems.
		  First, the justification of extensive multithreading
		  is based on an overly simplistic view of the storage
		  hierarchy. Second,the local greedy scheduling policy
		  embodied in dataflow is inadequate in many
		  circumstances. A more realistic model of the storage
		  hierarchy imposes significant constraints on the
		  scheduling of computation and requires a degree of
		  parsimony in the scheduling policy. In particular,
		  it is important to establish a scheduling hierarchy
		  that reflects the underlying storage hierarchy.
		  However, even with this improvement, simple local
		  scheduling policies are unlikely to be adequate."
}

@Article{Culler93,
  author = 	 "David E. Culler and Seth C. Goldstein and Klaus Erik
		  Schauser and Thorsten von Eicken",
  title = 	 "{TAM}-a compiler controlled threaded abstract machine",
  journal =	 jpal,
  year =	 "1993",
  volume =	 "18",
  number =	 "3",
  pages =	 "347--370",
  month =	 "Jul",
  note2        = "HAR",
  url =     "ftp://ftp.cs.berkeley.edu/ucb/TAM/jpdc93.ps",
  abstract =	 "The Threaded Abstract Machine (TAM) refines dataflow
		  execution models to address the critical constraints
		  that modern parallel architectures place on the
		  compilation of general-purpose parallel programming
		  languages. TAM defines a self-scheduled machine
		  language of parallel threads, which provides a path
		  from dataflow-graph program representations to
		  conventional control flow. The most important
		  feature of TAM is the way is exposes the interaction
		  between the handling of asynchronous message events,
		  the scheduling of computation, and the utilization
		  of the storage hierarchy. The paper provides a
		  complete description of TAM and codifies the model
		  in terms of a pseudo machine language TL0. Issues in
		  compilation from a high level parallel language to
		  TL0 are discussed in general and specifically in
		  regard to the Id90 language. The implementation of
		  TL0 and CM-5 multiprocessor is explained in detail.
		  Using this implementation, a cost model is developed
		  for the various TAM primitives. The TAM approach is
		  evaluated on sizable Id90 programs on a 64 processor
		  system. The scheduling hierarchy of quanta and
		  threads is shown to provide substantial locality
		  while tolerating long latencies. This allows the
		  average thread scheduling cost to be extremely low."
}

@InProceedings{Daddis91,
  author = 	 "George E. Daddis Jr. and H. C. Torng",
  title = 	 "The concurrent execution of multiple instruction
		  streams on superscalar processors",
  volume =	 "I",
  pages =	 "76--83",
  booktitle =	 icpp91,
  year =	 "1991",
  month =	 "Aug",
  note2        = "HAR",
  abstract =	 "We propose and evaluate an issuing mechganism to
		  boost superscalar performance with multiple and
		  out-of-order instruction dispatching from two or
		  more independent instruction streams. This
		  mechanism, an instruction window, expands and
		  enhances the multiple instruction stream dispatching
		  implemented in HEP. Simulation of the MIMD and
		  single stream processors were performed on both
		  numeric and general computation benchmarks. The
		  effects of instruction window size, maximum fetch
		  count, and instruction execution times are
		  examined. We demonstrate through simulation a
		  90%--100% increase in the processor througput when
		  interleaving the execution of just two instruction
		  streams relative to the performance of an analogous
		  single stream processor. A second benefit discovered
		  in this MIMD processor is the increased utilization
		  of processor resources - turbulences in an
		  instruction stream caused by conditional branches
		  and instruction cache faults are massked by the
		  concurrent execution of other independent
		  instruction streams. The hardware required to
		  implement the instruction window and support the
		  execution of multiple instruction streams in a
		  superscalar processor is outlined."
}

@TechReport{Dageville93,
  author = 	 "Benoit Dageville and Kam-Fai Wong",
  title = 	 "Supporting thousands of threads using a hyrbrid
		  stack sharing scheme",
  institution =  "European Computer-Industry Research Centre,
		  Germany",
  year = 	 "1993",
  number =	 "ECRC93-01",
  url = 	 "ftp://ftp.ecrc.de/pub/ECRC_tech_reports/reports/ECRC-93-01.ps.Z",
  abstract =	 ""
}

@InProceedings{Dally87,
  author = 	 "William J. Dally and Linda Chao and Andrew Chien and
		  Soha Hassoun and Waldemar Horwat and John Kaplan and
		  Paul Song and Brian Totty and Scott Wills",
  title = 	 "Architecture of a Message-Driven Processor",
  pages =	 "189--196",
  booktitle =	 isca14,
  year =	 "1987",
  month =	 "Jun",
  abstract =	 ""
}

@InProceedings{Dally88a,
  author = 	 "William J. Dally",
  title = 	 "Fine-grain message-passing concurrent computers",
  volume =	 "1",
  pages =	 "2--12",
  booktitle =	 "Proceedings of  Third Conference on Hypercube
		  Concurrent Computers and Applications",
  year =	 "1988",
  month =	 "Jan",
  abstract =	 "The J-Machine efficiently executes fine-grain
		  concurrent programs by providing mechanisms that
		  reduce the overhead of message-passing, translation,
		  and context switching to approximately=5 mu s.
		  Reducing overhead to a time comparable with the
		  natural grain size of many concurrent programs
		  allows the programmer to exploit all of the
		  concurrency present in these programs rather than
		  grouping many grains together - reducing the
		  concurrency to improve the efficiency.
		  Low-dimensional k-ary n-cube networks that use
		  wormhole routing and virtual channels can send a
		  6-word message across the diameter of a 4K-node
		  concurrent computer in 4 mu s. These low-dimensional
		  networks (8<or=k<or=64 and 2<or=n<or=4) outperform
		  binary n-cubes (k=2) because they balance the
		  component of latency due to message length with the
		  component due to distance. The Message-Driven
		  Processor (MDP) can perform a task switch on message
		  arrival in 1 mu s. The MDP performs message
		  reception, buffering, and scheduling in hardware to
		  eliminate the software overhead of 100 mu s or more
		  associated with these functions."
}

@TechReport{Dally88b,
  author = 	 "William J. Dally and Andrew Chien and Stuart Fiske
		  and Waldemar Horwat and John Keen and Peter Nuth and
		  Jerry Larivee and Paul Song and Brian Totty ",
  title = 	 "Message-driven processor architecture : version 11",
  institution =  "Massachusetts Institute of Technology. Artificial
		  Intelligence Laboratory.",
  year = 	 "1988",
  number =	 "A.I. memo ; 1069",
  abstract =	 "The Message Driven Processor is a node of a
		  large-scale multiprocessor being developed by the
		  Concurrent VLSI Architecture Group.  It is intended
		  to support fine-grained, message passing, parallel
		  computation.  It contains several novel
		  architectural features, such as a low-latency
		  network interface, extensive type-checking hardware,
		  and on-chip memory that can be used as an
		  associative lookup table. This document is a
		  programmer's guide to the MDP.  It describes the
		  processor's register architecture, instruction set,
		  and the data types supported by the processor.  It
		  also details the MDP's message sending and exception
		  handling facilities."
}

@InProceedings{Dally89,
  author = 	 "William J. Dally and Andrew Chien and Stuart Fiske
		  and Waldemar Horwat and John Keen and Jerry Larivee
		  and Richard Lethin and Peter Nuth and Scott Wills",
  title = 	 "The J-machine: A Fine-grain concurrent computer",
  pages =	 "1147--1153",
  booktitle =	 "Proceedings of the IFIP 11th World Computer
		  Congress, Information Processing 89.",
  year =	 "1989",
  abstract =	 "The J-Machine is a fine-grain concurrent computer
		  that provides low-overhead primitive mechanisms for
		  communication, synchronisation, and translation.
		  Communication mechanisms are provided that permit a
		  node to send a message to any other node in the
		  machine in <2 mu s. On message arrival, a task is
		  created and dispatched in <1 mu s. A translation
		  mechanism supports a global virtual address space.
		  These mechanisms efficiently support most proposed
		  models of concurrent computation. The hardware is an
		  ensemble of up to 65536 nodes each containing a
		  36-bit processor, 4K 36-bit words of memory, and a
		  router. The nodes are connected by a high-speed 3-D
		  mesh network. This design was chosen to make the
		  most efficient use of available chip and board
		  area."
}

@Article{Dally92a,
  author = 	 "William J. Dally and Stuart Fiske and John S. Keen
		  and Richard A. Lethin and Michael D. Noakes and
		  Peter R. Nuth and Roy E. Davidson and Gregory A.
		  Fyler",
  title = 	 "The message-driven processor: a multicomputer
		  processing node with efficient mechanisms",
  journal =	 ieeemicro,
  year =	 "1992",
  volume =	 "12",
  number =	 "2",
  pages =	 "23--39",
  abstract =	 "The message-driven processor (MDP), a 36-b,
		  1.1-million transistor, VLSI microcomputer,
		  specialized to operate efficiently in a
		  multicomputer, is described. The MDP chip includes a
		  processor, a 4096-word by 36-b memory, and a network
		  port. An on-chip memory controller with error
		  checking and correction (ECC) permits local memory
		  to be expanded to one million words by adding
		  external DRAM chips. The MDP incorporates primitive
		  mechanisms for communication, synchronization, and
		  naming which support most proposed parallel
		  programming models. The MDP system architecture,
		  instruction set architecture, network architecture,
		  implementation, and software are discussed."
}

@Article{Dally92b,
  author = 	 "William J. Dally and Andrew Chien and Roy Davidson
		  and Stuart Fiske and S. Furman and Gregory Fyler and
		  D. B. Gaunce and Waldemar Horwat and S. Kaneshiro
		  and John S. Keen and Richard A. Lethin and Michael
		  D. Noakes and Peter R. Nuth and Ellen Spertus and Brian
		  Totty and Deborah Wallach and Scott Wills", 
  title = 	 "The J-machine: a fine-grain parallel computer",
  journal =	 "Computing Systems in Engineering",
  year =	 "1992",
  volume =	 "3",
  number =	 "1-4",
  pages =	 "7--15",
  month =	 "Dec",
  abstract =	 "The MIT J-machine is a fine-grain concurrent
		  computer that provides low-overhead mechanisms for
		  parallel computing. Prototype J-machines have been
		  operational since July 1991. The J-machine
		  communication mechanism permits a node to send a
		  message to any other node in the machine in <2 mu s.
		  On message arrival, a task is created and dispatched
		  in <1 mu s. A translation mechanism supports a
		  global virtual address space. These mechanisms
		  efficiently support most proposed models of
		  concurrent computation and allow parallelism to be
		  exploited at a grain size of 10 operations. The
		  hardware is an ensemble of up to 65536 nodes each
		  containing a 36-bit processor, 4 K 36-bit words of
		  on-chip memory, 256 K words of DRAM and a router.
		  The nodes are connected by a high-speed
		  three-dimensional mesh network."
}

@Article{Das89,
  author = 	 "P. C. Das and G. Barua",
  title = 	 "A threads facility for IITKIX",
  journal =	 "Computer Science and Informatics",
  year =	 "1989",
  volume =	 "19",
  number =	 "2",
  pages =	 "13--18",
  abstract =	 "The design and implementation of lightweight
		  processes (threads) in IITKIX, an experimental
		  Unix-compatible operating system are described.
		  Viable shared memory multiprocessors and LAN-based
		  distributed systems have generated an interest in
		  threads. Within a conventional Unix process,
		  multiple threads execute, sharing all the code and
		  data but with separate stacks. These threads also
		  have their own state such as open files and current
		  directory. They form a hierarchy and parent-child
		  relationships as in Unix processes, are maintained.
		  The kernel schedules threads on the CPUs and
		  supports calls for thread creation, termination,
		  suspension and synchronisation with other threads.
		  This design differs from other, related work in that
		  it presents a programming model for threads that is
		  very nearly the same as for conventional processes,
		  something familiar to the Unix programmer and hence
		  likely to be more acceptable than other models."
}

@Article{Dasgupta91,
  author = 	 "Partha Dasgupta and R. Ananthanarayanan and Sathis
		  Menon and Ajay Mohindra and Raymond Chen",
  title = 	 "Distributed Programming With Objects and Threads in
		  the Clouds System",
  journal =	 "Computing Systems",
  year =	 "1991",
  volume =	 "4",
  number =	 "3",
  pages =	 "243--275",
  month =	 "Summer",
  note =	 "Also as Tech report GIT-CC 91/26,Georgia Institute
		  of Technology, College of Computing",
  url = 	 "ftp://helios.cc.gatech.edu/pub/papers/prog-env.ps.Z",
  abstract = 	 "The CLOUDS operating system supports a distributed
		  environment consisting of compute servers, data
		  servers and user workstations. The resulting
		  environment logically simulates an integrated,
		  centralized computing system. In addition, CLOUDS
		  supports a programming paradigm that makes
		  distributed programming simpler. Distributed
		  programs can be written in a centralized fashion and
		  yet they can exploit parallelism and distribution at
		  runtime. The system paradigm is based on an
		  object/thread model. Unlike most systems, CLOUDS
		  separates the notion of memory from computation.
		  Programming environments based on these
		  abstractions, though unconventional, provide
		  powerful tools for composing applications that
		  exploit concurrency and distribution. This paper
		  discusses programming techniques that use
		  persistence and distribution of memory. The examples
		  show how separation of computation from memory can
		  be used to the programmer's advantage. They also
		  present a distributed programming technique called
		  implicit distributed programming. The implementation
		  details of the programming support subsystems are
		  presented. The system performance measurements
		  demonstrate the usability of CLOUDS as a distributed
		  programming platform."
}

@InBook{Dasgupta92,
  author = 	 "Partas Dasgputa and Richard J. LeBlanc",
  title = 	 "The structure of the Clouds distributed operating
		  system",
  chapter = 	 "??",
  publisher =	 "IOS Press",
  year =	 "1992",
  pages =	 "36--60",
  note =	 " Mission critical operating systems",
  abstract =	 "A novel system architecture, based on the object
		  model, is the central structuring concept used in
		  the Clouds distributed operating system. This
		  architecture makes Clouds attractive over a wide
		  class of machines and environments. Clouds is a
		  native operating system. and runs on a set of
		  general purpose computers connected via a local area
		  network. The system architecture of Clouds is
		  composed of a systemwide global set of persistent
		  virtual address spaces, called objects that contain
		  persistent data and code. The object concept is
		  implemented at the operating system level, thus
		  presenting a single level storage view to the user.
		  Lightweight threads carry computational activity
		  through the code stored in the objects. The
		  persistent objects and threads gives rise to a
		  programming environment composed of shared permanent
		  memory, dispensing with the need for
		  hardware-derived concepts such as file systems and
		  message systems. Clouds provides a rich environment
		  for conducting research in distributed systems. Some
		  of the topics addressed include distributed
		  programming environments, consistency of persistent
		  data and fault-tolerance."
}

@InProceedings{Davidson80,
  author = 	 "Edward S. Davidson",
  title = 	 "A multiple stream microprocessor prototype system:
		  AMP-1",
  pages =	 "9--16",
  booktitle =	 isca7,
  year =	 "1980",
  month =	 "May",
  note2 =	 "HAR",
  abstract =	 "A general-purpose multiple-stream processor with
		  shared memory and a single time-multiplexed
		  synchronous bus has been implemented. The AMP-1
		  system uses eight standard microprocessors and 64k
		  bytes of memory. The design is highly efficient in
		  the use of processor, bus, and memory
		  resources. Preliminary performance measurements
		  agree closely with an analytical memory access
		  conflict model and show extremely low conflict-based
		  performance degradation. Heavy interleaving of the
		  memory and effective multitasking of a job can yield
		  significant performance speedups. Considerations for
		  future implementations are presented"
}

@Article{Davidson91,
  author = 	 "Jack W. Davidson and David B. Whalley",
  title = 	 "Methods for Saving and Restoring Register Values
		  across Function Calls",
  journal =	 spe,
  year =	 "1991",
  volume =	 "21",
  number =	 "2",
  pages =	 "459--472",
  month =	 "Feb",
  note2        = "HAR",
  url = 	 "ftp://ftp.cs.fsu.edu/pub/whalley/papers/spe91.ps.Z",
  abstract =	 "The method used to save and restore the values of
		  registers across function calls can affect
		  performance and influence the design of specific
		  instructions. The authors describe the results of an
		  experiment that empirically evaluated six different
		  schemes for saving and restoring registers on CISC
		  machines. The six schemes were logically divided
		  into two classes. Three of the techniques do not
		  require the compiler to perform data-flow analysis,
		  whereas the other three schemes do. Within each
		  class, one scheme delegates the responsibility of
		  preserving the state of the registers to the calling
		  function. The second scheme of each class delegates
		  the responsibility of saving the registers to the
		  function being called. In the third scheme of the
		  two classes, the registers are partitioned into two
		  disjoint sets. The calling function is responsible
		  for preserving register values in one of the sets
		  whereas the called function is responsible for the
		  other set. For each class the third scheme is shown
		  to produce the most effective code."
}

@Misc{Dean92,
  author =	 "Randall W. Dean",
  title =	 "Mach 3.0 cthreads PACKAGE",
  year =	 "1992",
  month =	 "Dec",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/unpublished/internal_slides/Cthreads/mi-cthreads.ps",
  abstract =	 "Slides from Randall Dean's talk on the Mach 3.0
		  cthread package"
}

@InProceedings{Dean93,
  author = 	 "Randall Dean",
  title = 	 "Using Continuations to Build a User-Level Threads Library",
  pages =	 "137--151",
  booktitle =	 "Proceedings of the Third USENIX Mach Conference",
  year =	 "1993",
  month =	 "Apr",
  note2        = "HAR",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/cont_threads.ps",
  abstract =	 "We have designed and built a user-level threads
		  library that uses continuations for transfers of
		  control. The use of contiuations reduces the amount
		  of state that needs to be saved and restored at
		  context switch time, thereby reducing the
		  instruction count in the critical sections. Our
		  multiprocessor contention benchmarks indicate that
		  this reduction and use of Busy Spinning, Busy
		  Waiting and Spin Polling increases throughput by as
		  much as on a multiprocessor. In addition, flattening
		  the locing hierarchy reduces context switch latency
		  by 5% to 49% on both uniprocessors and
		  multiprocessors. This paper describes the library's
		  design and compares its overall performance
		  characteristics to the existing implementations."
}

@InProceedings{Debaere87,
  author = 	 "E. Debaere",
  title = 	 "A Language Coprocessor for the Interpretation of
		  Threaded Code",
  pages =	 "593--602",
  booktitle =	 euromicro87,
  year =	 "1987",
  note =	 "Published in " # mm #  " Vol.21 No. ?",
  abstract =	 ""
}

@InProceedings{Degroot90,
  author = 	 "Doug DeGroot",
  title = 	 "Throttling and speculating on parallel
		  architectures",
  editor =	 "N. Rishe and S. Navathe and D. Tal",
  pages =	 "2--4",
  booktitle =	 "PARBASE-90 International Conference on Databases,
		  Parallel Architectures and Their Applications",
  year =	 "1990",
  publisher =	 "IEEE",
  address =	 "Miami Beach, FL, USA",
  month =	 "Mar",
  abstract =	 "Throttling of parallelism is of importance to
		  dynamic parallelism models in which the number and
		  sizes of possible parallel code segments (tasks,
		  processes, threads, etc.) are unknown at compile
		  time, and which, if left uncontrolled, may lead to
		  explosive parallelism with resulting slowdown rather
		  than speedup. The primary goal of throttling is
		  preventing a dynamically unfolding parallel
		  execution from creating so many parallel code
		  segments that either: the system runs out of
		  resources with which to manage the parallel
		  segments; the system begins to thrash as a result of
		  the increased process management and communication
		  requirements; or the system simply wastes CPU time
		  creating extra code segments which end up being
		  executed sequentially anyway. On the other hand,
		  speculative parallelism involves the parallel
		  execution of code segments which are not yet known
		  to be required. It can be exploited in two basic
		  forms: parallel search and parallel test.
		  Speculative parallelism can be used when there are
		  ideal resources and the currently executing set of
		  codes is not expected to yield any more parallel
		  threads of execution."
}

@MastersThesis{Dellarocas91,
  author = 	 "Chrysanthos N. Dellarocas",
  title = 	 "High-Performance Retargetable Simulator for Parallel
		  Architectures",
  school =       "Massachusetts Institute of Technology, Laboratory of
		  Computer Science",
  year = 	 "1991",
  month =	 "Jun",
  note =	 "97 pages. Also as tech report MIT/LCS/TR-505",
  abstract = 	 "In this thesis, we describe Proteus, a
		  high-performance simulation-based system for the
		  evaluation of parallel algorithms and system
		  software. Proteus is built around a retargetable
		  parallel architecture simulator and a flexible data
		  collection and display component. The simulator uses
		  a combination of simulation and direct execution to
		  achieve high performance, while retaining simulation
		  accuracy. Proteus can be configured to simulate a
		  wide range of shared-memory and message-passing MIMD
		  architectures and the level of simulation detail can
		  be chosen by the user. Detailed memory, cache and
		  network simulation is supported. Parallel programs
		  can be written using a programming model based on C
		  and a set of runtime system calls for thread and
		  memory management. The system allows nonintrusive
		  monitoring of arbitrary information about an
		  execution, and provides flexible graphical utilities
		  for displaying recorded data. To validate the
		  accuracy of the system, a number of published
		  experiments were reproduced on Proteus. In all cases
		  the results obtained by simulation are very close to
		  those published, a fact that provides support for
		  the reliability of the system. Performance
		  measurements demonstrate that the simulator is one
		  to two orders of magnitude faster than other similar
		  multiprocessor simulators. "
}

@InProceedings{Dennis88,
  author = 	 "Jack B. Dennis and Guang R. Gao",
  title = 	 "An efficient pipelined dataflow processor
		  architecture",
  pages =	 "368--373",
  booktitle =	 supcomp88,
  year =	 "1988",
  abstract =	 "It is demonstrated that the principles of pipelined
		  instruction execution can be effectively applied in
		  data-flow computers, yielding an architecture that
		  avoids the main sources of pipeline gaps during
		  program execution in many conventional designs. The
		  processing element uses an architecture called
		  argument-fetch data-flow architecture. It has two
		  parts: a data-flow instruction scheduling unit
		  (DISU) and a pipelined instruction processing unit
		  (PIPU). The PIPU is an instruction processor that
		  uses many conventional techniques to achieve fast
		  pipelined operation. The DISU holds the data-flow
		  signal graph of the collection of data-flow
		  instructions allocated to the processing element and
		  maintains a large pool of enabled instructions
		  available for execution by the PIPU. The
		  architecture provides a basis for achieving high
		  performance for many scientific applications. The
		  trial design and fabrication of an enable memory, a
		  key component of the DISU, are reported"
}

@TechReport{Dennis94a,
  author = 	 "Jack B. Dennis",
  title = 	 "Memory Models and Cache Management for a
		  Multithreaded Program Execution Model",
  institution =  "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1994",
  number =	 "CSG-Memo-363",
  month =	 "Oct",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-363.ps.gz",
  abstract =	 ""
}

@InBook{Dennis94b,
  author = 	 "Jack B. Dennis and Guang R. Gao",
  title = 	 "Multithreaded computer architecture: A summary of
		  the state of the art",
  chapter = 	 "Multithreaded Architectures: Principles, Projects,
		  and Issues",
  publisher =	 "Kluwer academic",
  year =	 "1994",
  pages =	 "1--74",
  note2        = "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo29.ps.gz",
  abstract =	 "The architecture of future high performance computer
		  systems will respond to the possibilities offered by
		  technology and to the increasing demand for
		  attention to issues of
		  programmability. Multithreaded processing element
		  architectures are a promising alternative to RISC
		  architecture and its multiple-instruction issue
		  extensions such as VLIW, superscalar, ad
		  superpipelined architectures. This paper presents an
		  overview of multithreaded computer architectures and
		  the technical issues affecting their prospective
		  evolution. We introduce the basic concepts of
		  multithreaded computer architecture and describe
		  several architectures representative of the design
		  space for multithreaded parallel computers. We
		  review design issues for multithreaded processing
		  elements intended for use as the node processor of
		  parallel computers for scientific computing. These
		  include the question of choosing an appropriate
		  program execution model, the organization of the
		  processing element to achive good utilization of
		  major resources, support for fine-grain
		  interprocfessor communication and global memory
		  access, compiling machine code for multithreaded
		  processors, and the challenge of implementing
		  virtual memory in large-scale multiprocessor
		  systems"
}

@InProceedings{Devarakonda92,
  author = 	 "Murty Devarakonda and Arup Mukherjee",
  title = 	 "Issues in implementation of cache-affinity
		  scheduling",
  pages =	 "345--357",
  booktitle =	 usenixw92,
  year =	 "1992",
  address =	 "San Francisco, CA, USA",
  month =	 "Jan",
  note2        = "HAR",
  abstract =	 "In a shared memory multiprocessor, a thread may have
		  an affinity to a processor because of the data
		  remaining in the processor's cache from a previous
		  dispatch. The authors show that two basic problems
		  should be addressed in a Unix-like system to exploit
		  cache affinity for improved performance: first, the
		  limitation of the Unix dispatcher model ('processor
		  seeking a thread'); second, pseudo-affinity caused
		  by low-cost waiting techniques used in a threads
		  package such as C threads. They demonstrate that the
		  affinity scheduling is most effective when used in a
		  threads package that supports multiplexing of user
		  threads on kernel threads."
}

@TechReport{Dibella92,
  author =       "Kathleen S. Dibella and Krithi Ramamritham and Panos
		  Chrysanthis and S. Raghuram",
  title =        "Scheduling Algorithms and their Performance on
		  Shared Memory Multiprocessors",
  institution =  "University of Massachusetts",
  number =       "COINS TR 92-37",
  year =         "1992",
  abstract =     "Many multithreaded multiprocessing systems have
		  simply adopted the scheduling structure used in
		  single processing systems: the single central queue
		  of ready processes maintained for the system creates
		  a source of contention. Hence researchers have been
		  trying to eliminate or redefine redundant and
		  inefficient data structures and deal with other
		  aspects that contribute to excessive waits. This
		  paper examines an alternative scheduling structure
		  which reduces the negative effects of the ready
		  queue bottleneck. The proposed structure is composed
		  of multiple ready queues which make it possible for
		  several processes to access the queue(s)
		  simultaneously. Each processor removes a ready
		  thread (request) from a selected ready queue for
		  execution. The scheduling policy determines which
		  ready queue is selected. The application processes
		  post ready to run threads in the ready queues
		  according to a queuing policy. Different
		  combinations of the scheduling and queuing policies
		  are possible but the policies have to be
		  compatible. This relative independence of the two
		  types of policies has allowed us to implement and
		  test many policy combinations. We also compare their
		  performance with existing scheduling
		  implementations. Results of tests based on our
		  implementation on a Sequent Symmetry multiprocessor
		  show that multi-access distributed ready queues
		  offer potential performance benefits for
		  multiprocessor systems and that the overheads of
		  managing multiple queues is more than overcome by
		  their improved performance."
}

@techreport{Doeppner87a,
  title   =      "A Threads Tutorial.",
  author  =      "Thomas W. Doeppner",
  year    =      "1987",
  month   =      "Mar",
  number  =      "CS-87-06",
  institution =  "Brown University, Department of Computer Science",
  abstract =     ""
}

@techreport{Doeppner87b,
  title   =      "Threads: A System for the Support of Concurrent
		  Programming",
  author  =      "Thomas W. Jr. Doeppner",
  institution =  "Brown University, Department of Computer Science",
  address =      "Providence, RI 02912",
  year    =      "1987",
  month   =      "Jun",
  number  =      "CS-87-11",
  abstract =     ""
}

@techreport{Doeppner88a,
  title   =      "A Threads Tutorial - Fortran Version",
  author  =      "Thomas W. Jr. Doeppner",
  institution =  "Brown University, Department of Computer Science",
  address =      "Providence, RI 02912",
  year    =      "1988",
  month   =      "Jun",
  number   =     "CS-88-19",
  abstract =     "Threads is a system for the efficient support of
		  concurrency. It runs on either single-processor or
		  multiprocessor computers and presents to the
		  programmer the same ``view of the world'' no matter
		  how many processors are available for use. This view
		  is that a number of concurrent {\em threads of
		  control} are executing in a single shared address
		  space and share a common view of which files are
		  open. Thus threads may communicate very efficiently
		  through this shared memory and all threads may
		  participate in I/O on any file. In this paper we
		  discuss the most important routines provided by the
		  Threads system and give examples of their use."
}

@InProceedings{Doeppner88b,
  author = 	 "Thomas W. {Doeppner, Jr.} and David D. Johnson",
  title = 	 "A Multi-Thread Debugger",
  pages =	 "295--297",
  booktitle =	 "Proceedings of the ACM SIGPLAN/SIGOPS Workshop on
		  Parallel and Distributed Debugging",
  year =	 "1988",
  month =	 "May",
  abstract =	 ""
}

@TechReport{Doeppner93,
  author = 	 "Thomas W. Jr. Doeppner ",
  title = 	 "The Thread-Monitor Library: A System Monitoring
		  Solaris-Threads Programs",
  institution =  "Brown University, Department of Computer Science",
  year = 	 "1993",
  number = 	 "CS93-53",
  month = 	 "Nov",
  url = 	 "ftp://ftp.cs.brown.edu/pub/techreports/93/cs93-53.ps.Z",
  abstract = 	 "Debugging a multi-threaded program involves all the
		  difficulties of debugging a single-threaded program
		  with the additional concern of the interaction among
		  threads. Multithreaded programs can be
		  nondeterministic-the outcome of the program depends
		  upon the ordering of the executions of the various
		  threads. When testing a program, one would like to
		  exert some control over this ordering, to force
		  certain execution sequences to occur so that they
		  can be tested. The Thread-Monitor Library is an
		  experimental system that allows one to monitor and
		  control the execution of a multithreaded program
		  (written using the Solaris Threads package). It
		  creates separate windows for controlling each thread
		  and separate windows for monitoring each
		  synchronization object (currently, only mutexes and
		  condition variables are supported). Whenever a
		  thread enters a synchronization construct, its
		  execution is immediately suspended and the
		  programmer is prompted for permission to resume
		  execution. The operations applied to each of the
		  synchronization objects are listed in the
		  synchronization object windows, allowing one to keep
		  track of their state. All of this is accomplished
		  through a collection of ``wrappers'' for the Solaris
		  Threads API. Rather than call the standard Solaris
		  routine, one instead calls the appropriate wrapper
		  which calls upon the monitor code as well as
		  completing the call to the Solaris routine."
}

@InProceedings{Doligez93,
  author = 	 "Damien Doligez and Xavier Leroy",
  title = 	 "A concurrent, generational garbage collector for a
		  multithreaded implementation of {ML}",
  pages =	 "113--123",
  booktitle =	 "Twentieth Annual ACM SIGPLAN-SIGACT Symposium on
		  Principles of Programming Languages",
  year =	 "1993",
  address =	 "Charleston, SC, USA",
  month =	 "Jan",
  url =     "ftp://ftp.inria.fr/INRIA/Projects/cristal/Xavier.Leroy/concurrent-gc.ps.gz",
  abstract =	 "The design and implementation are presented of a
		  'quasi real-time' garbage collector for Concurrent
		  Caml Light, an implementation of ML with threads.
		  This two-generation system combines a fast,
		  asynchronous copying collector on the young
		  generation with a non-disruptive concurrent marking
		  collector on the old generation. This design
		  crucially relies on the ML compile-time distinction
		  between mutable and immutable objects."
}

@InProceedings{Donalson93,
  author = 	 "D. Donalson and Mauricio J. Serrano and Roger C.
		  Wood and Mario Nemirovsky",
  title = 	 "DISC: dynamic instruction stream computer-an
		  evaluation of performance",
  pages =	 "448--456",
  volume =	 "I: Architecture",
  booktitle =	 hicss26,
  year =	 "1993",
  address =	 "Wailea, HI, USA",
  month =	 "Jan",
  abstract =	 "DISC is a simple processor architecture targeted for
		  real-time applications. The architecture is based on
		  dynamic fine-grained multithreading where the next
		  instruction is fetched from one of several possible
		  simultaneously active threads. The DISC architecture
		  uses a combination of concepts including, a register
		  stack file, a four stage pipeline, up to four active
		  threads, a dynamic scheduler, and special
		  input/output (I/O) and interrupt constructs to allow
		  maximization of performance for real-time control
		  applications. Previous stochastic results were very
		  encouraging and so a synthetic benchmark was
		  developed to allow more detailed testing. The
		  benchmark was based on a Hughes Aircraft Company
		  satellite control system, and assembled with the
		  DISC assembler. The model was designed and run in
		  the Verilog simulation language."
}

@InProceedings{Draves91a,
  author =       "Richard P. Draves and Brian N. Bershad and Richard
		  F. Rashid and Randall W. Dean",
  title =        "Using continuations to implement thread management
		  and communication in operating systems",
  booktitle =    sosp13,
  adresse =      "Asilomar, Pacific Grove, CA",
  publisher =    "Association for Computing Machinery SIGOPS",
  month =        oct,
  year =         "1991",
  pages =        "122--136",
  notes =        "Published in " # acmosr # " Vol.25, No.5, 1991. Also
		  Tech report CMU-CS-91-115, Carnegie Mellon University,
		  School of Computer Science",
  note2      =   "HAR",
  url =     "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/threadmgnt.ps",
  abstract =     "We have improved the performance of the Mach 3.0
		  operating system by redesigning its internal thread
		  and interprocess communication facilities to use
		  continuations as the basis for control
		  transfer. Compared to previous versions of Mach 3.0,
		  our new system consumes 85% less space per
		  thread. Cross-address space remote procedure calls
		  execute 14%faster. Exception handling runs over 60%
		  faster. In addition to improving system performance,
		  we have used continuations to generalize many
		  control transfer optimizations that are common to
		  operating systems, and have recast those
		  optimizations in terms of a single implementation
		  methodology. This paper describes our experiences
		  with using continuations in the Mach operating
		  system."
}

@TechReport{Draves91b,
  author = 	 "Richard P. Draves",
  title = 	 "Continuations: unifying thread management and
		  communication in operating system",
  institution =  "Carnegie-Mellon University, School of Computer
		  Science",
  year = 	 "1991",
  number = 	 "CMU-CS-91-115",
  month = 	 "Mar",
  note =         "21 pages",
  abstract =     "Thread management and interprocess communication are
		  two key operating systems services. Within an
		  operating system, these services can be unified by
		  managing the state of a computation as a
		  continuation, a first class object that can be
		  explicitly examined and manipulated through a
		  well-defined interface. Continuations as first class
		  objects improve the performance of thread management
		  and interprocess communication facilities, and can
		  be used to generalize many optimizations that are
		  common to operating systems. These optimizations can
		  be recast in terms of a single, unifying
		  implementation methodology. We have used
		  continuations to redesign the internals of the Mach
		  operating system at Carnegie Mellon University. On a
		  DECstation 3100, our new system consumes over 85%
		  less space per thread and executes a cross-address
		  space procedure call 14% faster than earlier,
		  heavily optimized versions. This paper describes the
		  application of continuations to the Mach operating
		  system."
}

@InProceedings{Dubey94a,
  author = 	 "Pradeep Dubey and Arvind Krishna and Mark
		  Squillante",
  title = 	 "Analytic Performance Modeling for a Spectrum of
		  Multithreaded Processor Architectures",
  pages =	 "??--??",
  booktitle =	 "Intl Workshop on Modeling, Analysis and
		  Simulation of Comp. and Telecomm. Systems
		  (MASCOTS)",
  year = 	 "1995",
  month =	 "Jul",
  note =	 "Also as Tech report RC 19661 1994, IBM Research
		  division, T.J.Watson Research center",
  note2 =	 "HAR",
  url = 	 "http://www.watson.ibm.com:8080/main-cgi-bin/search_paper.pl/entry_ids=121",
  abstract =	 "The throughput of pipelined processors suffers due
		  to delays associated with instruction dependencies
		  and memory latencies. Multithreaded architectures
		  try to tolerate such delays by sharing the pipeline
		  with independent instruction threads. This paper
		  proposes a comprehensive analytical framework to
		  quantitate the performance potential of a wide
		  spectrum of multithreaded machines ranging from
		  those that are capable of switching threads every
		  cycle to those that switch threads only on long
		  inter-instruction latencies. For machines in the
		  former category, the proposed analytic model
		  provides an exact solution for pipeline utilization
		  which is significantly better than lower and upper
		  bounds obtainable from simple approximation
		  techniques. Unlike previously published analytic
		  models of such systems, the Markov model developed
		  here accepts a general distribution for the
		  interlock delays with multiple latencies. For
		  machines in the latter category, the paper provides
		  an approximate analytic model which is simpler than
		  previously published analytic models. The models
		  have been verified using previously published
		  analytical and simulation- based results. As
		  compared to the simulation alternative, the models
		  provide a much quicker estimate of pipeline
		  utilization as a function of a number of threads."
}

@InProceedings{Dubey94b,
  author = 	 "Pradeep K. Dubey and Arvind Krishna and Michael J.
		  Flynn",
  title = 	 "Analytical modeling of multithreaded pipeline
		  performance",
  editor =	 "T. Mudge and B. D. Shriver",
  volume =	 "I",
  pages =	 "361--367",
  booktitle =	 hicss27,
  year =	 "1994",
  address =	 "Wailea, HI, USA",
  month =	 "Jan",
  note =	 "Also as Research Report RC 19549 - May 03, 1994.IBM
		  Research division, T.J.Watson Research center",
  url = 	 "http://www.watson.ibm.com:8080/main-cgi-bin/search_paper.pl/entry_ids=151",
  abstract =	 "The throughput of pipelined processors suffers due
		  to delays associated with instruction dependencies
		  and memory latencies. Multithreaded architectures
		  try to tolerate such delays by sharing the pipeline
		  with independent instruction threads. This paper
		  proposes an analytic model which is used to
		  quantitate the advantage of multithreaded
		  architectures. The analytic model provides an exact
		  solution, which is significantly better than bounds
		  obtainable from simpler approximate techniques.
		  Unlike previous analytic models of such systems, the
		  model presented here accepts a general distribution
		  for the interlock delays with multiple latencies.
		  The model provides a much quicker performance
		  estimate than simulation. The model has been
		  validated for a variety of input distributions using
		  previously published simulation-based results."
}

@inProceedings{Dubey95,
  author = 	 "Pradeep K. Dubey and Kevin O'Brien and Kathryn M.
		  O'Brien and  Charles Barton",
  title = 	 "Single-Program Speculatives Multithreading (SPSM)
		  Architecture: Compiler-Assisted Fine-Grained
		  Multithreading",
  year = 	 "1995",
  booktitle =    pact95,
  pages=         "??--??",
  month =	 "Jun",
  note =         "Also as Tech report RC 19928, IBM Research division
		  T.J.Watson Research center, Feb.1995",
  note2 =	 "HAR",
  url = 	 "http://www.watson.ibm.com:8080/main-cgi-bin/search_paper.pl/entry_ids=32",
  abstract =	 "Recent limit studies on instruction-level parallel
		  processing, based on non-numeric applications, have
		  reported significant performance gains from
		  speculative execution of multiple control flows.
		  This paper describes a new single- program
		  speculative multithreading (SPSM) architecture,
		  which can be viewed as an extension of any existing
		  single- thread architecture. It enables speculative
		  fetch, decode, and execution from multiple program
		  locations simultaneously. Instruction threads are
		  generated at compile-time using control dependence
		  analysis. Inter- thread data dependences are also
		  analyzed at compile-time. However, resource binding
		  of instructions is performed only at run time, to
		  offer binary compatibility across different
		  implementations. New thread generation algorithms,
		  being prototyped in a version of the TOBEY compiler,
		  are also described. The SPSM architecture includes
		  novel fork/suspend instructions which are used to
		  identify independent instruction threads, and also
		  to specify compile-time control flow speculations
		  associated with inter-thread dependences"
}

@InProceedings{Duchamp91,
  author = 	 "Dan Duchamp",
  title = 	 "Experience with threads and {RPC} in Mach",
  pages =	 "87--104",
  booktitle =	 "Proceedings of 2nd symposium on Experiences with
		  Distributed and Multiprocessor systems (SEDMS II)",
  year =	 "1991",
  address =	 "Atlanta, GA, USA",
  month =	 "Mar",
  abstract =	 "Undertaking the design of a multi-threaded
		  event-driven program requires the programmer to
		  grapple not only with how to control concurrency,
		  but also with mechanisms and policies specified by
		  underlying software, including the scheduler, the
		  RPC stub compiler, and of course the thread facility
		  itself. The author relates the experience acquired
		  with Mach's implementation of these functions while
		  writing a particular multi-threaded program. Advice
		  is given to future designers of both multi-threaded
		  applications and underlying systems software."
}

@Article{Eager93,
  author =       "Derek L. Eager and John Zahorjan",
  title =        "Chores: Enhanced Run-Time Support for Shared-Memory
		  Parallel Computing",
  journal =      tocs,
  pages =        "1--32",
  volume =       "11",
  number =       "1",
  note  =        "Also as Tech report UW-CSE-91-08-05, University of
		  Washington, Department of Computer Science and
		  Engineering, Feb 1991",
  year =         "1993",
  month =        "Feb",
  abstract =  "Parallel computing is increasingly important in the
		  solution of large-scale numerical problems. The
		  difficulty of efficiently hand-coding parallelism,
		  and the limitations of parallelizing compilers, have
		  nonetheless restricted its use by scientific
		  programmers. In this paper we propose a new
		  paradigm, chores, for the run-time support of
		  parallel computing on shared-memory multiprocessors.
		  We consider specifically uniform memory access
		  shared-memory environments, although the chore
		  paradigm should also be appropriate for use within
		  the clusters of a large-scale non-uniform memory
		  access machine. We argue that chore systems attain
		  both the high efficiency of compiler approaches for
		  the common case of data parallelism, and the
		  flexibility and performance of user-level thread
		  approaches for functional parallelism. These
		  benefits are achieved within a single, simple
		  conceptual model that almost entirely relieves the
		  programmer and compiler from concerns of
		  granularity, scheduling, and enforcement of
		  synchronization constraints. Performance
		  measurements are presented for an implemention of
		  chores on a Sequent Symmetry multiprocessor."
}

@InProceedings{Eickemeyer87,
  author = 	 "Richard Eickemeyer and Janak H. Patel",
  title = 	 "Performance evaluation of Multiple register sets",
  pages =	 "264--271",
  booktitle =	 isca14,
  year =	 "1987",
  note2        = "HAR",
  abstract =	 "In this paper a DEC VAX with multiple register sets
		  is evaluated under many differently sized register
		  sets. Both the number of registers per set were
		  varied. Performance, measured in terms of memory
		  traffic, is compared to that of a standard
		  VAX. Memory traffic is measured from many real
		  program traces on the standard processor and from
		  transformations of the trace for the multiple
		  register set processors. Results are presented for
		  each program; an empirical formula is derived which
		  describes the average programs behaviour. A decrease
		  in memory references of approximately 16% can be
		  expeceted using multpile register sets"
}

@PhdThesis{Eickemeyer88,
  author = 	 "Richard Eickemeyer",
  title = 	 "Performance evaluation of multiple register set
		  architectures and cache memories",
  school = 	 "University of Illinois at Urbana-Champaign,
		  Department of Electrical and Computer Engineering",
  year = 	 "1988",
  month =	 "Jan",
  note =	 "As tech report UILU-ENG-88-2207",
  note2 = 	 "HAR",
  abstract =	 "Multiple register set architectures have been
		  proposed as a method to reduce the large amount of
		  memory traffic associated with high-level language
		  procedure calls. In this thesis, the effect on
		  overall processor memory traffix of multiple
		  register sets is characterized for a set of real
		  programs. To accomplish this, traces of programs
		  address and register references were collected on a
		  VAX-11/780. These traces were then mapped to the
		  traces that could have come from a hypothetical VAX
		  with multiple register sets. The memory traffic for
		  various numbers of register sets and numbers of
		  registers per set were compared to the memory
		  traffic in the original trace. The results show a
		  10-20% reduction in memory traffic using the
		  multiple register sets; the results vary among
		  programs. The effect of context switch was also
		  measured and found not to be significant. A
		  synthetic benchmark program, Dhampstone was written
		  in the C language with the goal to accurately
		  characterize program behaviour with and without
		  multiple register sets. To construct the program,
		  existing program statistics wre supplemented with
		  measurements of I/O and C language register
		  declarations. The change in memory traffic with
		  multiple register sets on Dhampstone was compared to
		  the change measured in the real programs; there was
		  good agreement fort a variety of numbers of sets and
		  numbers of registers per set. Multiple register sets
		  were compared to other local memory organizations
		  using a chip area model and a memory access time
		  model. These organizations consisted of a variety of
		  different cache organizations. Combinations of
		  different caches and multiple register sets were
		  also examined. With a small chip area, a top of
		  stack cache performed better than multiple register
		  sets. For larger chip areas , an instruction cache,
		  or better yet, an instruction cache plus a small
		  top-of-stack cache perform well. At the largest
		  cache sizes studied, a full cache is required so
		  that each memory reference could potentially be in
		  the cache. Adding multiple register sets to a full
		  cache improves performance slightly for larger chip
		  areas"
}

@TechReport{Ekanadham86,
  author = 	 "Kattamuri Ekanaddham",
  title = 	 "Multi-tasking on a dataflow-like architecture",
  institution =  "IBM Research division, T.J.Watson Research center",
  year = 	 "1986",
  number =	 "RC 12307",
  month =	 "Nov",
  abstract =	 ""
}

@InProceedings{Elmasri95,
  author = 	 "Nasser Elmasri and Herbert H.J. Hum and Guang R.
		  Gao",
  title = 	 "The Threaded Communication Library: Preliminary
		  Experience on a Multiprocessor with Dual-Processor
		  Nodes",
  pages =	 "??--??",
  booktitle =	 "??",
  year =	 "1995",
  month =	 "Jul",
  note =	 "Also as Tech memo McGill University ,School of
		  computer Science, ACAPS-89",
  note2 =	 "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo89.ps.gz",
  abstract =	 ""
}

@InProceedings{Emer78,
  author = 	 "Joel S. Emer and Edward S. Davidson",
  title = 	 "Control store organization for multiple stream
		  pipelined processor",
  pages =	 "43--48",
  booktitle =	 icpp78,
  year =	 "1978",
  abstract =	 ""
}

@PhdThesis{Emer79,
  author = 	 "Joel S. Emer",
  title = 	 "Shared resources for multiple instruction stream
		  pipelined processors",
  school = 	 "University of Illinois at Champaign-Urbana,
		  Department of Electrical and Computer Engineering",
  year = 	 "1979",
  note =	 "As tech report Coordinated Science Lab R-838",
  abstract =	 ""
}

@TechReport{Engelhardt94,
  author = 	 "Dean Engelhardt and Andrew Wendelborn",
  title = 	 "Expressing Nested data parallel operations through
		  multithreading", 
  institution =  "University of Adelaide, Department of Computer
		  Science",
  year = 	 "1994",
  number =	 "??",
  address =	 "South Australia",
  month =	 "May",
  note =	 "28 pages",
 url = 	 "ftp://ftp.cs.adelaide.edu.au/pub/ParFP/techreports/nested-dp-threads.ps.Z",
  abstract =	 ""
}

@TechReport{Engler93,
  author = 	 "Dawson R. Engler and Gregory R. Andrews and David K.
		  Lowenthal",
  title = 	 "Filaments : Efficient support for fine-grain
		  parallelism",
  institution =  "University of Arizona. Dept. of Computer Science",
  year = 	 "1993",
  number =	 "93-13",
  month =	 "Apr",
  note2        = "HAR",
  url =     "ftp://ftp.cs.arizona.edu/reports/1993/TR93-13a.ps.Z",
  abstract =	 "It has long been thought that coarse-grain
		  parallelism is much more efficient than fine-grain
		  parallelism due to the overhead ofprocess (thread)
		  creation, context switching, and synchronization. On
		  the other hand, there are several advantages to
		  fine-grain parallelism: ease of programming for many
		  applications, architecture independence, and load-
		  balancing potential. This paper describes techniques
		  that support efficient execution of fine-grain
		  parallel programs on shared- memory multiprocessors.
		  These are implemented in a package called Filaments,
		  which supports three kinds of threads: run-to-
		  completion, barrier (iterative), and
		  fork/join.Efficiency results primarily from making
		  threads stateless, i.e., they have no private stack;
		  this also greatly reduces memory consumption.The
		  gains in performance are such that a fine-grain
		  implementation of Jacobi Iteration with a thread per
		  point is within 11 percent of a coarse- grain
		  program with only one task per processor on a
		  Sequent Symmetry. Execution times for problems with
		  more work per thread -- such as matrix
		  multiplication -- are usually indistinguishable from
		  coarse-grain programs and can be substantially
		  faster when the amount of work per thread varies."
}

@Article{English95,
  author = 	 "John English",
  title = 	 "Multithreading in C++.  ",
  journal =	 "Acm sigplan notices  ",
  year =	 "1995",
  volume =	 "30",
  number =	 "4",
  pages =	 "21--??",
  month =	 "Apr",
  abstract =	 ""
}

@InProceedings{Eykholt92,
  author = 	 "Joseph R. Eykholt and Steve R. Kleiman and Steve
		  Barton and Jim Voll and Roger Faulkner and Anil
		  Shivalingiah and Mark Smith and and Dan Stein and
		  Mary Weeks and Dock Williams",
  title = 	 "Beyond multiprocessing: multithreading the SunOS
		  kernel",
  pages =	 "11--18",
  booktitle =	 usenixs92,
  year =	 "1992",
  address =	 "San Antontio, TX, USA",
  month =	 "Jun",
  note2        = "HAR",
  url = 	 "http://www.sun.com/sunsoft/Developer-products/sig/threads/papers/beyond_mp.ps",
  abstract =	 "Preparing the SunOS/SVR4 kernel for today's
		  challenges: symmetric multiprocessing,
		  multi-threaded applications, real-time, and
		  multimedia, led to the incorporation of several
		  innovative techniques. In particular, the kernel was
		  re-structured around threads. Threads are used for
		  most asynchronous processing, including interrupts.
		  The resulting kernel is fully preemptible and
		  capable of real-time response. The combination
		  provides a robust base for highly concurrent,
		  responsive operation."
}

@InProceedings{Fahringer95,
  author = 	 "Thomas Fahringer and Matthew Haines and Piyush
		  Mehrotra",
  title = 	 "On the Utility of Threads for Data Parallel
		  Programming",
  pages =	 "??--??",
  booktitle =	 "??",
  year =	 "1995",
  month =	 "Jul",
  note =	 "Also as Tech report 95-35  Institute for Computer
		  Applications in Science and Engineering, NASA
		  Langley Research Center",
  url = 	 "ftp://ftp.icase.edu/pub/techreports/95/95-35.ps.Z",
  abstract =	 "Threads provide a useful programming model for
		  asynchronous behavior because of their ability to
		  encapsulate units of work that can then be scheduled
		  for execution at runtime, based on the dynamic state
		  of a system. Recently, the threaded model has been
		  applied to the domain of data parallel scientific
		  codes, and initial reports indicate that the
		  threaded model can produce performance gains over
		  non-threaded approaches, primarily through the use
		  of overlapping useful computation with communication
		  latency. However, overlapping computation with
		  communication is possible without the benefit of
		  threads if the communication system supports
		  asynchronous primitives, and this comparison has not
		  been made in previous papers. This paper provides a
		  critical look at the utility of lightweight threads
		  as applied to data parallel scientific programming."
}

@InProceedings{Fan92a,
  author = 	 "Xiaoming Fan",
  title = 	 "Queue-based primitives for a multithreaded architecture",
  pages =	 "113--116",
  booktitle =	 euromicro91,
  year =	 "1991",
  address =	 "Vienna, Austria",
  month =	 "Sep",
  note =	 "Published in Microprocessing {\&} Microprogramming,
		  Vol.34, No.1--5, Feb 1992",
  abstract =	 "Multithreaded architectures have recently gained
		  attention either as uniprocessor or as Processing
		  Element (PE) in multiprocessing systems. In the
		  paper, simple, but powerful queue-based primitives
		  are presented for such an architecture being
		  developed. The conventional queue structure is
		  extended, then used as the central structure for
		  scheduling of threads, communication and
		  synchronisation of concurrent threads, resource
		  management and so on. It has been shown that such
		  primitives are efficient and easy to implement in
		  hardware."
}

@Article{Fan92b,
  author = 	 "Xiaoming Fan",
  title = 	 "Realization of multiprocessing on a RISC-like
		  architecture",
  journal =	 mm,
  year =	 "1992",
  volume =	 "33",
  number =	 "4",
  pages =	 "195--206",
  month =	 "Jun",
  abstract =	 "A processor in a multiprocessor environment must be
		  able to tolerate memory latency and handle
		  synchronization efficiently. Multithreaded execution
		  is a technique which can support not only
		  multiprocessing directly at the architecture level
		  but also hiding memory latency by using parallelism
		  in multiprocessor environments. The author presents
		  a processor architecture MTMA that combines the
		  RISC- and multithreaded properties. Instead of
		  tagged memory and its complex control logic, he uses
		  queue-based operations as primitives for
		  synchronization of different threads and other
		  issues. It has been shown that this approach can
		  reduce the complexity of hardware implementation
		  greatly. Furthermore, queue-based primitives can
		  also be applicable far beyond synchronization of
		  parallel threads. The MTMA's architecture is simple,
		  hence, it is expected to integrate the architecture
		  on a single chip"
}

@InProceedings{Fan93a,
  author = 	 "Xiaoming Fan",
  title = 	 "Analysis of multithreaded architectures: a case
		  study",
  pages =	 "87--90",
  booktitle =	 euromicro92,
  year =	 "1993",
  note =	 "Published in Microprocessing {\&} Microprogramming,
		  37,1993",
  note2 =	 "HAR",
  abstract =	 ""
}

@PhdThesis{Fan93b,
  author = 	 "Xiaoming Fan",
  title = 	 "Latency-directed multithreaded computation and its
		  architectural support",
  school = 	 "University of Hamburg, Germany. Fachbereicht
		  Informatikk",
  year = 	 "1993",
  month =	 "Mar",
  note2 =	 "HAR",
  url = 	 "http://tech-www.informatik.uni-hamburg.de/Paper/1994/fan-diss/dissertation_fan.ps.gz",
  abstract =	 ""
}

@InProceedings{Farquhar93,
  author = 	 "W. G. Farquhar and Paraskevas Evripidou",
  title = 	 "DART: a data-driven processor architecture for
		  real-time computing",
  pages =	 "141--152",
  booktitle =	 "Architectures and Compilation Techniques for Fine
		  and Medium Grain Parallelism. IFIP WG10.3 Working
		  Conference",
  year =	 "1993",
  organization = "IFIP",
  address =	 "Orlando, FL, USA",
  month =	 "Jan",
  note =	 "Published in IFIP Transactions A Vol. A-23, 1993",
  abstract = 	 "The paper presents the design of DART, a data-driven
		  processor architecture for real-time computing. The
		  DART processor is designed to be the key building
		  block in real-time multiprocessor systems that can
		  handle multiple 'hard' and 'soft' real-time
		  processes concurrently. The DART processor is a
		  data-flow/control-flow hybrid design that schedules
		  instructions based not only on data availability but
		  also on priority. The highest priority instruction
		  having data available will be executed regardless of
		  the thread in which it is contained. The DART
		  processor retains fine grain actors but utilizes an
		  operand forwarding scheme to reduce the latency
		  between data dependent instructions in sequential
		  threads. Data-driven context switch support is
		  provided at the instruction level allowing for
		  inordinately fast response and context switch time
		  characteristics. These features provide the
		  processor level support required to build a
		  multiprocessor system, comprised of DART processing
		  elements, specifically for real-time computing."
}

@InProceedings{Farrens91,
  author = 	 "Matthew K. Farrens and Andrew R. Pleszkun",
  title = 	 "Strategies for achieving improved processor
		  througput",
  pages =	 "362--369",
  booktitle =	 isca18,
  year =	 "1991",
  note2 =	 "HAR",
  abstract =	 "Deeply pipelined processors have relatively low
		  issue rates due to dependencies between
		  instructions. In this paper the authors examine the
		  possibility of interleaving a second stream of
		  instructions into the pipeline, which would issue
		  instructions during the cycles the first stream was
		  unable to. Such an interleaving has the potential to
		  significantly increase the throughput of a processor
		  without seriously impairing the execution of either
		  process. The authors propose a dynamic interleaving
		  of at most two instructions streams, which share the
		  pipelined functional units of a machine. To support
		  the interleaving of two instruction streams a number
		  of interleaving policies are described and
		  discussed. Finally, the amount of improvement in
		  processor throughput is evaluated by simulating the
		  interleaving policies for several machine variants."
}

@InProceedings{Faust90,
  author = 	 "John E. Faust and Henry M. Levy",
  title = 	 "The performance of an object-oriented threads
		  package",
  pages =	 "278--288",
  booktitle =	 "OOPSLA ECOOP '90 Conference on Object-Oriented
		  Programming: Systems, Languages, and Applications",
  year =	 "1990",
  address =	 "Ottawa, Canada",
  month =	 "Oct",
  note =	 "Published in SIGPLAN Notices Vol.25, No.10.
		  Oct.1990",
  note2        = "HAR",
  abstract = 	 "Presto is an object-oriented threads package for
		  writing parallel programs on a shared-memory
		  multiprocessor. The system adds thread objects and
		  synchronization objects to C++ to allow programmers
		  to create and control parallelism. Presto's
		  object-oriented structure, along with its user-level
		  thread implementation, simplifies customization of
		  thread management primitives to meet
		  application-specific needs. The performance of
		  thread primitives is crucial for parallel programs
		  with fine-grained structure; therefore, the
		  principal objective of this effort was to
		  substantially improve Presto's performance under
		  heavy loads without sacrificing the benefits of its
		  object-oriented interface. The authors discuss
		  design and implementation issues for shared-memory
		  multiprocessors and the performance impact of
		  various designs is shown through measurements on a
		  20-processor Sequent Symmetry multiprocessor"
}

@TechReport{Feeley93,
  author = 	 "Michael Feeley and Jeffrey Chase and Edward
		  Lazowska",
  title = 	 "User-Level Threads and Interprocess Communication",
  institution =  "University of Washington, Department of Computer
		  Science and Engineering",
  year = 	 "1993",
  number = 	 "UW-CSE-93-02-03",
  month = 	 "Feb",
  note2        = "HAR",
  url  =   "ftp://ftp.cs.washington.edu/tr/1993/02/UW-CSE-93-02-03.PS.Z",
  abstract = 	 ""
}

@Article{Feitelson92,
  author = 	 "Dror G. Feitelson and Larry Rudolph",
  title = 	 "Gang scheduling performance benefits for fine-grain
		  synchronization",
  journal =	 jpal,
  year =	 "1992",
  volume =	 "16",
  number =	 "4",
  pages =	 "306--318",
  month =	 "Dec",
  note =	 "Also as Tech report 91-8 , Hebrew University of
		  Jerusalem, Dept. of Computer Science",
  abstract =	 "Multiprogrammed multiprocessors executing fine-grain
		  parallel programs appear to require new scheduling
		  policies. A promising new idea is gang scheduling,
		  where a set of threads are scheduled to execute
		  simultaneously on a set of processors. This has the
		  intuitive appeal of supplying the threads with an
		  environment that is very similar to a dedicated
		  machine. It allows the threads to interact
		  efficiently by using busy waiting, without the risk
		  of waiting for a thread that currently is not
		  running. Without gang scheduling, threads have to
		  block in order to synchronize, thus suffering the
		  overhead of a context switch. While this is
		  tolerable in coarse-grain computations, and might
		  even led to performance benefits if the threads are
		  highly unbalanced, it causes severe performance
		  degradation in the fine-grain case. The authors have
		  developed a model to evaluate the performance of
		  different combinations of synchronization mechanisms
		  and scheduling policies, and validated it by an
		  implementation on the Makbilan multiprocessor. The
		  model leads to the conclusion that gang scheduling
		  is required for efficient fine-grain synchronization
		  on multiprogrammed multiprocessors."
}

@InProceedings{Feldman89,
  author = 	 "S. I. Feldman and C. B. Brown",
  title = 	 "IGOR: a system for program debugging via reversible
		  execution",
  pages =	 "112--123",
  booktitle =	 "Proceedings of ACM SIGPLAN and SIGOPS Workshop on
		  Parallel and Distributed Debugging",
  year =	 "1988",
  publisher =	 "ACM",
  address =	 "Madison, WI, USA",
  month =	 "May",
  note =	 "Published in SIGPLAN Notices Vol.24 No.1, Jan.1989",
  abstract =	 "Typical debugging tools are insufficiently powerful
		  to find the most difficult types of program
		  misbehaviors. A new debugging system, IGOR provides
		  a great deal more useful information and offers new
		  abilities that are quite promising. The system runs
		  fast enough to be quite useful while providing many
		  features that are usually available only in an
		  interpreted environment. The authors describe some
		  improved facilities (reverse execution, selective
		  searching of execution history, substitution of data
		  and executable parts of the programs) that are
		  needed for serious debugging and are these
		  capabilities at reasonable cost without modifying
		  the executable code and running fairly close to full
		  speed. The prototype runs under the DUNE distributed
		  operating system. The paper describes planned
		  extensions to make use of extra processors to speed
		  the system and for applying the technique to
		  multi-thread and time-dependent executions."
}

@InProceedings{Feldman92,
  author = 	 "Jerome A. Feldman and Chu-Cheow Lim and T. Rauber",
  title = 	 "The shared-memory language pSather on a
		  distributed-memory multiprocessor",
  pages =	 "17--20",
  booktitle =	 "Workshop on Languages, Compilers and Run-Time
		  Environments for Distributed Memory
		  Multiprocessors",
  year =	 "1992",
  address =	 "boulder, CO, USA",
  month =	 "Sep",
  note =	 "Published in SIGPLAN Notices Vol.28, No.1,
		  Jan. 1993",
  abstract =	 "Parallel Sather (abbreviated as pSather) is a
		  parallel extension of the object oriented language
		  Sather. pSather adds constructs to create
		  asynchronously executed threads and a special
		  MONITOR class that provides features for
		  implementing synchronisation mechanisms. The paper
		  gives an overview of the language and discusses its
		  implementation on distributed memory machines. The
		  current status of and future work on the language
		  are also presented."
}

@InProceedings{Felten92,
  author = 	 "Edward W. Felten and Dylan McNamee",
  title = 	 "Improving the Performance of Message-Passing
		  Applications by Multithreading",
  booktitle =    "Proceedings of the Scalable High Performance
		  Computing Conference SHPCC-92",
  year = 	 "1992",
  month =	 "Sep",
  pages =	 "84--89",
  note =	 "Also as Tech report UW-CSE-92-09-07, University of
		  Washington, Department of Computer Science and
		  Engineering",
  note2 =	 "HAR",
  url =     "ftp://ftp.cs.washington.edu/tr/1992/09/UW-CSE-92-09-7.PS.Z",
  abstract =	 "Achieving maximum performance in message-passing
		  programs requires that calculation and communication
		  be overlapped. However, the program transformations
		  required to achieve this overlap are error-prone and
		  add significant complexity to the application
		  program. We argue that calculation/communication
		  overlap can be achieved easily and consistently by
		  executing multiple threads of control on each
		  processor, and that this approach is practical on
		  message-passing architectures without any special
		  hardware support. We present timing data for a
		  typical message-passing application, to demonstrate
		  the advantage of our scheme."
}

@Unpublished{Ferrari95,
  author = 	 "Adam Ferrari and V. S. Sunderam",
  title = 	 "TPVM: Distributed concurrent computing with
		  lightweight processes",
  year =	 "1995",
  note2 =	 "HAR",
  url =     "http://uvacs.cs.virginia.edu/~ajf2j/docs/tpvm_paper.ps",
  abstract =	 ""
}

@InProceedings{Fiers92,
  author = 	 "C. Fiers and W Van de Velde and Erik H. D'Hollander",
  title = 	 "The generation of light-weight threads for the
		  parallel execution of a Taskgraph",
  pages =	 "47--60",
  booktitle =	 "NFWO Workshop on Parallel Computing",
  year =	 "1992",
  month =	 "May",
  abstract =	 ""
}

@TechReport{Fillo95,
  author = 	 "Marco Fillo and Stephen W. Keckler and William J.
		  Dally and Nicholas P. Carter and Andrew Chang and
		  Yevgeny Gurevich and Whay S. Lee",
  title = 	 "The {M}-Machine Multicomputer",
  institution =  "Artificial Intelligence Laboratory, Massachusetts
		  Institute of Technology",
  year = 	 "1995",
  number =	 "AI-Memo 1532",
  url = 	 "ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1532.ps.Z",
  abstract = 	 "The M-Machine is an experimental multicomputer being
		  developed to test architectural concepts motivated
		  by the constraints of modern semiconductor
		  technology and the demands of programming systems.
		  The M-Machine computing nodes are connected with a
		  3-D mesh network; each node is a multithreaded
		  processor incorporating 12 function units, on-chip
		  cache, and local memory. The multiple function units
		  are used to exploit both instruction-level and
		  thread-level parallelism. A user accessible message
		  passing system yields fast communication and
		  synchronization between nodes. Rapid access to
		  remote memory is provided transparently to the user
		  with a combination of hardware and software
		  mechanisms. This paper presents the architecture of
		  the M-Machine and describes how its mechanisms
		  maximize both single thread performance and overall
		  system throughput."
}

@InProceedings{Fiske95,
  author = 	 "Stuart Fiske and William J. Dally",
  title = 	 "Thread Prioritization: A Thread Scheduling Mechanism
		  for Multiple-Context Parallel Processors",
  pages =	 "210--221",
  booktitle =	 "First International Symposium on High Performance
		  Computer Architecture",
  year =	 "1995",
  month =	 "Jan",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.ai.mit.edu/pub/cva/hpca95.ps.Z",
  abstract =	 "Multiple-context processors provide register
		  resources that allow rapid context switching between
		  several threads as a means of tolerating long
		  communication and synchronization latencies. When
		  scheduling threads on such a processor, we must
		  first decide which threads should have their state
		  loaded into the multiple contexts, and second, which
		  loaded thread is to execute instructions at any
		  given time. In this paper we show that both
		  decisions are important, and that incorrect choices
		  can lead to serious performance degradation. We
		  propose thread prioritization as a means of guiding
		  both levels of scheduling. Each thread has a
		  priority that can change dynamically, and that the
		  scheduler uses to allocate as many computation
		  resources as possible to critical threads. We
		  briefly describe its implementation, and we show
		  simulation performance results for a number of
		  simple benchmarks in which synchronization
		  performance is critical. "
}

@InProceedings{Flynn68,
  author = 	 "Michael J. Flynn",
  title = 	 "A multiple instruction stream processor with shared
		  resources",
  pages =	 "251--286",
  booktitle =	 "Proceedings of conference on parallel processing",
  year =	 "1968",
  publisher =	 "Spartan Press",
  address =	 "Monterey, California",
  abstract =	 ""
}

@InCollection{Flynn70,
  author = 	 "Michael J. Flynn and A. Podvin and K. Shimizu",
  title = 	 "A multiple instruction stream processor with shared
		  resources",
  booktitle =	 "Parallel processor systems: Technologies and
		  applications",
  publisher =	 "MacMillan New York / Spartan Books",
  year =	 "1970",
  editor =	 "L. C. Hobbs et al",
  pages =	 "251--286",
  abstract =	 ""
}

@Article{Flynn72,
  author = 	 "Michael J. Flynn and A. Podvin",
  title = 	 "Shared resource multiprocessing",
  journal =	 ieeecomp,
  year =	 "1972",
  volume =	 "5",
  pages =	 "20--28",
  month =	 "Mar",
  abstract =	 ""
}

@TechReport{Ford93a,
  author = 	 "Bryan Ford and Mike Hiblerand and Jay Lepreau",
  title = 	 "Notes on Thread Models in Mach 3.0",
  institution =  "University of Utah, Department of Computer Science",
  year = 	 "1993",
  number =	 "UUCS-93-012",
  month =	 "Apr",
  note = 	 "7 pages",
  note2 = 	 "HAR",
  url = 	 "ftp://cs.utah.edu/techreports/1993/UUCS-93-012.ps.Z",
  abstract = 	 "During the Mach In-Kernel Servers work, we explored
		  two alternate thread models that could be used to
		  support traps to in-kernel servers.  In the
		  ``migrating threads'' model we used, the client's
		  thread temporarily moves into the server's task for
		  the duration of the call. In the ``thread switching''
		  model, an actual server thread is dispatched to
		  handle client traps. Based on our experience, we
		  find that the migrating threads model is quite
		  complex and difficult to implement in the context of
		  the current design of Mach and the Unix single
		  server.  The thread switching model would fit more
		  naturally and would probably be much simpler and
		  more robust than migrating threads, making it a
		  valuable approach to explore in the near future.
		  However, we believe migrating threads inherently to
		  be faster than thread switching, and ultimately to
		  be the best long term direction."
}

@InProceedings{Ford94,
  author = 	 "Bryan Ford and Jay Lepreau",
  title = 	 "Evolving Mach 3.0 to a migrating thread model",
  pages =	 "97--114",
  booktitle =	 usenixw94,
  year =	 "1994",
  month = 	 "Jan",
  note =	 "Also tech report UUCS-93-022, University of Utah,
		  Department of Computer Science",
  url = 	 "ftp://cs.utah.edu/pub/thread_migrate.ps.Z",
  abstract =	 "We have modified Mach 3.0 to treat cross-domain
		  remote procedure call (RPC) as a single entity,
		  instead of a sequence of message passing operations.
		  With RPC thus elevated, we improved the transfer of
		  control during RPC by changing the thread model.
		  Like most operating systems, Mach views threads as
		  statically associated with a single task, with two
		  threads involved in an RPC.  An alternate model is
		  that of migrating threads, in which, during RPC, a
		  single thread abstraction moves between tasks with
		  the logical flow of control, and ``server'' code is
		  passively executed.  We have compatibly replaced
		  Mach's static threads with migrating threads, in an
		  attempt to isolate this aspect of operating system
		  design and implementation.  The key element of our
		  design is a decoupling of the thread abstraction
		  into the {\em execution context} and the {\em
		  schedulable thread of control}, consisting of a
		  chain of contexts.  A key element of our
		  implementation is that threads are now ``based'' in
		  the kernel, and temporarily make excursions into
		  tasks via upcalls.  The new system provides more
		  precisely defined semantics for thread manipulation
		  and additional control operations, allows scheduling
		  and accounting attributes to follow threads,
		  simplifies kernel code, and improves RPC
		  performance.  We have retained the old thread and
		  IPC interfaces for backwards compatibility, with no
		  changes required to existing client programs and
		  only a minimal change to servers, as demonstrated by
		  a functional Unix single server and clients.  The
		  logical complexity along the critical RPC path has
		  been reduced by a factor of nine. Local RPC, doing
		  normal marshaling, has sped up by factors of
		  1.7--3.4.  We conclude that a migrating-thread model
		  is superior to a static model, that kernel-visible
		  RPC is a prerequisite for this improvement, and that
		  it is feasible to improve existing operating systems
		  in this manner."
}

@Unpublished{Foster94,
  author = 	 "Ian Foster and Carl Kesselman and Steven Tuecke",
  title = 	 "Portable Mechanisms for Multithreaded Distributed
		  Computations",
  year =	 "1994",
  note2 =	 "HAR",
  url =          "ftp://ftp.mcs.anl.gov/pub/nexus/reports/nexus_perf.ps.Z",
  abstract =	 ""
}

@TechReport{Fowler92a,
  author = 	 "Robert J. Fowler and Leonidas I. Kontothanassis",
  title = 	 "Improving Processor and Cache Locality in Fine-Grain
		  Parallel Computations using Object-Affinity
		  Scheduling and Continuation Passing (Revised)",
  institution =  "University of Rochester, Department of Computer Science",
  year = 	 "1992",
  number =	 "TR 411",
  month =	 "Jun",
  url = 	 "ftp://ftp.cs.rochester.edu/pub/papers/systems/92.tr411.processor_cache_locality.ps.Z",
  abstract =	 "On recent high-performance multiprocessors, there is
		  a potential conflict between the goals of achieving
		  the full performance potential of the hardware and
		  providing a parallel programming environment that
		  makes effective use of programmer effort. On one
		  hand, an explicit coarse-grain programming style may
		  appear to be necessary, both to achieve good cache
		  performance and to limit the amount of overhead due
		  to context switching and synchronization. On the
		  other hand, it may be more expedient to use more
		  natural and finer-grain programming styles based on
		  abstractions such as task heaps, light-weight
		  threads, parallel loops, or object-oriented
		  parallelism. Unfortunately, using these styles can
		  cause a loss of performance due to poor locality and
		  high overhead. We claim that the locality issue in
		  fine-grain parallel programs can be addressed
		  effectively by using object-affinity scheduling and
		  that the overhead can be reduced substantially by
		  representing tasks as templates that are managed
		  using continuation-passing style mechanisms. We
		  present supporting evidence for these claims in the
		  form of experimental measurements of programs
		  running on Mercury, an object-oriented system
		  implemented on an SGI 4D/480 multiprocessor"
}

@InProceedings{Fowler92b,
  author = 	 "Robert. J. Fowler and  Leonidas. I. Kontothanassis",
  title = 	 "Supporting User-Level Exception Handling on a
		  Multiprocessor Micro-Kernel: Experiences with
		  PLATINUM",
  pages =	 "217--232",
  booktitle =	 "Proceedings of 3rd symposium on Experiences with
		  Distributed and Multiprocessor systems (SEDMS III)",
  year =	 "1992",
  publisher =	 "Usenix",
  month =	 "Mar",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.cs.rochester.edu/pub/papers/systems/92.SEDMS.PLAT_Exceptions.ps.Z",
  abstract = 	 "We describe the facilities provided by \PL{}, a
		  multiprocessor micro-kernel, to support user-level
		  exception handling. The principal design goal for
		  these facilities is to provide a very simple
		  mechanism with sufficient flexibility and efficiency
		  to experiment with the implementation of a wide
		  variety of exception-driven, user-level services in
		  a parallel system. These services provide
		  conventional exception handling, an interface
		  between parallel programs and debugging or
		  performance monitoring tools, user-level paging and
		  I/O services, and support for user-level lightweight
		  process packages. The exception handling facility is
		  based on fast message passing using a reconfigurable
		  hierarchy of mailboxes. The mechanisms we describe
		  have performance comparable to their uniprocessor
		  signal handling under Unix, and retain that
		  performance when used on a multiprocessor to support
		  the kind of exception-driven services demanded by a
		  parallel programming environment"
}

@InProceedings{Fowler94,
  author = 	 "Robert J. Fowler, Leonidas I. Kontothanassi",
  title = 	 "Mercury: Object-Affinity Scheduling and Continuation
		  Passing on Multiprocessors",
  pages = 	 "??-??",
  booktitle =    parle94,
  year = 	 "1994",
  month = 	 "Jul",
  series =	 "Lecture Notes in Computer Science ???",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/94.PARLE.Mercury.ps.Z",
  abstract =     "Mercury is a system designed to explore methods for
		  improving the performance of natural grain parallel
		  object-oriented programs on shared memory
		  multiprocessors with hardware-coherent caches. The
		  novel aspects of Mercury are a locality-conscious
		  implementation of user-level threads, new scheduling
		  techniques based on object affinity, and a
		  lightweight task management mechanism that uses
		  coarse-grain dataflow   techniques to reduce the
		  costs of scheduling, synchronizing, and dispatching
		  units of work. By using this combination of
		  techniques to control the overhead of parallelism
		  and to increase locality of reference, Mercury can
		  run relatively fine-grain programs with levels of
		  performance approaching those obtained by hand-tuned
		  coarse-grain programs. We describe the
		  implementation of Mercury and present experimental
		  results on its performance"
}

@InProceedings{Francis87,
  author = 	 "Rhys S. Francis and Ian D. Mathieson",
  title = 	 "Synchronised execution on shared memory
		  multiprocessors",
  pages =	 "165--175",
  booktitle =	 "Proceedings from International Conference on Vector
		  and Parallel Processors in Computational Science
		  III",
  year =	 "1987",
  address =	 "Liverpool, UK",
  month =	 "Aug",
  note =	 "Published in Parallel Computing Vol.8 No.1-3",
  abstract =	 "Threads provides a mechanism for simulating the
		  execution of parallel algorithms on a simplified
		  model of a shared-memory multiprocessor. The
		  algorithms can be expressed in a high-level
		  block-structured language which supports multiple
		  threads of execution within a common body of program
		  code. Results show an ability to achieve good
		  speedup for small problems using algorithms derived
		  by simple modifications of sequential algorithms. As
		  well, a sibling thread synchronisation feature
		  provides the basis for the synchronous execution of
		  threads. k-parallel algorithms tailored to the
		  machine size and implemented as synchronously
		  executing iterations, can provide near linear
		  speedup as the problem size is increased. The
		  techniques described in this paper seem to promise
		  an effective synchronous execution mode for
		  shared-memory MIMD architectures."
}

@InProceedings{Francis90,
  author = 	 "Rhys S. Francis and Arnold N. Pears",
  title = 	 "Self scheduling and execution threads",
  pages =	 "586--590",
  booktitle =	 ispdp2,
  year =	 "1990",
  address =	 "Dallas, TX, USA",
  month =	 "Dec",
  abstract =	 "Two major strategies exist for the exploitation of
		  parallel execution on shared memory multiprocessors.
		  Multiple threads of execution can be created within
		  a shared program context and loop iterations can be
		  scheduled between processors. While the two
		  techniques are well established in isolation, the
		  design of large, general purpose multi-user
		  multiprocessors requires a seamless blend. The
		  authors examine a set of system structures which
		  combine the two parallel computing models to exploit
		  inherent program parallelism. The objective is a
		  general purpose parallel programming environment in
		  which programs can create as many threads as
		  desired, which are scheduled on the share of
		  processor resource allocated to the program. From
		  within any of these threads, the self scheduling of
		  iterations interacts with the thread management so
		  that free processor resource is dynamically
		  allocated to parallel iterations."
}

@InProceedings{Freeh94,
  author =       "Vincent W. Freeh and David K. Lowenthal and Gregory
		  R. Andrews",
  title =        "Distributed Filaments: Efficient Fine-Grain
		  Parallelism on a Cluster of Workstations",
  booktitle =    osdi,
  year =         "1994",
  pages =        "201--213",
  url = 	 "ftp://ftp.cs.arizona.edu/reports/1994/TR94-11a.ps",
  abstract =     "A fine-grain parallel program is one in which
		  processes are typically small, ranging from a few to
		  a few hundred instructions. Fine-grain parallelism
		  arises naturally in many situations, such as
		  iterative grid computations, recursive fork/join
		  programs, the bodies of parallel FOR loops, and the
		  implicit parallelism in functional or dataflow
		  languages. It is useful both to describe massively
		  parallel computations and as a target for code
		  generation by compilers. However, fine-grain
		  parallelism has long been thought to be inefficient
		  due to the overheads of process creation, context
		  switching, and synchronization. This paper describes
		  a software kernel, Distributed Filaments (DF), that
		  implements fine-grain parallelism both portably and
		  efficiently on a workstation cluster. DF runs on
		  existing, off-the-shelf hardware and software. It
		  has a simple interface, so it is easy to use. DF
		  achieves efficiency by using stateless threads on
		  each node, overlapping communication and
		  computation, employing a new reliable datagram
		  communication protocol, and automatically balancing
		  the work generated by fork/join computations."
}

@PhdThesis{Fujita87,
  author = 	 "Tetsuya Fujita",
  title = 	 "Multithreaded processor architecture for parallel
		  symbolic computing",
  school = 	 "Massachusetts Institute of Technology, Laboratory of
		  Computer Science",
  year = 	 "1987",
  month =	 "Sep",
  note =	 "As Tech report MIT/LCS/TM-338. 71 pages",
  abstract =	 ""
}

@InProceedings{Gallmeister91,
  author = 	 "Bill O. Gallmeister and C. Lanier",
  title = 	 "Early experience with POSIX 1003.4 and POSIX 1003.4A",
  pages =	 "190--198",
  booktitle =	 "Proceedings of Twelfth Real-Time Systems Symposium",
  year =	 "1991",
  address =	 "San Antonio, TX, USA",
  month =	 "Dec",
  abstract =	 "Two proposed IEEE standards for real-time operating
		  systems support, POSIX.4 and POSIX.4a, are
		  proceeding towards IEEE approval and will eventually
		  become international standards. The authors provide
		  a brief overview of the facilities of POSIX.4 and
		  POSIX.4a. They concentrate on a few of the critical
		  features that POSIX.4 and POSIX.4a provide and
		  describe the POSIX.4 scheduling interface. The
		  POSIX.4a support for multiple threads of control is
		  also described. The features found in POSIX.4 and
		  POSIX.4a for synchronization of multiple threads,
		  are discussed, and the POSIX.4 interprocess
		  communication facility is presented. The performance
		  numbers are given to allow comparisons of the
		  facilities of traditional Unix systems, the
		  facilities of a representative hard real-time system
		  (LynxOS), and the facilities of POSIX.4 and
		  POSIX.4a."
}

@InProceedings{Gao88,
  author = 	 "Guang R. Gao and Rene Tio and Herbert H. J. Hum",
  title = 	 "Design of an efficient dataflow architecture without
		  data flow",
  booktitle =	 "Int. Conference on fifth generation computer
		  systems",
  year =	 "1988",
  address =	 "Tokyo, Japan",
  month =	 "Nov",
  note =	 "Also as Tech report TR-SOCS- 88.14, McGill
		  University, School of computer Science",
  abstract =	 "An efficient static dataflow architecture based on
		  an argument-fetching data-driven principle has
		  recently been proposed (as reported by Dennis and
		  Gao [5]). This architecture opens possibilities in
		  combining the techniques of existing high-
		  performance conventional pipelined architectures
		  with the strengths of the dataflow model of parallel
		  computation. The key feature is that data never
		  'flows' in the new architecture even though
		  instruction scheduling remains data-driven. The new
		  architecture answers some speculations about the
		  efficiency of practical dataflow architectures --
		  data-driven instruction scheduling need not mean
		  higher traffic due to data token flow in the
		  processor architecture.This paper outlines the
		  instruction set architecture design of the
		  argument-fetching dataflow architecture. We describe
		  the program and theinstruction format, as well the
		  instruction scheduling and execution of the new
		  nachine. The basic architecture as presented in [5]
		  is augmented by a structure memory for handling
		  array structures. Two important aspects, I-structure
		  style array accesses and interprocessor
		  communication support in the instruction set design,
		  are described. We briefly discuss extensions for a
		  dynamic argument-fetching dataflow architecture
		  where multiple function invocations can be
		  effectively supported. A multiprocessor machine for
		  AI applications based on the new processor
		  architecture is currently being investigated."
}

@InProceedings{Gao89a,
  author = 	 "Guang R. Gao and Rene Tio",
  title = 	 "Instruction set architecture of an efficient
		  pipelined dataflow architecture",
  pages =	 "385--392",
  booktitle =	 "Proceedings of the Twenty-Second Annual Hawaii
		  International Conference on System Sciences.",
  volume = 	 "I",
  year =	 "1989",
  month =	 "Jan",
  abstract =	 "A highly pipelined static dataflow architecture
		  based on an argument-fetching data-driven principle
		  has recently been proposed. It separates the
		  data-driven instruction scheduling mechanism from
		  the actual instruction execution unit, avoiding the
		  unnecessary overhead of data token movement that
		  exists in other proposals of dataflow architectures.
		  Work carried out on the instruction set design and
		  machine program format is described. The
		  implementation of long-latency operations-the
		  structure memory operations and interprocessor
		  communication operations-is discussed, as is the
		  implementation of FIFO (first-in-first-out) buffers.
		  The design of an assembler and instruction-set
		  interpreter is outlined."
}

@TechReport{Gao89b,
  author = 	 "Guang R. Gao",
  title = 	 "A flexible architecture model for hybrid dataflow
		  and control- flow evaluation",
  institution =  "McGill University. School of Computer Science",
  year = 	 "1989",
  number =	 "MCGS 89-10",
  note =	 "Appears in Proceedings of the International
		  Workshop: Dataflow - a Status Report  in conjunction
		  with the ACM Annual Symposium on Computer
		  Architecture",
  abstract =	 ""
}

@InProceedings{Gao91,
  author = 	 "Guang R. Gao and Herbert H. J. Hum and Jean-Marc
		  Monti",
  title = 	 "Towards an efficient hybrid dataflow architecture
		  model",
  volume =	 "I",
  pages =	 "355--371",
  booktitle =	 parle91,
  year =	 "1991",
  series =	 "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  address =	 "Eindhoven, Netherland",
  month =	 "Jun",
  abstract =	 "The dataflow model and control-flow model are
		  generally viewed as two extremes of computation
		  models on which a spectrum of architectures are
		  based. The authors present a hybrid architecture
		  model (McGill Dataflow architecture, MDFA) which
		  employs conventional architecture techniques to
		  achieve fast pipelined operation, while exploiting
		  fine-grain parallelism by data-driven instruction
		  scheduling. A mechanism for supporting concurrent
		  operations of multiple instruction threads on the
		  hybrid architecture model is presented and a
		  compiling paradigm for dataflow software pipelining
		  which efficiently exploits loop parallelism in loops
		  is outlined. Simulation results attest that hybrid
		  evaluations can indeed be beneficial."
}

@Article{Gao93a,
  author = 	 "Guang R. Gao",
  title = 	 "An efficient hybrid dataflow architecture model",
  journal =	 jpal,
  year =	 "1993",
  volume =	 "19",
  number =	 "4",
  pages =	 "293--307",
  month =	 "Dec",
  abstract =	 "The author presents an efficient hybrid architecture
		  model which employs: a conventional architecture
		  technique to achieve fast pipelined instruction
		  execution, while exploiting fine-grain parallelism
		  by data-driven instruction scheduling; an efficient
		  mechanis which supports concurrent operation of
		  multiple instruction threads on the hybrid
		  architecture model; and a compiling paradigm for
		  dataflow software pipelining which efficiently
		  exploits loop parallelism through limited balancing.
		  He establishes a set of basic results which show
		  that the fine-grain parallelism in a loop exposed
		  through limited balancing can be fully exploited by
		  a simple greedy runtime data-driven scheduling
		  scheme, achieving both time and space efficiency
		  simultaneously. Simulation results are briefly
		  discussed. This architecture model is utilized as a
		  research vehicle to study various architecture
		  issues for hybrid dataflow computers, as well as
		  compiling techniques for such machines."
}

@Article{Gao93b,
   author =      "Guang Gao and Jean-Luc Gaudiot and Lubomir Bic",
   title =       "Dataflow and Multithreaded Architectures: Guest
		  Editor's Introduction",
   journal =     jpal,
   pages =       "271--272",
   volume =      "18",
   number =      "3",
   month =       jul,
   year =        "1993",
   abstract =	 ""
}

@InProceedings{Gerndt91,
  author = 	 "Michael Gerndt and H. Moritsch",
  title = 	 "Parallelization for multiprocessors with memory
		  hierarchies",
  editor =	 "H. P. Zima",
  pages =	 "89--101",
  booktitle =	 "Parallel Computation. First International ACPC
		  Conference. Proceedings",
  year =	 "1991",
  month =	 "Oct",
  series =	 "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  address =	 "Salzburg, Austria",
  abstract =	 "Programming shared memory multiprocessors seems to
		  be easier than developing programs for distributed
		  memory systems. The reason is the existence of a
		  global names space for parallel threads providing
		  uniform access to all global data. This programming
		  model seems to be inadequate for systems with a
		  larger number of processors since memory hierarchies
		  are integrated to eliminate the bottleneck of global
		  memory access. Therefore programming these systems
		  has to be done with respect to the distribution of
		  data, thus to enforce the parallel processes
		  exploiting locality of references. This paper
		  describes an ongoing project in which the authors
		  investigate the applicability of the distributed
		  memory parallelization strategy for shared memory
		  systems with memory hierarchies."
}

@article{Gheith93,
  author       = "Ahmed Gheith and Karsten Schwan",
  title        = "CHAOS-arc:  Kernel Support for Multiweight Objects,
		  Invocations, and Atomicity in Real-Time
		  Multiprocessor  Applications",
  journal      = tocs,
  pages        = "33--72",
  volume       = "11",
  number       = "1",
  month        = "Feb",
  year         = "1993",
  abstract =	 "CHAOS/sup arc/ is an object-based multiprocessor
		  operating system kernel that provides primitives
		  with which programmers may easily construct objects
		  of differing types and object invocations of
		  differing semantics, targeting multiprocessor
		  systems, and real-time applications. The CHAOS/sup
		  arc/ kernel can guarantee desired performance and
		  functionality levels of selected computations in
		  real-time applications. Such guarantees can be made
		  despite possible uncertainty in execution
		  environments by allowing programs to adapt in
		  performance and functionality to varying operating
		  conditions. This paper reviews the primitives
		  offered by CHAOS/sup arc/ and demonstrates how the
		  required elements of the CHAOS/sup arc/ real-time
		  kernel are constructed with those primitives."
}

@InProceedings{Ghosh94,
  author = 	 "Kaushik Ghosh",
  title = 	 "Experimentation with Configurable, Lightweight
		  Threads on a {KSR} Multiprocessor.",
  pages =	 "??--??",
  booktitle =	 "Proceedings of 1st International Workshop on
		  Parallel Processing",
  year =	 "1994",
  address =	 "Bangalore, India",
  month =	 "Dec",
  note =	 "Also as Tech report GIT-CC-93/37, Georgia Tech,
		  College of Computing",
  url =     "ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-37.ps.Z",
  abstract =	 ""
}

@InProceedings{Giering93,
  author = 	 "Edward W. Giering and Frank Mueller and Theodore .P.
		  Baker",
  title = 	 "Implementing Ada 9X features using POSIX Threads:
		  Design Issues",
  pages =	 "214--228",
  booktitle =	 "Proceedings of TRI-Ada 1993",
  year =	 "1993",
  month =	 "Sep",
  url = 	 "http://www.informatik.hu-berlin.de/~mueller/ftp/pub/PART/triada93.ps.Z",
  abstract =	 ""
}

@InProceedings{Giloi92,
  author = 	 "Wolfgang K. Giloi and A. Schramm and Wolfgang
		  Schroeder-Preikschat",
  title = 	 "A new programming model for massively parallel
		  systems",
  pages =	 "231--244",
  booktitle =	 "Programming Environments for Parallel Computing.
		  IFIP WG 10.3 Workshop",
  year =	 "1992",
  organization = "IFIP",
  address =	 "Edinburgh, UK",
  month =	 "Apr",
  note =	 "Published in IFIP Transactions A. Vol.A-11, 1992",
  abstract =	 "The authors present a new programming model for
		  massively parallel distributed memory systems called
		  the virtual processor model. This programming
		  methodology eliminates the restrictions of other
		  models with respect to the degree of parallelism and
		  the connectivity of the interconnection structure.
		  Therefore, it allows the user to write parallel
		  programs in an application-oriented, fine-grain
		  fashion, in which the message-passing mechanisms are
		  hidden from the user. The aggregation of a virtual
		  processor topology into executable threads of code
		  and the mapping of these threads onto the physical
		  resources of the system can be performed by the
		  compiler. The compiler also takes care of
		  synchronization, which is required only once per
		  iteration as opposed to once per communication or
		  process invocation. The concepts of an
		  object-oriented implementation of the virtual
		  processor model and its implications on the system
		  architecture, the operating system, and the
		  compilers are discussed. As a means for an efficient
		  implementation, the notions of featherweight
		  processes and featherweight communication are
		  introduced."
}

@InProceedings{Glenn88,
  author = 	 "Raymond R. Glenn",
  title = 	 "Performance Prediction for the Horizon Supercomputer",
  pages =	 "48--52",
  booktitle =	 "Proceedings Supercomputing'88",
  year =	 "1988",
  month =	 "Nov",
  abstract =	 ""
}

@InProceedings{Glenn91a,
  author = 	 "Raymond R. Glenn and Daniel V. Pryor and John M.
		  Conroy and Theodore Johnson",
  title = 	 "Characterizing memory hot spots in a shared memory
		  {MIMD} machine",
  pages =	 "554--566",
  booktitle =	 ics91,
  year =	 "1991",
  address =	 "Albuquerque, NM, USA",
  month =	 "Nov",
  abstract =	 "The authors analyze two memory hot spot problems
		  associated with massively parallel MIMD computers.
		  The first is the memory stride problem, which is
		  similar to stride problems found in existing
		  supercomputers. Pseudorandom interleaving, as
		  proposed by A. Norton and E. Melton (1987), is the
		  preferred solution. The second hot spot problem
		  occurs in designs that use two separate memory
		  accesses to lock and unlock critical sections (split
		  transaction) and employ a first come/first serve
		  queuing mechanism for shared memory locations. A
		  bistability in throughput brought about by these
		  conditions is analyzed and experimentally
		  demonstrated. Simple equations are presented which
		  predict the throughput at a critical section of code
		  as a function of the number of applied threads.
		  These equations also express the maximum number of
		  threads that can safely be applied without the
		  possibility of stalling. "
}

@InProceedings{Glenn91b,
  author = 	 "Raymond R. Glenn and Daniel V. Pryor",
  title = 	 "Instrumentation for a massively parallel MIMD
		  application",
  pages =	 "208--209",
  booktitle =	 icommcs91,
  year =	 "1991",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  abstract =	 "An application implemented on a simulated machine
		  called Horizon is described. One purpose of the
		  study is to investigate some of the features of a
		  possible future machine (or class of machines) with
		  a view toward deciding, early on in the research
		  cycle, where problems may come up, what features
		  should be added or strengthened, and what proposed
		  features seem to be unnecessary. Another purpose is
		  to learn more about how to program, instrument and
		  debug a shared memory, massively parallel MIMD
		  computer and to begin to answer some of the
		  questions: what tools does a programmer need to
		  debug this type of machine? How can a programmer
		  know if the machine is performing well? How can
		  bottlenecks be identified? How can the massive
		  amount of instrumentation information be condensed
		  and presented to a user in a way that makes sense?
		  The machine model (M.R. Thistle et al., 1988), is a
		  massively parallel MIMD computer having a global
		  address space, and memory-based synchronization
		  through the use of full/empty bits at each location.
		  In addition, the processors each support 128
		  independent instruction streams. The processor
		  performs a complete context switch at every clock
		  tick, and can thereby execute from a different
		  instruction stream (I-stream) at each new cycle. The
		  machine is arranged to have up to 256 of these
		  processing elements (PEs) and up to 512 memory
		  modules on a 16*16*6 node internal network."
}

@Article{Glenn93,
  author = 	 "Raymond R. Glenn and Daniel V. Pryor and John M.
		  Conroy and Theodore Johnson",
  title = 	 "A bistability throughput phenomenon in a
		  shared-memory MIMD machine",
  journal =	 jsup,
  year =	 "1993",
  volume =	 "7",
  number =	 "3",
  pages =	 "357--375",
  month =	 "Sep",
  abstract =	 "Examines a previously unanalyzed bistability
		  phenomenon with respect to the number of threads
		  that are doing useful work. This phenomenon is
		  illustrated by a single work queue on a
		  shared-memory machine. An analysis of designs that
		  use two separate memory accesses to lock and unlock
		  critical sections (split transaction) and that
		  employ a first come/first serve queuing mechanism
		  for shared-memory locations is presented. A
		  bistability in the number of threads working,
		  brought about by these conditions, is analyzed and
		  experimentally demonstrated. A simple analysis is
		  presented which predicts the throughput at a
		  critical section of code as a function of the number
		  of applied threads. The study concludes that the
		  mean size of the work items that can be executed in
		  parallel without the possibility of stalling is
		  proportional to the square of the number of threads
		  applied."
}
@InProceedings{Gmach92,
  author = 	 "R. Gmach",
  title = 	 "The {GASO} threads package an implementation of {POSIX}
		  threads for {VM/CMS}",
  pages =	 "265--282",
  booktitle =	 "Proceedings {SHARE} Europe Anniversary Meeting",
  year =	 "1992",
  publisher =	 "SHARE Eur. Assoc",
  address =	 "Davos, Switzerland",
  month =	 "Sep",
  abstract =	 "Threads are a kind of lightweight processes, sharing
		  the same address space. GASO is a user-level threads
		  package for the IBM VM/SP operating system based on
		  the Pthreads' interface specified by POSIX in their
		  1003.4A draft. This threads package is an
		  easy-to-use library, which enables the programmer to
		  write application in C, which are running in the
		  normal CMS environment with multiple threads
		  executing in parallel. It is possible for instance
		  to have one thread waiting for user keyboard input,
		  while all other threads keep performing any
		  computations. Threads can be useful in structuring a
		  program. Threads are also useful for having multiple
		  copies of the same code running simultaneously; for
		  example when a server program executes requests for
		  clients, a thread can be used to execute each
		  client's request. Threads communicate through shared
		  variables-one thread may set a variable that another
		  thread will read later. Provision for
		  synchronization is implemented by means of mutexes
		  and condition variables."
}

@MastersThesis{Goldstein94,
 author = 	 "Seth C. Goldstein",
  title = 	 "The Implementation of a Threaded Abstract Machine", 
  school =       "University of California at Bekerley, Computer
		  Science Division",
  year = 	 "1994",
  month =	 "May",
  note =	 "Also as Tech report UCB/CSD 94/818",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-94-818",
  url2=    "ftp://ftp.cs.berkeley.edu/ucb/TAM/UCB-94-818.ps",
  abstract =	 "This report describes TL0 and a translator for TL0,
		  which targets both commodity workstations and
		  commercial multiprocessors. TL0 is a portable
		  parallel assembly language based on the Threaded
		  Abstract Machine (TAM). TAM defines an execution
		  model of self-scheduled threads which meets the
		  demands that modern parallel architectures place on
		  the compilation of general-purpose parallel
		  programming languages. The most important features
		  of TAM are its utilization of the storage hierarchy
		  and the way it exposes asynchronous message events
		  and the scheduling of computation. This report
		  defines TL0 in two ways: by defining the operational
		  semantics of TL0 and by describing TLC, the target
		  of the TL0- to-C translator. We discuss the
		  constraints imposed on the design of the system by
		  asynchronous messages handling. We then present a
		  detailed analysis of the implementation of dynamic
		  scheduling and how it is influenced by the two-level
		  scheduling hierarchy. In addition to describing TL0,
		  we explain how to eliminate many of the sources of
		  overhead introduced by using C as an assembler. The
		  translator described is currently being used as the
		  back end for an Id90 compiler which produces the
		  fastest Id90 executables in the world. By defining
		  TL0 and describing the translator, this report
		  should allow others both to build high-level
		  language compilers that use TL0 as a target and to
		  port TL0 to other parallel machines."
}

@InProceedings{Gottlieb92,
  author = 	 "Israel Gottlieb and Y. Singer",
  title = 	 "Dynamically partitioned dataflow",
  editor =	 "V. Milutinovic et al",
  volume =	 "I",
  pages =	 "535--546",
  booktitle =	 hicss25,
  year =	 "1992",
  address =	 "Kauai, HI, USA",
  month =	 "Jan",
  abstract =	 "Discusses the dynamically partitioned dataflow
		  architecture. It executes maximally persistent
		  threads along data dependent paths in the dataflow
		  graph. Only where such a path cannot be pursued
		  further, does the architecture resort to the more
		  conventional approach: pushing the pipeline with an
		  instruction from elsewhere. To mask the latency of
		  next instruction address calculation, a novel
		  lookahead mechanism is employed."
}

@InProceedings{Govindarajan92a,
  author = 	 "R. Govindarajan and Shashank Nemawarkar",
  title = 	 "Small : A scalable multithread architecture to
		  explot large locality",
  pages =	 "32--39",
  booktitle =	 ispdp4,
  year =	 1992,
  month =	 "Dec",
  abstract =	 "The authors propose a multithreaded architecture
		  that performs synchronization efficiently by
		  following a layered approach, exploits larger
		  locality by using large, resident activations, and
		  reduces the number of load stalls with the help of a
		  novel high-speed buffer organization. The
		  performance of the proposed architecture is
		  evaluated using deterministic discrete-event
		  simulation. Initial simulation results indicate that
		  the architecture can achieve high performance in
		  terms of both speedup and processor utilization."
}

@InProceedings{Govindarajan92b,
  author = 	 "R. Govindarajan and S. S. Nemawarkar",
  title = 	 "A large context multithreaded architecture",
  pages =	 "423--428",
  booktitle =	 "Proceedings of CONPAR 92-VAPP V. Second Joint
		  International Conference on Vector and Parallel
		  Processing",
  year =	 1992,
  month =	 "Sep",
  abstract =	 "Multithreaded architectures synthesize von Neumann
		  computation model with data-driven evaluation to
		  take advantage of both computation models. The
		  authors propose the design and performance
		  evaluation of a large context multithreaded (LCM)
		  architecture. The salient features of the proposed
		  architecture are: (i) a layered approach to
		  synchronization and scheduling of the three levels
		  of program hierarchy, (ii) support for a large
		  resident context to improve locality, and (iii) a
		  novel high-speed buffer organization to ensure 100%
		  data availability. Initial simulation results
		  indicate that the proposed architecture is capable
		  of sustaining very high processor utilization."
}

@InProceedings{Govindarajan95,
  author = 	 "R. Govindarajan and Shashank S. Nemawarkar and
		  Philip LeNir",
  title = 	 "Design and Performance Evaluation of a Multithreaded
		  Architecture",
  pages =	 "298--307",
  booktitle =	 "First International Symposium on High Performance
		  Computer Architecture",
  year =	 1995,
  month =	 "Jan",
  abstract =	 "Multithreaded architectures have the ability to
		  tolerate long memory latencies and unpredictable
		  synchronization delays. In this paper we propose a
		  multithreaded architecture that is capable of
		  exploiting both coarse-grain parallelism and
		  fine-grain instruction-level parallelism in a
		  program. Instruction-level parallelism is exploited
		  by grouping instructions from a number of active
		  threads at runtime. The architecture supports
		  multiple resident activations to improve the extent
		  of locality exploited. Further, a distributed data
		  structure cache organization is proposed to reduce
		  both the network traffic and the latency in
		  accessing remote locations.  Initial performance
		  evaluation using discrete-event simulation indicates
		  that the architecture is capable of achieving very
		  high processor throughput. The introduction of the
		  data structure cache reduces the network latency
		  significantly. The impact of various cache
		  organizations on the performance of the architecture
		  is also discussed in this paper."
}

@InProceedings{Grafe90a,
  author = 	 "V. G. Grafe and J. E. Hoch",
  title = 	 "Implementation of the epsilon psilon dataflow
		  processor",
  volume =	 1,
  pages =	 "19--29",
  booktitle =	 hicss23,
  year =	 1990,
  month =	 "jan",
  abstract =	 "Based on an operation-level, static dataflow model
		  of computation, the processor uses a directly
		  addressed matching store rather than a traditional
		  associative matching store. The design is
		  implemented with off-the-shelf ICs on a single
		  circuit board. Sustained performance comparable to
		  that of commercial minisupercomputers is
		  demonstrated. Two variations of the processor
		  architecture are constructed, with the second
		  building extensively on the experience gained from
		  the first. The second prototype is studied in great
		  detail, and the resulting insights lead to the
		  development of a new architecture, epsilon psilon-2.
		  The architectures and specific implementations
		  involved in this progression are described. The
		  knowledge gained in the physical construction and
		  the performance assessment of each are presented
		  along with the resulting architectural
		  modifications."
}

@InProceedings{Grafe90b,
  author = 	 "V.G. Grafe and J. E. Hoch",
  title = 	 "The Epsilon-2 Hybrid dataflow architecture",
  pages =	 "88--93",
  booktitle =	 compcon90,
  year =	 1990,
  note2 =	 "HAR",
  abstract =	 "Epsilon-2 is a general parallel computer
		  architecture that combines the fine-grain
		  parallelism of dataflow computing with the
		  sequential efficiency common to von Neumann
		  computing. Instruction-level synchronization,
		  single-cycle context switches, and
		  reduced-instruction-set-computer-like sequential
		  efficiency are all supported in Epsilon-2. Epsilon-2
		  is based on an intrinsically parallel computation
		  model. The instruction scheduling model of Epsilon-2
		  is a generalization of both the von Neumann and
		  dataflow models. The storage model of Epsilon-2 is a
		  parallel generalization of a traditional stack-based
		  storage model. The system is built around a module
		  consisting of a processor board and structure memory
		  board, connected by a four-by-four crossbar, an
		  input/output port, and the global interconnect. In
		  this way, each additional unit of processing brings
		  with it a unit of structure memory, a unit of I/O
		  bandwidth, and a unit of global interconnect
		  bandwidth. A sample code is presented in detail, and
		  the progress of the physical implementation
		  discussed."

}

@Article{Grafe90c,
  author = 	 "V. G. Grafe and J. E. Hoch",
  title = 	 "The Epsilon-2 multiprocessor system",
  journal =	 jpal,
  year =	 1990,
  volume =	 10,
  number =	 4,
  pages =	 "309--318",
  month =	 "Dec",
  abstract =	 "Epsilon-2 is a general purpose parallel computer
		  architecture that addresses the twin goals of high
		  performance and programmability. Epsilon-2
		  implements a hybrid computation model that combines
		  the fine-grain parallelism of dataflow computing
		  with the sequential efficiency characteristic of von
		  Neumann computing. It provides instruction-level
		  synchronization, single-cycle context switches, and
		  RISC-like sequential execution. The general parallel
		  computing model and the architecture of Epsilon-2
		  are described. A sample code is also presented to
		  illustrate Epsilon-2's operation."
}

@unpublished{GranLarsen92,
  author =       "{\O}. Gran Larsen",
  title  =       "Swinging register sets - an architectural concept
		  for multithreading",
  month  =       "October",
  year   =       "1992",
  note   =       "Unpublished memo"
}
@TechReport{Grimshaw91, 
  author =       "Andrew S. Grimshaw and Edmond C. {Loyot, Jr.}",
  title =        "{ELFS:} Object-oriented Extensible File Systems",
  year =         1991,
  month =        jul,
  number =       "TR-91-14",
  institution =  "University of Virginia, Department of  Computer
		  Science",
  url =     "ftp://uvacs.cs.virginia.edu/pub/techreports/CS-91-14.ps.Z",
  abstract =     "Provide the high bandwidth and
		  low latency, reduce the cognitive burden on the
		  programmer, and manage proliferation of data formats
		  and architectural changes. Details of the plan to
		  make an extensible OO interface to file system.
		  Objects each have a separate thread of control, so
		  they can do asynchronous activity like prefetching
		  and caching in the background, and support multiple
		  outstanding requests. The Mentat object system makes
		  it easy for them to support pipelining of I/O with
		  I/O and computation in the user program. Let the
		  user choose type of consistency needed. See
		  grimshaw:objects for more results."
}

@InProceedings{Guffick89,
  author = 	 "I. M. Guffick and G. S. Blair",
  title = 	 "{KITARA}: a parallel object-oriented language for
		  distributed applications",
  editor =	 "D. J. Evans and G. R. Joubert and F. J. Peters",
  pages =	 "449--454",
  booktitle =	 "Parallel Computing 89. Proceedings of the
		  International Conference",
  year =	 1989,
  publisher =	 "North-Holland, Amsterdam, Netherlands",
  month =	 "Aug",
  abstract =	 "KITARA is an object-oriented language for
		  programming in a parallel/distributed environment.
		  The language adopts a tailored object model based on
		  encapsulation, abstraction and polymorphism, and
		  supports strong type checking. Communication between
		  objects is asynchronous, using tuples and
		  distributed tuple-spaces, which decouples object
		  interaction. Both inter- and intra-object
		  concurrency can be described; objects can execute
		  concurrently and can consist of multiple threads of
		  control. The paper briefly describes the suitability
		  of the object metaphor for programming in a
		  parallel/distributed environment, and thereafter
		  presents the design of the KITARA programming
		  language."
}

@MastersThesis{Gulati95,
  author = 	 "Manu Gulati",
  title = 	 "Multithreading on a superscalar microprocessor",
  school = 	 "University of California at Irvine, Department of
		  Electrical and Computer Engineering",
  year = 	 1995,
  month =	 "Jan",
  note2 =	 "HAR",
  url = 	 "http://www.eng.uci.edu/comp.arch/papers/gulati/thesis.ps",
  abstract =	 ""
}

@PhdThesis{Gunther93,
  author = 	 "Bernard K. Gunther",
  title = 	 "Superscalar Performance in a Multithreaded
		  Microprocessor",
  school = 	 "University of Tasmania, Department of Electrical
		  Engineering and Computer Science", 
  year = 	 "1993",
  address =	 "Australia",
  month =	 "Dec",
  note2 = 	 "HAR",
  url = 	 "ftp://probitas.cs.utas.edu.au/pub/Theses/Multithreaded_micropro_thesis.ps.gz",
  abstract =	 "Multithreading is introduced in its traditional
		  context of multiprocessing. The evolution of
		  parallel machines, both coarse and fine-grained,
		  is heading towards implementations placing greater
		  reliance on massive integrated circuits to reduce
		  physical constraints. Multithreading in a
		  microprocessor takes advantage of this opportunity,
		  maximizing hardware utilization and gaining
		  performance from appropriate processing paradigms.
		  This strategy is developed into a new multithreaded
		  architecture, Concurro, which serves as a research
		  vehicle for massively integrated processors.
		  Concurro borrows from superscalar, dataflow, and
		  conventional multithreaded architectures in a design
		  that aims to be implementable despite ambitious
		  performance goals."
}
 
@InProceedings{Gupta91,
  author = 	 "Anoop Gupta and John Hennessy and Kourosh Gharachorloo
		  and Todd Mowry and Wolf-Dietrich Weber",
  title = 	 "Comparative evaluation of latency reducing and
		  tolerating techniques",
  pages =	 "309--318",
  booktitle =	 isca18,
  year =	 "1991",
  month =	 "May",
  note2 =	 "HAR",
  url = 	 "http://www.eecg.toronto.edu/~tcm/isca91.ps.Z",
  abstract =	 ""
}

@InProceedings{Ha94,
  author = 	 "Sangho Ha and Junghwan Kim and Eunha Rho and Yoonhee
		  Nah and Seungho Cho and Daejoon Hwang and Sangyong Han",
  title = 	 "A Massively Parallel Multithreaded Architecture:
		  DAVRID",
  pages =	 "70--74",
  booktitle =	 iccd94,
  year =	 "1994",
  month =	 "Oct",
  abstract =	 "MPAs(Massively Parallel Architectures) should
		  address two fundamental issues for scalability:
		  synchronization and communication latency. Dataflow
		  architectures cause problems of excessive
		  synchronization costs and inefficient execution of
		  sequential programs while they offer the ability to
		  exploit massive parallelism inherent in programs. In
		  contrast, MPAs based on von Neumann computational
		  model may suffer from inefficient synchronization
		  mechanism and communication latencies.
		  DAVRID(DAtaflow Von Neumann RISC hybrID) is a
		  massively parallel multithreaded architecture which,
		  by combining advantages of von Neumann model and
		  dataflow model, preserves good single thread
		  performance as well as tolerates latency and
		  synchronization costs. In this paper , we describe
		  the DAVRID architecture and evaluate it through
		  simulation results over several benchmarks."
}

@InProceedings{Ha95,
  author = 	 "Sangho Ha and Sangyong Han and Heunghwan Kim",
  title = 	 "Partitioning a Lenient Parallel Language into
		  Sequential Threads",
  volume =	 "?",
  pages =	 "??--??",
  booktitle =	 hicss28,
  year =	 "1995",
  month =	 "Jan",
  abstract =	 "Multithreading is attractive in a large-scale
		  parallel system  since it allows split-phase memory
		  operations and fast context  switching between
		  computations without blocking the processor.
		  Performance of multithreaded architectures depends
		  significantly  on the quality of multithreaded
		  codes. In this paper, we describe the enhanced
		  thread formation scheme to produce efficient
		  sequential threads from programs written in a
		  lenient parallel language Id^-. This scheme features
		  graph partitioning based on only long latency
		  instructions, combination of multiple switches and
		  merges introducing a generalized switch and merge,
		  thread merging, and redundant arc elimination using
		  thread precedence relation. The simulation results
		  show that our scheme reduces control and branch
		  instructions effectively. "
}

@Article{Hagino93,
  author = 	 "T. Hagino and K. Yamagishi",
  title = 	 "The {ToM} (Thread on Modules) microkernel",
  journal =	 "Systems and Computers in Japan",
  year =	 "1993",
  volume =	 "24",
  number =	 "9",
  pages =	 "14--21",
  abstract =	 "The authors have been developing a distributed
		  operating system called Threads on Modules (ToM). It
		  supports a programming model which is suited for
		  distributed environments. For this purpose, modules,
		  threads, houses, users, RPCs and capabilities are
		  introduced. To make the ToM system as portable as
		  possible, it was implemented using the microkernel
		  technology. The ToM microkernel implements the
		  minimal functionalities and the others are
		  implemented as user level programs. Although the ToM
		  microkernel is designed to support the ToM
		  programming model, it has sufficient functionalities
		  to implement other operating systems. This paper
		  presents some of the main features of the ToM
		  microkernel. A UNIX emulator is also presented."
}

@Techreport{Haines95,
  author = 	 "Matthew Haines and Piyush Mehrotra and David Cronk",
  title = 	 "Ropes: Support for collective operations among
		  distributed threads",
  institution =  "Institute for Computer Applications in Science and
		  Engineering, NASA Langley Research",
  number =	 "ICASE Report No. 95-36",
  year =	 "1995",
  month =	 "May",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.icase.edu/pub/techreports/95/95-36.ps.Z",
  abstract =	 "Lightweight threads are becoming increasingly useful
		  in supporting parallelism and asynchronous control
		  structures in applications and language
		  implementations. Recently, systems have been
		  designed and implemented to support interprocessor
		  communication between lightweight threads so that
		  threads can be exploited in a distributed memory
		  system. Their use, in this setting, has been largely
		  restricted to supporting latency hiding techniques
		  and functional parallelism within a single
		  application. However, to execute data parallel codes
		  independent of other threads in the system,
		  collective operations and relative indexing among
		  threads are required. This paper describes the
		  design of {it ropes:} a scoping mechanism for
		  collective operations and relative indexing among
		  threads. We present the design of ropes in the
		  context of the Chant system, and provide performance
		  results evaluating our initial design decisions."
}
@TechReport{Halbherr94,
  author = 	 "Michael Halbherr and Yuli Zhou and Chris F. Joerg",
  title = 	 "MIMD-Style Parallel Programming Based on
		  Continuation-Passing Thread",
  institution =  "Massachussets Institute of Technology,Laboratory for
		  Computer Science, Computation Structures Group",
  year = 	 "1994",
  type =	 "Memo",
  number =	 "CSG Memo-335",
  month =	 "Mar",
  note =	 "Also appears in Proc. of 2nd Int. Workshop on Massive
		  Parallelism: Hardware, Software and Applications,
		  October 1994, Capri, Italy",
  availabe = 	 "ftp://csg-ftp.lcs.mit.edu/pub/users/zhou/csg355.ps.Z",
  abstract =	 "Today's message passing architectures are
		  characterized by high communication costs and they
		  typically lack hardware support for synchronization
		  and scheduling. These deficiencies present a severe
		  obstacle to obtaining efficient implementations of
		  parallel applications whose communication patterns
		  are either highly irregular or dependent on dynamic
		  information. In this paper we present a model based
		  on continuation-passing threads in which we try to
		  overcome these difficulties. The model incorporates
		  two effective software mechanisms targeted towards
		  lengthening sequential threads in order to offset
		  the costs of dynamic scheduling, and towards
		  preserving the locality of computations to reduce
		  the network traffic. The model is currently
		  implemented as a C language extension along with a
		  runtime system implemented on the CM-5 that embodies
		  a work stealing scheduler. Real world applications
		  written in this package, such as ray-tracing and
		  protein folding, have shown impressive speedup
		  results."
}

@InProceedings{Halstead88,
  author = 	 "Robert H. Halstead and Tetsuya Fujita",
  title = 	 "MASA: A multithreaded processor architecture for
		  parallel symbolic computing",
  pages =	 "443--451",
  booktitle =	 isca15,
  year =	 "1988",
  month =	 "May",
  note2 =	 "HAR",
  abstract =	 "MASA is a ``first cut'' at at processor architecture
		  intended as a building block for a multiprocessor
		  that can execute parallel lisp programs
		  efficiently. MASA features a tagged architecture,
		  multiple contexts, fast trap handling, and a
		  synchronization bit in evert memory word. MASAs
		  principal novelty is its use of multiple contexts
		  both to support multithreaded execution-interleaved
		  execution from separate instruction streams-and to
		  speed up procedure calls and trap handling in the
		  same manner as register windows. A project is under
		  way to evaluate MASA-like architectures for
		  executing programs written in Multilisp"
}

@Article{Halstead94,
  author = 	 "Robert Halstead and David Callahan and Jack B. Dennis",
  title = 	 "Panel Session II: Programming, Complication, and
		  Resource Management Issues for Multithreading",
  journal =	 canews,
  year =	 "1994",
  volume =	 "22",
  number =	 "1",
  pages =	 "19--??",
  month =	 "Mar",
  note2 =	 "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo30.ps.gz",
  abstract =	 ""
}

@TechReport{Han93,
  author = 	 "Jay Han",
  title = 	 "Porting FastThreads to the {KSR1}",
  institution =  "Projet SOR, INRIA Rocquencourt",
  year = 	 "1993",
  month =	 "Jun",
  url  =   "ftp://ftp.inria.fr/INRIA/Projects/SOR/FastThreads-KSR1.ps.
gz",
  abstract =	 ""
}

@InProceedings{Hansen90,
  author = 	 "Gilbert J. Hansen and Charles A. Linthicum and Gary
		  Brooks",
  title = 	 "Experience with a Performance Analyzer for
		  Multithreaded Applications",
  pages =	 "123--131",
  booktitle =	 "Proceedings Supercomputing'90",
  year =	 "1990",
  month =	 "Nov",
  abstract =	 ""
}

@TechReport{Harbour91,
  author =       "Michael Gonzalez Harbour and Lui Sha",
  title =        "An Application-Level Implementation of the Sporadic
		  Server",
  institution =  "Carnegie Mellon University Software Engineering
		  Institute",
  number =       "CMU/SEI-91-TR-26",
  year =         "1991",
  note =         "54 pages",
  abstract =     "The purpose of this paper is to introduce a sporadic
		  server algorithm that can be implemented as an
		  application-level task, and that can be used when no
		  runtime or operating system level implementation of
		  the sporadic server is available. The sporadic
		  server is a simple mechanism that both limits and
		  guarantees a certain amount of execution power
		  dedicated to servicing aperiodic requests with soft
		  or hard deadlines in a hard real-time system. The
		  sporadic server is event-driven from an application
		  viewpoint but appears as a periodic task for the
		  purpose of analysis and, consequently, allows the
		  use of analysis methods such as rate monotonic
		  analysis to predict the behavior of the real-time
		  system. When the sporadic server is implemented at
		  the application-level, without modification to the
		  runtime executive or the operating system, some of
		  its requirements cannot be met strictly and,
		  therefore, some simplifications need to be
		  assumed. We show that even with these
		  simplifications, the application-level sporadic
		  server proposed in this paper has the same
		  worst-case performance as the full-featured runtime
		  sporadic server algorithm, although the average case
		  performance is slightly worse. The implementation
		  requirements are a runtime prioritized preemptive
		  scheduler and system calls to change a task's or
		  thread's priority. Two implementations are
		  introduced in this paper, one for Ada and the other
		  for POSIX 1003.4a, Threads Extension to Portable
		  Operating Systems."
}

@InProceedings{Hauser93,
  author = 	 "Carl Hauser and Christian Jacobi and Marvin Theimer
		  and Brent Welch and Mark Weiser",
  title = 	 "Using threads in interactive systems: a case study",
  pages =	 "94--105",
  booktitle =	 sosp14,
  year =	 "1993",
  address =	 "Ashville, NC, USA",
  month =	 "Dec",
  note =	 "Published in " # acmosr # "Vol.27, No.5, Dec. 1993",
  note2 =	 "HAR",
  abstract =	 "We describe the results of examining two large
		  research and commercial systems for the ways that
		  they use threads. We used three methods: analysis of
		  macroscopic thread statistics, analysis of the
		  microsecond spacing between thread events, and
		  reading the implementation code. We identify ten
		  different paradigms of thread usage: defer work,
		  general pumps, slack processes, sleepers, one-shots,
		  deadlock avoidance, rejuvenation, serializers,
		  encapsulated fork and exploiting parallelism. While
		  some, like defer work, are well known, others have
		  not been previously described. Most of the paradigms
		  cause few problems for programmers and help keep the
		  resulting system implementation understandable. The
		  slack process paradigm is both particularly
		  effective in improving system performance and
		  particularly difficult to make work well. We observe
		  that thread priorities are difficult to use and may
		  interfere in unanticipated ways with other thread
		  primitives and paradigms. Finally, we glean from the
		  practices in this code several possible future
		  research topics in the area of thread abstractions. "
}

@InProceedings{Heide86,
  author = 	 "K. von der Heide",
  title = 	 "A general purpose pipelined ring architecture",
  series =	 "Lecture Notes in Computer Science 237",
  pages =	 "198--205",
  booktitle =	 "CHECK THIS",
  year =	 "1986",
  abstract =	 ""
}

@InProceedings{Heuser90,
  author =       "Mark Heuser",
  title =        "An Implementation of Real-Time Thread
		  Synchronization", 
  booktitle =    usenixs90,
  pages =        "97--106",
  publisher =    "USENIX",
  address =      "Anaheim, CA",
  year =         "1990",
  abstract =	 ""
}

@PhdThesis{Heytens92,
  author = 	 "Michael L. Heytens",
  title = 	 "The Design and Implementation of a Parallel
		  Persistent Object System",
  school = 	 "Massachusetts Institute of Technology, Laboratory of
		  Computer Science",
  year = 	 "1992",
  month =	 "Feb",
  note =	 "238 pages. Also as tech report MIT/LCS/TR-529",
  abstract = 	 "It is widely recognized that the expressive power of
		  relational database systems is inadequate for
		  applications that manipulate complex, non
		  record-oriented data. Much recent research has been
		  focused on the design of more expressive database
		  language models that seamlessly integrate the data
		  modeling, abstraction, and general computation of
		  full programming languages with the features of
		  traditional database systems such as persistence,
		  failure recovery, and security. Such additional
		  flexibility gives expressive power to the
		  programmer, but complicates matters for the compiler
		  and run-time system in their efforts to implement
		  database programs efficiently. AGNA, an experimental
		  persistent object system is described that was
		  designed and built that uses parallelism in a
		  fundamental way to enhance performance. Parallelism
		  is incorporated into the design of the system at all
		  levels. We begin with an implictly parallel
		  transaction language that includes a full
		  higher-order programming language and the List
		  comprehension, a notation similar to SQL but more
		  general. Transactions are compiled into code for a
		  multi- threaded abstract machine called P.RISC,
		  whose central feature is fine grain parallelism with
		  data-driven execution. P-RISC code is emulated on
		  each processor of a MIMD machine with multiple
		  disks"
}

@Article{Hicks89,
  author = 	 "James Hicks and Derek Chiou and Boon Seong Ang and
		  Arvind",
  title = 	 "Performance studies of the Monsoon dataflow
		  processor",
  journal =	 jpal,
  year =	 "1993",
  volume =	 "18",
  number =	 "3",
  pages =	 "273--300",
  note =	 "Also as 1992 Tech Memo CSG 345-3 ,Massachussets Institute
		  of Technology, Laboratory for Computer Science",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-345-3.ps",
  abstract =	 "In this paper, we examine the performance of Id, an
		  implicitly parallel language, on Monsoon, an
		  experimental dataflow machine. One of the precepts
		  of our work is that the Id run-time system and
		  compiled Id programs should run on any number of
		  Monsoon processors without change. Our experiments
		  running Id programs on Monsoon show that speedups of
		  more than seven are easily achieved on eight
		  processors for most of the applications that we
		  studied. We explain the sources of overhead that
		  limit the speedup of each of our benchmark programs.
		  We also compare the performance of Id on a single
		  Monsoon processor with C/Fortran on a DEC Station
		  5000 (MIPS R3000 processor),to establish a baseline
		  for the efficiency of Id execution on Monsoon. We
		  find that the execution of Id programs on one
		  Monsoon processor takes up to three times as many
		  cycles as the corresponding C or Fortran programs
		  executing on a MIPS R3000 processor. We identify the
		  sources of inefficiency on Monsoon and suggest
		  improvements, where possible. In many cases,
		  however, improving single processor performance will
		  reduce parallel processor performance."
}

@InProceedings{Hidaka93,
  author = 	 "Yasuo Hidaka and Hanpei Koike and Hidehiko Tanaka",
  title = 	 "Multiple threads in cyclic register windows",
  pages =	 "131--142",
  booktitle =	 isca20,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  note =	 "Published in " # canews # " Vol.21, No.2, May
		  1993",
  note2 =	 "HAR",
  abstract =	 "Multithreading is often used to compile logic and
		  functional languages, and implement parallel C
		  libraries. Fine-grain multithreading requires rapid
		  context switching, which can be slow on
		  architectures with register windows. In the past,
		  researchers have either proposed new hardware
		  support for dynamic allocation of windows to
		  threads, or have sacrificed fast procedure called by
		  fixed allocation of windows to threads. A novel
	 	  window management algorithm, which retains both fast
		  procedure calls and fast context switching, is
		  proposed. The algorithm has been implemented on the
		  SPARC processor by modifying window trap handlers. A
		  quantitative evaluation of the scheme using a
		  multithreaded application with various concurrency
		  and granularity levels is given. The evaluation
		  shows that the proposed scheme always does better
		  than the other schemes. Some implications for
		  multithreaded architectures are also presented."
}

@Article{Hirata92a,
  author = 	 "Hiroaki Hirata and Yoshiyuki Mochizuki and Akio
		  Nishmura and Yoshimori Nakase",
  title = 	 "A multithreaded processor architecture with
		  simultaneous instruction issuing",
  journal =	 "Supercomputer",
  year =	 "1992",
  volume =	 "9",
  number =	 "3",
  pages =	 "23--39",
  month =	 "May",
  abstract =	 "The authors propose a multithreaded architecture
		  which can improve machine throughput. In this
		  architecture, an instruction from one thread is
		  issued simultaneously with instructions from other
		  threads, and these instructions can begin execution
		  unless they are competing with one another for the
		  same functional unit. The architecture also makes it
		  possible to parallelize a still wider range of loops
		  than on vector or VLW machines. Simulation results
		  show that a 1.9 and a 2.8 times speed-up can be
		  gained by permitting, respectively, two and four
		  independent instruction streams to be executed on a
		  processor."
}

@InProceedings{Hirata92b,
  author = 	 "Hiroaki Hirata and Kozo Kimura and Satoshi Nagamine
		  and Yoshiyuki Mochizuki and Akio Nishimura and
		  Yoshimori Nakase and Teiji Nishizawa",
  title = 	 "An elementary processor architecture with
		  simultaneous instruction issuing from multiple threads",
  pages =	 "136--145",
  booktitle =	 isca19,
  year =	 "1992",
  address =	 "Gold Coast, Australia",
  month =	 "May",
  note =	 "Published in " # canews # "Vol.20, No.2, May 1992",
  abstract =	 "The article proposes a multithreaded processor
		  architecture which improves machine throughput. In
		  the processor architecture, instructions from
		  different threads (not a single thread) are issued
		  simultaneously to multiple functional units, and
		  these instructions can begin execution unless there
		  are functional unit conflicts. This parallel
		  execution scheme greatly improves the utilization of
		  the functional unit. Simulation results show that by
		  executing two and four threads in parallel on a
		  nine-functional-unit processor, a 2.02 and a 3.72
		  times speed-up, respectively, can be achieved over a
		  conventional RISC processor. The architecture is
		  also applicable to the efficient execution of a
		  single loop. In order to control functional unit
		  conflicts between loop iterations, a new static code
		  scheduling technique has been developed. Another
		  loop execution scheme by using the multiple control
		  flow mechanism of the architecture makes it possible
		  to parallelize loops which are difficult to
		  parallelize in vector or VLIW machines."
}

@InProceedings{Hironaka91,
  author = 	 "Tetsou Hironaka and Takashi Hashimoto and Keizo
		  Okazaki and Kazuaki Murakami and Shinji Tomita",
  title = 	 "A Single-Chip Vector-Processor Prototype Based on
		  Multithreaded Streaming/FIFO Vector (MSFV)
		  Architecture",
  pages =	 "77--86",
  booktitle =	 "Proceedings of International Symposium on
		  Supercomputing'91",
  year =	 "1991",
  month =	 "Nov",
  abstract =	 ""
}

@InProceedings{Hitchcock85,
  author = 	 "Charles Y. Hitchcock III and Brinkley Sprunt",
  title = 	 "Analyzing multiple register sets",
  pages =	 "55--63",
  booktitle =	 isca12,
  year =	 "1985",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Hoch91,
  author = 	 "J. E. Hoch and D. M. Davenport and V. G. Grafe and
		  K. M. Steele",
  title = 	 "Compile-time partitioning of a non-strict language
		  into sequential threads",
  pages =	 "180--189",
  booktitle =	 "Proceedings of 1991 Symposium on Parallel and Distributed
		  Processing",
  year =	 "1991",
  month =	 "Dec",
  abstract =	 "Presents a practical algorithm for partitioning a
		  program into sequential threads. A thread is a
		  sequence of instructions, possibly including
		  branches, that can be scheduled as an indivisible
		  unit on a von Neumann-like processor. The primary
		  target of the proposed compilation strategy is
		  large-scale parallel systems that rely on
		  multithreading at the processor level to tolerate
		  long communication latencies. As such, the algorithm
		  incorporates a mechanism to balance the desire to
		  maximize thread length with the desire to expose
		  useful high-level parallelism. It can also exploit
		  known dependency information (gathered through
		  subscript analysis, for example). Although this
		  paper focuses on non-strict (but not lazy) language
		  semantics, the partitioning analysis is equally well
		  suited to a non-strict language on a sequential
		  machine or a strict language on a parallel
		  multithreaded machine."
}

@InProceedings{Holm94,
  author = 	 "John Holm and Antonio Lain and Prithviraj Banerjee",
  title = 	 "Compilation of scientific programs into
		  multithreaded and message driven computation",
  pages =	 "518--525",
  booktitle =	 "Proceedings of the 1994 Scalable High Performance
		  Computing Conference",
  year =	 "1994",
  note2 =	 "HAR",
  url =     "ftp://ftp.crhc.uiuc.edu/pub/Paradigm/shpcc94.hlb.ps.Z",
  abstract =	 "Many programs written in the SPMD programming model
		  send messages asynchronously, and block when
		  receiving messages.  Multiple threads can make use
		  of the processor while other threads wait for
		  messages. This paper describes and evaluates two
		  techniques for multithreading on the nodes of
		  distributed memory message passing systems.  One
		  method is a purely runtime threads package.  The
		  second method requires the SPMD code to be
		  systematically transformed into message driven code
		  which can be run under a message driven model.  The
		  multithreading of scientific applications is
		  evaluated on the iPSC2 and the CM5."
}

@Article{Horn91,
  author = 	 "C. Horn and Vinny Cahill",
  title = 	 "Supporting Distributed Applications in the Amadeus
		  Environment",
  journal = 	 "Computer Communications",
  year =	 "1991",
  volume =	 "14",
  pages =	 "358-365",
  month =	 "Jul/Aug",
  note =	 "Also technical report TCD-CS-92-23, Dept. of
		  Computer Science, Trinity College Dublin",
  url = 	 "ftp://ftp.dsg.cs.tcd.ie:/pub/doc/TCD-CS-92-23.ps.gz", 
  abstract =	 "Distributed programming is becoming commonplace,
		  typically based on remote procedure call (RPC) and
		  lightweight threads packages, possibly with an
		  underlying distributed file service. In this article
		  we argue that there are significant merits in
		  providing an integrated distributed application
		  environment, rather than merely augmenting one or
		  more programming languages individually with an RPC
		  package, threads support and remote file access. Our
		  Amadeus environment is a proof of concept
		  implementation, currently extending C++ for
		  distributed and persistent programming above Unix."
}

@TechReport{Hseush92,
  author = 	 "Wenwey Hseush and James C. Lee and Gail Kaiser",
  title = 	 "MeldC Threads: Supporting Large-Scale Dynamic
		  Parallelism",
  institution =  "Columbia University, Department of Computer Science",
  year = 	 "1992",
  number =	 "CUCS-010-92",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.cs.columbia.edu/pub/reports/reports-1992/cucs-010-92.ps.Z", 
  abstract =	 "We present a new thread model that supports large-
		  scale dynamic parallelism, i.e., the number of
		  co-existing threads is large andthe life-time for
		  each thread is short. We introduce the interleaving
		  stack (IS), which melds the frames of multiple
		  threads into one shared stack. The pure interleaving
		  stack is not scalable due to severe internal
		  fragmentation. We propose and evaluate an
		  improvement, circular interleaving stack (CIS). Our
		  simulation shows that in the domains of large-scale
		  dynamic parallelism, CIS performs better than the
		  traditional one- stack-per-thread (1SPT) mechanism
		  with respect to memory utilization. The 1SPT
		  mechanism gains better memory utilization as thread
		  life-time increases. CIS and 1SPT show similar CPU
		  utilization"
}

@InProceedings{Hsieh93,
  author = 	 "Wilson C. Hsieh and P. Wang and William E. Weihl",
  title = 	 "Computation migration: enhancing locality for
		  distributed-memory parallel systems",
  pages =	 "239--248",
  booktitle =	 ppopp4,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  note =	 "Published in SIGPLAN Notices. Vol.28, No.7, July
		  1993",
  abstract =	 "Computation migration is a technique that is based
		  on compile-time program transformation, for
		  accessing remote data in a distributed-memory
		  parallel system. In contrast with RPC-style access,
		  where the access is performed remotely, and with
		  data migration, where the data is moved so that it
		  is local, computation migration moves put of the
		  current thread to the processor where the data
		  resides. The access is performed at the remote
		  processor, and the migrated thread portion continues
		  to run on that same processor; this makes subsequent
		  accesses in the thread portion local. The authors
		  describe an implementation of computation migration
		  that consists of two parts: a implementation that
		  migrates single activation frames, and a high-level
		  language annotation that allows a programmer to
		  express when migration is desired. They performed
		  experiments using two applications; these
		  experiments demonstrate that computation migration
		  is a valuable alternative to RPC and data
		  migration."
}

@TechReport{Hsieh94,
  author = 	 "Wilson C. Hsieh and Kirk L. Johnson and M. Frans
		  Kaashoek and Deborah A. Wallach William E. Weihl",
  title = 	 "Efficient Implementation of High-Level Languages on
		  User-Level Communications Architecture",
  institution =  "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1994",
  number = 	 "MIT-LCS-TR-616",
  month = 	 "May",
  url =     "ftp://ftp-pubs.lcs.mit.edu/common/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-616.ps.gz",
  abstract =     "User-level communication architectures --- parallel
		  architectures that give user code direct but
		  protected access to the network --- provide
		  communication performance that is an order of a
		  higher than previous-generation message-passing
		  architectures. Unfortunately, in order to take
		  advantage of this level of performance, programmers
		  must concern themselves with low-level issues that
		  are often hardware-dependent ({\em e.g.}, what
		  primitives to use for large and small data
		  transfers, and whether to use interrupts or
		  polling). As a result, programs are difficult to
		  design, implement, maintain, and port. New compiler
		  and runtime system mechanisms are needed to allow
		  programs written in high-level languages ---
		  languages where the programmer does not orchestrate
		  communication --- to achieve the full potential of
		  user-level communication architectures. We propose a
		  software architecture (compiler and runtime system)
		  for implementing high-level languages with dynamic
		  parallelism on user-level communication
		  architectures. The compiler uses a simple runtime
		  interface, and a new strategy called optimistic
		  active messages to eliminate overhead due to context
		  switching and thread creation. The runtime supports
		  user-level message handlers and multithreading. We
		  developed an implementation of the runtime for the
		  CM-5; microbenchmarks demonstrate that our runtime
		  has excellent base performance. We compare our
		  compilation strategy and runtime with a portable
		  runtime that uses traditional network interfaces. On
		  our system, the microbenchmarks perform up to 30
		  times better; three hand-compiled applications run
		  10% to 50% faster. We also compare our approach on
		  these applications with hand-crafted C programs that
		  use active messages; the microbenchmarks perform
		  within 25% of C, and almost all of the applications
		  perform within a factor of two of C."
}

@InProceedings{Hsu89,
  author = 	 "M. Hsu",
  title = 	 "Parallel computing with distributed shared data",
  pages =	 "485--??",
  booktitle =	 "Proceedings Fifth International Conference on Data
		  Engineering",
  year =	 "1989",
  publisher =	 "IEEE Comput. Soc. Press",
  address =	 "Los Angeles, CA, USA",
  month =	 "??",
  abstract =	 "Summary form only given. The issue of ease of using
		  shared data in a data-intensive parallel computing
		  environment is discussed. An approach is
		  investigated for transparently supporting data
		  sharing in a loosely coupled parallel computing
		  environment, where a moderate to a large number of
		  individual computing elements are connected via a
		  high-bandwidth network without necessarily
		  physically sharing memory. A system called VOYAGER
		  is discussed which serves as the underlying system
		  facility that supervises the distributed shared
		  virtual memory. VOYAGER allows shared-data parallel
		  applications to take advantage of parallel and
		  distributed processing with relative ease. The
		  application program merely maps the shared data onto
		  its virtual address space replicates itself on
		  distributed machines and spawns appropriate
		  execution threads; the threads would automatically
		  be given coordinated access to the shared data
		  distributed in the network. Multiple computation
		  threads migrate and populate the processors of a
		  number of computing elements, making use of the
		  multiple processors to achieve a high degree of
		  parallelism. The low-level resource management
		  chores are made available once and for all in the
		  underlying facility VOYAGER, usable by many
		  different data-intensive applications."
}

@MastersThesis{Huguet85a,
  author = 	 "Miquel Huguet",
  title = 	 "A C-oriented register set design",
  school = 	 "University of California, Los Angeles. Computer
		  Science Dept.",
  year = 	 "1985",
  note =	 "As Tech report CSD-850019.",
  abstract =	 ""
}

@Article{Huguet85b,
  author = 	 "Miquel Huguet and Tomas Lang",
  title = 	 "A reduced register file for RISC architectures",
  journal =	 canews,
  year =	 "1985",
  volume =	 "27",
  number =	 "10",
  pages =	 "22--31",
  month =	 "Sep",
  abstract =	 ""
}

@TechReport{Huguet88,
  author = 	 "Miquel Huguet and Tomas Lang",
  title = 	 "Reduced register saving/restoring for small register
		  files",
  institution =  "University of California, Los Angeles. Computer
		  Science Dept",
  year = 	 "1988",
  number =	 "CSD 880066",
  month =	 "Aug",
  abstract =	 "With the current trend towards VLSI load/store
		  architectures, registers have become one of the
		  critical resources to increase processor
		  performance. Since registers are conventionally
		  saved/restored across function calls, the
		  corresponding saving and restoring (RSR) traffic can
		  almost eliminate the data memory traffic reduction
		  obtained. To reduce the RSR overhead some current
		  processors rely on hardware support, such as
		  multiple-window register files, intra-procedural
		  compiler optimizations, such as live-variable
		  analysis and leaf-function optimization, and/or
		  inter-procedural register allocation. The authors
		  present an intra-procedural optimization to reduce
		  the RSR traffic when registers are saved/restored at
		  the caller side (Policy A-IvOpt). They show that
		  this policy generates at least RSR traffic with
		  respect to the conventional policies of
		  saving/restoring the registers at the caller with
		  live-variable analysis and at the callee with
		  leaf-function optimization. Moreover, they present a
		  new RSR policy which makes use of dynamic
		  information to know which registers have been used
		  during program execution so that unnecessary RSR
		  traffic is eliminated. They also show that the
		  dynamic policy with leaf-function optimization
		  (Policy G-If) generates less RSR traffic than Policy
		  A-IvOpt. Finally, they compare the RSR traffic
		  generated by Policy G-If to the one generated by the
		  already-existing schemes for multiple-window
		  architectures (fixed-size windows, variable-size
		  windows, and multi-size windows) and show that
		  Policy G-If generates the least RSR traffic when a
		  small register file "
}

@PhdThesis{Huguet89,
  author = 	 "Miquel Huguet",
  title = 	 "Architectural and compiler support for efficient
		  function calls",
  school = 	 "University of California, Los Angeles. Computer
		  Science Dept.",
  year = 	 "1989",
  month =	 "Sep",
  note =	 "As tech report CSD-890051",
  abstract =	 ""
}

@article{Huguet91,
  author       = "Miquel Huguet and Tomas Lang",
  title        = "Architectural Support for Reduced Register
		  Saving/Restoring in Single-Window Register Files",
  journal      = tocs,
  pages        = "66-97",
  volume       = "9",
  number       = "1",
  month        = "Feb",
  year         = "1991",
  note2 =	 "HAR",
  abstract =	 "The use of registers in a processor reduces the data
		  and instruction memory traffic. Since this
		  reductions is a significant factor in the
		  improvement of the program execution time, recent
		  VLSI processors have a large number of registers
		  which can be used efficiently because of the
		  advances in compiler technology. However, since
		  registers have to be saved/restored across function
		  calls, the corresponding register saving and
		  restoring (RSR) memory traffic can almost eliminate
		  the overall reduction. This traffic has been reduced
		  by compiler optimizations and by providing
		  multiple-window register files. Although these
		  multiple-window architectures produce a large
		  reduction in the RSR traffic, they have several
		  drawbacks which make the single-window file
		  preferable. The authors consider a combination of
		  hardware support and compiler optimizations to
		  reduce the RSR traffic for a single-window register
		  file, beyond the reductions achieved by compiler
		  optimizations along. Basically, this hardware keeps
		  track of the registers that are written during
		  execution, so that the number of registers saved is
		  minimized. Moreover, hardware is added so that a
		  register is saved in the activation record of the
		  function that uses it (instead of in the record of
		  the current function); in this way a register is
		  restored only when it is needed, rather than
		  wholesale on procedure return. They present a
		  register saving and restoring policy that makes use
		  of this hardware, discuss its implementation, and
		  evaluate the traffic reduction when the policy is
		  combined with intraprocedural and interprocedural
		  compiler optimizations. On average for the four
		  general-purpose programs measured, the RSR traffic
		  is reduced by about 90 percent for a small register
		  file (i.e. 32 registers), which results in an
		  overall data memory traffic reduction of about 15
		  percent."
}

@InProceedings{Hum91a,
  author = 	 "Herbert H. J. Hum and Guang R. Gao",
  title = 	 "A novel high-speed memory organization for
		  fine-grain multi-thread computing",
  volume =	 "I",
  pages =	 "34--51",
  booktitle =	 parle91,
  year =	 "1991",
  month =	 "Jun",
  series =	 "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  address =	 "Eindhoven, Netherland",
  note =	 "Also in Future generation Computer Systems Vol.8
		  No.4, pp.287--301",
  note2 =	 "HAR",
  abstract =	 "The authors propose a novel organization of
		  high-speed memories, known as the register-cache,
		  for a multi-threaded architecture. Viewed from the
		  execution unit, its contents are addressable similar
		  to ordinary CPU registers using relatively short
		  addresses. From the main memory perspective, it is
		  content addressable, i.e., its contents are tagged
		  just as in conventional caches. The register
		  allocation for the register-cache is adaptively
		  performed at runtime, resulting in a dynamically
		  allocated register file. A program is compiled into
		  a number of instruction threads called super-actors.
		  A super-actor becomes ready for execution only when
		  its input data are physically residing in the
		  register-cache and space is reserved in the
		  register-cache to store its result. Therefore, the
		  execution unit will never stall or 'freeze' when
		  accessing instruction or data. Another advantage is
		  that since registers are dynamically assigned at
		  runtime, register allocation difficulties at
		  compile-time, e.g., allocating registers for
		  subscripted variables of large arrays, can be
		  avoided. Architectural support for overlapping
		  executions of super-actors and main memory
		  operations are provided so that the available
		  concurrency in the underlying machine can be better
		  utilized. The preliminary simulation results seem to
		  be very encouraging: with software pipelined loops,
		  a register-cache of moderate size can keep the
		  execution unit usefully busy."
}

@InProceedings{Hum91b,
  author = 	 "Herbert H. J. Hum and Guang R. Gao",
  title = 	 "Efficient support of concurrent threads in a hybrid
		  dataflow/von Neumann architecture",
  pages =	 "190--193",
  booktitle =	 ispdp3,
  year =	 "1991",
  address =	 "Dallas, TX, USA",
  month =	 "Dec",
  note =	 "Also published in Supercomputing 91 workshop on
		  multithreading",
  abstract =	 "Examines the thread support in a multi-threaded
		  processor architecture, called the Super-Actor
		  Machine (SAM). The SAM employs a novel organization
		  of high-speed buffer memory known as the
		  register-cache-a memory device organized both as a
		  register file and a cache, and is used as a buffer
		  between the execution unit and main memory. To
		  characterize the floating-point performance of the
		  machine on scientific benchmarks, a new performance
		  measure is introduced, called the Floating-point
		  Arithmetic and logic operations per machine Beat, or
		  FAB for short. Results from a detailed simulation
		  are very encouraging: for the memory-intensive SAXPY
		  scientific loop, a processing element of the SAM can
		  attain a .85 FAB rating as compared to a value of
		  nearly 1 for the IBM RS/6000 which employs the
		  combined floating-point add-multiply operation (this
		  type of instruction has not been implemented on the
		  SAM yet). Furthermore, the SAM can attain this
		  rating with less execution resources (i.e.
		  high-speed registers) than typical multi-threaded
		  architectures."
}

@Article{Hum92a,
  author = 	 "Herbert H. J. Hum and Guang R. Gao",
  title = 	 "A high-speed memory organization for hybrid
		  dataflow/von Neumann computing",
  journal =	 "Future Generation Computer Systems",
  year =	 "1992",
  volume =	 "8",
  number =	 "4",
  pages =	 "287--301",
  month =	 "Sep",
  abstract =	 "The paper proposes a novel organization of
		  high-speed memories, known as the register-cache,
		  for a multi-threaded architecture. Viewed from the
		  execution unit, its contents are addressable as
		  ordinary CPU registers using relatively short
		  addresses. From the main memory perspective, it is
		  content addressable. In this register-cache
		  organization, a number of registers are grouped into
		  a block of registers where a register in a block is
		  accessed using an offset from the address of the
		  block, an offset value which is embedded in the
		  compiler generated code. The binding of register
		  block locations to register-cache line addresses is
		  adaptively performed at runtime, thus resulting in a
		  dynamically allocated register file. In this
		  execution model, a program is compiled into a number
		  of instruction threads called super-actors. A
		  super-actor becomes ready for execution only when
		  its input data are physically residing in the
		  register-cache and space is reserved in the
		  register-cache to store its result."
}

@PhdThesis{Hum92b,
  author = 	 "Herbert H. J. Hum",
  title = 	 "The Super-Actor Machine: A hybrid dataflow/von
		  Neuman architecture",
  school = 	 "McGill University",
  year = 	 "1992",
  address =	 "Montreal, Canada",
  month =	 "May",
  abstract =	 ""
}

@InProceedings{Hum93,
  author = 	 "Herbert H.J. Hum and Guang R. Gao",
  title = 	 "Supporting a dynamic SPMD model in a multithreaded
		  architecture",
  pages =	 "165--175",
  booktitle =	 compcon93,
  year =	 "1993",
  month =	 "Feb",
  note2 = 	 "HAR",
  abstract =	 "The authors present a multithreaded architecture
		  model which can efficiently support a single-program
		  multiple-data (SPMD) computation of programs with
		  dynamic data structures. It is based on a dynamic
		  SPMD model where the access delay due to a remote
		  reference of a dynamic data structure can be
		  tolerated by having multiple threads of control
		  concurrently in execution within each processor.
		  However, the present model permits the exploitation
		  of locality of references through the use of caches
		  for remote memory operations. When a remote memory
		  access operation is encountered and cannot be
		  satisfied locally, the processor can have the
		  flexibility of migrating the thread to a remote
		  processor when (and only when) such migration is
		  desirable."
}

@InProceedings{Hum94,
  author = 	 "Herbert H. J. Hum and Kevin B. Theobald and Guang R.
		  Gao",
  title = 	 "Building multithreaded architectures with
		  off-the-shelf microprocessors",
  pages =	 "288--294",
  booktitle =	 ipps8,
  year =	 "1994",
  address =	 "Cancun, Mexico",
  month =	 "Apr",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo68.ps.gz",
  note =	 "Also as McGill University ACAPS Technical Memo 68,
		  Oct.1993",
  note2 =	 "HAR",
  abstract =	 "Present day parallel computers often face the
		  problems of large software overheads for process
		  switching and inter-processor communication. These
		  problems are addressed by the Multi-Threaded
		  Architecture (MTA), a multiprocessor model designed
		  for efficient parallel execution of both numerical
		  and non-numerical programs. We begin with a
		  conventional processor, and add the minimal external
		  hardware necessary for efficient support of
		  multithreaded programs. The article begins with the
		  top-level architecture and the program execution
		  model. The latter includes a description of
		  activation frames and thread synchronization. This
		  is followed by a detailed presentation of the
		  processor. Major features of the MTA include the
		  Register-Use Cache for exploiting temporal locality
		  in multiple register set microprocessors, support
		  for programs requiring non-determinism and
		  speculation, and local function invocations which
		  can utilize registers for parameter passing."
}

@InProceedings{Hum95,
  author = 	 "Herbert H.J. Hum and Olivier Maquelin and Kevin B.
		  Theobald and Xinmin Tian and Xinan Tang and Guang R.
		  Gao and Phil Cupryk and Nasser Elmasri and Laurie J.
		  Hendren and Alberto Jimenez and Shoba Krishnan and
		  Andres Marquez and Shamir Merali and Shashank
		  Nemawarkar and Praash Panangaden and Xun Xue and
		  Yingchun Zhu",
  title = 	 "The Multi-threaded architecture multiprocessor",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "June",
  note =	 "Also as Memo-88, McGill University, School of
		  Computer Science, ACAPS Lab, Dec. 1994",
  note2 =	 "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo88.ps.gz",
  abstract =	 "Multithreaded node architectures have been proposed
		  for future multiprocessor systems. However, some
		  open issues remain: can efficient multithreading
		  support be provided in a multiprocessor machine such
		  that it is capable of tolerating the synchronization
		  and communication latencies, with little intrusion
		  on the performance of sequentially-executed code?
		  And how much (quantitatively) does such
		  non-intrusive multithreading support contribute to
		  the scalable parallel performance in the presence of
		  increasing interprocessor communication and
		  synchronization demands?  In this paper, we describe
		  the design of the EARTH (Efficient Architecture for
		  Running THreads) which attempts to address the above
		  issues. Each processor in EARTH has an off-the-shelf
		  RISC processor for executing threads, and an ASIC
		  Synchronization Unit (SU) supporting dataflow-like
		  thread synchronizations, scheduling, and remote
		  memory requests. In preparation for an
		  implementation of the SU, we have emulated a basic
		  EARTH model on MANNA 2.0, an existing multiprocessor
		  whose hardware configuration closely matches EARTH.
		  ThisEARTH-MANNA emulation testbed has been fully
		  functional, enabling us to experiment with
		  large-scale benchmarks with impressive speed.  With
		  this platform, we demonstrate that multithreading
		  support can be efficiently implemented (with little
		  emulation overhead) in a multiprocessor without a
		  major impact on uniprocessor performance. Also, we
		  give our first quantitative indications of how much
		  the basic multithreading support can help in
		  tolerating increasing communication/synchronization
		  demands."
}

@InProceedings{Huong94,
  author = 	 "Chengchang Huang and Yih Huang and Philip K. McKinley",
  title = 	 "A Thread-Based Interface for Collective
		  Communication on ATM Networks",
  pages =	 "??--??",
  booktitle =	 icdcs15,
  year =	 "1995",
  month =	 "??",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.cps.msu.edu/pub/crg/PAPERS/icdcs95.ps.Z",
  abstract =	 "This paper presents the results of an investigation
		  of collective communication operations for
		  distributed computing across asynchronous transfer
		  mode (ATM) networks. Several collective operations
		  have been implemented and studied on a three-switch
		  ATM network testbed at Michigan State University.
		  The methods use virtual topologies constructed from
		  ATM virtual channels. A particular type of virtual
		  topology is described that efficiently implements
		  several collective operations through the use of
		  hardware-supported ATM multicast channels.
		  Performance measurements are presented that
		  illustrate how a thread-based software design can
		  take advantage of such underlying hardware
		  features."

}

@InProceedings{Hwang93,
  author =       "Dae J. Hwang and S. H. Cho and Y. D. Kim and Sang Y.
		  Han",
  title =        "Exploiting Spatial and Temporal Parallelism in the
		  Multithreaded Node Architecture Implemented on
		  Superscalar {RISC} Processors",
  booktitle =    icpp93,
  volume =       "I - Architecture",
  pages =        "I--51--I--54",
  publisher =    "CRC Press",
  address =      "Boca Raton, FL",
  month =        aug,
  year =         "1993",
  abstract =	 ""
}

@PhdThesis{Iannucci88a,
  author = 	 "Robert A. Iannucci",
  title = 	 "A Dataflow/Von Neumann Hybrid Architecture",
  school = 	 "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1988",
  month =	 "Jul",
  note =	 "182 Pages. As Tech report MIT/LCS/TR-418",
  abstract =	 "This report examines the spectrum of architectures
		  from von Neumann to dataflow by proposing a new
		  architecture which is a hybrid  of the two
		  paradigms.  The analysis attempts to discover those
		  features of the dataflow architecture, lacking in a
		  von Neumann machine, which are essential for
		  tolerating latency and synchronization costs.  These
		  features are captured in the concept of a  parallel
		  machine  language  wherein the units of scheduling,
		  called scheduling quanta , are bound at compile time
		  rather than at instruction set design time.  It is
		  shown that the combination of dataflow-style
		  explicit synchronization and von Neumann-style
		  implicit synchronization results in an architectural
		  synergism. Using an instruction set which is
		  strictly less powerful than that of the MIT
		  Tagged-Token Dataflow Architecture (TTDA), the
		  hybrid architecture can exploit the same kinds of
		  parallelism while executing fewer instructions,
		  demonstrating the power of passing state implicitly
		  between program-counter sequenced instructions"
}

@InProceedings{Iannucci88b,
  author = 	 "Robert A. Iannucci",
  title = 	 "Toward a dataflow/von neumann hybrid architecture",
  pages =	 "131--140",
  booktitle =	 isca15,
  year =	 "1988",
  note =	 "Also as CSG-Memo-275, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  note2 =	 "HAR",
  abstract =	 ""
}

@Book{Iannucci90,
  author = 	 "Robert A. Iannucci",
  title = 	 "Parallel machines: parallel machine languages: the
		  emergence of hybrid dataflow computer
		  architectures",
  publisher = 	 "Kluwer Academic",
  year = 	 "1990",
  address =	 "Massachussetts",
  note =	 "240 pages. ISBN:0-7923-9101-2",
  abstract =	 "Book version of authors PhD thesis"
}

@Article{Iannucci94,
  author = 	 "Robert Iannucci and Anant Agarwal and William
		  Dally",
  title = 	 "Panel Session I: Architectural and Implementation
		  Issues for Multithreading",
  journal =	 canews,
  year =	 "1994",
  volume =	 "22",
  number =	 "1",
  pages =	 "3--16",
  month =	 "Mar",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/memos/memo30.ps.gz",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Inhoara93,
  author = 	 "Shigekazu Inohara and Kazuhiko Kato and Takashi
		  Masuda",
  title = 	 "'Unstable threads' kernel interface for minimizing
		  the overhead of thread switching",
  pages =	 "149--155",
  booktitle =	 ipps7,
  year =	 "1993",
  address =	 "Newport, CA, USA",
  month =	 "Apr",
  note2 =	 "HAR",
  abstract =	 "The performance of threads is limited primarily by
		  the overhead of two kinds of switching: vertical
		  switching (user/kernel domain switching) and
		  horizontal switching (context switching between
		  threads). Although these switchings are
		  indispensable in some situations, existing thread
		  mechanisms involve unnecessary switchings on
		  multiprogrammed systems, because of inappropriate
		  interfaces between the operating system kernel and
		  user-level programs. This paper presents a set of
		  interfaces between the kernel and user-level
		  programs that minimizes the overhead of the two
		  kinds of switchings. The kernel provides 'unstable
		  threads,' which are controlled solely by the kernel,
		  while each user-level program monitors them and
		  gives suggestions on their activities to the kernel
		  through a shared memory area between the kernel and
		  user address spaces. This new way of separating
		  thread management minimizes the overhead of vertical
		  and horizontal switchings."
}

@InProceedings{Inohara92,
  author = 	 "Shigekazu Inohara and Kazuhiko Kato and AAtsunobu
		  Narita and Takashi Masuda",
  title = 	 "A thread facility based on user/kernel cooperation
		  in the XERO operating system",
  booktitle =	 "Proceedings of the Fifteenth Annual International
		  Computer Software and Applications Conference",
  year =	 "1992",
  pages =	 "398--405",
  month =	 "Sep",
  note =	 "Also published in IEICE Transactions on Information
		  and Systems, Vol.E75-D, No.5, pp.627--634, Sep.
		  1992. Also as tech report 91-007, University of Tokyo
		  Dept. of Information Science",
  abstract =	 "The mechanisms for executing concurrent applications
		  proposed so far fall into one of three groups:
		  processes, kernel-level threads, and user-level
		  threads. Each of them is insufficient in terms of
		  either parallelism, the flexibility to combine
		  separately developed programs at run-time, or costs
		  of operations such as creation, switching, and
		  termination. A thread facility in the XERO operating
		  system overcomes this problem and provides a uniform
		  framework for executing concurrent applications. To
		  achieve parallelism of threads, the flexibility to
		  combine separately developed programs at run-time,
		  and fast thread operations, the operating system
		  kernel and a thread management module in a user
		  address space manage threads cooperatively. The
		  authors implement the cooperative thread management
		  mechanism and measure its performance to examine the
		  effectiveness of the approach."
}

@TechReport{Inohara94,
 author =        "Shigekazu Inohara and Takashi Masuda", 
 title =         "A Framework for Minimizing Thread Management
		  Overhead Based on Asynchronous Cooperation between
		  User and Kernel Schedulers",  
 institution =   "Department of Information Science, Faculty of
		  Science, University of Tokyo", 
 number =        "94-02",
 year =          "1994", 
 month =         "jan",
 note2 =	 "HAR", 
 url =      "ftp://ftp.is.s.u-tokyo.ac.jp/pub/tech-reports/TR94-02-a4.ps.Z",
 abstract =      "The performance of thread mechanism is dominated
		  primarily by two kinds of thread-switching
		  overheads: vertical switching (user/kernel domain
		  switching) and horizontal switching (context
		  switching between threads). Ideally, vertical
		  switchings should occur only when system calls or
		  external interrupts are invoked. Horizontal
		  switchings within a user address space should occur
		  only when threads synchronize with one another.
		  Horizontal switchings in the kernel should occur
		  only when processors are redistributed among user
		  address spaces. Existing thread mechanisms, however,
		  do not function ideally in a multiprogrammed system
		  because (1) user-level programs de termine how many
		  threads to use, (2) the kernel does not inform the
		  user level of which nor how many threads are
		  actually being assigned processors, and/or (3)
		  interaction between the kernel and user-level
		  schedulers is synchronous. This paper presents a
		  thread mechanism that minimizes the thread-switching
		  overhead by avoiding these three problems; the
		  kernel scheduler determines how many threads to use
		  in each user address space, the kernel scheduler
		  lets user-level schedulers know which threads are
		  actually being assigned processors, and all
		  interaction between the kernel scheduler and
		  user-level schedulers is asynchronous. Employing
		  these three ideas, the proposed mechanism minimizes
		  thread switchings {\em both in the kernel and in the
		  user level}, under the assumption that threads are
		  managed without off-line information on applications
		  to run."
} 

@InProceedings{Ito92,
  title =        "On {PaiLisp} Continuation and its Implementation",
  author =       "Takayasu Ito and Tomohiro Seino",
  pages =        "73--90",
  booktitle =    cwe92,
  year     =     "1992",
  url =     "http://elib.stanford.edu:80/TR/STAN:CS-TR-92-1426",
  abstract =     "Introduces call-once continuations. That is,
		  continuations that are hobbled to the point of being
		  useful for nothing more complex than thread
		  switching, but that thereby can use stacks."
}

@InProceedings{Jagannathan91,
  author = 	 "Suresh Jagannathan",
  title = 	 "Expressing fine-grained parallelism using concurrent
		  data structures",
  editor =	 "J. B. Banatre and D. Le Metayer",
  pages =	 "77--92",
  booktitle =	 "Research Directions in High-Level Parallel
		  Programming Languages. Proceedings",
  year =	 "1991",
  month =	 "Jun",
  series =	 "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  address =	 "Mont Saint-Michel, France",
  url =     "ftp://ftp.nj.nec.com/pub/pls/hlpl91.ps",
  abstract =	 "Efficient support for fine-grained parallelism on
		  stock hardware has been the focus of much recent
		  research interest. The realization of this goal
		  involves the design of linguistic devices for
		  expressing concurrency, compile-time analysis
		  techniques for constructing efficient
		  implementations of shared data objects, and a
		  runtime support system for managing lightweight
		  threads. The author examines one specific approach
		  to realizing fine-grained parallelism based on this
		  general strategy. The author's starting point is an
		  explicitly parallel language that relies on
		  concurrent data structures for expressing
		  synchronization and communication constraints. The
		  author argues that given a toolbox of compile-time
		  analyses for optimizing the representation these
		  structures, and a robust runtime system tailored for
		  managing dynamically instantiated lightweight
		  processes, fine-grained parallel applications can be
		  made to run efficiently on stock multiprocessor
		  hardware."
}

@InProceedings{Jagannathan92,
  author =       "Suresh Jagannathan and Jim Philbin",
  title =        "A Foundation for an Efficient Multi-Threaded Scheme
		  System",
  booktitle =    "Proceedings of the 1992 ACM Conference on Lisp and
		  Functional Programming",
  pages =        "345--357",
  address =      "San Francisco, USA",
  month =        jun,
  year =         "1992",
  series =	 "Lecture Notes in Computer Science ??",
  url = 	 "ftp://ftp.nj.nec.com/pub/pls/lfp92.ps",
  abstract =	 ""
}

@InProceedings{Jog89,
  author = 	 "Rajeev Jog and P. L. Vitale and J. R. Callister",
  title = 	 "Performance evaluation of a commercial
		  cache-coherent shared memory multiprocessor",
  pages =	 "173--182",
  booktitle =	 icommcs90,
  year =	 "1990",
  address =	 "Boulder, CO, USA",
  month =	 "May",
  note =	 "Published in " # perfrev # " Vol.18 No.1, 1990",
  abstract =	 "The paper describes an approximate mean value
		  analysis (MVA) model developed to project the
		  performance of a small-scale shared-memory
		  commercial symmetric multiprocessor system. The
		  system, based on Hewlett Packard Precision
		  Architecture processors, supports multiple active
		  user processes and multiple execution threads within
		  the operating system. Using detailed timings for
		  hardware delays, a customized approximate closed
		  queueing model is developed for the multiprocessor
		  system. The model evaluates delays due to bus and
		  memory contention, and cache interference. It
		  predicts bus bandwidth requirements and utilizations
		  for the bus memory controllers. An extension to
		  handle I/O traffic is outlined. Applications are
		  profiled on the basis of execution traces on
		  uniprocessor systems to provide input parameters for
		  the model. Performance effects of various detailed
		  architectural tradeoffs (memory interleaving, lower
		  memory latencies) are examined. The sensitivity of
		  overall system performance to various parameters is
		  explored. Preliminary measurements of uniprocessor
		  systems are compared against the model predictions.
		  A prototype multiprocessor system is under
		  development. It is intended to validate the modeling
		  results against measurements."
}

@InProceedings{Jones91,
  author = 	 "Michael B. Jones",
  title = 	 "Bringing the {C} Libraries With Us into a
		  Multi-Thread Future",
  pages =	 "81--91",
  booktitle =	 usenixw91,
  year =	 "1991",
  month =	 "Jan",
  note2 =	 "HAR",
  url =  	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/cmultithread.ps",
  abstract =	 "An enormous amount of UNIX (and UNIX-like) code has
		  been written (by a likewise enormous amount of
		  programmers) that uses the standard C libraries. Use
		  is made throughout much of this code of the
		  knowledge that traditional UNIX programs have
		  exactly one thread in control. However increasing
		  numbers of UNIX-like systems are beginning to
		  provide support for programs with multiple threads
		  of control. To the possible extent, it is highly
		  desirable to preserve the existing C library
		  interfaces for multi-threaded programs; this will
		  aid both in code and programmer portability between
		  traditional UNIX environments and new multi-threaded
		  ones.  A number of issues must be confronted in
		  order to produce versions of the C libraries which
		  can be used in multi-threaded programming
		  environments. Among these are: functions with
		  non-reentrant interfaces, functions which maintain
		  state between invocations, use of macros in the
		  library interfaces, interactions with signals,
		  compatibility with single-threaded library data
		  structures, performance issues, and of course, erno.
		  Despite these and other problems, experience has
		  shown that reasonable solutions are available. This
		  paper presents both a detailed explanation of the
		  inherent problems in producing multi-thread-safe C
		  libraries and the different solutions which are
		  available. Finally, the solutions to these problems
		  adopted by a number of research and industry groups
		  are presented."
}

@TechReport{Jonsson94,
  author = 	 "Jan Jonsson",
  title = 	 "Pipeline interleaved programmable digital signal
		  processors",
  institution =  "Chalmers Un iversity of Technology, Department of
		  Computer Engineering",
  year = 	 "1994",
  number =	 "1994-07-14",
  url =  	 "TBD",
  abstract =	 ""
}


@InProceedings{Jordan83,
  author = 	 "Harry F. Jordan",
  title = 	 "Performance measurements on {HEP:} A pipelined {MIMD}
		  computer",
  pages =	 "207--212",
  booktitle =	 isca10,
  year =	 "1983",
  month =	 "Jun",
  note2 =	 "HAR",
  abstract =	 "A pipelined implementation of MIMD operation is
		  embodied in the HEP computer. This architectural
		  concept should be carefully evaluated now that suchg
		  a computer is available commercially. This paper
		  studies the degree of utilization of pipelines in
		  the MIMD environment. A detailed analysis of two
		  extreme cases indicates that pipeline utilization is
		  quite high. Although no direct comparisons are made
		  with other computers, the low pipeline idle time in
		  this machine indicates that this architectural
		  technique may be more beneficial in an MIMD machine
		  than in either SISD or SIMD machines"
}

@Article{Jordan84,
  author = 	 "Harry F. Jordan",
  title = 	 "Experiences with pipelined multiple instruction streams",
  journal =	 ieeep,
  year =	 "1984",
  volume =	 "72",
  number =	 "1",
  pages =	 "113--123",
  month =	 "Jan",
  abstract =	 ""
}

@InProceedings{Jsmith92,
  author = 	 "J. A. Smith",
  title = 	 "The multi-threaded {X (MTX)} window system and {SMP}",
  pages =	 "131--143",
  booktitle =	 "Distributed Computing, Practice and Experience.
		  Proceedings of the Autumn 1992 OpenForum Technical
		  Conference",
  year =	 "1992",
  address =	 "Utrecht, Netherlands",
  month =	 "Nov",
  abstract =	 "The current UNIX implementation of the X11R5 Window
		  System provides only single-threaded support of
		  client requests, event processing, and device input.
		  Each request and event is processed one at a time to
		  the exclusion of all other processing. This approach
		  leads to noninteractive server. With X clients that
		  use the Phigs+ X Extension (PEX), X Image Extension
		  (XIE), and integrated multimedia, the performance
		  and usability of the X11R5 server is severely
		  degraded. This paper presents the object-oriented
		  design and implementation of an X Window Server with
		  multi-threaded concurrent support and shows how
		  multimedia X clients can take advantage of the
		  resulting gains in interactivity using Symmetric
		  Multi-Processor SMP) platforms such as OSF/1 and
		  DG/UX. Lessons learned can be applied to client side
		  threads design."
}

@InProceedings{Kalas94,
  author = 	 "Ivan Kalas and Eshrat Arjomandi and Guang R. Gao and
		  Bill O'Farrell",
  title = 	 "FTL: A multithreaded environment for parallel
		  computation",
  pages =	 "??--??",
  booktitle =	 "Proceedings of CASCON'94",
  year =	 "1994",
  url = 	 "http://www-acaps.cs.mcgill.ca/cascon94/htm/english/abs/kalas.htm",
  note2 =	 "HAR",
  abstract =	 "The arrival of high-performance ``killer micros'' and
		  the availability of high-performance networks (e.g.,
		  ATM) offer potential for building clusters of
		  workstations with a significantly higher level of
		  scalability than before. A promising approach to
		  exploiting parallel computation on these systems is
		  to use multithreading to overlap computation and
		  communication while offering a simple programming
		  model that smoothly integrates these two functions.
		  This paper describes the design and implementation
		  of a portable software platform for multithreaded
		  computation in distributed memory systems. The goal
		  is to provide a runtime environment that efficiently
		  integrates computation and communication, and runs
		  on off-the-shelf workstations without any hardware
		  or operating system modifications. The target
		  configurations are networked clusters of UNIX
		  workstations, such as workstation farms and
		  high-speed interconnect clusters. The FTL software
		  platform is being implemented as a runtime library
		  that can be used either directly by a programmer, or
		  by a compiler. Portability and programmability are
		  among the important objectives in our design."
}

@PhdThesis{Kaminsky77,
  author = 	 "William J. Kaminsky",
  title = 	 "Architecture for multiple instruction stream LSI
		  processors",
  school = 	 "University of Illinois at Champaign-Urbana,
		  Department of Electrical and Computer Engineering",
  year = 	 "1977",
  note =	 "As Tech report Coordinated Science Lab R-796",
  abstract =	 ""
}

@Article{Kaminsky79,
  author = 	 "William J. Kaminsky and Edward S. Davidson",
  title = 	 "Developing a multiple-instruction-stream single-chip
		  processor",
  journal =	 ieeecomp,
  year =	 "1979",
  volume =	 "12",
  number =	 "12",
  pages =	 "66--76",
  month =	 "Dec",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Karlin91,
  author = 	 "Anna R. Karlin and Kai Li and Mark S. Manasse and
		  Susan Owicki",
  title = 	 "Empirical studies of competitive spinning for a
		  shared-memory multiprocessor",
  pages =	 "41--55",
  booktitle =	 sosp13,
  year =	 "1991",
  address =	 "Pacific Grove, CA, USA",
  month =	 "Oct",
  note =	 "Published in " # acmosr # " Vol.25, No.5, 1991. Also
		  as Tech report CS-TR-319-91, Princeton University.
		  Dept. of Computer Science",
  url =     "http://www.cs.princeton.edu:80/TR/PRINCETONCS:TR-319-91",
  abstract =	 "A common operation in multiprocessor programs is
		  acquiring a lock to protect access to shared data.
		  Typically, the requesting thread is blocked if the
		  lock it needs is held by another thread.
		  Alternatively, the thread could spin until the lock
		  is free, or spin for a while and then block. The
		  authors study seven strategies for determining
		  whether and how long to spin before blocking. Of
		  particular interest are competitive strategies, for
		  which the performance can be shown to be no worse
		  than some constant factor times an optimal off-line
		  strategy. The performance of five competitive
		  strategies is compared with that of always blocking,
		  always spinning, or using the optimal off-line
		  algorithm. Measurements of lock-waiting time
		  distributions for five parallel programs were used
		  to compare the cost of synchronization under all the
		  strategies. Additional measurements of elapsed time
		  for some of the programs and strategies allowed
		  assessment of the impact of synchronization strategy
		  on overall program performance. Both types of
		  measurements indicate that the standard blocking
		  strategy performs poorly compared to mixed
		  strategies. Among the mixed strategies studied,
		  adaptive algorithms perform better than non-adaptive
		  ones."
}

@InProceedings{Kats89,
  author = 	 "Morry Katz and Daniel Weise",
  title = 	 "Continuing into the future: on the interaction of
		  futures and first-class continuations",
  editor =	 "T. Ito and R. H. Jr. Halstead",
  pages =	 "101--102",
  booktitle =	 "Parallel Lisp: Languages and Systems. US/Japan
		  Workshop on Parallel Lisp Proceedings",
  year =	 "1989",
  month =	 "Jun",
  series =	 "Lecture Notes in Computer Science ???",
  publisher =	 "Springer-Verlag",
  address =	 "Sendai, Japan",
  note  =	 "Also in Proceedings of the 1990 ACM Conference on
		  LISP and Functional Programming, pp.176--184, June 1990",
  abstract =	 "One of the nicest features of the future construct
		  originally presented in Multilisp is its near
		  orthogonality with respect to a functional subset of
		  Scheme. Introducing futures into most functional
		  programs does not affect the value returned, even
		  though the parallel execution order might differ
		  from the sequential. When futures and continuations
		  are used in the same program, however, parallel and
		  sequential executions can yield different results.
		  No existing implementation of futures has yet
		  addressed this issue. The authors make futures and
		  continuations interact properly through a simple,
		  yet important, change to the implementation of the
		  future construct. This change causes a second
		  problem to manifest itself: the creation of
		  extraneous computation threads. The second problem
		  is addressed by making an additional change to the
		  future construct."
}

@InProceedings{Kavi95,
  author = 	 "Krishna Kavi and Ali R. Hurson and Phenil Patadia
		  and Elizabeth Abraham and Ponnarasu Shanmugam",
  title = 	 "Design of Cache Memories for Multi-Threaded
		  Dataflow Architecture",
  pages =	 "??--??",
  booktitle =	 isca22,
  year =	 "1995",
  month =	 "Jun",
  abstract =	 "Cache memories have proven their effectiveness in
		  the von Neumann architecture when localities of
		  reference govern the execution loci of programs. A
		  pure dataflow program, in contrast, contains no
		  locality of reference since the execution sequence
		  is enforced only by the availability of arguments.
		  Instruction locality may be enhanced if, dataflow
		  programs are reordered. Enhancing the locality of
		  data references in the dataflow architecture is a
		  more challenging problem. In this paper we report
		  our approaches to the design of instruction, data
		  (operand) and I-Structure cache memories using the
		  Explicit Token Store (ETS) model of dataflow
		  systems. We will present the performance results
		  obtained using various benchmark programs."
}

@InProceedings{Kawano95,
  author = 	 "Tetsuo Kawano and Shigeru Kusakabe and Rin-ichiro
		  Taniguchi and Makoto Amamiya",
  title = 	 "Fine-grain Multi-thread Processor Architecture for
		  Massively Parallel Processing",
  pages =	 "??--??",
  booktitle =	 "First International Symposium on High Performance
		  Computer Architecture",
  year =	 "1995",
  month =	 "Jan",
  abstract =	 "Latency, caused by remote memory access and remote
		  procedure call, is one of the most serious problems
		  in massively parallel computers. In order to
		  eliminate the processors' idle time caused by these
		  latencies, processors must perform fast context
		  switching among fine-grain concurrent processes. In
		  this paper, we propose a processor architecture,
		  called Datarol-II, that promotes efficient
		  fine-grain multi-thread execution by performing fast
		  context switching among fine-grain concurrent
		  processes. In the Datarol-II processor, an implicit
		  register load/store mechanism is embedded in the
		  execution pipeline in order to reduce memory access
		  overhead caused by context switching. In order to
		  reduce local memory access latency, a two-level
		  hierarchical memory system and a load control
		  mechanism are also introduced. We describe the
		  Datarol-II processor architecture, and show its
		  evaluation results."
}

@MastersThesis{Keckler92a,
  author = 	 "Stephen W. Keckler",
  title = 	 "A Coupled Multi-ALU Processing Node for a Highly
		  Parallel Computer",
  school = 	 "Massachusetts Institute of Technology, Artificial
		  Intelligence Laboratory",
  year = 	 "1992",
  month =	 "Sep",
  note =	 "165 pages. Also AI Lab Technical Report 1355",
  url =  	 "ftp://ftp.ai.mit.edu/pub/users/skeckler/cva/smthesis.ps.Z",
  abstract = 	 "This report describes Processor Coupling, a
		  mechanism for controlling multiple ALUs on a single
		  integrated circuit to exploit both instruction-level
		  and inter-thread parallelism. A compiler statically
		  schedules individual threads to discover available
		  intra-thread instruction-level parallelism. The
		  runtime scheduling mechanism interleaves threads,
		  exploiting inter-thread parallelism to maintain high
		  ALU utilization. ALUs are assigned to threads on a
		  cycle by cycle basis, and several threads can be
		  active concurrently. Simulation results show that
		  Processor Coupling performs well both on single
		  threaded and multi-threaded applications. The
		  experiments address the effects of memory latencies,
		  function unit latencies, and communication bandwidth
		  between function units."
}

@InProceedings{Keckler92b,
  author = 	 "Stephen W. Keckler and William J. Dally",
  title = 	 "Processor coupling: integrating compile time and
		  runtime scheduling for parallelism",
  pages =	 "202--213",
  booktitle =	 isca19,
  year =	 "1992",
  address =	 "Gold Coast, Australia",
  month =	 "May",
  note =	 "Published in " # canews # "Vol.20, No.2, May
		  1992",
  url = 	 "ftp://ftp.ai.mit.edu/pub/cva/pc-isca92.ps.Z", 
  abstract =	 "The technology to implement a single-chip node
		  composed of four high-performance floating-point ALUs
		  will be available by 1995. This paper presents
		  processor coupling, a mechanism for controlling
		  multiple ALUs to exploit both instruction-level and
		  inter-thread parallelism, by using compile time and
		  runtime scheduling. The compiler statically
		  schedules individual threads to discover available
		  intra-thread instruction-level parallelism. The
		  runtime scheduling mechanism interleaves threads,
		  exploiting inter-thread parallelism to maintain high
		  ALU utilization. ALUs are assigned to threads on a
		  cycle by cycle basis, and several threads can be
		  active concurrently. The authors provide simulation
		  results demonstrating that, on four simple numerical
		  benchmarks, processor coupling achieves better
		  performance than purely statically scheduled or
		  multi-processor machine organizations. They examine
		  how performance is affected by restricted
		  communication between ALUs and by long memory
		  latencies. They also present an implementation and
		  feasibility study of a processor coupled node."
}

@InProceedings{Kepecs85,
  author = 	 "Jonathan Kepecs",
  title = 	 "Lightweight processes for {UNIX} implementation and
		  applications",
  pages =	 "299--308",
  booktitle =	 usenixs85,
  year =	 "1985",
  month =	 "Jun",
  abstract =	 ""
}

@TechReport{Keppel91,
  author =       "David Keppel",
  title =        "Register Windows and User-Space Threads on the
		  {SPARC}",
  institution =  "University of Washington",
  number =       "TR 91-08-01",
  year =         "1991",
  note2 =	 "HAR",
  url =     "ftp://ftp.cs.washington.edu/tr/1991/08/UW-CSE-91-08-01.PS.Z",
  abstract =     "Multiple lightweight processes or {\em threads} have
		  multiple stacks, and a thread context switch moves
		  execution from one stack to another. On the SPARC
		  architecture, parts of a thread's stack can be
		  cached in register windows while the thread is
		  running. The cached data must be flushed to memory
		  when the thread is suspended. Doing the flushing
		  both efficiently and correctly can be tricky. This
		  document discusses the implementation of a
		  non-preemptive user-space threads package under
		  SunOS."
}

@TechReport{Keppel93,
  author = 	 "David Keppel",
  title = 	 "Tools and Techniques for Building
		  Fast Portable Threads Package",
  institution =  "University of Washington, Department of Computer
		  Science and Engineering",
  year = 	 "1993",
  number = 	 "UW-CSE-93-05-06",
  month = 	 "May",
  note2 =	 "HAR",
  url =     "ftp://ftp.cs.washington.edu/tr/1993/05/UW-CSE-93-05-06.PS.Z",
  abstract = 	 "Threads are units of concurrent execution that can
		  be viewed as abstract data types (ADTs) with
		  operations to initialize and run them. It is common
		  to improve performance by hard-coding allocation and
		  scheduling policies, but that has led to the
		  development of many threads packages, each with
		  policies tuned for a particular set of applications.
		  Further, the machine-dependence of threads packages
		  restricts the availability of applications built on
		  top of them. This paper examines techniques for
		  building threads packages and discusses tradeoffs
		  between performance, flexibility, and portability.
		  It then introduces QuickThreads, a simple threads
		  toolkit with a portable interface. This paper shows
		  how QuickThreads can be used to implement a basic
		  uniprocessor threads package and discusses the
		  implementation of portable threads packages tuned to
		  the needs of particular applications. For example,
		  QuickThreads can be used to build barrier
		  synchronization that runs in O(lg2 processors) time
		  units instead of the traditional O(lg2 threads).
		  This paper also reports on the performance and
		  implementation of QuickThreads and describes some
		  experiences using it to reimplement and port
		  existing multiprocessor threads packages."
}

@TechReport{Khandker95,
  author = 	 "Abu Masud Khandker and Peter Honeyman and Toby
		  Teorey",
  title = 	 "Performance of DCE RPC",
  institution =  "University of Michigan Center for IT??",
  year = 	 "1995",
  number =	 "citi-tr-95-2",
  month =	 "Jan",
  url = 	 "ftp://citi.umich.edu/afs/umich.edu/group/itd/citi/public/techreports/PS.Z/citi-tr-95-2.ps.Z",
  abstract =	 "This report focuses on the performance of the Open
		  Software Foundation's Distributed Computing
		  Environment (OSF/DCE) remote procedure call (RPC).
		  We test the performance of DCE RPC with no security
		  over the connectionless datagram protocol on IBM
		  RS/6000s running AIX 3.2.4 connected with 10 Mbps
		  Ethernet, and report the round trip time and
		  throughput as measures of the overall performance of
		  DCE RPC. We also investigate the effect of using
		  application level DCE threads for improving the
		  throughput."
}

@Unpublished{Kish94,
  author = 	 "Bradley J. Kish and Bruno R. Preiss",
  title = 	 "Hobbes: A Multi-Threaded Superscalar Architecture",
  year =	 "1994",
  month =	 "Oct",
  note2 =	 "HAR",
  url = 	 "http://www.pads.uwaterloo.ca/Bruno.Preiss/papers/rejects/hobbes/paper.ps",
  abstract =	 "Several recently proposed microprocessor
		  architectures provide increased performance through
		  out-of-order andor speculative superscalar
		  execution. These architectures are complex and may
		  require changes to the instruction set.
		  Alternatively, multi-threaded architectures attempt
		  to increase pipeline utilization by concurrently
		  executing instructions from different threads. This
		  paper describes a multi-threaded superscalar
		  architecture called Hobbes. In the proposed
		  architecture, up to two instructions can be issued
		  per cycle from any of the four on-board threads.
		  Instructions are issued in-order, without branch
		  prediction. It is argued that using multi-threading
		  increases the available instruction-level
		  parallelism without needing to support complex
		  techniques such as out-of-order issue,
		  register-renaming, branch prediction, and load
		  by-passing of stores. Results from trace-driven
		  simulations of the architecture are presented. It is
		  shown that, with similar execution, cache and memory
		  resources, Hobbes can provide better performance
		  than a more complex, out-of-order, superscalar
		  processor."
}

@InProceedings{Kleiman92,
  author = 	 "Steve Kleiman and Bart Smaalders and Dan Stein and
		  Devang Shah",
  title = 	 "Writing multithreaded code in solaris",
  pages =	 "187--192",
  booktitle =	 compcon92,
  year =	 "1992",
  url = 	 "http://www.sun.com/sunsoft/Developer-products/sig/threads/papers/writing_mt_code.ps",
  abstract =	 "SunOS 5.0, the operating system component of Solaris
		  2.0 contains the kernel support for multiple threads
		  of control in a single process address space. This
		  allows a single application to efficiently overlap
		  I/O operations and to take advantage of more than
		  one processor, if available. The authors describe
		  some of the issues in using and converting libraries
		  to the multithreaded environment. In addition, they
		  give several examples of different uses of threads
		  in user applications."
}

@TechReport{Kleinoeder94,
  author = 	 "Juergen Kleinoeder and Thomas Riechmann",
  title = 	 "Hierarchical schedulers in the PM
		  System-Architecture",
  institution =  "University of Erlangen-Nuernberg, Germany, IMMD IV",
  year = 	 "1994",
  number =	 "TR-I4-94-16",
  month =	 "Jun",
  url =  	 "ftp://ftp.uni-erlangen.de/pub/papers/immd4/TR/TR-I4-94-16.ps.Z",
  abstract =	 "Today's computer and network architectures provide
		  means for parallel execution of computing intensive
		  applications. The primary operating system
		  abstraction for supporting parallelization of
		  applications is the thread. However, current
		  implementations have severe deficiencies: kernel
		  threads have too much overhead, and user-level
		  threads are not integrated well enough into the
		  operating system. Furthermore, an application may
		  gain an unfair advantage from the scheduler by
		  forking many threads. In this paper, we propose a
		  hierarchically structured scheduling system. With a
		  homogeneous integration into an open operating
		  system architecture, the disadvantages of other
		  thread concepts are avoided. In addition, a
		  distribution of computing time among all
		  applications and application subsystems is easily
		  possible, independent of the number of threads."
}

@Article{Kodama92a,
  author = 	 "Yuetsu Kodama and Shuichi Sakai and Yoshinory
		  Yamaguchi",
  title = 	 "A prototype of a highly parallel dataflow machine
		  EM-4 and its preliminary evaluation",
  journal =	 "Future Generation Computer Systems",
  year =	 "1992",
  volume =	 "7",
  number =	 "2-3",
  pages =	 "199--209",
  month =	 "Apr",
  abstract =	 "The paper discusses the design and implementation of
		  a prototype of a highly parallel dataflow machine,
		  the EM-4, mainly from the viewpoint of the system
		  architecture and maintenance. The EM-4 is a next
		  generation dataflow computer whose target structure
		  has more than a thousand PEs. A prototype with 80
		  PEs was completed in April 1990. Distinctive
		  features of the EM-4 system architecture are: (1) a
		  strongly connected arc model, (2) two simple and
		  fast synchronization mechanisms, (3) versatile
		  pipeline design, (4) a RISC based parallel
		  architecture and (5) an interconnection network with
		  extra facilities. Distinctive features of the
		  maintenance architecture are: (1) tree structured
		  parallel maintenance mechanisms dedicated to
		  testing, initializing, debugging and monitoring the
		  system, (2) maintenance operations executed
		  independently of the system operations and (3)
		  simplified and effective implementation of
		  maintenance functions. After examining these
		  features, the implementation and preliminary
		  evaluation of the EM-4 prototype are described. The
		  design features and the evaluation results show that
		  an efficient and highly testable parallel computer
		  is realized, based on the modified dataflow schemes
		  and the versatile maintenance schemes."
}

@InProceedings{Kodama92b,
  author = 	 "Yuetsu Kodama and Shuichi Sakai and Yoshinori
		  Yamaguchi",
  title = 	 "Evaluation of the EM-4 highly parallel computer
		  using a game tree searching problem",
  pages =	 "731--738",
  booktitle =	 "Proceedings of FGCS '92. Fifth Generation Computer
		  Systems",
  year =	 "1992",
  month =	 "Jun",
  abstract =	 "EM-4 is a highly parallel computer whose eventual
		  target implementation has more than 1000 processing
		  elements (PEs). The EM-4 prototype consists of 80
		  PIEs and has been fully operational at the
		  Electrotechnical Laboratory since April 1990. EM-4
		  was designed to execute in parallel not only static
		  or regular problems, but also dynamic and irregular
		  problems. The paper presents an evaluation of the
		  EM-4 prototype for dynamic and irregular problems.
		  For this evaluation, the authors chose a checkers
		  program as an example of the game tree searching
		  problem. The game tree is dynamically expanded and
		  its structure is irregular because the number and
		  the depth of subtrees of each node depend heavily
		  upon the status of the game. They examine effects of
		  the load balancing by function distribution, data
		  transfer, control of parallelism, and searching
		  algorithms on the EM-4 prototype. The results show
		  that the EM-4 is effective in dynamic load
		  balancing, fine grain packet communication and high
		  performance of instruction execution."
}

@InProceedings{Kodama95,
  author = 	 "Yuetsu Kodama and Hirohumi Sakane and Mitsuhisa Sato
		  and Hayato Yamana and Shuichi Sakai and Yoshinori
		  Yamaguchi",
  title = 	 "The EM-X Parallel Computer: Architecture and Basic
		  Performance",
  pages =	 "??--??",
  booktitle =	 isca22,
  year =	 "1995",
  abstract =	 "Latency tolerance is essential in achieving high
		  performance on parallel computers for remote
		  function calls and fine-grained remote memory
		  accesses.  EM-X supports interprocessor
		  communication on an execution pipeline with small
		  and simple packets.  It can create a packet in one
		  cycle, and receive a packet from the network in the
		  on-chip buffer without interruption.  EM-X invokes
		  threads on packet arrival, minimizing the overhead
		  of thread switching.  It can tolerate communication
		  latency by using efficient multi-threading and
		  optimizing packet flow of fine grain communication.
		  EM-X also supports the synchronization of two
		  operands, direct remote memory read/write operations
		  and flexible packet scheduling with priority. This
		  paper describes distinctive features of the EM-X
		  architecture and reports the performance of small
		  synthetic programs and larger more realistic
		  programs."
}

@InProceedings{Kono91,
  author = 	 "Shinji Kono and Masaki Yamada and Mario Tokoro",
  title = 	 "Thread Diagram",
  year = 	 "1990",
  month =	 "Feb",
  booktitle =    "WOOC 90, Japanese Workshop on Object Oriented
		  Computation",
  note =	 "Also as Tech memo SCSL-TM-90-010, Sony Computer
		  System Lab",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.csl.sony.co.jp/CSL/CSL-Papers/90/SCSL-TM-90-010.ps.Z",
  abstract =	 ""
}

@TechReport{Konsek87,
  author = 	 "Marian B. Konsek and Daniel A. Reed and Wittaya
		  Watcharawittayakul",
  title = 	 "Context switching with multiple register windows : a
		  RISC performance study",
  institution =  "University of Illinois at Urbana-Champaign, Dept. of
		  Computer Science",
  year = 	 "1987",
  number =	 "UIUCDCS-R-87-1377",
  note =	 "32 pages",
  abstract =	 ""
}

@TechReport{Kontothanassi93,
  author = 	 "Leonidas Kontothanassi",
  title = 	 "The Mercury User's Manual",
  institution =  "University of Rochester, Department of Computer Science",
  year = 	 "1993",
  number =	 "TR 465",
  month =	 "Aug",
  url =  	 "93.tr465.Mercury_users_manual.ps.Z",
  abstract = 	 "It is common for parallel applications to require a
		  large number of threads of control, often much
		  larger than the number of processors provided by the
		  underlying hardware. Using heavyweight (Unix style)
		  processes to implement those threads of control is
		  prohibitively expensive. Mercury is an environment
		  for writing object-oriented parallel programs in C++
		  that provides the user with simple primitives for
		  inexpensive thread creation and blocking and
		  spinning synchronization. If required, Mercury
		  primitives allow the user to control scheduling
		  decisions in order to achieve good locality of
		  reference in non uniform memory access (NUMA)
		  multiprocessors. This paper describes the basic
		  Mercury primitives and provides examples of their
		  use"
}

@InProceedings{Korry95,
  author = 	 "Richard Korry and Cathy McCann and Burton J. Smith",
  title = 	 "Memory management in the Tera Computer",
  pages =	 "??--??",
  booktitle =	 "SC95??",
  year =	 "1995",
  organization = "Tera",
  note2 =	 "HAR",
  url = 	 "http://www.tera.com/SC95/mem-mgmt.html",
  abstract =	 "This paper describes memory scheduling for the Tera
		  MTA (Multi Threaded Architecture) computer system.
		  The Tera MTA is intended to support a mixture of
		  large and small tasks running in parallel, and
		  ensure that they all make progress commensurate with
		  their importance. We describe the memory scheduling
		  algorithms used to schedule these tasks fairly. Some
		  of the issues encountered and solutions proposed are
		  novel, due in part to the highly multiprogrammed
		  nature of our architecture. In particular, we
		  present an algorithm for swapping a set of tasks to
		  and from memory that achieves minimal overhead,
		  largely independent of the order in which tasks are
		  swapped."
}

@InProceedings{Korty89,
  author =       "Joseph A. Korty",
  title =        "Sema: a Lint-like Tool for Analyzing Semaphore Usage
		  in a Multithreaded {UNIX} Kernel",
  booktitle =    usenixw89,
  pages =        "113--123",
  publisher =    "USENIX",
  address =      "San Diego, CA",
  year =         "1989",
  abstract =	 ""
}

@Book{Kowalik85,
  author = 	 "Janusz S. Kowalik",
  title = 	 "Parallel MIMD computation: the {HEP} supercomputer
		  and its applications",
  publisher = 	 "MIT Press",
  year = 	 "1985",
  note = 	 "ISBN : 0-262-11101-2",
  abstract =	 "Collected papers on the HEP supercomputer"
}

@inProceedings{Kubiatowics92,
  author = 	 "John Kubiatowics and David Chaiken and Anant Agarwal",
  title = 	 "Closing the window of vulnerability in multiphase
		  memory transactions",
  pages =	 "274--284",
  booktitle =	 asplos5,
  year =	 "1992",
  note2 =	 "HAR",
  url =  "ftp://cag.lcs.mit.edu/pub/papers/window-of-vulnerability.ps.Z",
  abstract =	 "Multiprocessor architects have begun to explore
		  several mechanisms such as prefetching,
		  context-switching and software-assisted dynamic
		  cache-coherence, which transform single-phase memory
		  transactions in conventional memory systems into
		  multiphase operations. Multiphase operations
		  introduce a window of vulnerability in which data
		  can be invalidated before it is used. Losing data
		  due to invalidations introduces damaging livelock
		  situations. This paper discusses the origins of the
		  window of vulnerability and proposes an
		  architectural framework that closes it. The
		  framework is implemented in Alewife, a large-scale
		  multiprocessor being built at MIT."
}

@MastersThesis{Kubiatowics93,
  author = 	 "John Kubiatowics",
  title = 	 "Closing the Window of Vulnerability in Multiphase
		  Memory Transactions: The Alewife Transaction Store",
  school = 	 " Massachusetts Institute of Technology, Department
		  of Electrical Engineering and Computer Science",
  year = 	 "1993",
  month =	 "Feb",
  note =	 "Also as MIT/LCS Technical Report 594",
  url =  	 "ftp://cag.lcs.mit.edu/pub/papers/kubi-thesis.ps.Z",
  abstract = 	 "Multiprocessor architects have begun to explore
		  several mechanisms such as prefetching,
		  context-switching and software-assisted dynamic
		  cache-coherence, which transform single-phase memory
		  transactions in conventional memory systems into
		  multiphase operations. Multiphase operations
		  introduce a window of vulnerability in which data
		  can be invalidated before it is used. Losing data
		  due to invalidations introduces damaging livelock
		  situations. This paper discusses the origins of the
		  window of vulnerability and proposes an
		  architectural framework that closes it. The
		  framework employs fully-associative
		  transaction-buffers and an algorithm called
		  thrashlock. It has been implemented as one facet of
		  the Alewife machine, a large-scale cache-coherent
		  multiprocesso"
}

@InProceedings{Kucera89,
  author =       "Julie Kucera",
  title =        "Making {\em libc}\/ Suitable for Use by Parallel
		  Programs",
  booktitle =    "Proceedings of 1st symposium on Experiences with
		  Distributed and Multiprocessor systems (SEDMS I)",
  year =         "1989",
  pages =        "145--152",
  abstract =     "Experience making libc reentrant, adding semaphores,
		  {\em etc.}, on a Convex. Some problems with
		  I/O. Added semaphores and private memory to make
		  libc calls reentrant, i.e., callable in parallel by
		  multiple threads.",
}

@InProceedings{Kuechlin90,
  author = 	 "Wolfgang Kuechlin",
  title = 	 "The S-threads environment for parallel symbolic
		  computation",
  editor =	 "R. E. Zippel",
  pages =	 "1--18",
  booktitle =	 "Computer Algebra and Parallelism. Second
		  International Workshop Proceedings",
  year =	 "1990",
  month =	 "May",
  series =	 "Lecture Notes in Computer Science 584",
  publisher =	 "Springer-Verlag",
  address =	 "Ithaca, NY, USA",
  abstract =	 "Presents a programming environment, based on threads
		  of control, that is suitable for parallel symbolic
		  computation on shared memory multiprocessors. The
		  S-threads system offers a solution to the problem of
		  whether to have heap memory shared and global, or
		  distributed, and local to threads. The memory
		  structure makes it particularly easy to reclaim,
		  without garbage collection, all intermediate list
		  memory used by an algorithm; under some additional
		  restrictions, S-threads may also perform independent
		  garbage collections. The S-threads environment is
		  being used in the construction of the PARSAC system,
		  a parallel version of the SAC-2 Computer Algebra
		  System. S-threads and PARSAC-2 are implemented on an
		  Encore Multimax, based on the C Threads environment
		  emulated by Encore Parallel Threads."
}

@TechReport{Kuechlin91a,
  author = 	 "Wolfgang W. Kuechlin",
  title = 	 "On the Multi-Threaded Computation of Integral
		  Polynomial Greatest Common Divisors",
  institution =  "Ohio State University, Department of CI",
  year = 	 "1991",
  number =	 "OSU-CISRC-2-91-TR",
  note =	 "12 pages",
  abstract =	 "We report experiences and practical results from
		  parallelizing the Brown-Collins polynomial g.c.d.
		  algorithm, starting from Collins' SAC-2
		  implementation IPGCDC.  The parallelization
		  environment is PARSAC-2, a multi-threaded version of
		  SAC-2 programmed in C with the parallelization
		  constructs of the C Threads library.  IPGCDC
		  computes the g.c.d. and its co-factors of two
		  polynomials in Z[z1, ... , zr]. and then recovering
		  the result by Chinese remaindering.  After studying
		  timings of the SAC-2 algorithm, we first parallelize
		  the Chinese remaindeer algorithm, and then we
		  parallelize the main loop of IPGCDC by executing the
		  modular g.c.d. computations concurrently. Finally,
		  we determine speed-ups and speed-up efficiencies of
		  our parallel algorithms over a wide range of
		  polynomials.  Our experiments were conducted on a 12
		  processor Encore Multimax 320 under the Mach
		  operating system."
}

@InProceedings{Kuechlin91b,
  author = 	 "Wolfgang W. Kuechlin and Nicholas J. Nevin",
  title = 	 "On multi-threaded list-processing and garbage
		  collection",
  pages =	 "894--897",
  booktitle =	 ispdp3,
  year =	 "1991",
  address =	 "Dallas, TX, USA",
  month =	 "Dec",
  note =	 "Also as Tech report OSU-CISRC-3-91-TR11, Ohio State
		  University",
  abstract =	 "The authors discuss the problem of parallel
		  list-processing and garbage collection in an
		  environment based on lightweight processes
		  (threads). Their main insight is that the threads
		  paradigm suggests a heap memory layout and garbage
		  collection technique which is quite different from
		  existing Lisp and Prolog systems. They introduce a
		  hierarchy of fork constructs and a memory structure
		  which support new garbage collection schemes which
		  are local to threads. For example, the new technique
		  of preventive garbage collection can recover all
		  intermediate list memory used by a function at the
		  small expense of copying its output parameters."
}

@InProceedings{Kuechlin92a,
  author = 	 "Wolfgang W. Kuechlin and Jeffrey A. Ward",
  title = 	 "Experiments with Virtual C Threads",
  pages =	 "50-55",
  booktitle =	 ispdp4,
  year =	 "1992",
  address =	 "Arlington, TX, USA",
  month =	 "Dec",
  abstract =	 "Virtual C Threads is a user-level threads system
		  built transparently on top of C Threads. Its goal is
		  to insulate the application from much of the
		  implementation differences and limitations in C
		  Threads systems and to make multithreading of large
		  codes practical. The number of available threads is
		  virtually unlimited, and at the same time there is a
		  significant reduction in the number of C thread
		  context switches through lazy evaluation of virtual
		  threads. They evaluate Virtual C Threads from an
		  application point of view. They measure its
		  performance on parallel symbolic algebra codes and
		  suggest an application programming style that
		  requires little tuning and thus makes multithreading
		  easy and efficient."
}

@TechReport{Kuechlin92b,
  author = 	 "Wolfgang W. Kuechlin and Jeffrey A. Ward",
  title = 	 "Experiments with Virtual Threads",
  institution =  "Ohio State University",
  year = 	 "1992",
  number =	 "OSU-CISRC-9-92-TR23",
  note =	 "21 pages",
  url = "ftp://ftp.cis.ohio-state.edu/pub/tech-report/1992/TR23.ps.gz",
  abstract =	 "The goal of a virtual threads system is to provide
		  to a user much the same convenience in dealing with
		  threads that a virtual memory system provides in
		  dealing with memory. Most importantly, the number of
		  available threads is virtually unlimited, so that
		  the parallelization of a application software can
		  follow the opportunities in the program rather than
		  the restrictions of the hardware. The convenience of
		  programming is also complemented by efficiency in
		  execution, with a significant reduction in the
		  number of kernel context switches through lazy
		  evaluation of virtual threads.  These ideas have
		  been investigated in the past by Vandevoorde and
		  Roberts for WorkCrews under Modula2+, and by Mohr,
		  Kranz, and Halstead, for futures in a parallel
		  Scheme (Multilisp). Our main contribution is the
		  adaptation of these concepts for the specific case
		  of the widely used C Threads environment, the
		  discussion of scheduling alternatives, and detailed
		  performance measurements.  Our prototype
		  implementation extends our ``S-threads'' environment
		  for parallel symbolic computation in C. While in
		  S-threads each thread fork maps to a C thread fork,
		  many virtual S-threads now execute on the same C
		  thread. S-threads supports the parallel Computer
		  Algebra system PARSAC-2. We compare the performance
		  of PARSAC-2 algorithms for list sorting and
		  polynomial g.c.d. computation on S-threads with that
		  on virtual S-threads. We exhibit a case where the
		  virtual threads concept allows us to ignore
		  grain-size issues in the code, because the
		  un-optimized code performs nearly as well on virtual
		  threads as a carefully optimized version which
		  avoids below grainsize forks in S-threads. We
		  discuss the issue of virtual threads inducing
		  deadlock in the presence of signals and waits and
		  propose various solutions to this problem. We
		  present performance measurements on a program using
		  signal and waits."
}

@InProceedings{Kuehn88,
  author = 	 "James T. Kuehn and Burton J. Smith",
  title = 	 "The Horizon supercomputing system: architecture and
		  software",
  pages =	 "28--34",
  booktitle =	 supcomp88,
  year =	 "1988",
  month =	 "Nov",
  note =	 "Also as Tech report SRC-TR-88-009, Supercomputing
		  Research Center",
  abstract =	 "An overview is given of Horizon, a shared-memory
		  multiple-instruction-stream-multiple-data-stream
		  computer architecture currently under study. Its
		  performance target is a sustained rate of 100 GFLOPS
		  (billions of floating-point operators per second),
		  using a few hundred identical scalar processors. The
		  Horizon architecture is described and its
		  performance is estimated. The processor instruction
		  set and a simple programming example are given."
}

@InProceedings{Kurihara91,
  author = 	 "Kiyoshi Kurihara and David Chaiken and Anant
		  Agarwal",
  title = 	 "Latency tolerance through multi-threading in
		  large-scale multiprocessors",
  pages =	 "91--101",
  booktitle =	 "Proceedings of the International Symposium on Shared
		  Memory Multiprocessing",
  year =	 "1991",
  publisher =	 "Inf. Process. Soc. Japan",
  address =	 "Tokyo, Japan",
  month =	 "Apr",
  note =	 "Appears in the book ``Shared Memory
		  Multiprocessing'', MIT Press Chap.14",
  note2 =	 "HAR",
  url = 	 "ftp://cag.lcs.mit.edu/pub/papers/issmm.ps.Z",
  abstract =	 "In large-scale distributed-memory multiprocessors,
		  remote memory accesses suffer significant latencies.
		  Caches help alleviate the memory latency problem by
		  maintaining local copies of frequently used data.
		  However, they cannot eliminate the latency caused by
		  first-time references and invalidations needed to
		  enforce cache coherence. Multithreaded processors
		  tolerate such latencies by rapidly switching between
		  threads when they encounter cache misses. The paper
		  evaluates the effectiveness of multithreading in
		  Alewife, a scalable multiprocessor that is being
		  developed at MIT. For the applications used in the
		  study, multithreading results in a modest 20%
		  improvement in execution time on a 64-processor
		  machine. The impact of multithreading is expected to
		  be far more significant in larger machines, when
		  remote memory latency becomes a dominant term in the
		  performance equation."
}

@InProceedings{Kuwayama94,
  author = 	 "M. Kuwayama and K. Saisho and A. Fukuda",
  title = 	 "Organization scheme of system servers in
		  microkernel-based operating systems-multi-process
		  and multi-thread methods",
  pages =	 "307--312",
  booktitle =	 "Proceedings of Eighteenth Annual International
		  Computer Software and Applications Conference
		  (COMPSAC 94)",
  year =	 "1994",
  month =	 "Nov",
  abstract =	 "We compare organization schemes of out-of-kernel
		  functions in the microkernel-based operating
		  systems, from the viewpoints of concurrent or
		  parallel executions of the functions. We describe
		  two methods; multiprocess method and multi-thread
		  one. With the multiprocess method, the functions are
		  partitioned into some modules to be allocated to
		  processes. With the multi-thread one, the functions
		  are allocated to a single process including multiple
		  threads. We implement system servers according to
		  the methods on uni-processor environment.
		  Performance evaluation shows that the multi-process
		  method gives good performance."
}

@Article{Lass89,
  author = 	 "Stanley E. Lass",
  title = 	 "Some innovations in computer architecture",
  journal =	 canews,
  year =	 "1989",
  volume =	 "17",
  number =	 "1",
  pages =	 "73--77",
  month =	 "Mar",
  abstract =	 "Reviews several innovations in computer
		  architecture. In particular the author looks at: the
		  test instruction, the pack cache, thread switching,
		  and emulation of popular architectures."
}

@InProceedings{Laudon92,
  author = 	 "James Laudon and Anoop Gupta and Mark Horowitz",
  title = 	 "Architectural and Implementation Tradeoffs in the
		  Design of Multiple-Context Processors",
  booktitle   =  isca19,
  year = 	 "1992",
  month =	 "May",
  note =	 "24 pages. Published in " # canews # " Vol.20,
		  No.2, May 92. Also Tech report CSL-TR-92-523,
		  Stanford University, Computer Systems Laboratory",
  note2 = 	 "HAR",
  url =     "ftp://www-flash.stanford.edu/pub/flash/csl-tr-92-523.ps.Z",
  abstract =	 "The authors examine two multiple-context schemes in
		  the context of scalable shared-memory
		  multiprocessors. The blocked scheme switches between
		  contexts at cache misses. The proposed interleaved
		  scheme switches between available contexts on a
		  cycle-by-cycle basis, while providing full pipeline
		  interlocks for good single-context performance. They
		  show the interleaved scheme to have a performance
		  advantage over the blocked scheme due to its ability
		  to hide pipeline dependencies and to reduce the
		  context switch cost. They also show that, while the
		  implementation of the interleaved scheme is more
		  complex, this complexity is not overwhelming."
}

@InProceedings{Laudon94,
  author = 	 "James Laudon and Anoop Gupta and Mark Horowitz",
  title = 	 "Interleaving: A Multithreading Technique targeting
		  multiprocessors and workstations",
  pages =	 "308--318",
  booktitle =	 asplos6,
  year =	 "1994",
  month =	 "Oct",
  note2 =	 "HAR",
  url = 	 "ftp://www-flash.stanford.edu/pub/flash/asplos6.ps.Z",
  abstract =	 ""
}

@PhdThesis{Laudon94a,
  author = 	 "James P. Laudon",
  title = 	 "Architectural and Implementation tradeoffs in the
		  design of multiple-context processors",
  school = 	 "Stanford University",
  year = 	 "1994",
  month =	 "May",
  note =	 "Also as tech report CSL-TR-94-634",
  note2 = 	 "HAR",
  url = 	 "http://elib.stanford.edu:80/TR/STAN:CSL-TR-94-634",
  abstract =	 "Tolerating memory latency is essential to achieving
		  high performance in scalable shared-memory
		  multiprocessors. In addition, tolerating instruction
		  (pipeline dependency) latency is essential to
		  maximize the performance of individual processors.
		  Multiple-context processors have been proposed as a
		  universal mechanism to mitigate the negative effects
		  of latency. These processors tolerate latency by
		  switchin to a concurrent thread of execution
		  whenever one of the threads blocks due to a
		  high-latency operation. Multiple context processors
		  built so far, however, either have a high
		  context-switch cost which disallows tolerance of
		  short latencies (e.g., due to pipeline
		  dependencies), or alternatively they require
		  excessive concurrency from the software. We propose
		  a multiple-context architecture that combines full
		  single-thread support with cycle-by-cycle context
		  interleaving to provide lower switch costs and the
		  ability to tolerate short latencies. We compare the
		  performance of our proposal with that of earlier
		  approaches, showing that our approach offers
		  substantially better performance for parallel
		  applications. We also explore using our approach for
		  uniprocessor workstations --- an important
		  environment for commodity microprocessors. We show
		  that our approach also offers much better
		  performance for multiprogrammed uniprocessor
		  workloads. Finally, we explore the implementation
		  issues for both our proposed and existing
		  multiple-context architectures. One of the larger
		  costs for a multiple-context processor arises in
		  providing a cache capable of handling multiple
		  outstanding requests, and we propose a lockup-free
		  cache which provides high performance at a
		  reasonable cost. We also show that amount of
		  processor state that needs to be replicated to
		  support multiple contexts is modest and the extra
		  complexity required to control the multiple contexts
		  under both our proposed and existing approaches is
		  manageable. The performance benefits and reasonable
		  implementation cost of our approach make it a
		  promising candidate for addition to future
		  microprocessors."
}

@TechReport{LeBlanc91a,
  author =       "Evangelos P. Markatos and Thomas J. LeBlanc",
  title =        "Load Balancing Vs. Locality Management in
		  Shared-Memory Multiprocessors",
  institution =  "University of Rochester, Department of Computer
		  Science",
  year =         "1991",
  month =         oct,
  number =       "TR 399",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/92.TR399.locality_vs_load_balancing.ps.Z",
  abstract =     "A parallel application executes most efficiently
		  when its workload is evenly distributed among the
		  available processors, and processes are located
		  close to their data. There is often a conflict
		  between the goals of load balancing and locality
		  management policies however, which many existing
		  systems resolve in favor of load balancing. In this
		  paper we use both experimentation and simulation to
		  investigate the relationship between load balancing
		  and locality management in shared-memory
		  multiprocessors. Our experiments with applications
		  on the BBN Butterfly multiprocessor show that
		  although both techniques can improve application
		  performance, locality management should be done
		  before load balancing in many cases. Our simulations
		  show that even in cases where there is a significant
		  variation in the completion time of processes, or
		  where the remote-to-local access time ratio is
		  small, maximizing locality first and balancing load
		  second usually results in the best performance. We
		  conclude that the scheduler should not be oblivious
		  to the location of data, as is the case with the
		  popular central work queue model; instead, the
		  location of data should be the primary factor
		  influencing the initial assignment of processes to
		  processors."
}

@Book{LeBlanc91b,
  author =       "Evangelos P. Markatos and Thomas J. LeBlanc",
  title =        "Memory-Conscious Scheduling in Shared-Memory
		  Multiprocessors", 
  year =         "1991",
  month =        dec,
  publisher =    "submitted for publication",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/Memory-Conscious_Scheduling.ps.Z",
  abstract =     "Many parallel applications are implemented using
		  lightweight thread packages. The low overhead
		  associated with user-level thread management
		  encourages programmers to use threads to exploit
		  fine-grain parallelism in an application. Although
		  the overhead of explicit thread management can be
		  very small, there is other overhead associated with
		  lightweight threads: the time required to load data
		  into the local memory (or cache) where the thread
		  executes. In many cases this overhead is a
		  significant percentage of the execution time of a
		  thread, so much so that fine-grain parallel programs
		  typically perform much worse than coarse-grain
		  implementations of the same application, even when
		  the cost of thread management is negligible. In this
		  paper we propose a technique for thread scheduling,
		  called {\em \mcs }, that reduces this overhead by
		  executing a thread close to some of the data it will
		  access. We illustrate the use of this scheduling
		  technique on two different architectures: a
		  bus-based, cache-coherent multiprocessor, and a
		  distributed-memory machine without caches. Our
		  experimental results show that this technique
		  significantly improves the performance of fine-grain
		  parallel applications on both machines. In fact,
		  fine-grain parallel applications using \mcs\ are
		  often comparable in performance to coarse-grain
		  parallel applications. Using simulation, we quantify
		  the benefits of \mcs\ under varying distributions of
		  data and load, and changes in the remote-to-local
		  memory access time ratio. We conclude that \mcs\
		  offers substantial performance benefits on current
		  multiprocessors, and has even greater potential on"
}

@InProceedings{LeSergent92,
  author = 	 "T. Le Sergent and B. Berthomieu",
  title = 	 "Incremental Multi-threaded Garbage Collection on
		  Virtually Shared Memory Architectures",
  pages =	 "179-199",
  booktitle =	 "International Workshop on Memory Management, St
		  Malo, France",
  year =	 "1992",
  organization = "Natl Centre for Sci Res, Automation and Systems Analysis",
  month =	 "Sep",
  note =         "Also as Rapport de Recherche LAAS-CNRS 92077",
  url =  	 "ftp://ftp.laas.fr/pub/lcs/reports/iwmm92.ps.Z",
  abstract = 	 "Distributed garbage collection for parallel
		  implementations of LCS or other high level parallel
		  languages"
}

@Article{Leblanc92,
  author =       "Parthas Dasgupta and Richard J. LeBlanc and Mustaque
		  Ahamad and Umakishore Ramachandran",
  title =        "The Clouds Distributed Operating System",
  journal = 	 ieeecomp,
  year = 	 "1992",
  volume = 	 "24",
  number =       "11",
  month =        "Nov",
  url =     "ftp://helios.cc.gatech.edu/pub/papers/intro.ps.Z",
  abstract =     "Clouds is a distributed operating system that runs
		  on general purpose computers connected via a
		  local-area network. The system is composed of
		  compute servers, data servers, and user
		  workstations. It implements an object-thread model
		  of computation that is based on the object-oriented
		  programming concepts. Clouds supports course-grained
		  objects that provide long term storage for
		  persistent data and associated code. Lightweight,
		  concurrent threads provide support for computational
		  activity through the code in the objects. Persistent
		  objects and threads give rise to a programming
		  environment composed of shared permanent memory,
		  dispensing with the need for facilities such as file
		  systems. Though the hardware may be distributed,
		  Clouds provides applications with a logically
		  centralized system, based on a shared
		  ``single-level'' store. The implementation of Clouds
		  separates the operating system policies from the
		  mechanisms used to implement them. The structure of
		  Clouds is a three-level hierarchy. The lowest level
		  is a minimalist kernel that provides mechanisms for
		  memory and processor control. At the next level, a
		  set of trusted system-objects provide low-level
		  operating system services. High-level services are
		  implemented at the top level in application
		  objects."
}

@Article{Lee87a,
  author = 	 "Edward A. Lee and David G. Messerschmitt",
  title = 	 "Pipelined interleaved programmable DSP's:
		  Architecture",
  journal =	 ieeeassp,
  year =	 "1987",
  volume =	 "ASSP-35",
  number =	 "9",
  pages =	 "??--??",
  month =	 "Sep",
  abstract =	 ""
}

@Article{Lee87b,
  author = 	 "Edward A. Lee and David G. Messerschmitt",
  title = 	 "Pipeline interleaved programmable DSP's: Synchronous
		  data flow programming",
  journal =	 ieeeassp,
  year =	 "1987",
  volume =	 "ASSP-35",
  number =	 "9",
  pages =	 "??--??",
  month =	 "Sep",
  abstract =	 ""
}

@TechReport{Lee92,
  author = 	 "James C. Lee and Wenwey Hseush and Scott Meyer",
  title = 	 "MeldC Runtime Kernel Implementation Guide",
  institution =  "Columbia University, Department of Computer Science",
  year = 	 "1992",
  number =	 "CUCS-045-92",
  month =	 "??",
  url = 	 "ftp://ftp.cs.columbia.edu/reports/reports-1992/cucs-045-92.ps.Z",
  abstract =	 ""
}

@Article{Lee94,
  author = 	 "Ben Lee and Ali R. Hurson",
  title = 	 "Dataflow Architectures and Multithreading",
  journal =	 ieeecomp,
  year =	 "1994",
  volume =	 "27",
  number =	 "8",
  pages =	 "27--??",
  month =	 "Aug",
  note =	 "Har",
  abstract =	 "Contrary to initial expectations, implementing
		  dataflow computers has presented a monumental
		  challenge. Now, however, multithreading offers a
		  viable alternative for building hybrid architectures
		  that exploit parallelism"
}

@InProceedings{Leeuwrik90,
  author = 	 "G. Leeuwrik and D. Richard Miller and Donna Quammen
		  and R. Senko and Daniel Tabak",
  title = 	 "Hardware design of the MULTRIS microprocessor",
  pages =	 "117--122",
  booktitle =	 euromicro89,
  year =	 "1989",
  address =	 "Cologne, West Germany",
  month =	 "Sept",
  note =	 "Published in Microprocessing {\&} Microprogramming
		  Vol.28, No.1--5, March 1990",
  note2 = 	 "HAR",
  abstract =	 "The MULTRIS (MULTitasking RISC) is a RISC-type
		  processor, developed at George Mason University and
		  is targeted for multitasking languages. It
		  implements an efficient register management policy
		  called threaded register windows. Each task running
		  on MULTRIS can directly access six windows at a
		  time. Window assignments can be varied by
		  instructions such as a procedure call, etc. There
		  are currently 64 non-overlapping windows. When a
		  task switch occurs, the new task acquires other
		  windows in the CPU, while the interrupted task keeps
		  its own windows. As long as there are free windows
		  in the CPU, no CPU-memory traffic occurs during a
		  task switch. This policy yields a very efficient
		  multitasking system, particularly useful in
		  supporting real-time applications and tasking
		  languages. This paper reports the second stage of
		  the MULTRIS development, namely its architectural
		  and hardware design. The distinguishing features of
		  this architecture include multiple register resident
		  stacks, and queues; burst mode register-memory
		  transfer; and low latency interrupt support."
}

@TechReport{Lenir92a,
  author = 	 "Phillip Lenir and Vincent Collini",
  title = 	 "A large context multithreaded architecture with
		  multiple pipelining",
  institution =  "McGill University. Department of Electrical
		  Engineering",
  year = 	 "1992",
  month =	 "April",
  abstract =	 ""
}

@InProceedings{Lenir92b,
  author = 	 "Philip Lenir and R. Govindarajan and Shashank
		  Nemawarkar",
  title = 	 "Exploiting Instruction-Level Parallelism: The
		  Multithreaded Approach",
  booktitle =	 isma25,
  address =	 "Portland, OR, USA",
  year =	 "1992",
  pages =	 "189--192",
  month =	 "Dec",
  note =	 "Published in SIGMICRO newsletter Vol.23, No.1--2,
		  Dec 1992",
  note2 = 	 "HAR",
  abstract =	 "The main challenge in the field of Very Large
		  Instruction Word (VLIW) and superscalar
		  architectures is exploiting as much
		  instruction-level parallelism as possible. In the
		  paper an execution model which uses multiple
		  instruction sequences and extracts instruction-level
		  parallelism at runtime from a set of enabled threads
		  has been presented. A new multi-ring architecture
		  has been proposed to support the execution model.
		  The multithreaded architecture features (i) large
		  resident activations to improve program and data
		  locality, (ii) a novel high-speed buffer
		  organization which ensures zero load/store stalls
		  for the local variables of an activation, and (iii)
		  a dynamic instruction scheduler that groups
		  operations from multiple threads for execution.
		  Initial performance evaluation studies predict that
		  the proposed architecture is capable of executing
		  seven concurrent operations per cycle with eight
		  execution pipes and six rings."
}

@PhdThesis{Leutenegger90a,
  author = 	 "Scott T. Leutenegger",
  title = 	 "Issues in Multiprogrammed Multiprocessor Scheduling",
  school = 	 "University of Wisconsin-Madison, Department of
		  Computer Science",
  year = 	 "1990",
  month =	 "Aug",
  note =	 "Also Tech report TR 954",
  abstract = 	 "Scheduling policies for general purpose
		  multiprogrammed multiprocessors are not well
		  understood. This thesis examines various policies to
		  determine which characteristics of a scheduling
		  policy are the most significant determinants of
		  performance. In particular we consider three
		  scheduling policy characteristics: allocation of
		  processing power among competing jobs, support for
		  inter-process synchronization, and preemption
		  frequency. We find that allocation of processing
		  power among competing jobs is at least as important
		  as the other two scheduling policy characteristics.
		  We compare a more comprehensive set of policies than
		  previous work, including four scheduling policies
		  that have not previously been examined. We also
		  compare the policies under workloads that may be
		  more realistic than previous studies have used.
		  Using these new workloads, we arrive at different
		  conclusions than reported in earlier work. In
		  particular, we find that the ``smallest number of
		  processes first'' (SNPF) scheduling discipline
		  performs poorly, even when the number of processes
		  in a job is positively correlated with the total
		  service demand of the job. We also find that
		  policies that allocate an equal fraction of the
		  processing power to each job in the system perform
		  better than practical policies that allocate
		  processing power unequally. We find that allocation
		  of processing power among competing jobs is at least
		  as important as explicit support for spin-lock and
		  barrier synchronization. (Minimizing spin-waiting is
		  achieved by coscheduling processes within a job, or
		  by using a thread management package that avoids
		  preemption of processes that hold spinlocks.) We
		  also find that allocation of processing power among
		  competing jobs is a more important characteristic of
		  a scheduling policy than preemption frequency for a
		  wide range of preemption overhead values. Our
		  studies are done by simulating abstract models of
		  the system and the workloads"
}

@TechReport{Leutenegger90b,
  author = 	 "Scott T. Leutenegger and Mary K. Vernon",
  title = 	 "The Performance of Multiprogrammed Multiprocessor
		  Scheduling Policies",
  institution =  "University of Wisconsin-Madison, Department of
		  Computer Sciences",
  year = 	 "1990",
  number =	 "TR 913",
  month =	 "Feb",
  abstract = 	 "Scheduling policies for general purpose
		  multiprogrammed multiprocessors are not well
		  understood. This paper examines various policies to
		  determine which properties of a scheduling policy
		  are the most significant determinants of
		  performance. We compare a more comprehensive set of
		  policies than previous work, including one important
		  scheduling policy that has not previously been
		  examined. We also compare the policies under
		  workloads that we feel are more realistic than
		  previous studies have used. Using these new
		  workloads, we arrive at different conclusions than
		  reported in earlier work. In particular, we find
		  that the ``smallest number of processes first'' (SNPF)
		  scheduling discipline performs poorly, even when the
		  number of processes in a job is positively
		  correlated with the total service demand of the job.
		  We also find that policies that allocate an equal
		  fraction of the processing power to each job in the
		  system perform better, on the whole, than policies
		  that allocate processing power unequally. Finally,
		  we find that for lock access synchronization,
		  dividing processing power equally among all jobs in
		  the system is a more effective property of a
		  scheduling policy than the property of minimizing
		  synchronization spin-waiting, unless demand for
		  synchronization is extremely high. (The latter
		  property is implemented by \fIcoscheduling\fR
		  processes within a job, or by using a thread
		  management package that avoids preemption of
		  processes that hold spinlocks.) Our studies are done
		  by simulating abstract models of the system and the
		  workloads"
}

@InProceedings{Li89,
  author = 	 "Kai Li and R. Schaefer",
  title = 	 "Shared virtual memory for a hypercube multiprocessor",
  volume =	 "I",
  pages =	 "371--378",
  booktitle =	 "Proceedings of the Fourth Conference on Hypercubes,
		  Concurrent Computers and Applications",
  year =	 "1989",
  publisher =	 "Golden Gate Enterprises",
  address =	 "Monterey, CA, USA",
  month =	 "Mar",
  abstract =	 "The paper describes the design and implementation of
		  Shiva, a shared virtual memory system for hypercube
		  multiprocessors. The Shiva system provides clients
		  with a large, coherent, shared virtual memory and a
		  multithread interface. Threads operating within the
		  same shared virtual memory address space can send
		  messages to each other and migrate from one node to
		  another at low cost. Thus, the system supports both
		  the shared-memory and message-passing models of
		  parallel programming, expanding the application
		  domain of hypercube multiprocessors. The prototype
		  implementation on the Intel iPSC/2 hypercube is
		  already operational. The preliminary performance
		  results indicate that shared virtual memory is a
		  beneficial strategy for implementing the next
		  generation of operating systems for hypercube
		  multiprocessors."
}

@InProceedings{Li95,
  author = 	 "Yamin Li and Wanming Chu",
  title = 	 "The Effects of STEF in Finely Parallel Multithreaded
		  Processors",
  pages =	 "318--325",
  booktitle =	 "First International Symposium on High Performance
		  Computer Architecture",
  year =	 "1995",
  month =	 "Jan",
  abstract =	 "The throughput of a multiple-pipelined processor
		  suffers due to lack of sufficient instructions to
		  make multiple pipelines busy and due to delays
		  associated with pipeline dependencies. Finely
		  Parallel Multithreaded Processor (FPMP)
		  architectures try to solve these problems by
		  dispatching multiple instructions from multiple
		  instruction threads in parallel. This paper proposes
		  an analytic model which is used to quantify the
		  advantage of FPMP architectures. The effects of four
		  important parameters in FPMP, S, T, E, and F, (STEF)
		  will be evaluated. Unlike previous analytic models
		  of multithreaded architecture, the model presented
		  here concerns the performance of multiple pipelines.
		  It deals not only with pipeline dependencies but
		  also with structure conflicts. The model accepts the
		  configuration parameters of a FPMP, the distribution
		  of instruction types, and the distribution of
		  interlock delay cycles. The model provides a quick
		  performance prediction and a quick utilization
		  prediction which are helpful in the processor
		  design."
}

@Article{Liedtke91,
  author = 	 "Jochen Liedtke and U. Bartling and U. Beyer and D.
		  Heinrichs and R. Ruland and G. Szalay",
  title = 	 "Two years of experience with a mu -kernel based OS",
  journal =	 acmosr,
  year =	 "1991",
  volume =	 "25",
  number =	 "2",
  pages =	 "57--62",
  month =	 "Apr",
  note2 = 	 "HAR",
  abstract =	 "Describes the basic components of the L3 operating
		  system and the experiences of the first two years
		  using it. The system results from scientific
		  research, but is addressed to commercial
		  applications. It is based on a small kernel handling
		  tasks, threads and dataspaces. User level device
		  drivers and file systems are described as examples
		  of flexible OS services realized outside the
		  kernel."
}

@InProceedings{Liedtke92,
  author = 	 "Jocken Liedtke",
  title = 	 "Fast thread management and communication without
		  continuations",
  pages =	 "213--221",
  booktitle =	 "Proceedings of the USENIX Workshop on Micro-Kernels
		  and Other Kernel Architectures",
  year =	 "1992",
  address =	 "Seattle, WA, USA",
  month =	 "Apr",
  note2 =	 "HAR",
  abstract =	 "Draves, Bershad, Rashid, and Dean (see proc. 13 the
		  ACM Symposium on operating systems principles, 1991)
		  described some Mach kernel optimizations based on
		  continuations resulting in e.g. 14% faster
		  communication and 85% less kernel stack space. The
		  present paper was stimulated by their work and
		  presents results in some way contradicting it. The
		  main thesis is that highly efficient thread
		  management and communication in an operating system
		  must (and can) be obtained by a thorough and
		  comprehensive design, not by one special trick. The
		  arguments come from the operating system L3. A null
		  remote procedure call in L3 executes 8.9 times
		  faster than in Mach MK40 and exception handling 11.6
		  times faster. These large factors can hardly be
		  explained by slight differences between the Mach and
		  L3 IPC semantics. A second reason for the factor
		  coming mainly from implementation and not from
		  different kernel concepts is the fact that the L3
		  kernel performance itself was significantly improved
		  by a"
}

@TechReport{Liedtke93,
  author = 	 "Jochen Liedtke",
  title = 	 "Lazy Context Switching Algorithms for Sparc-like
		  Processors", 
  institution =  "German National Research Center for Computer
		  Science {(GMD)}",
  year = 	 "1993",
  number =	 "776",
  address =	 "Sankt Augustin, Germany",
  month =	 "Sep",
  note2 =	 "HAR",
  url =     "ftp://ftp.gmd.de/GMD/os/gmd-tr-776.ps.Z ",
  abstract =	 ""
}

@Article{Liedtke94,
  author = 	 "Jochen Liedtke",
  title = 	 "A short note on implementing thread exclusiveness
		  and address space locking",
  journal =	 acmosr,
  year =	 "1994",
  volume =	 "28",
  number =	 "3",
  pages =	 "38--42",
  month =	 "Jul",
  abstract =	 "A thread can make itself exclusive by disabling
		  execution of all other threads running in the same
		  address space. This brute force synchronization
		  mechanism is useful in cases otherwise requiring
		  numerous semaphore or mutex operations or when
		  synchronization has to be added to existing
		  software. Based on the L3 experiences, the paper
		  describes how to implement exclusiveness by locking
		  the relevant region of a task's address space. This
		  optimistic method scales better than suspending and
		  later reactivating all threads explicitly."
}

@MastersThesis{Lim91,
  author = 	 "Beng-Hong Lim",
  title = 	 "Waiting Algorithms for Synchronization in
		  Large-Scale Multiprocessors ",
  school = 	 "Massachussets Institute of Technology, Laboratory of
		  Computer Science",
  year = 	 "1991",
  month =	 "Feb",
  note =	 "103 pages, Also as tech report MIT/LCS/TR-498",
  abstract = 	 "A program running on a parallel machine consists of
		  multiple asynchronous threads that have to
		  synchronize at various points to ensure program
		  correctness. At a synchronization point, a thread
		  may be forced to wait until the synchronization
		  condition is satisfied. Waiting incurs a cost. This
		  thesis investigates various types of synchronization
		  mechanisms and techniques to minimize wasted
		  processor cycles while waiting on a synchronization
		  condition. In particular, two-phase waiting
		  algorithms and multithreading are used to reduce the
		  cost of waiting. The utility of these methods is
		  analyzed with mathematical models and demonstrated
		  through simulations. Two-phase waiting algorithms
		  that rely on efficient waiting mechanisms provided
		  by multithreaded processors are shown to be very
		  robust under most operating circumstances."
}

@article{Lim93,
  author       = "Beng-Hong Lim and Anant Agarwal",
  title        = "Waiting Algorithms for Synchronization  in
		  Large-Scale Multiprocessors",
  journal      = tocs,
  pages        = "253--294",
  volume       = "11",
  number       = "3",
  month        = "August",
  year         = "1993",
  note         = "Also available as MIT VLSI Memo 91-632",
  note2 =	 "HAR",
  url =  	 "ftp://cag.lcs.mit.edu/pub/papers/waiting-algs.ps.Z",
  abstract     = "Through analysis and experiments, the authors
		  investigate two-phase waiting algorithms to minimize
		  the cost of waiting for synchronization in
		  large-scale multiprocessors. In a two-phase
		  algorithm, a thread first waits by polling a
		  synchronization variable. If the cost of polling
		  reaches a limit L/sub poll/ and further waiting is
		  necessary, the thread is blocked, incurring an
		  additional fixed cost, B. The choice of L/sub poll/
		  is a critical determinant of the performance of
		  two-phase algorithms. They focus on methods for
		  statically determining L/sub poll/ because the
		  run-time overhead of dynamically determining L/sub
		  poll/ can be comparable to the cost of blocking in
		  large-scale multiprocessor systems with lightweight
		  threads. Their experiments show that always-block
		  (L/sub poll/=O) is a good waiting algorithm with
		  performance that is usually close to the best of the
		  algorithms compared. They show that even better
		  performance can be achieved with a static choice of
		  L/sub poll/ based on knowledge of likely wait-time
		  distributions."
}

@InProceedings{Lim95,
  author = 	 "Beng-Hong Lim and Ricardo Bianchini",
  title = 	 "Limits on the Performance Benefits of
		  Multithreading",
  pages =	 "??--??",
  booktitle =	 isca22 # ". Workshop on Shared-memory
		  multiprocessing",
  year =	 "1995",
  month =	 "Jun",
  abstract =	 ""
}

@InProceedings{Lindsay93,
  author = 	 "Scott K. Lindsay and Bruno R. Preiss",
  title = 	 "On the Performance of a Multi-Threaded {RISC}
		  Architecture",
  pages =	 "369--372",
  volume =       "1",
  booktitle =	 "Proceedings of 1993 Canadian Conference on
		  Electrical and Computer Engineering",
  year =	 "1993",
  publisher =	 "Canadian Society for Electrical and Computer
		  Engineering",
  address =	 "Vancouver",
  month =	 "Sep",
  url = 	 "http://www.pads.uwaterloo.ca/Bruno.Preiss/papers/published/1993/ccece/paper.ps",
  abstract =	 "Multi-threading is a form of parallel processing in
		  which the processor contains several independent
		  contexts which share a single execution pipeline.
		  We propose a new multi-threaded architecture which
		  differs from previous architectures in that context
		  switches are performed only when the running program
		  cannot execute an instruction in the next cycle. We
		  argue that this strategy can improve pipeline
		  utilization in environments which do not have a
		  large enough number of processes to fully utilize
		  earlier multi-threaded machines."
}

@InProceedings{Liskov87,
  author =       "Barbara Liskov",
  title =        "Implementation of {Argus}",
  booktitle =    sosp11,
  pages =        "111--122",
  month =        "11",
  year =         "1987",
  note     =     "Published in " # acmosr # " Vol.21, No.5",
  abstract =     "Argus is a programming language and system developed
		  to support the construction and execution of
		  distributed programs. This paper describes the
		  implementation of Argus, with particular emphasis on
		  the way we implement atomic actions, because this is
		  where Argus differs most from other implemented
		  systems. The paper also discusses the performance of
		  Argus. The cost of actions is quite reasonable,
		  indicating that action systems like Argus are
		  practical.",
  abstract2 =     "Comment 1 by schlenk, Mon Aug 15 15:12:42 1988 The
		  Argus prototype is implemented on top of UNIX. For
		  each guardian a process is created. Each handler
		  call creates a new thread within this UNIX
		  process. Threads are implemented as a state vector
		  and may be preempted at procedure calls. A new
		  protocol similiar to UDP (called ACP) is used for
		  remote handler calls. Actions are serialized by
		  classifying them as reader or writer and strict two
		  phase locking. Transactions generate new object
		  (guardian) versions in volatile memory, which are
		  written to stable storage whenever the top level
		  transaction commits. This paper shows how systems
		  like Argus can be implemented on top of UNIX without
		  extensive modifications of the kernel. The only
		  kernel support is ACP."
}

@InProceedings{Littman91,
  author = 	 "Jay Littman",
  title = 	 "Applying Threads",
  pages =	 "209--221",
  booktitle =	 usenixw92,
  year =	 "1992",
  organization = "USENIX Assoc.",
  address =	 "San Francisco, CA, USA",
  month =	 "Jan",
  note2 =	 "HAR",
  abstract =	 "Multithreading components of a software system can
		  increase performance, but it can also increase
		  complexity. Hewlett-Packard has developed a
		  workstation based medical product, called the
		  Monitoring Full Disclosure Review Station, or
		  M1251A, that uses multithreading to achieve
		  performance requirements. The M1251A continuously
		  acquires physiological waveforms and arrhythmia
		  information for presentation to a clinician in an
		  intensive care unit. This paper describes the
		  benefits the M1251A gains from multithreading,
		  identifies the problems the development team had
		  with multithreading and explains how those problems
		  were resolved."
}

@InProceedings{Lux93,
  author = 	 "Wolfgang Lux and Hermann Hartig and Winfried Kinhauser",
  title = 	 "Migrating Multi-threaded shared objects",
  volume =	 "??",
  pages =	 "??--??",
  booktitle =	 hicss26,
  year =	 "1993",
  url = 	 "ftp://ftp.gmd.de/GMD/BirliX/Migration.ps.Z",
  abstract =	 ""
}

@TechReport{MHaines92a,
  author = 	 "Matthew Haines and A.P. Willem Bohm",
  title = 	 "Thread management in a distributed memory
		  implementation of Sisal",
  institution =  "Colorado State University, Department of Computer
		  Science",
  year = 	 "1992",
  number =	 "CS-92-122",
  url = 	 "ftp://ftp.cs.colostate.edu/pub/visa/HaBo92c.ps.Z",
  abstract =	 "Today's most powerful computers are distributed
		  memory multiprocessors. Programming these machines
		  requires both the detection of abundant parallelism
		  in the application and efficient management of the
		  parallelism. Functional languages have demonstrated
		  their ability to expose parallelism in an
		  application as well as ease the burden of parallel
		  programming. Latency tolerance and avoidance are the
		  key issues in managing this exposed parallelism for
		  efficient execution on conventional distributed
		  memory multiprocessors. We therefore consider the
		  issue of providing both latency tolerance and
		  avoidance for Sisal programs executing on
		  distributed memory multiprocessors. Sisal is a
		  functional language designed to exploit the
		  parallelism available in scientific applications,
		  and execute them in parallel on a variety of
		  multiprocessor architectures.To date, Sisal has been
		  successful in achieving these goals. Current
		  implementations of Sisal exist for sequential
		  machines and a variety of multiprocessor
		  architectures, including shared memory, vector,
		  hierarchical memory, and dataflow machines. The
		  absence of conventional distributed memory
		  multiprocessors from the list stems from the
		  compiler and runtime organization. The current
		  compiler assumes that all data structures exist in a
		  flat, shared addressing space, and the runtime
		  system is implemented using global shared data
		  structures.This paper describes the design and
		  implementation of the thread management portion of
		  the runtime system for distributed memory execution.
		  In particular, the efficiency problem of very large
		  flat loops is examined and a proposed solution is
		  presented. We give performance results and outline
		  future work."
}

@TechReport{MHaines92b,
  author = 	 "Matthew Haines and A. P. Willem Bohm",
  title = 	 "Software multithreading in a conventional
		  distributed memory multiprocessor",
  institution =  "Colorado State University,  Dept. of Computer
		  Science",
  year = 	 "1992",
  number =	 "CS-92-126",
  month =	 "Sep",
  note =         "25 pages",
  url = 	 "ftp://ftp.cs.colostate.edu/pub/visa/HaBo92e.ps.Z",
  abstract =	 "Today's most powerful computers are distributed
		  memory multiprocessors. Although they possess
		  massive amounts of available resources, it is often
		  difficult to exploit these resources efficiently.
		  Compilers that can cope with the complexities of
		  these systems are being constructed, but their scope
		  of effect is often limited due to the complexity of
		  the analysis and the lack of runtime information.
		  Novel architectures that can better tolerate
		  latencies are under construction, but their
		  effectiveness is unproven, and they do little to
		  ease the burden on current commercial
		  machines.Therefore we are designing a runtime
		  system, called VISA, that attempts to avoid and
		  tolerat latencies on conventional distributed memory
		  multiprocessors, as well as provide a single
		  addressing space to ease the burden of programming
		  or code generation. The goal of our runtimesystem is
		  to serve as a tool for studying the effects of
		  latency avoidance and latency tolerance on programs
		  running on these conventional architectures. In this
		  paper we describe the design and implementation of
		  multithreading in the VISA runtime system for the
		  purpose of latency tolerance. In particular, we
		  examine machine-independent designs for thread
		  representation, thread switching, and split-phased
		  transactions. We quantify the cost of multithreading
		  for our environment, present a test program for
		  which multithreading degrades performance, and
		  present a program for which multithreading enhances
		  performance."
}

@TechReport{MHaines92c,
  author = 	 "Matthew Haines and A. P. Willem Bohm",
  title = 	 "On the design of distributed memory Sisal",
  institution =  "Colorado State University, Dept. of Computer Science",
  year = 	 "1992",
  number =	 "92-144",
  month =	 "??",
  url = 	 "ftp://ftp.cs.colostate.edu/pub/visa/HaBo93a.ps.Z",
  abstract =	 "This paper describes the design of task management
		  and distribution, virtual shared addressing, and
		  multithreading in a distributed memory
		  implementation of the functional programming
		  language Sisal. Tasks are lightweight processes,
		  representing portions of code that can be executed
		  independently. We study flat task distribution,
		  where one master process spawns a number of slave
		  processes over the same number of processors, and
		  multi- level task distribution, where a (large)
		  number of slave processes are spawned over the
		  machine in tree fashion. We measure the trade-offs
		  for these two distribution methods. We describe a
		  flexible loop distribution scheme, allowing loop
		  distribution to be tied in with data
		  distribution.The Virtual Shared Addressing (VISA)
		  system provides a shared memory abstraction for
		  distributed memory machines, implementing remote
		  memory references by implicit message passing. VISA
		  implements a variety of data distribution functions
		  and access patterns efficiently. We investigate
		  block cyclic data structure allocation schemes for
		  one dimensional arrays, which map a certain number
		  of contiguous array elements in to each memory. We
		  present a variety of distribution functions for two
		  dimensional arrays, and study their effectiveness in
		  reducing remote references. Finally, we study the
		  effectiveness of multithreading in hiding latency.In
		  particular, we examine a machine-independent design
		  for thread representation, thread switching, and
		  split-phase transactions. Our system is currently
		  implemented on the nCUBE/2 using the Vertex 3.0
		  operating system message passing primitives, but has
		  been carefully designed to be portable to other
		  distributed memory multiprocessor architectures."
}

@InProceedings{MHaines93,
  author = 	 "Matthew Haines and A. P. Willem Bohm",
  title = 	 "Task management, virtual shared memory, and
		  multithreading in a distributed memory
		  implementation of SISAL",
  pages =	 "12--23",
  booktitle =	 parle93,
  year =	 "1993",
  address =	 "Munich, Germany",
  month =	 "Jun",
  series =	 "Lecture Notes in Computer Science ??",
  note =	 "Also as Tech report Colorado State University. Dept.
		  of Computer Science CS-92-122",
  url = 	 "ftp://ftp.cs.colostate.edu/pub/visa/HaBo93b.ps.Z",
  abstract =	 "This paper describes the design and initial
		  performance of a runtime system for implementing
		  Sisal on a distributed memory multiprocessor. The
		  runtime system provides support for task management
		  and distribution, virtual shared memory, and
		  multithreading. Tasks represent portions of code
		  that can be executed in parallel, and the authors
		  examine the performance effects of both flat and
		  multi-level task distribution strategies. They
		  introduce their virtual shared memory scheme, called
		  VISA, and investigate a block cyclic mapping
		  function using fixed and variable address
		  translation. Finally, they study the effectiveness
		  of multithreading in hiding latency for their
		  current nCUBE/2-based implementation. In particular,
		  they examine a machine-independent design for thread
		  representation, thread switching, and split-phase
		  transactions. They analyze the costs of
		  multithreading and provide initial performance results."
}

@InProceedings{MHaines94a,
  author = 	 "Matthew Haines and David Cronk and Piyush Mehrotra",
  title = 	 "On the design of chant: A Talking Threads Package",
  booktitle =    "Proceedings of Supercomputing '94",
  pages =	 "??--??",
  year = 	 "1994",
  note =	 "Also as Tech report NASA CR-194903 ICASE Report No.
		  94-25, Institute for Computer Applications in
		  Science and Engineering, NASA Langley Research",
  url = 	 "ftp://ftp.icase.edu/pub/techreports/94/94-25.ps.Z",
  abstract =	 "Lightweight threads are becoming increasingly useful in
		  supporting parallelism and asynchronous control
		  structures in applications and language
		  implementations. However, lightweight thread
		  packages traditionally support only shared memory
		  synchronization and communication primitives,
		  limiting their use in distributed memory
		  environments. We introduce the design of a runtime
		  interface, called Chant, that supports lightweight
		  threads with the capability of communication using
		  both point-to-point and remote service request
		  primitives, built from standard message passing
		  libraries. This is accomplished by extending the
		  POSIX pthreads interface with global thread
		  identifiers, global thread operations, and message
		  passing primitives. This paper introduces the Chant
		  interface and describes the runtime issues in
		  providing an efficient, portable implementation of
		  such an interface. In particular, we present
		  performance results of the initial portion of our
		  runtime system: point-to-point message passing among
		  threads. We examine the issue of thread scheduling
		  in the presence of polling for messages, and measure
		  the overhead incurred when using this interface as
		  opposed to using the underlying communication layer
		  directly. We show that our design can accommodate
		  various polling methods, depending on the level of
		  support present in the underlying thread system, and
		  imposes little overhead in point-to-point message
		  passing over the existing communication layer."
}

@TechReport{MHaines94b,
  author = 	 "Matthew Haines and Bryan Hess and Piyush Mehrotra
		  and John van Rosendale and Hans Zim",
  title = 	 "Runtime Support for Data Parallel Tasks",
  institution =  "Institute for Computer Applications in Science and
		  Engineering, NASA Langley Research Cente",
  year = 	 "1994",
  number =	 "NASA CR-194904 ICASE Report No. 94-2",
  month =	 "Apr",
  note =	 "22 pages ",
  url = 	 "ftp://ftp.icase.edu/pub/techreports/94/94-26.ps.Z",
  abstract =	 "We have recently introduced a set of Fortran
		  language extensions that allow for integrated
		  support of task and data parallelism, and provide
		  for shared data abstractions, (SDAs) as a method for
		  communication and synchronization among these tasks.
		  In this paper we discuss the design and
		  implementation issues of the runtime system
		  necessary to support these extensions, and discuss
		  the underlying requirements for such a system. To
		  test the feasibility of this approach, we implement
		  a prototype of the runtime system and use this to
		  support an abstract multidisciplinary optimization
		  (MDO) problem for aircraft design. We give initial
		  results and discuss future plans"
}

@InProceedings{Mabbs89,
  author = 	 "Stephen A. Mabbs and Kevin E. Forward",
  title = 	 "A survey of parallel processing architectures",
  pages =	 "22--28",
  booktitle =	 "Conference on Computing Systems and Information
		  Technology",
  year =	 "1989",
  publisher =	 "Instn. Eng. Australia",
  address =	 "Sydney, Australia",
  month =	 "Aug",
  abstract = 	 "Currently, there is a substantial research emphasis
		  on alternative parallel processing architectures.
		  Parallelism can be exploited in many ways within a
		  computer system. A common method is to use multiple
		  processors to allow simultaneous threads of
		  execution within a single program. Many research
		  institutes have designed and built working
		  prototypes of novel parallel architectures with the
		  hope that extensive simulation and testing of these
		  new systems will guide the development of the
		  architecture and operating systems of future
		  supercomputers. In order to assist with the
		  understanding of the bewildering number of proposals
		  for multiprocessor systems, the authors have
		  developed a classification scheme, which is used by
		  their group at the University of Melbourne to
		  categorize parallel architectures. A survey of some
		  parallel architectures is presented to illustrate
		  the classification scheme."
}

@MastersThesis{MacIntyre89,
  author = 	 "Ian D. MacIntyre",
  title = 	 "A Study of horizontal pipelining in a RISC
		  processor",
  school = 	 "University of Waterloo, Canada, Department of
		  Computer Science",
  year = 	 "1989",
  abstract =	 ""
}

@InProceedings{MacIntyre91,
  author = 	 "Ian D. MacIntyre and Bruno R. Preiss",
  title = 	 "Multi-Threaded Pipelining in a RISC Processor",
  pages =	 "7.3.1--7.3.6",
  booktitle =	 "Proceedings of 1991 Canadian Conference on
		  Electrical and Computer Engineering",
  year =	 "1991",
  address =	 "Quebec",
  month =	 "Sep",
  note2 = 	 "HAR",
  url = 	 "http://www.pads.uwaterloo.ca/Bruno.Preiss/papers/published/1991/ccece/paper.ps",
  abstract =	 "This paper examines the effects of multithreaded
		  pipelining on the CPI (cycles per instruction) of a
		  RISC processor. The desired CPI in a conventional
		  (single-threaded) RISC processor is one instruction
		  per cycle. However, the CPI is typically more than
		  one because of data hazards, control hazards, and
		  resource hazards in the pipeline. A multi-threaded
		  processor performs a context switch between every
		  instruction. Multi-threaded pipelining holds out the
		  promise of achieving a lower CPI because it can
		  eliminate data and control hazards, and mask the
		  effects of memory latency. However, multi-threaded
		  pipelining reduces cache hit ratios and requires
		  more chip area to implement. In this paper, we
		  present a model for predicting the CPI of a
		  multi-threaded pipelined processor. We also present
		  the results of trace-driven simulations of single-
		  and multi-threaded processors. These data show that,
		  for reasonable implementation technologies, and
		  taking into account the chip area penalty, a
		  multi-threaded processor can achieve a lower CPI
		  than a single-threaded processor."
}

@InProceedings{Mankovich87,
  author = 	 "Theodore E. Mankowich and Val Popescu and Herbert
		  Sullivan",
  title = 	 "CHoPP principles of operation",
  pages =	 "2--10",
  booktitle =	 "Second International conference on Supercomputing",
  year =	 "1987",
  address =	 "Santa Clara, CA",
  abstract =	 ""
}

@InProceedings{Maquelin92,
  author = 	 "Olivier C. Maquelin",
  title = 	 "Load Balancing and Resource Management in the ADAM
		  Machine ",
  pages =	 "??--??",
  booktitle =	 "Second Workshop on Dataflow Computing, Hamilton
		  Island, Australia,",
  year =	 "1992",
  month =	 "May",
  note =	 "Published in Advanced Topics in Dataflow Computing
		  and Multithreading, Lubomir Bic, Guang R. Gao,
		  Jean-Luc Gaudiot editors, IEEE Computer Society,
		  1995.",
  url = 	 "http://www-acaps.cs.mcgill.ca/~maquelin/dfws92om.ps.gz",
  abstract =	 "The dataflow architectures are well-known for their
		  ability to expose large amounts of parallelism.
		  However, when running highly parallel programs this
		  property can become a serious drawback. Due to the
		  aggressive exposure of parallelism, a large amount
		  of storage may be needed to hold intermediate
		  results of the computation. Diverse throttling
		  mechanisms have been proposed so far, both in
		  hardware and in software, but none of them seems to
		  offer a definitive solution. For such a mechanism to
		  work, it should not only be able to limit the
		  exploitation of further parallelism depending on the
		  load of the machine, but it should also be able to
		  guess with sufficient accuracy which portions of the
		  code should be executed next. This goal can be very
		  difficult to achieve, especially in conjunction with
		  a nonstrict execution model.  Load balancing
		  mechanisms were incorporated from the beginning into
		  the ADAM architecture. In contrast to most other
		  dataflow designs, this machine has a strict
		  execution model, which may restrict the exploitable
		  parallelism in some cases, but makes resource
		  management much easier. The ADAM architecture is
		  actually a von Neumann / dataflow hybrid, which uses
		  blocks of code as scheduling quanta instead of
		  individual instructions. Codeblocks behave like
		  small processes, which can be started and stopped
		  with just a few instructions. Split-phase
		  transactions are used to access remote data and
		  implicit synchronization mechanisms are used to stop
		  the execution of codeblocks trying to access results
		  which are not yet available.  This paper will first
		  describe briefly the execution model of the ADAM
		  machine and then explain the algorithms and
		  heuristics used for load management. The resources
		  needed to run several example programs exhibiting
		  widely different execution patterns will be shown,
		  based on simulation results. The total storage
		  requirements during program execution will be shown,
		  as well as other interesting quantities, like the
		  number of executable codeblocks, the number of
		  codeblock tokens, i.e. the number of codeblocks
		  which have not yet been scheduled for execution, and
		  the ratio of local calls versus external calls,
		  which shows how well locality is being exploited"
}

@PhdThesis{Maquelin94,
  author = 	 "Olivier C. Maquelin",
  title = 	 "The ADAM Architecture and its Simulation",
  school = 	 "Swiss Federal Institute of Technology, Computer
		  Engineering and Networks Lab",
  year = 	 "1994",
  month =	 "??",
  abstract =	 "This thesis describes the ADAM (Advanced DAtaflow
		  Machine) architecture and its implementation as a
		  simulator. This parallel computer architecture,
		  which was developed within the framework of the ADAM
		  project at the Swiss Federal Institute of
		  Technology, was developed with three main goals in
		  mind: It should be easily programmable, i.e. such a
		  machine should not be significantly more difficult
		  to program than a conventional sequential computer.
		  Furthermore, that architecture should be scalable,
		  allowing machine configurations with up to hundreds
		  of processors. Finally, it should be general enough
		  to allow the efficient implementation of all kinds
		  of parallel algorithms.  The ADAM architecture was
		  strongly influenced by the data-flow architectures.
		  Like these machines, it focuses on the
		  implementation of single-assignment languages
		  because of their inherent ability to express
		  parallelism. However, instead of trying to exploit
		  parallelism at the level of individual instructions,
		  it exploits parallelism at the level of whole
		  functions or loop bodies. Its machine model is based
		  on the notion of codeblocks, which behave like small
		  sequential processes. Codeblock synchronization and
		  scheduling is directly supported by the hardware,
		  reducing the duration of a context switch to a few
		  machine cycles. In addition, a dynamic load
		  balancing mechanism based on the codeblocks as units
		  of work is built into the architecture.  The main
		  memory is physically distributed, each node
		  containing part of the global memory. There is,
		  however, a global address space and accesses to
		  external data are performed transparently by the
		  hardware. In order to reduce the impact of network
		  delays, all external data accesses are implemented
		  as split-phase transactions. Starting an external
		  access and waiting for its termination are separate
		  operations. This allows external accesses to overlap
		  with program execution, and also makes it possible
		  to issue several requests before waiting for their
		  results. If this is not enough to keep the processor
		  busy, the processor tries to switch to a different
		  codeblock, taking advantage of codeblock-level
		  parallelism to hide the network latencies.  Because
		  single-assignment languages are well suited to the
		  automatic generation of parallel code, and because
		  the machine offers good run-time support for load
		  balancing, interprocessor communication and
		  synchronization, it is not necessary for the
		  programmer to deal with these issues explicitly.
		  Moreover, support for interleaved and replicated
		  data structures and the ability of the machine to
		  tolerate network latencies allow it to perform well
		  on all kinds of parallel algorithms. These features,
		  together with the fact that the ADAM architecture
		  was designed to be scalable up to hundreds of
		  processors, allow it to achieve an impressive
		  predicted performance on parallel benchmarks.  The
		  ADAM architecture is currently implemented as a
		  simulator which runs on Apple Macintosh computers
		  and takes advantage of a cluster of 32 Inmos T800
		  Transputers to speed up large simulations.
		  Simulating the architecture, instead of working from
		  the beginning on a hardware prototype, made it
		  possible to try new, unproven ideas relatively
		  quickly. The simulator is based on a detailed
		  machine model which completely describes the
		  architecture at register-transfer level. This level
		  of detail was necessary in order to guarantee that
		  no unanticipated bottlenecks would remain during
		  simulations. However, simulating large benchmarks at
		  that level necessitates an enormous amount of CPU
		  time.  The simulator was designed for interactive
		  use and takes advantage of multiple windows and
		  graphical elements to display the machine state. The
		  user interface is fully interactive, allowing a
		  ``hands on'' approach to architecture studies, and
		  even includes a source-level debugger. This good
		  high-level support, coupled with the detailed
		  machine model and the good simulation performance,
		  turns the simulator into a very efficient
		  architecture testbed."
}

@InProceedings{Maquelin95,
  author = 	 "Olivier C. Maquelin and Herbert H.J. Hum and Guang
		  R. Gao",
  title = 	 "Costs and Benefits of Multithreading with
		  Off-the-Shelf RISC Processors",
  pages =	 "??--??",
  booktitle =	 "Proceedings of EURO-PAR'95",
  year =	 "1995",
  month =	 "Aug",
  url = 	 "http://www-acaps.cs.mcgill.ca/info/EARTH/papers/europar95.ps.gz",
  note2 =	 "HAR",
  abstract =	 "Multithreaded architectures have been proposed for
		  future multiprocessor systems due to their ability
		  to cope with network latencies. Some of these
		  architectures depart significantly from current RISC
		  processor designs, while others retain most of the
		  RISC core unchanged. However, in light of the very
		  low cost and excellent performance of off-the-shelf
		  microprocessors it seems important to determine
		  whether it is possible to build efficient
		  multithreaded machines based on unmodified RISC
		  processors, or if such an approach faces inherent
		  limitations. This paper describes the costs and
		  benefits of running multithreaded programs on the
		  EARTH-MANNA system, which uses two Intel i860 XP
		  microprocessors per node. Even though this processor
		  was not designed for multithreading, benchmark
		  results show that good speedups can be achieved,
		  that it is possible to efficiently overlap
		  communication and computation, and that the costs of
		  multithreading can be minimal if parallelism is
		  exploited at a somewhat coarser level. A detailed
		  analysis of these costs shows that they could be
		  reduced significantly without having to switch to a
		  custom processor design."
}

@Article{Marinescu94,
  author = 	 "Dan C. Marinescu and John R. Rice ",
  title = 	 "On high level characterization of parallelism",
  journal =	 jpal,
  year =	 "1994",
  volume =	 "20",
  number =	 "1",
  pages =	 "107--113",
  month =	 "Jan",
  note =	 "Also as Tech report 1011, 1990, Purdue University,
		  Department of Computer Science",
  abstract =	 "Discusses issues pertinent to performance analysis
		  of massively parallel systems. A model of parallel
		  execution based on threads of control and events is
		  then introduced. The key ingredient of this model is
		  a measure of the communication complexity, which
		  gives the number of events E as a function of the
		  number of threads of control P and provides a
		  signature of a parallel computation. Various
		  consequences for speedup and load balancing are
		  presented."
}

@InProceedings{Markatos91,
  author =       "Brian D. Marsh and Michael L. Scott and Thomas J.
		  LeBlanc and Evangelos P. Markatos", 
  title =        "First-Class User-Level Threads",
  year =         "1991",
  month =        oct,
  booktitle =    sosp13,
  pages =        "110--121",
  address =      "Pacific Grove, CA",
  note =         "Published in " # acmosr # " Vol.25, No.5, 1991",
  note2 =        "HAR",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/91.SOSP.Psyche_First_Class_Threads.ps.Z ",
  abstract =     "It is often desirable, for reasons of clarity,
		  portability, and efficiency, to write parallel
		  programs in which the number of processes is
		  independent of the number of available processors.
		  Several modern operating systems support more than
		  one process in an address space, but the overhead of
		  creating and synchronizing kernel processes can be
		  high. Many runtime environments implement
		  lightweight processes (threads) in user space, but
		  this approach usually results in second-class status
		  for threads, making it difficult or impossible to
		  perform scheduling operations at appropriate times
		  (e.g. when the current thread blocks in the kernel).
		  In addition, a lack of common assumptions may also
		  make it difficult for parallel programs or library
		  routines that use dissimilar thread packages to
		  communicate with each other, or to synchronize
		  access to shared data. We describe a set of kernel
		  mechanisms and conventions designed to accord .i
		  {"}first-class status{"} to user-level threads,
		  allowing them to be used in any reasonable way that
		  traditional kernel-provided processes can be used,
		  while leaving the details of their implementation to
		  user-level code. The key features of our approach
		  are (1) shared memory for asynchronous communication
		  between the kernel and the user, (2) software
		  interrupts for events that might require action on
		  the part of a user-level scheduler, and (3) a
		  scheduler interface convention that facilitates
		  interactions in user space between dissimilar kinds
		  of threads. We have incorporated these mechanisms in
		  the Psyche parallel operating system, and have used
		  them to implement several different kinds of
		  user-level threads. We argue for our approach in
		  terms of both flexibility and performance."
}

@TechReport{Markatos93a,
  author = 	 "Evangelos P. Markatos and Thomas J. LeBlanc",
  title = 	 "Locality-Based Scheduling in Shared-Memory Multiprocessors",
  institution =  "Inst for ICS-FORTH, Heraklio, Crete, Greec",
  year = 	 "1993",
  number = 	 "94",
  month = 	 "Aug",
  note =	 "in Current and Future Trends in Parallel and
		  Distributed Computing, Albert Zomaya (Ed.), World
		  Scientific Publishing",
  note2 = 	 "HAR",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/93.TR94.Locality_Based_Scheduling.ps.Z",
  abstract = 	 "The last decade has produced enormous improvements
		  in microprocessor performance without a
		  corresponding improvement in memory or
		  interconnection network performance. As a result,
		  the relative cost of communication in shared-memory
		  multiprocessors has increased dramatically. Although
		  many applications could ignore the cost of
		  communication and still achieve good performance on
		  the previous generations of shared-memory machines,
		  good performance on modern machines requires that
		  communication be reduced or eliminated. One way to
		  reduce the need for communication is to use
		  scheduling polices that exploit knowledge of the
		  location of data when assigning processes to
		  processors, improving locality of reference by
		  co-locating a process with the data it will require.
		  This chapter presents an overview of the tradeoffs
		  to be made in process scheduling, and evaluates
		  locality-based scheduling techniques at the level of
		  the operating system kernel,  thread package, and
		  parallelizing compiler"
}

@PhDthesis{Markatos93b,
  author =       "Evangelos P. Markatos",
  title =        "Scheduling for Locality in Shared-Memory
		  Multiprocessors",
  school =       "University of Rochester, Department of Computer Science",
  month =        may,
  year =         "1993",
  note =         "110 pages. Also as Tech report 457",
  note2 = 	 "HAR",
  url =     "ftp://ftp.cs.rochester.edu/pub/papers/systems/93.tr457.scheduling_for_locality_in_shared_memory_multiprocessors.ps.Z",
  abstract =     "The last decade has produced enormous improvements
		  in processor speed without a corresponding
		  improvement in bus or interconnection network
		  speeds. As a result, the relative costs of
		  communication and computation in shared-memory
		  multiprocessors have changed dramatically. An
		  important consequence of this trend is that many
		  parallel applications, whose performance depends on
		  a delicate balance between the cost of communication
		  and computation, do not execute efficiently on
		  today's shared-memory multiprocessors. In this
		  dissertation we quantify the effect of this trend in
		  multiprocessor architecture on parallel program
		  performance, explain the implications of this trend
		  on popular parallel programming models, and propose
		  system software to efficiently map parallel programs
		  and programming models to modern shared-memory
		  multiprocessors. Our experiments with application
		  programs on bus-based, cache-coherent machines like
		  the Sequent Symmetry, and large-scale
		  distributed-memory machines like the BBN Butterfly,
		  confirm that applications scale much better on
		  previous-generation machines than on current
		  machines due to the rising cost of
		  communication. Our experiments also suggest that
		  shared-memory programming models, which can be
		  implemented efficiently on the machines of
		  yesterday, do not readily port to state-of-the-art
		  machines. As a solution, we propose new
		  decomposition and scheduling algorithms that
		  significantly reduce communication overhead. Our
		  scheduling algorithms, which apply equally well to
		  run-time libraries and parallelizing compilers,
		  attempt to co-locate processes and data, assigning
		  processes to processors based on the location of the
		  date they will access. Our experiments over a wide
		  variety of shared-memory multiprocessors demonstrate
		  that the performance benefits of these
		  scheduling-for-locality algorithms are significant,
		  improving performance by up to 60\% for some
		  applications on modern machines. We conclude that
		  communication overhead need not dominate performance
		  on present or future multiprocessors, given an
		  appropriate programming model, multiprogramming
		  scheduling policy, and user-level decomposition and
		  scheduling algorithms."
}

@Article{Marrin94,
  author = 	 "Ken Marrin",
  title = 	 "Multithreading support grows among realtime
		  operating systems",
  journal =	 "Computer design",
  year =	 "1994",
  volume =	 "32",
  number =	 "3",
  pages =	 "77--??",
  month =	 "Mar",
  abstract =	 "From embedded kernels to desktop operating systems
		  like Windows NT, the drive to enhance realtime
		  capabilities and multithreaded support is gathering
		  momentum"
}

@TechReport{Mascarenhas95,
  author = 	 "Edward Mascarenhas",
  title = 	 "Ariadne: Architecture of a Portable Threads system
		  supporting Mobile Processes",
  institution =  "Purdue University, Department of Computer Sciences",
  year = 	 "1995",
  number =	 "CSD-TR 95-017",
  note2 =	 "HAR",
  abstract =	 "This paper provides details of the architecture of
		  the Ariadne portable threads system. It describes
		  the features of thread migration and the use of
		  threads in distributed environments like PVM and on
		  shared memory multiprocessors."
}

@InProceedings{Massalin89,
  author = 	 "Henry Massalin and Calton Pu",
  title = 	 "Threads and input/output in the Synthesis kernel",
  booktitle =	 sosp12,
  year =	 "1989",
  pages =	 "191--201",
  note =	 "Published in " # acmosr # "Vol.23 No.5, 1989",
  url =  	 "TBD",
  abstract =	 "The Synthesis operating system kernel combines
		  several techniques to provide high performance,
		  including kernel code synthesis, fine-grain
		  scheduling, and optimistic synchronization. Kernel
		  code synthesis reduces the execution path for
		  frequently used kernel calls. Optimistic
		  synchronization increases concurrency within the
		  kernel. Their combination results in significant
		  performance improvement over traditional operating
		  system implementations. Using hardware and software
		  emulating a SUN 3/160 running SUNOS, Synthesis
		  achieves several times to several doze times speedup
		  for UNIX kernel calls and context switch times of 21
		  microseconds or faster."
}

@TechReport{Massalin91,
  author =       "Henry Massalin and Calton Pu",
  title =        "A Lock-Free Multiprocessor {OS} Kernel",
  institution =  "Columbia University",
  number =       "CUCS-005-91",
  year =         "1991",
  note2 =	 "HAR",
  url =     "ftp://ftp.cs.columbia.edu/reports/reports-1991/cucs-005-91.ps.Z",
  abstract =     "Typical shared-memory multiprocessor OS kernels
		  use interlocking implemented as spin-locks or
		  waiting semaphores. We have implemented that many
		  useful abstractions cannot reasonably be implemented in
		  their full generality. For example, the natural
		  numbers are a useful abstraction, but they can't all
		  be represented in a finite-state machine. When a
		  program specifies an abstract operation that exceeds
		  a particular implementations' capacity, some
		  exceptional action must be taken, even if only to
		  print {"}computation aborted due to arithmetic
		  overflow.{"}When a single language is used to code
		  systems with many layers of abstraction, it is
		  attractive to provide a control structure for
		  signalling and handling these circumstances. Hence
		  Modula-2+'s exception facility. Apparently the
		  Modula-2 abstraction called {"}process{"} fails to
		  hide the nature of its first implementation by a
		  multiprocessor. Therefore, Modula-2+ introduces a
		  more suitable abstraction for a thread of control
		  (called {"}Thread{"}). Compile-time checking in
		  Modula-2 guarantees the absence of a class of errors
		  but gives no warning of the error of deallocating
		  storage while references to it remain. Modula-2+
		  guarantees the absence of the dangerous errors by a
		  combination of run-time and compile-time checking: A
		  garbage collector provides automatic deallocation at
		  run-time, and checking at compile-time enforces a
		  discipline on the use of references that guarantees
		  the validity of the storage system's invariants. The
		  authors assume that the reader is already familiar
		  with Modula-2. Most of the paper is a general
		  overview, but it drops to the detail necessary for a
		  reference manual in the appendixes, which describe
		  the changes made to the syntax, the type-checking
		  rules, and formatting conventions. Greg Nelson"
}

@Unpublished{Mathiske95,
  author = 	 "B. Mathiske and F. Matthes and J. W. Schmidt",
  title = 	 "On Migrating Threads",
  note = 	 "Accepted for publications",
  year =	 "1995",
  month =	 "Jun",
  url =    "http://idom-www.informatik.uni-hamburg.de/Paper/1995/MMS95a/",
  abstract =     "Based on the notion of persistent threads in Tycoon,
		  we investigate thread migration as a programming
		  construct for building activity-oriented distributed
		  applications. We first show how astraight-forward
		  extension of a higher-order persistent language can
		  be used to define activities that span multiple
		  (semi-) autonomous nodes in heterogeneous networks.
		  In particular, we discuss the intricate binding
		  issues that arise in systems where threads are
		  first-class language citizens that may access local
		  and remote, mobile and immobile resources. \\ We
		  also describe how our system model can be understood
		  as a promising generalization of the more static
		  architecture of first-order and higher-order
		  distributed object systems. Finally, we give some
		  insight into the implementation of persistent and
		  migrating threads and we explain how to represent
		  bindings to ubiquitous resources present at each
		  node visited by a migrating thread on the network to
		  avoid excessive communication or storage costs."
}

@Article{McCann93,
  author = 	 "Cathy McCann and Raj Vaswani and John Zahorjan",
  title = 	 "A dynamic processor allocation policy for
		  multiprogrammed shared-memory multiprocessors",
  journal =	 tocs,
  year =	 "1993",
  volume =	 "11",
  number =	 "2",
  pages =	 "146--178",
  month =	 "May",
  note =	 "Also as tech report 90-03-02, University of
		  Washington, Dept. of Computer Science and
		  Engineering, 1990",
  abstract =	 "The authors propose and evaluate empirically the
		  performance of a dynamic processor-scheduling policy
		  for multiprogrammed shared-memory multiprocessors.
		  The policy is dynamic in that it reallocates
		  processors from one parallel job to another based on
		  the currently realized parallelism of those jobs.
		  The policy is suitable for implementation in
		  production systems in that: it interacts well with
		  very efficient user-level thread packages, leaving
		  to them many low-level thread operations that do not
		  require kernel intervention; it deals with thread
		  blocking due to user I/O and page faults it ensures
		  fairness in delivering resources to jobs; its
		  performance, measured in terms of average job
		  response time, is superior to that of previously
		  proposed schedules; it provides good performance to
		  very short, sequential (e.g., interactive) requests."
}

@Article{McCrackin91,
  author = 	 "Daniel C. McCrackin",
  title = 	 "Eliminating interlocks in deeply pipelined
		  processors by delayed enforcing multistreaming",
  journal =	 ieeetc,
  year =	 "1991",
  volume =	 "40",
  number =	 "10",
  pages =	 "1125--1132",
  month =	 "oct",
  note2 =	 "HAR",
  abstract =	 "The delay enforced multistreaming (DEMUS) processor
		  architecture provides a simple, inexpensive way of
		  achieving high hardware utilization in deeply
		  pipelined processors. Multiple streams share the
		  pipeline in an interleaved fashion. Both the data
		  dependency problem and the jump problem are
		  prevented by enforcing enough interdispatch delay on
		  each individual stream to prevent successive
		  instructions from interfering with each others
		  execution. The structure and opeartion of a small
		  DEMUS processor are presented and several stream
		  dispatching algorithms are compared by means of a
		  simple simulation"
}

@InProceedings{McCrackin93,
  author = 	 "Daniel C. McCrackin",
  title = 	 "The synergistic effect of thread scheduling and
		  caching in mulithreaded computers",
  pages =	 "157--164",
  booktitle =	 compcon93,
  year =	 "1993",
  month =	 "Feb",
  note2 =	 "HAR",
  abstract = 	 "This paper investigates combining two techniques -
		  thread scheduling and context switching on cache
		  misses - in a multithreaded computer. By means of a
		  simple simulation model for an 8-stage pipeline it
		  is demonstrated that these two techniques act
		  togehter to allow higher overall processor
		  performance with fewer running threads than the
		  number of pipeline stages. This two of the problems
		  with multithreaded machines - a large number of
		  required threads and ppor cache performance are
		  reduced"
}

@Article{McCrackin94a,
  author = 	 "Daniel C. McCrackin and Sumathi Srinivasan",
  title = 	 "Trace Driven Pipeline and Cache Simulation of
		  Multithreaded Computers",
  journal =	 "Simulation",
  year =	 "1994",
  volume =	 "63",
  number =	 "2",
  pages =	 "75--82",
  month =	 "Aug",
  abstract =	 " A technique for applying trace-driven simulation to
		  cached multithreaded machines with dynamic thread
		  scheduling is presented. A small amount of
		  constraint information is added to each trace
		  record, permitting the correct order of execution in
		  the cache miss, in which the behaviour of the cache
		  effects the instruction dispatch order, can be
		  modelled with this technique. The design of a
		  multithreaded pipeline and cache trace-driven
		  simulation system is described. This system allows
		  thread-scheduled multithreaded processors, which are
		  not simulatable by conventional trace-driven
		  techniques, to be effectively and accurately
		  simulated. Sample simulation results illustrate the
		  flexibility of this simulation technique."
}

@Article{McCrackin94b,
  author = 	 "Daniel C. McCrackin",
  title = 	 "Using opcode information to control thread
		  scheduling in pipelined multithreaded processors",
  journal =	 "Canadian Journal of Electrical and Computer
		  Engineering",
  year =	 "1994",
  volume =	 "19",
  number =	 "3",
  pages =	 "139--141",
  month =	 "Jul",
  abstract =	 "This paper examines the performance of two delay
		  enforced multistream (DEMUS) thread scheduling
		  mechanisms for pipelined multithreaded processors.
		  The fixed-delay (FD) mechanism, which is equivalent
		  to the pessimistic thread scheduling technique of
		  classical cyclic pipeline computers, is simple to
		  implement, but requires as many running threads as
		  pipeline stages for complete pipeline utilization.
		  It is demonstrated by means of simulation that even
		  the simple technique of scheduling based on opcode,
		  as in the modified fixed-delay (MFD) mechanism, is
		  sufficient to greatly improve the performance
		  characteristics of a pipelined multithreaded
		  machine."
}

@Article{McCrackin95,
  author = 	 "Daniel C. McCrackin",
  title = 	 "Practical Delay Enforced Multistream (DEMUS) Control
		  of Deeply Pipelined Processors",
  journal =	 ieeetc,
  year =	 "1995",
  volume =	 "44",
  number =	 "3",
  pages =	 "??--??",
  month =	 "Mar",
  abstract =	 "The simulated performance of a practical
		  multithreaded mechanism  for achieving high
		  utilization of deeply pipelined (> 5 stage)
		  processors is presented. Threads are dynamically
		  interleaved in  one pipeline. After each instruction
		  is dispatched, enough delay is introduced so that
		  successive instructions cannot interfere. Four
		  scheduling algorithms, three of which are
		  realizable, are tested on a simple simulated
		  processor. Good pipeline utilization can be achieved
		  even when the number of running threads is less than
		  the number of pipeline stages"
}

@InProceedings{McJones87,
  author =       "Paul R. McJones and Garret F. Swart",
  title =        "Evolving the {UNIX} System Interface to Support
		  Multithreaded Programs",
  booktitle =    usenixw89,
  pages =        "393--404",
  publisher =    "USENIX",
  address =      "San Diego, CA",
  month =        dec,
  year =         "1989",
  note =         "Also as Tech report 21, Digital Equipment
		  Corp. (DEC), Systems Research Center, 1987",
  abstract =     "Multiple threads (program counters executing in the
		  same address space) make it easier to write programs
		  that deal with related asynchronous activities and
		  that execute faster on shared-memory
		  multiprocessors. Supporting multiple threads places
		  new constraints on the design of operating system
		  interfaces. Part I of this report presents
		  guidelines for designing (or redesigning) interfaces
		  for multithreaded clients. We show how these
		  guidelines were used to design an interface to
		  UNIX-compatible file and process management
		  facilities in the Topaz operating system. Two
		  implementations of this interface are in everyday
		  use: a native one for the Firefly multiprocessor,
		  and a layered one running within a UNIX process.
		  Part II is the actual programmer's manual for the
		  interface discussed in Part I" 
}

@InProceedings{Mehrotra89,
  author =       "Piyush Mehrotra and John van Rosendale",
  title =        "Concurrent Object Access in Blaze 2",
  booktitle =    "Proc. ACM SIGPLAN Workshop on Object-Based
		  Concurrent Programming",
  pages =        "40",
  month =        apr,
  year =         "1989",
  note =         "Published in ACM SIGPLAN Notices, volume 24, number
		  4, pp.40--42",
  abstract =     "Presents a concurrent object-oriented language
		  (Blaze 2) in which objects are internally concurrent
		  (have multiple threads)."

}

@TechReport{Mehrotra94,
  author = 	 "Piyush Mehrotra and Matthew Haines",
  title = 	 "An Overview of The Opus Language and Runtime System",
  institution =  "Institute for Computer Applications in Science and
		  Engineering, NASA Langley Research Center",
  year = 	 "1994",
  number =	 "NASA CR-194921 ICASE Report No. 94-39",
  month =	 "May",
  note =	 "16 pages",
  available  =   "ftp://ftp.icase.edu/pub/techreports/94/94-39.ps.Z",
  abstract = 	 "We have recently introduced a new language, called
		  {\it Opus}, which provides a set of Fortran language
		  extensions that allow for integrated support of task
		  and data parallelism. It also provides shared data
		  abstractions (SDAs) as a method for communication
		  and synchronization among these tasks. In this
		  paper, we first provide a brief description of the
		  language features and then focus on both the
		  language-dependent and language-independent parts of
		  the runtime system that support the language. The
		  language-independent portion of the runtime system
		  supports lightweight threads across multiple address
		  spaces, and is built upon existing lightweight
		  thread and communication systems. The
		  language-dependent portion of the runtime system
		  supports conditional invocation of SDA methods and
		  distributed SDA argument handling"
}

@InProceedings{Mellor-Crummey91,
  author = 	 "John M. Mellor-Crummey",
  title = 	 "On-the-fly detection of data races for programs with
		  nested fork-join parallelism",
  pages =	 "24--33",
  booktitle =	 "Supercomputer Debugging Workshop '91. Proceedings",
  year =	 "1991",
  publisher =	 "Los Alamos Nat. Lab, USA",
  address =	 "Albuquerque, NM, USA",
  month =	 "Nov",
  url = 	 "TBD",
  abstract =	 "Detecting data races in shared-memory parallel
		  programs is an important debugging problem. This
		  paper presents a new protocol for run-time detection
		  of data races in executions of shared-memory
		  programs with nested fork-join parallelism and no
		  other inter-thread synchronization. This protocol
		  has significantly smaller worst-case run-time
		  overhead than previous techniques. The worst-case
		  space required by our protocol when monitoring an
		  execution of a program P is O(VN), where V is the
		  number of shared variables in P, and N is the
		  maximum dynamic nesting of parallel constructs in
		  P's execution. The worst-case time required to
		  perform any monitoring operation is O(N). We
		  formally prove that our new protocol always reports
		  a non-empty subset of the data races in a monitored
		  program execution and describe how this property
		  leads to an effective debugging strategy. "
}

@InProceedings{Mendelson,
  author = 	 "Abram Mendelson and Dhiraj K. Pradhan and A. D.
		  Singh",
  title = 	 "A single cached copy data coherence scheme for
		  multiprocessor systems",
  pages =	 "36--49",
  booktitle =	 isca16,
  year =	 "1989",
  month =	 "May",
  note =	 "Published in " # canews # " Vol.17, No.6, Dec 89",
  abstract =	 "The authors present and evaluate a snoopy cache
		  memory protocol, the single cache copy data
		  coherence (SCCDC), for multiprocessors that allows
		  only a single cache to hold a given shared data at
		  any time. The simulations presented indicate that
		  despite its simplicity, the scheme has the potential
		  for good performance comparable with more complex
		  snoopy cache schemes. With low implementation cost,
		  and the efficient support for important operating
		  system functions, the SCCDC scheme appears
		  attractive for multi-user, multi-thread environments
		  where the actual shared rate is modest, and mostly
		  caused by synchronization mechanisms."
}

@TechReport{Michiels91,
  author = 	 "R. Michiels",
  title = 	 "Porting Amoeba Thread Management and Communications
		  to CSTools",
  institution =  "Edinburgh Parallel Computing Centre",
  year = 	 "1991",
  number =	 "tn9118",
  url = 	 "file://ftp.epcc.ed.ac.uk/pub/tn/91/tn9118.ps.Z",
  abstract =	 ""
}

@Article{Miller74,
  author = 	 "Edward F. Miller Jr",
  title = 	 "A multiple stream registerless shared-resource
		  processor",
  journal =	 ieeetc,
  year =	 "1974",
  volume =	 "C-23",
  number =	 "3",
  pages =	 "277--285",
  month =	 "Mar",
  note2 =	 "HAR",
  abstract =	 "A novel high-performance processor architecture for
		  processing a large number of independent instruction
		  streams is proposed and its operating behaviour
		  studied. The proposed processor operates on
		  instruction words in a two-address format (thereby
		  eliminating the ``operating registers''), and is
		  organized in a fashion which permits as high degree
		  of internal buffering and pipelining. The processor
		  has the following properties: 1) The hardware cost
		  grows only slightly more than linearly with the
		  overall implementation cost; 2) The overall
		  performance is primarily dependent on the processor
		  wordtime and is only secondarily dependent on the
		  supporting memory cycle time; 3) All instruction
		  stream interfaces with memory occur at special
		  queuing units which are used to unscramble the
		  instruction streams and continually provide work for
		  subsequent processing elements"
}

@Article{Miller90,
  author = 	 "D. Richard Miller and Donna J. Quammen",
  title = 	 "Exploiting Large Register Sets",
  journal =	 "Microprocessors and Microsystems",
  year =	 "1990",
  volume =	 "14",
  number =	 "6",
  pages =	 "333--340",
  month =	 "Jul",
  abstract =	 "The paper examines problems associated with the
		  application of load-store RISC architectures with
		  large register sets or compiler-driven register
		  assignment to realtime system design methodologies
		  involving many tasks and frequent context switches.
		  Approaches to on-chip storage are reviewed and the
		  threaded windows concept introduced as an efficient
		  mechanism for managing register resources. Under
		  this system the structure and use of registers may
		  be dictated by the programmer, compiler and
		  operating system according to the demands of
		  procedure activation records, hardware-supported
		  stacks, hardware supported queues, and concurrent
		  task contexts, as well as task and system global
		  storage. The use and benefits of the system with
		  sequential programs and in concurrent tasking
		  environments are explored. The abilities and
		  attributes of similar architectures are evaluated
		  and planned developments of the threaded windows
		  system are outlined."
}

@InProceedings{Miller93,
  author = 	 "William M. Miller and Walid A. Najjar and A. P.
		  Willem Bohm",
  title = 	 "A quantitative analysis of locality in dataflow
		  programs",
  pages =	 "12--18",
  booktitle =	 isma24,
  year =	 "1991",
  address =	 "Albuquerque, NM, USA",
  month =	 "Nov.",
  note =	 "Also as tech report 91-122, Colorado State
		  University, Dept. of Computer Science",
  abstract =	 "Substantial evidence suggests that exploiting some
		  forms of locality within dataflow programs can
		  impact performance dramatically. This is the basic
		  premise of several hybrid von Neumann-dataflow or
		  multithreaded architectures. Identifying and
		  exploiting locality, however, in a fine-grained
		  asynchronous execution model is not trivial. In the
		  paper, fine grained intra-thread locality is
		  defined, quantified and evaluated. These
		  experimental measurements are based on the
		  evaluation of a set of numeric and non-numeric
		  benchmarks. The results point to a very large degree
		  of thread locality: for example, over 70% of the
		  instructions have to wait less than 5 instruction
		  execution steps for their input data. Furthermore,
		  the remarkable uniformity and consistency of the
		  distribution of thread locality across a wide
		  variety of benchmarks suggests that thread locality
		  is highly dependent or the instruction set."
}

@InProceedings{Miyazaki94,
  author = 	 "T. Miyazaki and C. Sakamoto and M. Kuwayama and L.
		  Saisho and A. Fukuda",
  title = 	 "Parallel Pthread library (PPL): user-level thread
		  library with parallelism and portability",
  pages =	 "301--306",
  booktitle =	 "Proceedings of Eighteenth Annual International
		  Computer Software and Applications Conference
		  (COMPSAC 94)",
  year =	 "1994",
  month =	 "Nov",
  abstract =	 "Light-weight processes, threads, are fast vehicles
		  for concurrent/parallel execution in a single
		  program. There are two thread models: kernel-level
		  thread model; and user-level one. Although the
		  kernel-level threads are more light-weight than UNIX
		  processes; it have observed that they are less
		  light-weight than we expected. Therefore, the
		  user-level thread model has attracted attention of
		  researchers. There have been many user-level thread
		  libraries. However, theses do not seem to support
		  both of portability and parallelism, which are
		  important aspects of software engineering and
		  parallel processing. Parallel Pthread Library (PPL),
		  we are developing, aims at supporting the both. In
		  this paper, we describe PPL. Furthermore, through
		  implementing the first version of PPL on two
		  operating systems and hardware architectures, we
		  compare the basic performance of it with that of
		  other existing user-level thread libraries."
}

@InProceedings{Mogul91,
  author = 	 "Jeffrey C. Mogul and Anita Borg",
  title = 	 "The effect of context switches on cache
		  performance",
  pages =	 "75--84",
  booktitle =	 asplos4,
  year =	 "1991",
  address =	 "Santa Clara, CA, USA",
  month =	 "Apr",
  note2 =	 "HAR",
  abstract =	 "The sustained performance of fast processors is
		  critically dependent on cache performance. Cache
		  performance in turn depends on locality of
		  reference. When an operating system switches
		  contexts, the assumption of locality may be violated
		  because the instructions and data of the
		  newly-scheduled process may no longer be in the
		  cache(s). Context-switching thus has a cost above
		  that associated with that of the operations
		  performed by the kernel. The authors fed address
		  traces of the processes running on a multi-tasking
		  operating system through a cache simulator, to
		  compute accurate cache-hit rates over short
		  intervals. By marking the output of such a
		  simulation whenever a context switch occurs, and
		  then aggregating the post-context-switch results of
		  a large number of context switches, it is possible
		  to estimate the cache performance reduction caused
		  by a switch. Depending on cache parameters, the net
		  cost of a context switch appears to be in the
		  thousands of cycles, or tens to hundreds of
		  microseconds."
}

@PhdThesis{Moore94,
  author = 	 "Simon William Moore",
  title = 	 "Multithreaded processor design",
  school = 	 "University of Cambridge, United Kingdom",
  year = 	 "1994",
  month =	 "Oct",
  note2 = 	 "HAR",
  url = 	 "http://www.cl.cam.ac.uk:80/ftp/papers/reports/TR358-swm11-multithreaded-processors.ps.gz",
  abstract =	 "Multithreaded processors aim to improve upon both
		  control-flow and data-flow processor models by
		  forming some amalgam of the two. They combine
		  sequential behaviour from the control-flow model
		  with concurrent aspects from data-flow design.  Some
		  multithreaded processor designs have added just a
		  little concurrency to control-flow or limited
		  sequential execution to data-flow. This thesis
		  demonstrates that more significant benefits may be
		  obtained by a more radical amalgamation of the two
		  models. A data-driven microthread model is proposed,
		  where a microthread is a short control-flow code
		  sequence. To demonstrate the efficiency of this
		  model, a suitable multithreaded processor, called
		  Anaconda, is designed and evaluated.  Anaconda
		  incorporates a scalable temporally predictable
		  memory tree structure with distributed virtual
		  address translation and memory protection. A
		  temporally predictable cached direct-mapped matching
		  store is provided to synchronise data to
		  microthreads. Code is prefetched into an instruction
		  cache before execution commences.
		  Earliest-deadline-first or fixed-priority scheduling
		  is supported via a novel hardware priority queue.
		  Control-flow execution is performed by a modified
		  Alpha 21064 styled pipeline which assists comparison
		  with commercial processors."
}

@InProceedings{Morrisett91,
  author =       "J. Gregory Morrisett",
  title =        "Running your continuation threads in parallel",
  address =      "Pittsburgh, PA",
  booktitle =    "Proceedings of the Third International Workshop on
		  Standard {ML}",
  month =        sep,
  year =         "1991",
  abstract =	 ""
}

@TechReport{Morrisett92,
  author =       "J. Gregory Morrisett and Andrew P. Tolmach",
  title =        "A Portable Multiprocessor Interface for Standard
		  {ML} of New Jersey",
  institution =  "School of Computer Science, Carnegie Mellon
		  University",
  number =       "CMU-CS-92-155",
  address =      "Pittsburgh, PA",
  year =         "1992",
  note =         "Also issued by Princeton University, Dept. of
		  Computer Science, tech report no. CS-TR-376-92",
  url =     "http://www.cs.princeton.edu:80/TR/PRINCETONCS:TR-376-92",
  abstract =     "We have designed a portable interface between
		  shared-memory multiprocessors and Standard ML of New
		  Jersey. The interface is based on the conventional
		  kernel thread model and provides facilities that can
		  be used to implement user-level thread packages. The
		  interface supports experimentation with different
		  thread scheduling policies and synchronization
		  constructs. It has been ported to three different
		  multiprocessors and used to construct a general
		  purpose, user-level thread package. In this paper,
		  we discuss the interface and its implementation and
		  performance, with emphasis on the Silicon Graphics
		  4D/380S multiprocessor."
}

@InProceedings{Morrisett93,
  author = 	 "J. Gregory Morrisett and Andrew P. Tolmach",
  title = 	 "Procs and locks: a portable multiprocessing platform
		  for Standard {ML} of New Jersey",
  pages =	 "198--207",
  booktitle =	 ppopp4,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  url =     "http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jgmorris/web/papers/jgmorris-ppopp.ps",
  abstract =	 "A portable platform has been built for running
		  Standard ML of New Jersey programs on
		  multiprocessors. It can be used to implement
		  user-level thread packages for multiprocessors
		  within the ML language with first-class
		  continuations. The platform supports experimentation
		  with different thread scheduling policies and
		  synchronization constructs. It has been used to
		  construct a Modula-3 style thread package and a
		  version of Concurrent ML, and has been ported to
		  three different multiprocessors running variants of
		  Unix. The authors describe the platform's design,
		  implementation, and performance."
}

@TechReport{Motomura93,
  author = 	 "Masato Motomura and Gregory M. Papadopoulos",
  title = 	 "Local Memory reference behaviour of fine-grain
		  multithreaded execution",
  institution =  "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  number =	 "CSG Memo-346",
  month =	 "Nov",
  note2 =	 "HAR",
  url =  	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-346.ps",
  abstract =	 "Multithreading is a potentially important technique
		  to deal with the effects of large communications
		  latencies in multiprocessors. In this paper, we
		  study the behavior of local memory references under
		  a fine-grain multithreaded execution model. Based on
		  the threaded abstract machine (TAM), the model is a
		  compiling convention for distributed memory
		  multiprocessors and does not presuppose special
		  hardware support like multiple hardware contexts. We
		  believe we have made three basic contributions to
		  the understanding of local reference streams under
		  fine-grained multithreading: (1) we extensively
		  studied the execution traces from two example
		  programs run on a multiprocessor simulator, (2) we
		  have determined a power-law working set model for
		  local activation frame and instruction references,
		  and (3) using the working set model we have derived
		  an analytic cache model which provides excellent
		  agreement with actual cache simulation"
}

@InProceedings{Motomura95,
  author = 	 "Masato Motomura and  T. Inoue and S. Torii and A.
		  Konagaya",
  title = 	 "Ordered Multithreading: A Novel Technique for
		  Exploiting Thread-Level Parallelism",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "Jun",
  abstract =	 ""
}

@InProceedings{Mueller92,
  author = 	 "Frank Mueller",
  title = 	 "Implementing POSIX threads under UNIX: Description
		  of work in progress",
  pages =	 "253--261",
  booktitle =	 "Proceedings of the 2nd software engineering research
		  forum",
  year =	 "1992",
  address =	 "Melb., Florida",
  month =	 "Nov",
  note2 =	 "HAR",
  url = 	 "http://www.informatik.hu-berlin.de/~mueller/ftp/pub/PART/pthreads_serf92.ps.Z",
  abstract =	 ""
}

@InProceedings{Mueller93,
  author = 	 "Frank Mueller",
  title = 	 "A library implementation of {POSIX} threads under Unix",
  pages =	 "29--41",
  booktitle =	 usenixw93,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "Jan",
  note2 =	 "HAR",
  url = 	 "http://www.informatik.hu-berlin.de/~mueller/ftp/pub/PART/pthreads_usenix93.ps.Z",
  abstract =	 "There has been an effort to specify an IEEE standard
		  for portable operating systems for open systems,
		  called POSIX. One part of it, the POSIX 1003.4a
		  threads extension (Pthreads for short), describes
		  the interface for light-weight threads that rely on
		  shared memory and have a smaller context frame than
		  processes. The author describes and evaluates the
		  design and implementation of a library of Pthreads
		  calls that is solely based on Unix. He shows that a
		  library implementation is feasible and can result in
		  good performance. This work can also be used as a
		  comparison of the performance of other
		  implementations, or as a prototyping, testing, and
		  debugging system in the regular Unix environment.
		  Finally, some problems with the Pthreads standard
		  are identified."
}

@InProceedings{Mughal88,
  author =       "K. A. Mughal",
  title =        "Generation of incremental indirect threaded code for
		  language-based programming environments",
  booktitle =    "Compiler Compilers and High Speed Compilation 2nd
		  CCHSC Workshop Proceedings",
  pages =        "230--242",
  address =      "Berlin, GDR",
  month =        oct,
  year =         "1988",
  abstract =	 ""
}

@InProceedings{Muhlbacher91,
  author = 	 "Jrg R. Muhlbacher",
  title = 	 "Objects and lightweight processes using object
		  oriented Modula",
  pages =	 "312--321",
  booktitle =	 "Second International Modula-2 Conference. Modula-2
		  and Beyond",
  year =	 "1991",
  publisher =	 "Loughborough University of  Technology",
  address =	 "Loughborough, UK",
  month =	 "Sep",
  abstract =	 "A discussion is given on the advantages of object
		  oriented programming (OOP) for dealing with
		  multitasking. The author uses an object oriented
		  extension of Modula 2 and OS/2 for providing light
		  weight processes. It is shown how semaphores and
		  critical regions can be implemented and the
		  advantage of inheritance for further refinements of
		  these synchronisation primitives. The
		  consumer-producer problem is used to show how to
		  program with threads using type extensions together
		  with the concept of lively objects. These objects
		  have their own private thread assigned and are
		  executed concurrently. This in turn provides a
		  concept of how the module processes can be
		  programmed, so that StartProcess results in creating
		  a lively object and SEND,WAIT, etc., are virtual
		  methods of objects of that class. Any scheduling
		  algorithms behind these methods may be modified then
		  by overriding these methods appropriately."
}

@InProceedings{Mukherjee91,
  author =	 "Bodhisattwa Mukherjee",
  title =	 "A portable and reconfigurable threads package",
  booktitle =	 "Proceedings of Sun User group technical conference",
  pages =	 "101--112",
  year =	 "1991",
  month =	 "Jun",
  note =	 "Also as Tech report GIT-ICS-91/02, Georgia Tech,
		  College of Computing",
  note2 =	 "HAR",
  url =  	 "ftp://ftp.cc.gatech.edu/pub/tech_reports/1991/GIT-CC-91-02.ps.Z",
  abstract =	 "This document describes the interface and
		  implementation of a user-level thread library for
		  several computer systems.  The purpose of this
		  library is to allow programmers to develop
		  high-performance, concurrent applications that are
		  easily ported to future MACH-based machines.  Thus,
		  differences between the `standard' Mach C thread
		  library package and this library have been
		  minimized."
}

@InProceedings{Mukherjee93,
  author = 	 "Bodhisattwa C. Mukherjee and Karsten Schwan",
  title = 	 "Improving performance by use of adaptive objects:
		  experimentation with a configurable multiprocessor
		  thread package",
  pages =	 "59--66",
  booktitle =	 "Proceedings the 2nd International Symposium on High
		  Performance Distributed Computing",
  year =	 "1993",
  publisher =	 "IEEE",
  address =	 "Spokane, WA, USA",
  month =	 "Jul",
  abstract =	 "Since the mechanisms of an operating system can
		  significantly affect the performance of parallel
		  programs, it is important to customize operating
		  system functionality for specific application
		  programs. The authors first present a model for
		  adaptive objects and the associated mechanisms, then
		  they use this model to implement adaptive locks for
		  multiprocessors which adapt themselves according to
		  user-provided adaptation policies to suit changing
		  application locking patterns. Using a parallel
		  branch and bound program, they demonstrate the
		  performance advantage of adaptive locks over
		  existing locks."
}

@Article{Mukherjee94,
  author = 	 "Bodhisattwa Mukherjee and Greg Eisenhauer and
		  Kaushik Ghosh",
  title = 	 "A machine independent interface for lightweight
		  threads",
  journal =	 acmosr,
  year =	 "1994",
  volume =	 "28",
  number =	 "1",
  pages =	 "33--47",
  month =	 "Jan",
  note2 =	 "HAR",
  url =     "ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-53.ps.Z",
  abstract =	 "Lightweight thread libraries have become a common
		  entity to support concurrent programming on shared
		  memory multiprocessors. However, the disparity
		  between primitives offered by operating systems
		  creates a challenge for those who wish to create
		  portable lightweight thread packages. What should be
		  the interface between the machine-independent and
		  machine-dependent parts of the thread library? We
		  have implemented a portable lightweight thread
		  library on top of Unix on a KSR-1 supercomputer, BBN
		  Butterfly multiprocessor, SGI multiprocesser,
		  Sequent multiprocessor and Sun 3/4 family of
		  uniprocessors. This paper first compares the nature
		  and performance of the operating system primitives
		  offered by these machines. We then present a
		  procedure-level abstraction that is efficiently
		  implementable on all the architectures and is a
		  sufficient base upon which a user-level thread
		  package can be built."
}

@TechReport{Mukherjee95,
  author = 	 "Rajat Mukherjee and John K. Bennet and Jay
		  A. Greenwood",
  title = 	 "The effects of architecture on the performance of
		  latency hiding via rapid context switching",
  institution =  "Rice University, Department of Electrical and
		  Computer Engineering",
  year = 	 "1994",
  number =	 "TR9415",
  note2 =	 "HAR",
  month =	 "Dec",
  url =  	 "http://www-ce.rice.edu/ce/members/jayg/htdocs/rajat.ps",
  abstract =	 ""
}

@InProceedings{Mullender91,
  author = 	 "Sape J. Mullender",
  title = 	 "Experiences with Amoeba",
  pages =	 "1--11",
  booktitle =	 "EurOpen. UNIX Distributed Open Systems in
		  Perspective. Proceedings of the Spring 1991 EurOpen
		  Conference",
  year =	 "1991",
  publisher =	 "EurOpen",
  address =	 "Tromso, Norway",
  month =	 "May",
  abstract =	 "The Amoeba distributed operating system has been in
		  use now for a few years. It has been used in
		  experiments with parallel algorithms, as a
		  distributed UNIX-like system, in real-time
		  applications, and in event-processing for
		  proton-scattering high-energy physics experiments.
		  The author has discovered many of the strong and the
		  weak points of Amoeba. On the positive side, he is
		  very pleased with the RPC-based communication, with
		  capability-based protection, and a free-standing
		  naming service, the bootstrap service and with the
		  process-management facilities. On the negative side,
		  he is not happy with the incomplete,
		  non-binary-compatible UNIX functionality, with the
		  flat port name space which limits the ability to
		  scale, with an immutable-file service, and with a
		  kernel implementation of user threads. The author
		  presents a brief overview of Amoeba, discusses his
		  experiences with Amoeba, and presents some of his
		  work on the design of a new distributed system."
}

@InProceedings{Muller95,
  author = 	 "Henk L. Muller, Paul W. A. Stallard and David H. D.
		  Warren",
  title = 	 "Hiding Miss Latencies with Multithreading on the
		  Data Diffusion Machine",
  volume =	 "I",
  pages =	 "178--185",
  booktitle =	 icpp95,
  year =	 "1995",
  month =	 "Aug",
  available  =   "http://www.pact.srf.ac.uk/DDM/ps/threading.ps.gz",
  note2 =	 "HAR",
  abstract =	 "Large parallel computers require techniques to
		  tolerate the potentially large latencies of
		  accessing remote data. Multithreading is one such
		  technique. We extend previous studies of
		  multithreading by investigating its use on the Data
		  Diffusion Machine (DDM), a virtual shared memory
		  machine in which data migrates according to its use.
		  We use a detailed emulator to study DDM's with up to
		  72 nodes, allowing the scalability of multithreading
		  to be tested further than in other studies. The
		  results are promising and show that the applications
		  tested can all benefit from multithreading on the
		  DDM. Most applications however reach the ceiling of
		  their parallelism. We briefly discuss how the
		  results may generalise to other architectures. "
}

@InProceedings{Murakami92,
  author = 	 "Kazuaki Murakami and Tetsou Hironaka and Takashi
		  Hashimoto and Hiroto Yasuura",
  title = 	 "A node vectorprocessor architecture for massively
		  parallel processing",
  pages =	 "124--136",
  booktitle =	 "Proceedings of a JSPS Seminar. Parallel Programming
		  Systems",
  year =	 "1992",
  month =	 "May",
  abstract =	 "Some architectural features suitable for node
 vectorprocessors used in massively parallel processing are proposed.
		  They are required to save the off-chip memory
		  bandwidth by exploiting the on-chip register
		  bandwidth instead. They should also broaden the
		  vectorizable part and improve the scalar performance
		  further. Those features include FIFO vector
		  registers, multithreading, and several ISP
		  architectures. The authors compare the effects of
		  three ISP architectures: ISP/sub p/ (purely scalar
		  ISP), ISP (separate scalar and vector ISP), and
		  ISP/sub u/ (unified scalar/vector ISP). The ISP/sub
		  u/ is the fastest model. For the ISP/sub u/ model,
		  the authors investigate the effects of
		  multithreading and FIFO vector registers.
		  Multithreading and FIFO vector registers improve
		  performance by 44% and by 11% respectively. The
		  authors conclude that the SIVR (scalar
		  instructions-vector registers) capability of the
		  ISP/sub u/ and the multithreading at the vector
		  instruction level are good candidates for the
		  architectural features suitable for node
		  vectorprocessors."
}

@InProceedings{Murer92,
  author = 	 "Stephan Murer and P. Farber",
  title = 	 "A scalable distributed shared memory",
  series =	 "Lecture notes in Computer Science ???",
  pages =	 "453--466",
  booktitle =	 "Proceedings of  Second Joint International
		  Conference on Vector and Parallel Processing. CONPAR
		  92-VAPP",
  year =	 "1992",
  month =	 "Sep",
  abstract =	 "Parallel computers of the future will require a
		  memory model which offers a global address space to
		  the programmer, while performing equally well under
		  various system configurations. The authors present a
		  logically shared and physically distributed memory
		  to match both requirements. This paper introduces
		  the memory system used in the ADAM coarse-grain
		  dataflow machine which preserves scalability by
		  tolerating latency and offers programmability
		  through its object-based structure. The authors show
		  how to support data objects of arbitrary size and
		  different access bandwidth and latency
		  characteristics, and present a possible
		  implementation of this model. The proposed system is
		  evaluated by analysis of the bandwidth and latency
		  characteristics of the three different object
		  classes and by examination of the impact of
		  different network topologies. Finally, they present
		  a number of simulation results which confirm the
		  previous analysis. "
}

@InProceedings{Murer93,
  author = 	 "Stephan Murer and P. Farber",
  title = 	 "Code generation for multi-threaded architectures
		  from dataflow graphs",
  pages =	 "77--90",
  booktitle =	 pact93,
  year =	 "1993",
  abstract =	 "Multi-threaded architectures require new code
		  generation methods. First the ADAM architecture as
		  one concrete implementation of the multi-threading
		  concept is introduced. Then the authors discuss the
		  problem of finding an optimal code sequence under
		  the presence of asynchronous split-phase
		  transactions, resulting in a new code generation
		  algorithm. Experimental results on the ADAM
		  simulator show that the new method performs better
		  with larger graphs. Techniques are demonstrated of
		  increasing the graph size (loop unrolling, function
		  expansion) and are validated by experiments."
}

@TechReport{NHaines93,
  author = 	 "Nicholas Haines and Darrell Kindred and J. Gregory
		  Morrisett and Scott M. Nettles and Jeannette M.
		  Wing",
  title = 	 "Tinkertoy Transactions",
  institution =  "Carnegie Mellon University, School of Computer Science",
  year = 	 "1993",
  number =	 "CMU-CS-93-202",
  url =  	 "http://rose.mercury.acs.cmu.edu:1082/TR/CMU:CS-93-202",
  abstract = 	 "We describe the design of a transaction facility for a
		  language that supports higher-order functions. We
		  factor transactions into four separable features:
		  persistence, undoability, locking, and threads.
		  Then, relying on function composition, we show how
		  we can put them together again. Our ``Tinkertoy''
		  approach towards building transactions enables us to
		  construct a model of concurrent, nested,
		  multi-threaded transactions, as well as other
		  non-traditional models where not all features of
		  transactions are present. Key to our approach is the
		  use of higher-order functions to make transactions
		  first-class. Not only do we get clean composability
		  of transactional features, but also we avoid the
		  need to introduce special control and
		  block-structured constructs as done in more
		  traditional transactional systems. We implemented
		  our design in Standard {ML} of New Jersey."
}

@InProceedings{Najjar92,
  author = 	 "Walid A. Najjar and William M. Miller and A. P.
		  Willem Bohm",
  title = 	 "An analysis of loop latency in dataflow execution",
  pages =	 "352--360",
  booktitle =	 isca19,
  year =	 "1992",
  address =	 "Gold Coast, Australia",
  month =	 "May",
  note =	 "Published in " # canews # "Vol.20, No.2, May
		  1992", 
  abstract =	 "Recent evidence indicates that the exploitation of
		  locality in dataflow programs could have a dramatic
		  impact on performance. The current trend in the
		  design of dataflow processors suggest a synthesis of
		  traditional non-strict fine grain instruction
		  execution and a strict coarse grain execution in
		  order to exploit locality. While an increase in
		  instruction granularity will favor the exploitation
		  of locality within a single execution thread, the
		  resulting grain size may increase latency among
		  execution threads. In the paper, the resulting
		  latency incurred through the partitioning of fine
		  grain instructions into coarser grain threads is
		  evaluated. The authors define the concept of a
		  cluster of fine grain instructions to quantify
		  coarse grain input and output latencies using a set
		  of numeric benchmarks. The results offer compelling
		  evidence that the inner loops of a significant
		  number of numeric codes would benefit from coarse
		  grain execution."
}

@InProceedings{Najjar93a,
  author = 	 "Walid Najjar and Lucas Roh and A. P. Willem Bohm",
  title = 	 "The initial performance of a bottom-up clustering
		  algorithm for dataflow graphs",
  pages =	 "91--102",
  booktitle =	 pact93,
  year =	 "1993",
  month =	 "June",
  note =	 "Also as Tech report CS-92-131, Colorado State
		  University, Dept. of Computer Science",
  abstract =	 "The objective of a hybrid von Neumann dataflow model
		  is to combine the advantages offered by the dataflow
		  cheap synchronization and latency hiding) and the
		  von Neumann models (exploiting locality) to improve
		  the overall performance. In order to do so, the
		  granularity of the execution is shifted from a fine
		  to a coarser grain thereby reducing the ratio of
		  matches (synchronizations) to instructions executed.
		  The authors present a cluster based model of coarse
		  grain dataflow execution where neighboring
		  instructions are grouped into clusters that are the
		  schedulable unit of execution. The goals of this
		  model are to preserve the loop and function level
		  parallelism to be exploited on the cluster level and
		  maximize the instruction level locality within a
		  cluster. They describe and evaluate a bottom-up
		  clustering algorithm that generates a coarse grain
		  graph from a fine grain one. The results show a
		  reduction in matching overhead by half. Furthermore,
		  the resulting clusters are highly sequential
		  indicating that the degree of parallelism in the
		  initial graph was preserved."
}

@Article{Najjar93b,
  author = 	 "Walid A. Najjar and A. P. Willem Bohm and William M.
		  Miller",
  title = 	 "A quantitative analysis of dataflow program
		  execution-preliminaries to a hybrid design",
  journal =	 jpal,
  year =	 "1993",
  volume =	 "18",
  number =	 "3",
  pages =	 "314--326",
  month =	 "Jul",
  abstract =	 "While the dataflow execution model can potentially
		  uncover all forms and levels of parallelism in a
		  program, in its traditional fine grain form it does
		  not exploit any form of locality. Recent evidence
		  indicates that the exploitation of locality in
		  dataflow programs could have a dramatic impact on
		  performance. The current trend in the design of
		  dataflow processors suggests a synthesis of
		  traditional nonstrict fine grain instruction
		  execution and strict coarse grain execution in order
		  to exploit locality. While an increase in
		  instruction granularity favors the exploitation of
		  locality within a single execution thread, the
		  resulting grain size may increase latency among
		  execution threads. The authors define fine grain
		  intrathread locality as a dynamic measure of
		  instruction level locality and quantify it using a
		  set of numeric and nonnumeric benchmarks. The
		  results point to a very large degree of intrathread
		  locality and a remarkable uniformity and consistency
		  of the distribution of thread locality across a wide
		  variety of benchmarks."
}

@InProceedings{Nakajima89,
  author =       "Tatsou Nakajima and Yasuhiko Yokote and Mario Tokoro
		  and Sinichi Ochiai and Tatsou Nagamatsu",
  title =        "Distributed Concurrent Smalltalk, {A} Language and
		  System for the Interpersonal Environment", 
  booktitle =    "Proceedings of the ACM SIGPLAN Workshop on
		  Object-Based Concurrent Programming",
  pages =        "43--45",
  month =        "4",
  year =         "1989",
  note =         "Published in ACM SIGPLAN Notices, volume 24, number 4",
  abstract =     "Extends concurrent Smalltalk to distributed systems,
		  taking inter-processor communication, name service,
		  and multiple thread control in an object into
		  consideration."
}

@InProceedings{Nandy93,
  author =       "S. K. Nandy and Ranjani Narayan and V. Visvanathan
		  and P. Sadayappan and Prashant S. Chauhan",
  title =        "A Parallel Progressive Refinement Image Rendering
		  Algorithm on a Scalable Multi-threaded {VLSI}
		  Processor Array",
  booktitle =    icpp93,
  volume =       "III - Algorithms {\&} Applications",
  pages =        "III-94--III--97",
  publisher =    "CRC Press",
  address =      "Boca Raton, FL",
  month =        aug,
  year =         "1993",
  abstract =	 ""
}

@TechReport{Neefs95,
  author = 	 "Henk Neefs",
  title = 	 "Architectural Support for and Research on Microthreading",
  institution =  "University of Gent, Department of Electronics and
		  Information Systems",
  year = 	 "1995",
  number =	 "Paris 95-02",
  month =	 "??",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.elis.rug.ac.be/pub/paris/tech_report_9502.ps.Z",
  abstract =	 ""
}

@InProceedings{Nemawarkar92,
  author = 	 "Shashank S. Nemawarkar and R. Govindarajan and Guang R.
		  Gao and V. K. Agarwal",
  title = 	 "Performance evaluation of latency tolerant
		  architectures",
  pages =	 "183--186",
  booktitle =	 "Proceedings. ICCI '92. Fourth International
		  Conference on Computing and Information",
  year =	 "1992",
  address =	 "Toronto, CA",
  month =	 "May",
  note2 =	 "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/papers/ICCI92.ps.gz",
  abstract =	 "The authors analyze a single processor multithreaded
		  architecture using stochastic timed Petri net (STPN)
		  model to study the effects of various parameters
		  such as memory latency and thread runlength, on
		  processor utilization. They first perform a simple
		  analysis of the basic model with constant values for
		  the parameters. This is followed by an extension
		  with stochastic parameters. A detailed simulation
		  study is conducted to validate the analysis. While
		  earlier researchers established that an increase in
		  the number of threads results in increased processor
		  utilization, their results, on the other hand,
		  indicate that average runlength and effective memory
		  latency have stronger impact on processor
		  utilization than the number of threads."
}

@InProceedings{Nemawarkar93,
  author = 	 "Shashank Nemawarkar and R. Govindarajan and Guang R.
		  Gao and V. K. Agarwal",
  title = 	 "Analysis of Multithread Multiprocessors with
		  Distributed Shared Memory",
  pages =	 "??--??",
  booktitle =	 ispdp5,
  year =	 "1993",
  address =	 "Dallas, TX",
  month =	 "Dec",
  note2 =	 "HAR",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/papers/SPDP93.ps.gz",
  abstract =	 "In this paper we propose an analytical model, based
		  on multi-chain closed queuing networks, to evaluate
		  the performance of multithreaded
		  multiprocessors. The queuing network is solved by
		  using approximate Mean Value Analysis. Unlike
		  earlier work which modeled individual subsystems in
		  isolation, our work models processor, memory, and
		  network subsystems in an integrated manner. Such an
		  approach brings out a strong coupling between each
		  pair of subsystems. For example, the processor and
		  memory utilization respond identically to the
		  variations in the network characteristics. Further
		  we observe that high performance on an application
		  is achieved when the memory request rate of a
		  processor equals the weighted sum of memory
		  bandwidth and the average round trip distance of the
		  remote memory across the network"
}

@InProceedings{Nemawarkar94,
  author = 	 "Shashank Nemawarkar and R. Govindarajan and Guang R.
		  Gao and V.K. Agarwal",
  title = 	 "Performance of Interconnection Networks in
		  Multithreaded Architectures",
  pages =	 "823--826",
  booktitle =	 parle94,
  year =	 "1994",
  month =	 "Jul",
  series =	 "Lecture Notes in Computer Science ???",
  url = 	 "ftp://ftp-acaps.cs.mcgill.ca/pub/doc/papers/PARLE94.ps.gz",
  abstract =	 "Analyses the performance of interconnection networks
		  in a multithreaded multiprocessor using a closed
		  queuing network model. The proposed integrated model
		  of the multiprocessor system captures the
		  interaction among subsystems faithfully. Our study
		  reveals a strong relationship of workload parameters
		  to the network performance and brings out a feedback
		  effect of the network response on the message rate
		  to the network."
}

@TechReport{Nemawarkar95a,
  author = 	 "Shashank S. Nemawarkar and Guang R. Gao",
  title = 	 "Latency Tolerance: A Metric for Performance Analysis
		  of Multithreaded Architectures",
  institution =  "McGill University, School of computer science",
  year = 	 "1995",
  number =	 "ACAPS Memo-85",
  abstract =	 ""
}

@TechReport{Nemawarkar95b,
  author = 	 "Shashank S. Nemawarkar and Guang R. Gao",
  title = 	 "Performance Analysis of Multithreaded Architectures
		  using an Integrated System Model",
  institution =  "McGill University, School of computer science",
  year = 	 "1995",
  number =	 "ACAPS Memo-84",
  abstract =	 ""
}

@PhdThesis{Nemirovsky90,
  author = 	 "Mario D. Nemirovsky",
  title = 	 "DISC: A Dynamic Instruction Stream Computer",
  school = 	 "University of California, Santa Barbara, Department
		  of Electrical and Computer Engineering",
  year = 	 "1990",
  month =	 "Sep",
  abstract =	 ""
}

@InProceedings{Nemirovsky91,
  author = 	 "Mario D. Nemirovsky and Forrest Brewer and Roger C.
		  Wood",
  title = 	 "DISC: Dynamic Instruction Stream Computer",
  pages =	 "163--171",
  booktitle =	 isma24,
  year =	 "1991",
  month =	 "Nov",
  abstract =	 "This paper applies a form of instruction stream
		  interleaving to the problem of high performance
		  real-time systems. Such systems are characterized by
		  high bandwidth, stochastically occurring interrupts
		  as well as high throughput requirements. The DISC
		  computer is based on dynamic interleaving where the
		  next instruction to be executed is dynamically
		  selected from several possible simultaneously active
		  streams. Each stream context is stored internally
		  making possible active task switching in a single
		  instruction cycle. For several RTS applications the
		  DISC concept promises higher computation throughput
		  at lower cost than is possible on contemporary RISC
		  processors. Implementation and register organization
		  details are presented as well as simulation
		  results."
}

@InProceedings{Neves95,
  author = 	 "Richard Neves and Robert B, Schnabel",
  title = 	 "Runtime Support for Execution of Fine Grain Parallel
		  Code on Coarse Grain Multiprocessors",
  pages =	 "??--??",
  booktitle =	 "Proceedings of The Fifth Symposium on the Frontiers
		  of Massively Parallel Computation (Frontiers '95)",
  year =	 "1995",
  url = 	 "http://www.cs.colorado.edu/~neves/pubs_ps/frontiers.ps",
  abstract =	 "The goal of this research is to provide systems
		  support that allows fine grain, data parallel code
		  to execute efficiently on much coarser grain
		  multiprocessors. The task of writing parallel
		  applications is simplified by allowing the
		  programmer to assume a number of processors
		  convenient to the algorithm being implemented. This
		  paper describes and evaluates a runtime approach
		  that efficiently manages thousands of ``virtual''
		  processors per actual processor. Tight integration
		  and specialization of scheduling, communication, and
		  context switching is used to significantly reduce
		  the overhead of running fine grain parallel code. A
		  Paragon prototype of this runtime approach is
		  evaluated by comparing implementations of two
		  numerical problems. The overhead due to scheduling,
		  communication, and context switching is analyzed.
		  The implementation and analysis show that fine grain
		  code can be efficiently executed in a coarse grain
		  multiprocessor using a runtime approach."
}

@InProceedings{Nikhil89,
  author = 	 "Rishiyur S. Nikhil and Arvind",
  title = 	 "Can dataflow subsume von Neumann computing?",
  pages =	 "262--272",
  booktitle =	 isca16,
  year =	 "1989",
  month =	 "Jun",
  note =	 "Also as CSG-Memo-292, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  note2 =	 "HAR",
  abstract =	 "We explore the question: ``what ccan a von Neumann
		  processor borrow from dataflow to make it more
		  suitable for a multiprocessor?''. Starting with a
		  simple, RISC-like instruction set, we show how to
		  change the underlying processor organization to make
		  it multithreaded. Then we extend it with three
		  instructions that give it a fine-grained dataflow
		  capability. We call the result P-RISC, for
		  ``parallel RISC'' Finally, we discuss memory support
		  for such multiprocessors. We compare our approach to
		  existing MIMD machines and to other dataflow
		  machines"
}

@Misc{Nikhil92a,
  author =	 "Rishiyur S. Nikhil",
  title =	 "Multithreaded Architectures tutorial slides",
  howpublished = "Tutorial at Various conferences (ISCA92,93 etc)",
  year =	 "1992",
  note2 =	 "HAR",
  url =     "http://www.research.digital.com/CRL/personal/nikhil/home.html",
  abstract =	 "Slides from tutorial on multithreaded architectures"
}

@InProceedings{Nikhil92b,
  author = 	 "Rishiyur S. Nikhil and Gregory M. Papadopoulos and
		  Arvind",
  title = 	 "{*T}: A multithreaded massively parallel
		  architecture",
  pages =	 "156--167",
  booktitle =	 isca19,
  year =	 "1992",
  month =	 "May",
  note =	 "Also as CSG-memo-325-1, Massachusetts Institute of
		  Technology, Laboratory for Computer Science",
  note2 =	 "HAR",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-325-1.ps.gz",
  abstract =	 "What should the architecture of each node in a
		  general purpose massively parallel architecture
		  (MPA) be?. We frame the question in concrete terms
		  by describing two fundamental problems that must be
		  solved well in any general purpose MPA. From this we
		  systematically develop the required logical
		  organization of an MPA node, and present some
		  details of *T (pronounced Start), a concrete
		  architecture designed to these requirements. *T is a
		  direct descendant of dynamic dataflow architectures,
		  and unifies them with von Neumann architectures. We
		  discuss a hand-compiled example and some compilation
		  issues."
}

@InProceedings{Nikhil93,
  author = 	 "Rishiyur S. Nikhil",
  title = 	 "A Multithreaded Implementation of Id using P-RISC
		  graphs",
  pages =	 "??--??",
  booktitle =	 "Proceedings of Workshop on Languages and Compilers
		  for Parallel Computing",
  year =	 "1993",
  address =	 "Portland, Oregon",
  month =	 "Aug",
  series =	 "Lecture notes in Computer Science ???",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/users/nikhil/mti/mti.ps",
  abstract =	 "P-RISC graphs are parallel, structured, executable
		  control-flow graphs that model locality properties
		  of distributed memory machines. P-RISC graphs have
		  coarse grain parallelism for distribution of work to
		  processors, and fine grain parallelism to overlap
		  latencies of remote accesses and synchronization.
		  These properties support dynamic and irregular
		  parallelism. We describe P-RISC graphs, how they are
		  used to implement the implicitly parallel
		  programming language Id, and how they are
		  implemented on distributed memory machines. We also
		  contrast our approach to Berkeley's TAM, a similar
		  system."
}

@TechReport{Nikhil95,
  author = 	 "Rishiyur S. Nikhil and Arvind and James E. Hicks and
		  Shail Aditya and Lennart Augustsson and Jan-Willem
		  Maessen and Y. Zhou",
  title = 	 "pH Language Reference Manual, Version 1.0",
  institution =  "Massachussets Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1995",
  number =	 "CSG-Memo-369",
  available= 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-369.ps",
  month =	 "Jan",
  abstract =	 ""
}

@InProceedings{Noakes90,
  author = 	 "Michael D. Noakes and William J. Dally",
  title = 	 "System design of the J-machine",
  pages =	 "179--194",
  booktitle =	 "Advanced Research in VLSI. Proceedings of the Sixth
		  MIT Conference",
  year =	 "1990",
  month =	 "Apr",
  abstract =	 "The J-Machine is a fine-grained concurrent computer
		  that provides low-overhead primitive mechanisms for
		  communication, synchronization, and translation. The
		  hardware is an ensemble of up to 65536 nodes each
		  containing a 36-bit processor, 4K 36-bit words of
		  on-chip memory, a router, and a static-column DRAM
		  controller with ECC capable of addressing up to an
		  additional 1M 36-bit words of external memory. The
		  nodes are connected by a high-speed, synchronous
		  3D-mesh network. The authors are currently in the
		  process of constructing a 4096 node prototype of the
		  J-Machine to serve as a testbench for studying
		  current topics in the execution of large
		  fine-grained concurrent programs on this class of
		  hardware. This machine is packaged in 4 chassis of
		  16 boards. Each board contains 64 processors. The
		  vertical interconnection between boards in the same
		  chassis uses elastomeric conductors. This technology
		  provides high-density signal distribution between
		  surface pads on PC-cards with limited alignment
		  constraints. Clock-skew between boards is minimized
		  by an automatic calibration mechanism that is
		  controlled by a boot-host."
}

@InProceedings{Noakes93,
  author = 	 "Michael D. Noakes and Deborah A. Wallach and
		  William J. Dally",
  title = 	 "The J-machine multicomputer: An architectural
		  evaluation",
  pages =	 "224--235",
  booktitle =	 isca20,
  year =	 "1993",
  month =	 "May",
  note2 =	 "HAR",
  url =  	 "ftp://ftp.ai.mit.edu/pub/cva/jm-netw.ps.Z",
  abstract =	 "The MIT J-Machine multicomputer has been constructed
		  to study the role of a set of primitive mechanisms
		  in providing efficient support for parallel
		  computing. Each J-Machine node consists of an
		  integrated multicomputer component, the
		  Message-Driven Processor (MDP), and 1 MByte of DRAM.
		  The MDP provides mechanisms to support efficient
		  communication, synchronization, and naming. A 512
		  node J-Machine is operational and is due to be
		  expanded to 1024 nodes in March 1993. The authors
		  discuss the design of the J-Machine and evaluate the
		  effectiveness of the mechanisms incorporated into
		  the MDP. They measure the performance of the
		  communication and synchronization mechanisms
		  directly and investigate the behavior of four
		  complete applications."
}

@PhDthesis{Nussbaum93,
  author =       "Daniel Nussbaum",
  title =        "Run-time Management for Large-Scale Distributed-Memory
		  Multiprocessors",
  school =       "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year =         "1993",
  month =        "Sep",
  number =       "MIT/LCS/TR-596",
  note   =       "216 pages",
  note2 = 	 "HAR",
  url =          "ftp://cag.lcs.mit.edu/pub/papers/dann-thesis.ps.Z",
  abstract =     "Effective thread management is crucial to achieving
		  good performance on large-scale distributed-memory
		  multiprocessors that support dynamic threads. For a
		  givenparallel computation with some associated task
		  constraints imposed by the task graph, a
		  thread-management algorithm produces a running
		  schedule as output, subject to the precedence
		  constraints imposed by the task graph and the
		  constraints imposed by the interprocessor
		  communications network. Optimal thread management is
		  an NP-hard problem, even given full a priori
		  knowledge of the entire task graph and assuming a
		  highly simplified architecture abstraction. Thread
		  management is even more difficult for dynamic
		  data-dependent computations which must use online
		  algorithms because their task graphs are not known a
		  priori. This thesis investigates online
		  thread-management algorithms and presents XTM, an
		  online thread-management system for large-scale
		  distributed-memory multiprocessors. XTM has been
		  implemented for the MIT Alewife Multiprocessor.
		  Simulation results indicate that XTM's behavior is
		  robust, even when run on very large machines."
}

@InProceedings{Nuth91,
  author = 	 "Peter R. Nuth and William J. Dally",
  title = 	 "A mechanism for efficient context switching",
  pages =	 "301--304",
  booktitle =	 iccd91,
  year =	 "1991",
  publisher =	 "IEEE",
  address =	 "Cambridge, MA, USA",
  month =	 "Oct",
  note2 = 	 "HAR",
  abstract =	 "Context switches are slow in conventional processors
		  because the entire processor state must be saved and
		  restored, even if much of the restored state is not
		  used before the next context switch. This
		  unnecessary data movement is required because of the
		  coarse granularity of binding between names and
		  registers. The context cache is introduced, which
		  binds variable names to individual registers. This
		  allows context switches to be very inexpensive,
		  since registers are only loaded and saved out as
		  needed. Analysis shows that the context cache holds
		  more live data than a multithreaded register file,
		  and supports more tasks without spilling to memory.
		  Circuit simulations show that the access time of a
		  context cache is 7% greater than a conventional
		  register file of the same size."
}

@PhdThesis{Nuth93,
  author = 	 "Peter R. Nuth",
  title = 	 "The Named-State Register File",
  school = 	 "Massachussets Institute of Technology, Aritifical
		  Intelligence Laboratory",
  year = 	 "1993",
  month = 	 "Aug",
  note =         "134 pages. Also as tech report AI-TR-1459",
  note2 = 	 "HAR",
  url =          "ftp://publications.ai.mit.edu/ai-publications/1993/AITR-1459.ps.Z",
  abstract = 	 "This thesis introduces the Named-State Register
		  File, a fine-grain, fully-associative register file.
		  The NSF allows fast context switching between
		  concurrent threads as well as efficient sequential
		  program performance. The NSF holds more live data
		  than conventional register files, and requires less
		  spill and reload traffic to switch between contexts.
		  This thesis demonstrates an implementation of the
		  Named-State Register File and estimates the access
		  time and chip area required for different
		  organizations. Architectural simulations of large
		  sequential and parallel applications show that the
		  NSF can reduce execution time by 9\% to 17\%
		  compared to alternative register files"
}

@InProceedings{Nuth95,
  author = 	 "Peter R. Nuth and William J. Dally",
  title = 	 "The Named-State Register File: Implementation and
		  Performance",
  pages =	 "??--??",
  booktitle =	 "Proceedings of the 1st International Symposium on
		  High-Performance Computer Architecture,",
  year =	 "1995",
  month =	 "Jan",
  url =  	 "ftp://ftp.ai.mit.edu/pub/cva/nsf-hpca95.ps.Z",
  abstract =	 "Context switches are slow in conventional processors
		  because the entire processor state must be saved and
		  restored, even if much of the state is not used
		  before the next context switch. This paper
		  introduces the Named- State Register File, a
		  fine-grain associative register file. The NSF uses
		  hardware and software techniques to efficiently
		  manage registers among sequential or parallel
		  procedure activations. The NSF holds more live data
		  per register than conventional register files, and
		  requires much less spill and reload traffic to
		  switch between concurrent contexts. The NSF speeds
		  execution of some sequential and parallel programs
		  by 9% to 17% over alternative register file
		  organizations. The NSF has access time comparable to
		  a conventional register file and only adds 5% to the
		  area of a typical processor chip."
}

@InProceedings{Oikawa93,
  author = 	 "Shuichi Oikawa and Hideyuki Tokuda",
  title = 	 "User-level real-time threads: an approach towards
		  high performance multimedia threads",
  pages =	 "66--76",
  booktitle =	 "Proceedings of 4th International Workshop on Network
		  and Operating System Support for Digital Audio and
		  Video.  NOSSDAV '93",
  year =	 "1993",
  month =	 "Nov",
  abstract =	 "Continuous-media applications may require more
		  efficient and flexible support from real-time
		  threads than traditional real-time systems. This is
		  because changes of system resource usage in a
		  workstations and network environment require dynamic
		  management of real-time threads behavior. If threads
		  are implemented in user-level, operations on threads
		  are processed in user-level. Then, managing threads
		  becomes more efficient avoiding kernel intervention.
		  Therefore, we can use dynamic management of thread
		  attributes effectively. The goal of our effort is to
		  realize high performance user-level real-time
		  threads which satisfy requirements from
		  continuous-media systems, such as efficiency,
		  flexibility and accuracy."
}

@InProceedings{Oikawa94,
  author = 	 "Shuichi Oikawa and Hideyuki Tokuda",
  title = 	 "User-level real-time threads", 
  pages =	 "7--11", 
  booktitle =	 "Proceedings 11th IEEE Workshop on Real-Time
		  Operating Systems and Software. RTOSS '94",
  year =	 "1994",
  address =	 "Seattle, WA, USA",
  month =	 "May",
  abstract =	 "Continuous-media applications require more efficient
		  and flexible support from real-time threads than
		  traditional real-time systems. It includes
		  functionalities such as the dynamic management of
		  thread attributes and the support of multiple thread
		  models. We describe the design and implementation of
		  user-level real-time threads on the RT-Mach micro
		  kernel. Since they are implemented at user-level,
		  both of the fast management of thread attributes and
		  the support of multiple thread models are possible."
}

@InProceedings{Okamoto92b,
  author = 	 "Toshio Okamoto and Hideo Segawa and Sung Ho Shin and
		  Ken-ichi Maeda and Mitsuo Saito",
  title = 	 "A micro kernel architecture for next generation
		  processors",
  pages =	 "83--94",
  booktitle =	 "Proceedings of the USENIX Workshop on Micro-Kernels
		  and Other Kernel Architectures",
  year =	 "1992",
  address =	 "Seattle, WA, USA",
  month =	 "Apr",
  note2 =	 "HAR",
  abstract =	 "The authors made the best use of the huge address
		  space provided by a 64-bit next generation
		  architecture. A new micro kernel was designed with
		  two outstanding features; single virtual space and
		  one level storage. Three major benefits from the
		  proposed kernel are fast context switching, fast
		  function call, and fast data access. This micro
		  kernel manages only two abstractions to simplify the
		  concept; the thread and the memory section. The
		  problem regarding how to prevent access by
		  unauthorized threads or programs which first
		  occurred due to the single virtual space has been
		  finally solved with newly designed rich-functioned
		  MMU hardware."
}

@InProceedings{Oldehoeft92,
  author =       "Gregory M. Papadopoulous and A. P. Willem Bohm and
		  Anton T. Dahbura and Rodney R. Oldehoeft",
  title =        "Minisymposium: Multithreaded Computer Systems",
  year =         "1992",
  month =        nov,
  booktitle =    ics92,
  publisher =    "IEEE",
  pages =        "772--775",
  address =      "Minn., MN, USA",
  note2 = 	 "HAR",
  abstract =	 "In this minisymposium speakers address architectural
		  principles and issues of multithreaded computer
		  systems, examine approaches to providing implicitly
		  parallel software for these machines, and give an
		  industrial perspective that includes the potential
		  wider influence of multithreaded computers.
		  Multithreaded computer systems are parallel machines
		  in which threads are sequentially executed code
		  segments. Each processor has hardware support for
		  quickly switching contexts among runnable threads.
		  Other features include some form of data matching,
		  split-phase memory accesses, and a dataflow-like
		  regime for thread instantiation. They combine
		  features of conventional von Neumann systems with
		  those of fine-grained dataflow architectures."
}

@InProceedings{Omondi86,
  author = 	 "Amos R. Omondi and J. Dean Brock",
  title = 	 "Shared Pipelines: Effective Pipelining in
		  Multiprocessor Systems",
  pages =	 "511--514",
  booktitle =	 isca13,
  year =	 "1986",
  abstract =	 ""
}

@Article{Omondi91,
  author = 	 "Amos R. Omondi",
  title = 	 "Design of a high performance instruction pipeline",
  journal =	 "Computer Systems Science and Engineering",
  year =	 "1991",
  volume =	 "6",
  number =	 "1",
  pages =	 "13--29",
  month =	 "Jan",
  abstract =	 ""
}

@Article{Onoye95,
  author = 	 "Takao Onoye and Toshihiro Masaki and Isao Shirakawa
		  and Hiroaki Hirata and Kozo Kimura and Shigeo
		  Asahara and Takayuki Sagishima",
  title = 	 "High-Level Synthesis of a Multithreaded Processor
		  for Image Generation",
  journal =	 "IEICE Transactions on Fundamentals of electronics
		  communications and computer sciences",
  year =	 "1995",
  volume =	 "E78-A",
  number =	 "3",
  pages =	 "322-330",
  month =	 "Mar",
  note2 =	 "HAR",
  abstract =	 "The design procedure of a multithreaded processor
		  dedicated to  the image generation is described,
		  which can be achieved by means of a  high-level
		  synthesis tool PARTHENON. The processor employs a
		  multithreaded architecture which is a novel
		  promising approach to the  parallel image
		  generation. This paper puts special stress on the
		  high- level synthesis scheme which can simplify the
		  behavioral description  for the structure and
		  control of a complex hardware, and therefore
		  enables the design of a complicated mechanism for a
		  multithreaded  processor. Implementation results of
		  the synthesis are also shown to  demonstrate the
		  performance of the designed processor. This
		  processor  greatly improves the throughput of the
		  image generation so far  attained by the
		  conventional approach."
}

@InProceedings{Opsommer93,
  author = 	 "Johan Opsommer and Wim Vande Velde and Erik H.
		  D'Hollander",
  title = 	 "Modeling and Visualizing Kernel Activity in a
		  Shared Memory Multiprocessor",
  pages =	 "512-517",
  booktitle =	 "European Simulation Symposium 1993",
  year =	 "1993",
  address =	 "Delft, Belgium",
  month =	 "Oct",
  url = 	 "ftp://ftp.elis.rug.ac.be/pub/parallel/delft93.ps.Z",
  abstract =	 "A graphical environment is presented to visualize
		  the kernel activity in a shared memory
		  multiprocessor. An existing thread scheduler was
		  modelled and simulated to study the behaviour of
		  parallel thread execution. The model reveals the
		  possible bottlenecks of the system and allows to
		  optimize several thread scheduling alternatives.
		  Using the MODLINE programming environment, it is
		  possible to obtain an animated execution showing the
		  evolution of thread creation, scheduling and
		  execution. The simulation was compared with a kernel
		  executing on a shared memory multiprocessor. In
		  addition a post processor was developed to visualize
		  the task execution on each processor and the shared
		  resource accesses.  In the corresponding Gantt
		  charts, task dependencies are graphically represented"
}

@InProceedings{Ostheimer91,
  author = 	 "Gerald Ostheimer",
  title = 	 "Parallel functional computation on STAR:DUST",
  pages =	 "??--??",
  booktitle =	 "1991 Workshop on parallel implementation of
		  functional languages",
  year =	 "1991",
  month =	 "June",
  note =	 "Also as Tech-report CSTR 91-07, University of
		  Southampton, England",
  note2 = 	 "HAR",
  url =  	 "ftp://ftp.dcs.st-and.ac.uk/pub/staple/stardust.ps.Z",
  abstract =	 "STAR:DUST ('St. Andrews RISC: Dataflow Using
		  Sequential Threads') is a processor design optimized
		  for efficient execution of sequential threads while
		  supporting plug-and-play construction of large
		  multiprocessor systems. Besides satisfying the major
		  RISC criteria (small instruction set, simple
		  instruction format, load/store principle,
		  pipelining), STAR:DUST employs a dataflow approach
		  to communication and parallelism. We describe the
		  architecture and propose a runtime model for
		  parallel functional computation based on STAR:DUST's
		  dataflow primitives."
}

@Article{Pancake93,
  author = 	 "Cherri M. Pancake",
  title = 	 "Multithreaded languages for scientific and technical
		  computing",
  journal =	 ieeep,
  year =	 "1993",
  volume =	 "81",
  number =	 "2",
  pages =	 "288--304",
  month =	 "Feb",
  abstract =	 "An overview of language support for parallel
		  technical computing is provided. The rationale for
		  multithreaded languages, in which the programmer
		  explicitly specifies what work is to be carried out
		  by multiple processors and how their activities
		  should be coordinated, is described. The discussion
		  begins with an introduction to the general models
		  for manipulating multiple threads and how they are
		  incorporated into programming languages. The wide
		  variety of features for creating multiple threads,
		  scheduling their execution, synchronizing their
		  activities, and sharing data among them are then
		  examined. Examples in a simplified, FORTRAN-like
		  notation are included. It is shown how the language
		  features are distributed among commercial compiler
		  implementations. Some less traditional approaches to
		  multithreaded language support are presented to
		  provide a glimpse at what might be expected in
		  future languages and compilers."
}

@InProceedings{Papadopoulos90,
  author = 	 "Gregory M. Papadopoulos and David E. Culler",
  title = 	 "Monsoon: an explicit token-store architecture",
  pages =	 "82--91",
  booktitle =	 isca17,
  year =	 "1990",
  month =	 "May",
  note =	 "Also as CSG-Memo-306, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  abstract =	 "Data-flow architectures tolerate long unpredictable
		  communication delays and support generation and
		  coordination of parallel activities directly in
		  hardware, instead of assuming that program mapping
		  will cause these issues to disappear. However, the
		  proposed mechanisms are complex and introduce new
		  mapping complications. A greatly simplified approach
		  to data-flow execution, called the explicit token
		  store (ETS) architecture, and its current
		  realization in Monsoon are presented. The essence of
		  dynamic data-flow execution is captured by a simple
		  transition on state bits associated with storage
		  local to a processor. Low-level storage management
		  is performed by the compiler in assigning nodes to
		  slots in an activation frame, rather than
		  dynamically in hardware. The processor is simple,
		  highly pipelined, and quite general. It may be
		  viewed as a generalization of a fairly primitive von
		  Neumann architecture. Although the addressing
		  capability is restrictive, there is exactly one
		  instruction executed for each action on the
		  data-flow graph. Thus, the machine-originated ETS
		  model provides new understanding of the merits and
		  the real cost of direct execution of data-flow
		  graphs."
}

@Book{Papadopoulos91a,
  author = 	 "Gregory M. Papadopoulos",
  title = 	 "Implementation of a general-purpose dataflow
		  multiprocessor",
  publisher = 	 "MIT Press",
  year = 	 "1991",
  note =	 "Book version of authors PhD thesis. Also as Tech
		  report LCS/TR-432, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  abstract =	 "General purpose multiprocessors have largely failed
		  to meet expectations for programmability and
		  performance. We blame the lack of usable parallel
		  programming languages and systems on the underlying
		  processor architecture. Machines built out of
		  conventional sequential processors simply do not
		  support the synchronization demands of parallel
		  execution, so the programmer focuses upon the
		  dangerous and arduous task of discovering a minimum
		  set of synchronization points without introducing
		  nondeterminism. We argue that processors must be
		  fundamentally changed to execute a parallel machine
		  language , in which parallel activities are
		  coordinated as efficiently as instructions are
		  scheduled. Dataflow architectures address this
		  challenge by radically reformulating the basic
		  specification of a machine program. These machines
		  directly execute dataflow graphs, which specify only
		  the essential prerequisites for the execution of an
		  instruction--the availability of operands.
		  Unfortunately, dataflow machines, including the
		  M.I.T. Tagged Token Dataflow Architecture (TTDA),
		  have labored under a number of implementation
		  burdens, notably the apparent need for a fully
		  associative operand matching store which discovers
		  when instructions are able to execute. We introduce
		  and develop a novel dataflow architecture, the
		  Explicit Token Store (ETS), which directly executes
		  tagged token dataflow graphs while correcting a
		  number of inherent inefficiencies of previous
		  dataflow machines. In the ETS model, operand
		  matching is performed at compiler-designated offsets
		  within an activation frame. We show that the ETS is
		  compatible with the TTDA by giving translations from
		  TTDA machine graphs to ETS machine graphs. Finally,
		  we describe an implementation of an ETS dataflow
		  multiprocessor, called Monson, now under
		  construction."
}

@InProceedings{Papadopoulos91b,
  author = 	 "Gregory M. Papadopoulus and Kenneth R. Traub",
  title = 	 "Multithreading: a revisionist view of dataflow
		  architectures",
  pages =	 "342--351",
  booktitle =	 isca18,
  year =	 "1991",
  address =	 "Toronto, Can",
  month =	 "May",
  note =	 "Also as CSG-memo-330, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  note2 = 	 "HAR",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-330.ps.gz",
  abstract = 	 "The authors develop a new machine-level programming
		  model which builds upon two previous improvements to
		  the dataflow execution model: sequential scheduling
		  of instructions and multiported registers for
		  expression temporaries. Surprisingly, these
		  improvements have required almost no architectural
		  changes to explicit token store (ETS) dataflow
		  hardware, only a shift in mindset when reasoning
		  about how that hardware works. Rather than viewing
		  computational progress as the consumption of tokens
		  and the firing of enabled instructions, the authors
		  instead reason about the evolution of multiple,
		  interacting sequential threads, where forking and
		  joining are extremely efficient. Because this new
		  paradigm has proven so valuable in coding resource
		  management operations and in improving code
		  efficiency, it is now the cornerstone of the Monsoon
		  instruction set architecture and macro assembly
		  language."
}

@InProceedings{Papadopoulos93,
  author = 	 "Gregory M. Papadopoulos and G. A. Broughton and
		  Robert Greiner and Michael J. Beckerle",
  title = 	 "{*T}: Integrated building blocks for parallel
		  computing",
  pages =	 "??--??",
  booktitle =	 ics93,
  year =	 "1993",
  note =	 "Also as CSG-Memo-351, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  abstract =	 ""
}

@InProceedings{Park91a,
  author = 	 "Won Woo Park and Donald S. Fussel and Roy M.
		  Jenevein",
  title = 	 "Performance advantages of multithreaded processors",
  volume =	 "I",
  pages =	 "97--101",
  booktitle =	 icpp91,
  year =	 "1991",
  month =	 "Aug",
  note2 =	 "HAR",
  abstract =	 ""
}

@PhdThesis{Park91b,
  author = 	 "Won Woo Park",
  title = 	 "Performance-area tradeoffs in a multithreaded
		  processing unit",
  school = 	 "University of Texas at Austin, Department of
		  Electrical and Computer Engineering",
  year = 	 "1991",
  note2 =	 "HAR",
  abstract =	 "Exploiting parallelism is a common approach for high
		  performance computers. There are two kinds of
		  parallelism we can exploit: The instruction-level
		  and task-level parallelism. The instruction-level
		  parallelism is exploited by super-scalar or VLIW
		  processor, and the task-level parallelism is
		  exploited by a multiprocessor, However, the
		  performance of a super-scalar processor is limited
		  by the available instruction-level parallelism and
		  machine parallelism, and it is significantly
		  affected by on-chip cache misses and incorrect
		  branch predictions. The utilization of processors in
		  a multiprocessor system drops due to contentions for
		  memory bussesm, memory conflicts, synchronizations,
		  and program structure. As the number of transistors
		  on a single chip increases, several basic building
		  blocks of a multiprocessor system can be placed on a
		  single chip. This gives an opportunity to share some
		  expensive functional blocks by multithreading
		  without sacrificing the instruction level
		  parallelism. In this circunstance, an optimal
		  configuration requires the analysis of
		  performance-area trade-offs between hardware
		  architectural design parameters. In this
		  dissertation we present studies on the
		  performance-area trade-offs in a multithreaded
		  processing unit."
}

@InProceedings{Peacock92a,
  author =       "J. Kent Peacock",
  title =        "File System Multithreading in System {V} Release 4
		  {MP}", 
  booktitle =    usenixs92,
  pages =        "19--30",
  publisher =    "USENIX",
  address =      "San Antonio, TX",
  year =         "1992",
  note2 = 	 "HAR",
  abstract =	 "An Intel-sponsored Consortium of computer companies
		  has developed a multiprocessor version of System V
		  Release 4 (SVR4MP) which has been released by Unix
		  System Laboratories. The Consortium's goal was to
		  add fine-grained locking to SVR4 with minimal change
		  to the kernel, and with complete backward
		  compatibility for user programs. To do this, a
		  locking strategy was developed which complemented,
		  rather than replaced, existing Unix synchronization
		  mechanisms. To multithread the file systems, some
		  general locking strategies were developed and
		  applied to the generic virtual file system (VFS) and
		  vnode interfaces. Of particular interest were the
		  disk-based S5 and UFS file system types, especially
		  with respect to their scalability. Contention points
		  were found and successively eliminated to the point
		  where the file systems were found to be disk-bound.
		  In particular, several file system caches were
		  restructured using a low-contention, highly-scalable
		  approach called a software set-associative cache.
		  This technique reduced the measured locking
		  contention of each of these caches from the 10-15%
		  range to less than 0.1%. A number of experimental
		  changes to disk queue sorting algorithms were
		  attempted to reduce the disk bottleneck, with
		  limited success. However, these experiments provided
		  the following insight into the need for balance
		  between I/O and CPU utilization in the system: that
		  attempting to increase CPU utilization to show
		  higher parallelism could actually lower system
		  throughput. Using the GAEDE benchmark with a
		  sufficient number of disks configured, the kernel
		  was found to obtain throughput scalability of 88% of
		  the theoretical maximum on five processors."
}

@InCollection{Peacock92b,
  author =       "J. Kent Peacock and Sunil Saxena and Dean Thomas and
		  Fred Yang and Wilfred Yu", 
  title =        "Experiences from Multithreading System {V} Release 4",
  booktitle =    "Proceedings of 3rd symposium on Experiences with
		  Distributed and Multiprocessor systems (SEDMS III)", 
  pages =        "77--92",
  publisher =    "USENIX",
  address =      "Newport Beach, CA",
  month =        mar # " 26-27",
  year =         "1992",
  note2 = 	 "HAR",
  abstract =	 ""
}

@TechReport{Petersen92,
  author =       "Karin Petersen and Kai Li",
  title =        "A Comparative Evaluation of Cache Coherence Schemes
		  Based on Virtual Memory Support",
  institution =  "Princeton University, Department of Computer
		  Science",
  number =       "TR-373-92",
  pages =        "23",
  month =        jun,
  year =         "1992",
  url =     "http://www.cs.princeton.edu/TR/PRINCETONCS:TR-373-92",
  abstract =     "This paper presents an evaluation of a new class of
		  software cache coherence schemes that use virtual
		  memory (VM) support to maintain multiprocessor cache
		  coherence. Traditional VM translation hardware in
		  each processor detects memory access attempts that
		  would violate cache coherence and system software is
		  used to enforce coherence. The implementation of
		  this class of coherence schemes is extremely
		  economical: it requires neither special
		  multiprocessor hardware nor compiler support, and
		  easily incorporates different consistency models. We
		  have evaluated four consistency models for the
		  VM-based approach: sequential consistency,
		  single-writer release consistency, release
		  consistency, and lazy release consistency. Our
		  trace-driven simulation results show that the
		  VM-based cache coherence schemes are practical for
		  small-scale multiprocessors and that the performance
		  of lazy release consistency for multi-threaded
		  parallel programs is close to the snoopy-cache
		  invalidation-based coherence approach. In this
		  paper, we address the problem of supporting SPMD
		  execution of programs that use recursively--defined
		  dynamic data structures on distributed memory
		  machines. The techniques developed for supporting
		  SPMD execution of array--based programs rely on the
		  fact that arrays are statically defined and directly
		  addressable. As a result, these techniques do not
		  apply to recursive data structures, which are
		  neither statically defined nor directly
		  addressable. We propose a three--pronged
		  approach. First, we describe a simple mechanism for
		  migrating a thread of control based on the layout of
		  heap--allocated data. Second, we explain how to
		  introduce parallelism into the model using a
		  technique based on futures and lazy task
		  creation. Third, we present the compiler analyses
		  and parallelization techniques that are required to
		  exploit the proposed mechanism."
}

@InProceedings{Petriu94,
  author = 	 "Dorina C. Petriu and Shikharesh Majumdar and
		  Jingping Lin and Curtis Hrischuk",
  title = 	 "Analytic performance estimation of client-server
		  systems with multi-threaded clients",
  pages =	 "96--100",
  booktitle =	 "MASCOTS '94. Proceedings of the Second International
		  Workshop on Modeling, Analysis, and Simulation of
		  Computer and Telecommunication Systems",
  year =	 "1994",
  address =	 "Durham, NC, USA",
  month =	 "Feb",
  abstract =	 "The authors present an analytical performance model
		  named rendezvous network with multi-threaded clients
		  (RNMTC) for performance analysis of client-server
		  systems. RNMTC is able to model systems with
		  multiple clients inter-communicating with multiple
		  servers which may represent either hardware or
		  software system components. Each system client is
		  described by a precedence graph, and may consist of
		  multiple concurrent execution threads whose number
		  can vary due to fork and join operations. The
		  analytic method for RNMTC proposed is based on
		  hierarchical decomposition: at the higher level the
		  system behaviour is represented by a Markov chain
		  (MC) model whose states correspond to all possible
		  combinations of client execution states; at the
		  lower level a stochastic rendezvous network (SRVN)
		  model with simple clients corresponds to each MC
		  slate. SRVN was previously introduced and MVA
		  approximate analytic solutions are known. The RNMTC
		  model has been used with a number of different test
		  cases and the analytic results were found to be in
		  close agreement with simulation results."
}

@InProceedings{Pham92,
  author = 	 "Thuan Q. Pham and Pankaj K. Garg",
  title = 	 "On migrating a distributed application to a
		  multi-threaded environment",
  pages =	 "45--53",
  booktitle =	 usenixs92,
  year =	 "1992",
  publisher =	 "USENIX Assoc.",
  address =	 "San Antonion, TX, USA",
  month =	 "Jun",
  note =	 "Also as Tech Report HPL-91-155, Hewlett-Packard Lab.,
		  Palo Alto, CA, USA",
  note2 = 	 "HAR",
  abstract =	 "Light-weight computation threads in a multi-threaded
		  operating system promise to provide low-overhead
		  computation and fully sharable addressing space not
		  available in conventional process-oriented operating
		  systems. Traditional distributed applications based
		  on processes can be re-architectured to use
		  concurrent threads in a multi-threaded platform to
		  take advantage of faster context switches and
		  shared-memory communication. The authors
		  investigated this expectation by porting an existing
		  distributed application to a multi-threaded
		  environment. As a result, the authors virtually
		  eliminated the cost of message-based IPC, replacing
		  it with shared-memory communication between threads.
		  The authors address the benefits, the difficulties,
		  and the trade-offs of such a re-architecture. They
		  also comment on some feasible architectures for
		  migrating currently distributed applications to
		  multi-threaded environments."
}

@TechReport{Polychronopoulos90,
  author = 	 "Constantine Polychronopoulos",
  title = 	 "Auto scheduling : control flow and data flow come
		  together",
  institution =  "University of Illinois at Urbana-Champaign. Center
		  for Supercomputing Research and Development",
  year = 	 "1990",
  number =	 "CSRD 1058",
  month =	 "Dec",
  note =	 "28 pages",
  abstract =	 "This paper presents a framework we term auto-
		  scheduling, which brings together the control flow
		  and data flow models by combining most of the
		  advantages and excluding the major disadvantages of
		  the two familiar models. Auto-scheduling can be
		  viewed either as an abstract architectural model or
		  as a parallel program compilation framework. While
		  in ordinary environments parallel task creation and
		  scheduling is done by the operating system, or at
		  best the run-time library, in auto- scheduling task
		  creation and scheduling is performed by the user
		  program itself, making parallel processing
		  affordable at fine- granularity levels.Under
		  auto-scheduling the compiler does not only generate
		  object code, but it 'lends' its knowledge about a
		  program to the parallel instruction threads of that
		  program, allowing them to manage, activate, and
		  schedule themselves at run-time, without the need of
		  an external monitor. This is done by means of
		  special drive-code injected by the compiler to each
		  schedulable unit of a program (task, thread,etc). We
		  argue that auto-scheduling offers an optimal
		  approach for exploiting parallelism on real parallel
		  computer systems."
}

@Article{Ponamgi91a,
  author = 	 "M. Krish Ponamgi and Wenwey Hseush and Gail E.Kaiser",
  title = 	 "Debugging Multithreaded Programs with MPD",
  journal =	 "IEEE software",
  year =	 "1991",
  volume =	 "8",
  number =	 "3",
  pages =	 "37-??",
  month =	 "May",
  note =         "Also as Tech report CUCS-014-91 Columbia University,
		  Department of Computer Science, 1991",
  url = 	 "ftp://ftp.cs.columbia.edu/reports/reports-1991/cucs-013-91a/b.ps.Z",
  abstract =	 "We have developed a Multi-processor Debugger, MpD,
		  based on Data Path Expressions and Predecessor
		  Automata. Data Path Expressions (DPEs) is a debugger
		  command language for the programmer to express both
		  sequential and parallel patterns of events.
		  Predecessor Automata (PAs) are automatically
		  generated recognizers for DPEs. The programmer
		  specifies DPEs to represent event patterns that may
		  happen during program execution, and the PAs analyze
		  the combined event streams collected from the
		  executing threads to determine whether or not each
		  DPE was actually matched."
}

@MastersThesis{Ponamgi91b,
  author = 	 "M. Krish Ponamgi",
  title = 	 "MpD: A Multiprocessor Debugger",
  school = 	 "Columbia University, Department of Computer Science",
  year = 	 "1991",
  note =	 "Also as tech report CUCS-022-91",
  url =     "ftp://ftp.cs.columbia.edu/reports/reports-1991/cucs-022-91.ps.Z",
  abstract =	 "MpD is a multiprocessor C debugger designed for
		  multithreaded applications running under the Mach
		  operating system. MpD is built on top of gdb, an
		  existing sequential debugger. The MpD layer utilizes
		  the modeling language Data Path Expressions
		  developed by Hseush and Kaiser to provide a rich set
		  of commands to trace sequential and parallel
		  execution of a program. Associated with each DPE are
		  actions that allow access to useful trace variables
		  and I/O facilities. DPEs are useful for describing
		  sequential and concurrent patterns of events, to be
		  verified during execution. The patterns include
		  conditions such as synchronization, race conditions,
		  and wrongly classified sequential/concurrent
		  behavior.We show in this thesis Data Path
		  Expressions are a viable language for multiprocessor
		  debuggers."
}

@InProceedings{Powell91,
  author =       "M. L. Powell and Steve R. Kleiman and Steve Barton
		  and Devang Shah and Dan Stein and Mary Weeks",
  title =        "Sun{OS} Multi-thread Architecture",
  booktitle =    usenixw91,
  pages =        "65--80",
  address =      "Dallas, TX, USA",
  month =        jan,
  year =         "1991",
  note2 =	 "HAR",
  url = 	 "http://www.sun.com/sunsoft/Developer-products/sig/threads/papers/sunos_mt_arch.ps",
  abstract =     "The authors describe a model for multiple threads of
		  control within a single UNIX process. The main 
		  goals are to provide extremely lightweight threads
		  and to rationalize and extend the UNIX Application
		  Programming Interface for a multi-threaded
		  environment. The threads are intended to be
		  sufficiently lightweight so that there can be
		  thousands present and that synchronization and
		  context switching can be accomplished rapidly
		  without entering the kernel. These goals are
		  achieved by providing lightweight user-level threads
		  that are multiplexed on top of kernel-supported
		  threads of control. This architecture allows the
		  programmer to separate logical (program) concurrency
		  from the required real concurrency, which is
		  relatively costly, and to control both within a
		  single programming model."
}

@InProceedings{Prasadh91,
  author = 	 "R. Guru Prasadh and Chuan-Lin Wu",
  title = 	 "A benchmark evaluation of a multi-threaded RISC
		  processor architecture",
  volume =	 "I",
  pages =	 "84--91",
  booktitle =	 icpp91,
  year =	 "1991",
  month =	 "Aug",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Probst94,
  author = 	 "David K. Probst",
  title = 	 "Programming, Compiling and Executing
		  Partially-Ordered Instruction Streams on Scalable
		  Shared-Memory Multiprocessors ",
  volume =	 "?",
  pages =	 "??--??",
  booktitle =	 hicss27,
  year =	 "1994",
  abstract =	 "Performance in large-scale shared-memory
		  multiprocessors depends on finding a scalable
		  solution to the memory-latency problem. In this
		  paper, we show that protect consistency (PRC)
		  relaxes previous consistency models with two
		  distinct performance benefits. First, PRC is used to
		  expose and exploit more parallelism in the
		  computation, giving better support to latency
		  tolerance. Second, assuming that visible
		  synchronization directly coordinates changes in the
		  ability to write shared data, PRC is used to create
		  more situations where cached data are reusable,
		  giving better support to latency avoidance. The
		  paper evaluates PRC in the context of relaxing
		  intrathread dependences for multithreaded
		  architectures. After the PRC programming notation is
		  described, programming and compiling aspects are
		  examined, and architectural support is discussed."
}

@InProceedings{Quammen89a,
  author = 	 "Donna J. Quammen and D. Richard Muller and Daniel
		  Tabak",
  title = 	 "Register window management for a real-time
		  multitasking RISC",
  volume =	 "1",
  pages =	 "135--142",
  booktitle =	 hicss22,
  year =	 "1989",
  month =	 "jan",
  abstract =	 "An architecture is proposed that allows fast
		  procedure calls, low-overhead task switches, and
		  primitives, which assist in queue-oriented intertask
		  communications. This is accomplished by managing the
		  registers as noncontiguous register windows. The
		  details of the register granularity are hidden from
		  the applications program. The architecture is based
		  on a VLSI CPU called the MULTIS, which is capable of
		  handling the dynamically created data of multiple
		  tasks in on-chip storage. This ability enables
		  tasking systems to benefit from the use of large
		  on-chip memories such as those found in RISC
		  (reduced-instruction-set computer) technologies.
		  Other features of the architecture include efficient
		  interrupt handling and provision for register-based
		  task local, procedure-global dynamic storage."
}

@InProceedings{Quammen89b,
  author = 	 "Donna Quammen and D.Richard Miller",
  title = 	 "Register window architecture for multitasking
		  applications",
  pages =	 "57--66",
  booktitle =	 isca16,
  year =	 "1989",
  note =	 "Published in " # canews # " Vol.17, No.6,1989",
  note2 =	 "HAR",
  abstract = 	 "The organization of large register banks into
		  windows has been shown to be effective in enhancing
		  the performance of sequential programs. One drawback
		  of such an organization, which is of minor
		  importance to sequential languages, is the overhead
		  encountered when the register bank must be replaced
		  during a task switch. With concurrent language
		  paradigms, such as are found in Ada, Occam, and
		  Modula-2, these switches will be more frequent. The
		  authors introduce a methodology, and an
		  architecture, which greatly reduces this overhead
		  while maintaining the inherent advantages of the
		  register window approach. In addition, they present
		  ways of implementing traditional stacks and queues,
		  as well as hierarchical storage structures using
		  windows."
}

@InProceedings{Quammen91,
  author = 	 "Donna J. Quammen and D. Richard Miller",
  title = 	 "Flexible Register Management for Sequential
		  Programs",
  pages =	 "320--329",
  booktitle =	 isca18,
  year =	 "1991",
  month =	 "May",
  abstract =	 "Most current architectures have registers organized
		  in one of two ways: single register sets; or
		  register stacks, implemented as either overlapping
		  register windows or register-caches. Each has
		  particular strengths and weaknesses. For example, a
		  single register set excels over a stack if a program
		  requires frequent access to globals. However, a
		  register stack performs better if deep recursive
		  chains exist. One drawback of all current systems is
		  that the hardware limits the manner in which the
		  software can use registers. In this paper, a
		  register hardware organization called threaded
		  windows or t-windows, which is being developed by
		  the authors to enhance the performance of concurrent
		  systems, is evaluated for sequential programs. The
		  organization allows the registers to be dynamically
		  restructured in any of the above forms, and any
		  combination of the above forms. This permits the
		  compiler, or the programmer, to capitalize upon each
		  register organization's strong points and avoid
		  their disadvantages."
}

@InProceedings{Queinnec90g,
  author =       "Christian Queinnec",
  title =        "{P}oly{S}cheme : {A} {S}emantics for a {C}oncurrent
		  {S}cheme",
  booktitle =    "Workshop on High Performance and Parallel Computing
		  in Lisp",
  year =         "1990",
  address =      "Twickenham (UK)",
  month =        nov,
  abstract =     "The Scheme language does not fully specify the
		  semantics of combination: the evaluation order of
		  the terms composing a combination is left
		  indeterminate. We investigate in this paper a
		  different semantics for Scheme where terms of
		  combinations are evaluated concurrently. The
		  resulting semantics models a language with
		  concurrent threads sharing a common workspace. The
		  semantics is given in terms of denotational
		  semantics and uses resumptions as well as a choice
		  operator: {\it oneof\/} which mimics a scheduler. An
		  alternate definition for this operator lets appear
		  the classical powerdomains. The main interest of
		  this operator is to offer a formalization that can
		  be read with an operational point of view while
		  keeping a firm theoretical base. Scheme also offers
		  first class continuations with indefinite extent; we
		  examine some semantics for continuations with
		  respect to concurrency. Each of these semantics is a
		  natural extension of the sequential case of regular
		  Scheme. Still they strongly differ in their observed
		  behaviours. The resulting language, named
		  PolyScheme, offers much of the features of current
		  concurrent Lisp (or Scheme) dialects thanks to the
		  sole extension of its combination semantics and
		  without any explicit specialized construct dealing
		  with concurrency."
}

@InProceedings{RChen89,
  author = 	 "Raymond C. Chen and Parthas Dasgupta",
  title = 	 "Linking Consistency with Object/Thread
		  Semantics: An Approach to Robust Computation",
  pages =	 "121-128",
  publisher =	 "IEEE Comput. Soc. Press",
  booktitle =	 icdcs9,
  address =	 "Newport Beach, CA, USA",
  year =	 "1989",
  month =	 "Jun",
  note2 =	 "HAR",
  url =  	 "ftp://helios.cc.gatech.edu/pub/papers/consistency.ps.Z",
  abstract =	 "An object/thread based paradigm is presented that
		  links data consistency with object/thread semantics.
		  The paradigm can be used to achieve a wide range of
		  consistency semantics from strict supports three
		  types of data consistency. Object programmers
		  indicate the type of consistency desired on a
		  per-operation basis, and the system performs
		  automatic concurrency control and recovery
		  management to ensure that those consistency
		  requirements are met. This allows programmers to
		  customize consistency and recovery on a
		  per-application basis without having to supply
		  complicated, custom recovery management schemes. The
		  paradigm allows robust and nonrobust computation to
		  operate concurrently on the same data in a
		  well-defined manner. The operating system need
		  support only one vehicle of computation-the thread."
}

@InProceedings{Ramachandran89,
  author = 	 "Umakishore Ramachandran and Mustaque Ahamad and
		  Yousef M. Khalidi",
  title = 	 "Coherence of distributed shared memory - Unifying
		  synchronization and data transfer",
  volume =	 "2",
  pages =	 "160--169",
  booktitle =	 icpp89,
  year =	 "1989",
  month =	 "Aug",
  note = 	 "10 pages",
  abstract =	 "Clouds is a distributed operating systems research
		  project. With threads and passive objects as the
		  primary building blocks, Clouds provides a
		  location-transparent protected procedure-call
		  interface to system services. The primary kernel
		  mechanism in Clouds is the mapping of the object
		  into the address space of the invoking thread.
		  Hence, the performance of such systems depends
		  crucially on the efficiency of memory mapping. The
		  problem is exacerbated with distribution, since the
		  object invoked by a thread may be located on a
		  remote node. Since a thread can potentially invoke
		  any object, the virtual address spaces of all
		  objects can be viewed as constituting a global
		  distributed shared memory. An organization and
		  mechanisms for supporting this abstraction of a
		  distributed shared memory are presented. The authors
		  propose a distributed shared-memory controller that
		  provides mechanisms for efficient access and
		  consistency maintenance of the distributed shared
		  memory. The novel feature of this approach is the
		  use of process synchronization to simplify
		  consistency maintenance. The distributed
		  shared-memory mechanisms serve as the backbone for
		  implementing object invocation, synchronization
		  mechanisms, and networkwide memory management in the
		  Clouds system."
}

@MastersThesis{Rangarajan91,
  author = 	 "Ganesh Rangarajan",
  title = 	 "A library implementation of POSIX threads",
  school = 	 "Florida State University, Department of Computer
		  Science",
  year = 	 "1991",
  month =	 "Jul",
  abstract =	 ""
}

@Article{Rashid86,
  author = 	 "Richard F. Rashid",
  title = 	 "Threads of a New System",
  journal =	 "Unix Review",
  year =	 "1986",
  pages =	 "37--49",
  month =	 "Aug",
  abstract =	 ""
}

@InProceedings{Rashid89,
  author = 	 "Richard Rashid and Daniel Julin and Douglas Orr and
		  Richard Sanzi and Robert Baron and Alesandro Forin
		  and David Golub and Michael B. Jones",
  title = 	 "Mach: a system software kernel",
  pages =	 "176-178",
  booktitle =	 compcon89,
  year =	 "1989",
  publisher =	 "IEEE Comput. Soc. Press",
  address =	 "San Francisco, CA, USA",
  month =	 "Feb",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/syskernel.ps",
  abstract =	 "The Mach operating system can be used as a system
		  software kernel which can support a variety of
		  operating system efficiently support system software
		  include integrated virtual memory management and
		  interprocess communication, multiple threads of
		  control within on address space, support for
		  transparent system trap callout, and an object
		  programming facility integrated with the Mach IPC
		  mechanisms."
}

@TechReport{Reppy91a,
  author =       "John H. Reppy",
  title =        "{Concurrent Programming with Events -- The
		  Concurrent ML Manual}",
  type =         "Reference Manual",
  number =       "Version 0.9.6",
  institution =  "Department of Computer Science, Cornell University",
  address =      "Ithaca, NY",
  month =        oct,
  year =         "1991",
  abstract =     "Concurrent ML (CML) is a system for concurrent
		  programming in Standard ML (SML). A CML program
		  consists of a set of {\em threads\/} (or
		  light-weight processes). A thread is the sequential
		  evaluation of a ML expression. It does not have to
		  be a terminating computation; in fact, infinitely
		  looping threads are often useful. The evaluation of
		  a thread may involve communication with other
		  threads, which is done by sending a message on a
		  channel. Message passing is synchronous and forms
		  the basis of communication and synchronization in
		  CML. This model is extended by {\em first-class
		  synchronous operations\/}, which provide a mechanism
		  for building new synchronization and communication
		  abstractions."
}

@TechReport{Reppy91b,
  author =       "John H. Reppy and Emden R. Gansner",
  title =        "{eXene}: {A} Multi-Threaded {X} Window System
		  Toolkit",
  address =      "Ithaca, NY 14853",
  institution =  "Department of Computer Science, Cornell University",
  note =         "In preparation, email jhr at research.att.com.",
  year =         "1991",
  abstract =	 ""
}

@Article{Reynolds90,
  author = 	 "Franklin D. Reynolds and J. Duane Northcutt and E.
		  Douglas Jensen and R. K. Clark and S. E. Shipman
		  and B. Dasarathy and D. P. Maynard",
  title = 	 "Threads: a programming construct for reliable
		  real-time distributed computing",
  journal =	 "International Journal of Mini and Microcomputers",
  year =	 "1990",
  volume =	 "12",
  number =	 "3",
  pages =	 "119--127",
  note =	 "Also in Proceedings of the ISMM International
		  Conference. Parallel and Distributed Computing, and
		  Systems, Oct 1990,  pp. 299--304, Published by Acta
		  Press",
  abstract =	 "This article describes the thread programming
		  abstraction and related facilities provided by the
		  kernel of the Alpha operating system. The design of
		  threads abstraction was driven by the need for an
		  integrated set of services to construct coherent,
		  distributed, real-time applications. An Alpha thread
		  is an active entity similar in some ways to
		  lightweight tasks. However, Alpha threads, unlike
		  Mach threads, for example, are not bound to any
		  particular address space and can migrate from one to
		  another by invoking operations o objects. Object
		  invocations are insensitive to the network location
		  of the object. Invocations are synchronous and
		  nested, similar to commonly found RPC mechanisms
		  layered on top of message-oriented operating
		  systems. Invocations, however, propagate the
		  identity and attributes of the thread, such as its
		  importance, urgency, and reliability requirements;
		  these service requirements are used by Alpha to
		  facilitate distributed (i.e., trans-node) resource
		  management. A collection of services is provided by
		  Alpha to enhance concurrency and reliability of
		  computations using threads in a distributed
		  real-time environment."
}

@TechReport{Ritchie93,
  author =       "Stuart Ritchie",
  title =        "The Raven Kernel: a Microkernel for Shared Memory
		  Multiprocessors",
  institution =  "University of British Columbia, Department of
		  Computer Science",
  number =       "TR-93-36",
  pages =        "166",
  month =        "30 " # apr,
  year =         "1993",
  available  =   "ftp://ftp.cs.ubc.ca/ftp/local/techreports/1993/TR-93-36.ps",
  abstract =     "The Raven kernel is a small, lightweight operating
		  system for shared memory multiprocessors. Raven is
		  characterized by its movement of several traditional
		  kernel abstractions into user space. The kernel
		  itself implements tasks, virtual memory management,
		  and low level exception dispatching. All thread
		  management, device drivers, and message passing
		  functions are implemented completely in user
		  space. This movement of typical kernel-level
		  abstractions into user space can drastically reduce
		  the overall number of user/kernel interactions for
		  fine-grained parallel applications."
}

@TechReport{Rogers92,
  author = 	 "Anne Rogers and John H. Reppy and Laurie Hendren",
  title = 	 "Supporting SPMD execution for dynamic data
		  structures",
  institution =  "Princeton University, Dept. of Computer Science",
  year = 	 "1992",
  number =	 "CS-TR-374-92",
  note =	 "Also in Conf Record of the 5th Workshop on Languages
		  and Compilers for Parallel Computing
		  pp.123-134,1992",
  note2 =        "Also in Languages and Compilers for Parallelism '92,
		  Springer-Verlag 1993 (Lecture Notes in Computer
		  Science ???)",
  url = 	 "http://www.cs.princeton.edu:80/TR/PRINCETONCS:TR-374-92",
  abstract =	 "In this paper, we address the problem of supporting
		  SPMD execution of programs that use
		  recursively-defined dynamic data structures on
		  distributed memory machines. The techniques
		  developed for supporting SPMD execution of
		  array-based programs rely on the fact that arrays
		  are statically defined and directly addressable. As
		  a result, these techniques do not apply to recursive
		  data structures, which are neither statically
		  defined nor directly addressable. We propose a three
		  pronged approach. First, we describe a simple
		  mechanism for migrating a thread of control based on
		  the layout of heap allocated data. Second, we
		  explain how to introduce parallelism into the model
		  using a technique based on futures and lazy task
		  creation [MKH91].Third, we present the compiler
		  analyses and parallelization techniques that are
		  required to exploit the proposed mechanism."
}

@TechReport{Rogers94,
  author = 	 "Anne Rogers and Martin C. Carlisle and John H. Reppy
		  and Laurie J.Hendren",
  title = 	 "Supporting dynamic data structures on distributed
		  memory machines",
  institution =  "Princeton University, Dept. of Computer Science",
  year = 	 "1994",
  number =	 "CS-TR-447-94",
  month =	 "Feb",
  note =	 "34 pages",
  url =  	 "http://www.cs.princeton.edu:80/TR/PRINCETONCS:TR-447-94",
  abstract = 	 "Compiling for distributed memory machines has been a
		  very active research area in recent years. Much of
		  this work has concentrated on programs that use
		  arrays as their primary data structures. To date,
		  little work has been done to address the problem of
		  supporting programs that use dynamic data
		  structures. The techniques developed for supporting
		  SPMD execution of array-based programs rely on the
		  fact that arrays are statically defined and directly
		  addressable. These techniques do not apply to
		  recursive data structures, which are neither
		  statically defined nor directly addressable. In this
		  paper, we describe a three part approach to
		  supporting programs that use dynamic data
		  structures. First, we use a simple mechanism for
		  migrating a thread of control based on the layout of
		  heap-allocated data. Second, we introduce
		  parallelism into the model using a technique based
		  on futures and lazy task creation. Third, we exploit
		  this execution model using compiler analyses and
		  parallelization techniques. We have implemented a
		  prototype system, which we call {\em Olden}, that
		  runs on the Intel iPSC/860 and the Thinking Machines
		  CM5. We discuss our implementation and report on
		  experiments with four benchmarks." 
}

@InProceedings{Roh94,
  author = 	 "Lucas Roh and Walid A. Najjar and Bhanu Shankar and
		  A. P. Willem Bohm",
  title = 	 "An evaluation of optimized threaded code
		  generation",
  pages =	 "37--46",
  booktitle =	 pact94,
  year =	 "1994",
  month =	 "Aug",
  abstract =	 "Multithreaded architectures hold many promises: the
		  exploitation of intra-thread locality and the
		  latency tolerance of multithreaded synchronization
		  can result in a more efficient processor utilization
		  and higher scalability. The challenge for a code
		  generation scheme is to make effective use of the
		  underlying hardware by generating large threads with
		  a large degree of internal locality without limiting
		  the program level parallelism. Top-down code
		  generation, where threads are created directly from
		  the compiler's intermediate form, is effective at
		  creating a relatively large thread. However, having
		  only a limited view of the code at any one time
		  limits the thread size. These top-down generated
		  threads can therefore be optimized by global,
		  bottom-up optimization techniques. In this paper, we
		  present such bottom-up optimizations and evaluate
		  their effectiveness in terms of overall performance
		  and specific thread characteristics such as size,
		  length, instruction level parallelism, number of
		  inputs and synchronization costs."
}

@InProceedings{Roh95,
  author = 	 "Lucas Roh and Walid A. Najjar",
  title = 	 "Analysis of Communications and Overhead Reduction
		  in Multithreading Execution",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "Jun",
  abstract =	 ""
}

@Article{Russel93,
  author = 	 "Gordon Russel and Paul Shaw",
  title = 	 "Shifting register windows",
  journal =	 ieeemicro,
  year =	 "1993",
  pages =	 "28--35",
  month =	 "Aug",
  note =	 "Also as tech report University of Strathclyde,
		  Department of Computer Science",
  note2 =	 "HAR",
  abstract =	 "Shifting register windows is a new register
		  windowing method that attempts to overcome some of
		  the difficulties of traditional fixed-and
		  variable-sized schemes. Using fewer register
		  elements than a seven-window Sparc organization,
		  shifting register windows more than halves
		  spill/refill memory traffic and reduces visible
		  spill/refill cycles by an order of magnitute. In
		  addition, shifting register windows, a scheme based
		  on fast hardware stack and register-memory
		  dribbling, has a very short register bus length. It
		  also zeros registers as they are being allocated,
		  making a common initialization unneccesary"
}

@InProceedings{Saavedra90,
  author = 	 "Rafael H. Saavedra-Barrera and David E. Culler and
		  Thorsten von Eicken",
  title = 	 "Analysis of Multithreaded Architectures for Parallel
		  Computing",
  booktitle   =  "SPAA '90. 2nd Annual ACM Symposium on Parallel
		  Algorithms and Architectures.",
  year = 	 "1990",
  month = 	 "Jul",
  pages = 	 "169--178",
  note = 	 "Also as Tech report UCB-CSD-90-569, University of
		  California Berkeley, Department of Computer
		  Science",
  note2 =	 "HAR",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-90-569",
  available2=   "ftp://ftp.cs.berkeley.edu/ucb/TAM/spaa90.ps",
  abstract=      "Multithreading has been proposed as an architectural
		  strategy for tolerating latency on multiprocessors and,
		  through limited empirical studies shows to offer
		  promise. This paper develops an analytical models of
		  multi threaded processor behavior based on a small
		  set of architectural and program parameters. The
		  model gives rise to a large Markov chain, which is
		  solved to obtain a formula for processor in terms of
		  the number of threads). transition, and saturation
		  efficiency depends only on the remorse reference
		  rate and switch case. Formulas for regime boundaries
		  are derived. The model is embellished to reflect
		  cache degradation due to multithreading, using an
		  analytical model of cache behavior, demonstrating
		  that returns diminish as the number threads becomes
		  large. Predictions from the embellished model
		  correlate will with published empirical
		  measurements. Prescriptive use of the model under
		  various scenarios indicates that multithreading is
		  effective But the number of useful threads per
		  processor is fairly small."
}

@TechReport{Saavedra91,
  author = 	 "Rafael H. Saavedra-Barrera and David E. Culler",
  title = 	 "An Analytical Solution for a Markov Chain Modeling
		  Multithreaded Execution",
  institution =  "University of California Berkeley, Department of
		  Computer Science",
  year = 	 "1991",
  number = 	 "UCB-CSD-91-623",
  month = 	 "Apr",
  note =         "24 pages",
  note2 =	 "HAR",
  url =     "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-91-623",
  abstract =     "Multithreading is an architectural technique aimed
		  at maintaining high processor utilization in the
		  presence of large memory or interprocessor
		  communication latency. While waiting for a remote
		  reference to complete, the processor switches to
		  another execution thread. Several realizations of
		  this concept have been proposed, but little data is
		  available on the actual costs and benefits. This
		  paper presents an analytical model of multithreaded
		  execution, which may serve to guide and explain
		  empirical studies. The model is based on three key
		  parameters: thread run-length, switch cost, and
		  latency. A closed-form expression for processor
		  utilization is obtained for deterministic and
		  stochastic run-lengths. The derivation involves
		  identifying specific patterns in the very large set
		  of equations forming the Markov chain. Using this
		  result, three operating regimes are identified for a
		  multithreaded processor subject to long latencies;
		  linear, where utiliation is proportional to the
		  number of threads per processor."
}

@MastersThesis{Sah91,
 author = 	 "Anuragh Sah",
 title = 	 "Parallel Language Support on Shared Memory Multiprocessor",
 school = 	 "University of California, Berkeley - Department Of
		  computer Science",
 year = 	 "1991",
 month = 	 "May",
 url =     "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-91-631",  
 abstract =      "The study of general purpose parallel computing
		  requires efficient and inexpensive platforms for
		  parallel program execution. This helps in ascertaining
		  tradeoff choices between hardware complexity and
		  software solutions for massively paralel systems
		  design. In this report, we present an implementation
		  of an efficient parallel execution model on shared
		  memory multiprocessors based on a Threaded Abstract
		  Machine. We discuss a k-way generalized locking
		  strategy suitable for our model. We study the
		  performance gains obtained by a queuing strategy
		  whicwhich uses multiple queues with reduced access
		  contention. We also present perforrformance models
		  in shared memory machines, related to lock
		  contention and serialization in shared memory
		  allocation. A bia-based memory management technique
		  which reduces the serialization is presented. These
		  issues are critical for obtaining an efficient
		  parallel execution environment"
}

@InProceedings{Saka91,
  author = 	 "Shuichi Sakai and Yuetsu Kodama and Yoshinori
		  Yamaguchi",
  title = 	 "Prototype implementation of a highly parallel
		  dataflow machine EM-4",
  pages =	 "278--286",
  booktitle =	 ipps5,
  year =	 "1991",
  month =	 "Apr",
  abstract =	 "The paper presents the implementation of the EM-4
		  prototype and reports initial performance
		  evaluation. The EM-4 is a highly parallel computer
		  whose design objectives are: to develop a feasible
		  parallel computer with more than 1000 processing
		  elements (PEs); and to pursue efficiency by
		  improving dataflow architectures. Key features of
		  the EM-4 are: (1) a strongly connected arc dataflow
		  model; (2) a Multiple-RISC concept; (3) a dataflow
		  single chip processor EMC-R and (4) a versatile
		  interconnection network with extra facilities. As a
		  first step the EM-4 prototype was implemented with
		  80 PEs. The EM-4 prototype has been fully
		  operational since May 1990, with peak performance of
		  1 GIPS and peak network performance of 14.63 GB/s.
		  It has performed 824 MIPS in the calculation of pi."
}

@InProceedings{Sakai89,
  author = 	 "Shuichi Sakai and Yoshinori Yamaguchi and J. Hiraki and
		  Yuetsu Kodama and Tositsugu Yuba",
  title = 	 "An architecture of a dataflow single chip processor",
  pages =	 "46--53",
  booktitle =	 isca16,
  year =	 "1989",
  month =	 "Jun",
  abstract =	 ""
}

@Article{Sakai90,
  author = 	 "Shuchi Sakai and Yoshinori Yamaguchi and K. Hiraki
		  and Yuetzu Kodama and Tositsugu Yuba",
  title = 	 "Design of the dataflow single-chip processor EMC-R",
  journal =	 "Journal of Information Processing",
  year =	 "1990",
  volume =	 "13",
  number =	 "2",
  pages =	 "165--173",
  abstract =	 "This paper presents the design of the dataflow
		  single-chip processor EMC-R, from the viewpoint of
		  advanced dataflow schemes and their implementations.
		  The EMC-R is a component chip of a highly parallel
		  (with more than a thousand processors) dataflow
		  machine, the EM-4. The distinctive features of the
		  chip are: (1) a refined dataflow model called a
		  strongly connected arc model, (2) two simple and
		  fast synchronization mechanisms, (3) a versatile
		  pipeline design, (4) a RISC-based architecture, (5)
		  a packet-switching unit design with extra facilities
		  and (6) a maintenance architecture for monitoring
		  the system. After these features have been examined,
		  the configuration architecture of the EMC-R that
		  makes them possible is shown. There are six units on
		  the chip: a switching unit, an input buffer unit, a
		  fetch and matching unit, an execution unit, a memory
		  control unit and a maintenance controller. The EMC-R
		  is a CMOS gate array chip containing 45, 788 CMOS
		  gates and using 255 signal pins. It is mounted on a
		  PGA ceramic package. The EM-4 prototype system with
		  80 EMC-Rs is available now. Its hardware system was
		  completed in April 1990. The purposes of the
		  prototype are (1) to evaluate several architectural
		  aspects by measuring dynamic characteristics of
		  practical programs, (2) to confirm the architectural
		  design, and (3) to provide an environment for
		  software development. Peak performance is 6.3
		  MIPS/chip."
}

@TechReport{Sakai92,
  author = 	 "Shuichi Sakai",
  title = 	 "Synchronization and Pipeline Design for a
		  Multithreaded Massively Parallel Computer",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  number =	 "CSG Memo 343-1",
  month =	 "Mar",
  note2 =	 "HAR",
  url = 	 "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-343-1.ps",
  abstract =	 "This paper examines two basic functions in a
		  massively parallel computer - synchronizations and
		  pipelining - and proposes efficient
		  implementations. The data-driven synchronization
		  mechanisms which currently exist are carefully
		  analyzed from the viewpoint of efficiency and
		  hardware complexity, and the optimized
		  synchronization mechanism is proposed. The pipeline
		  structure for a massively parallel computer
		  containing the proposed synchronization is
		  presented. Performance improvement methods for this
		  pipeline are proposed, cost-effectiveness of the
		  proposed method is considered, and related issues
		  are listed. Lastly, future problems including
		  software issues are presented"
}

@Article{Sakai93a,
  author = 	 "Shuichi Sakai and Yuetsu Kodama and Yoshinori
		  Yamaguchi",
  title = 	 "Design and implementation of a circular omega
		  network in the EM-4",
  journal =	 parcomp,
  year =	 "1993",
  volume =	 "19",
  number =	 "2",
  pages =	 "125--142",
  month =	 "Feb",
  abstract =	 "Presents the design principles and prototype
		  implementation of an interconnection network in a
		  highly parallel computer EM-4, that will have more
		  than a thousand processing elements. The
		  interconnection network of the EM-4 prototype adopts
		  a circular omega topology. The paper first examines
		  the features of this topology, then proposes a node
		  grouping method, a node addressing method and a
		  self-routing algorithm on each node. Then
		  store-and-forward deadlock prevention mechanisms are
		  proposed, and automatic load distribution mechanisms
		  attached to this network are presented. Next,
		  network implementation in the EM-4 prototype system
		  is described. The network consists of distributedly
		  controlled switching units and connection lines, and
		  actually performs 14.63 GB/s. The switching unit was
		  implemented as a unit of the EMC-R and performs
		  packet communication concurrently with and
		  independently of the other functional units on the
		  chip."
}

@InProceedings{Sakai93b,
  author = 	 "Shuichi Sakai and Kazuaki Okamoto and Hiroshi
		  Matsuoka and Hideo Hirono and Yuetsu Kodama and
		  Mitshuhisa Sato",
  title = 	 "Super-threading: Architectural and software
		  mechanisms for optimizing parallel computation",
  pages =	 "251--260",
  booktitle =	 ics93,
  year =	 "1993",
  month =	 "Jul",
  abstract =	 ""
}

@techreport{Samiotakis88,
  author =      "Yiannis Samiotakis and Karsten Schwan",
  title=        "A Thread Library for the BBN Butterfly Multiprocessor",
  institution = "Ohio State University, Department of Computer Science",
  year   =      "1988",
  month =       "Jun",
  number =      "OSU-CISRC-6/88-TR19",
  abstract =    ""
}

@TechReport{Samiotakis89a,
  author = 	 "Yannis Samiotakis",
  title = 	 "User interface of a thread library for the GP1000
		  multiprocessor",
  institution =  "Ohio State University, Computer and Information
		  Science Research Center",
  year = 	 "1989",
  number =	 "OSU-CISRC-4/89-TR14",
  month =	 "Apr",
  note =	 "35 pages",
  abstract =	 ""
}

@mastersthesis{Samiotakis89b,
  author  =      "Yiannis Samiotakis",
  title =        "A Thread Library for a Non Uniform Memory Access
		  Multiprocessor",
  school  =      "Ohio State University, Department of Computer Science",
  year    =      "1989",
  abstract =     ""
}

@TechReport{Samiotakis89c,
  author = 	 "Yannis Samiotakis and Prasad Vishnubhotla",
  title = 	 "Algorithms and internal organization of a thread
		  library for Non Uniform Memory Access
		  multiprocessors",
  institution =  "Ohio State University. Computer and Information
		  Science Research Center",
  year = 	 "1989",
  number =	 "OSU-CISRC-6/89-TR20",
  month =	 "Jun",
  abstract =	 "Carnegie-Mellon University Mach threads have been
		  well accepted as a paradigm for lightweight
		  processes but they have been implemented only on
		  uniprocessors and Uniform Memory Access (UMA)
		  multiprocessors such as the Encore Multimax. This
		  paper describes an implementation of a Mach thread
		  library designed for Non Uniform Memory Access
		  (NUMA) multiprocessor architectures such as the BBN
		  Butterfly. In NUMA architectures, remote memory
		  references are much more expensive than local memory
		  references. This should be considered in the design
		  and implementation of a thread library. Firstly, the
		  thread library should provide primitives that allow
		  user programs to organize threads such that locality
		  can be exploited.Secondly, the implementation of the
		  thread library itself should be organized so that
		  the code within the thread library exploits
		  locality. The thread library that we describe in
		  this paper satisfies these requirements. The
		  implementation algorithms presented are general and
		  are not limited to the Butterfly. Experimental
		  performance results are presented for a 32-node
		  Butterfly GP1000."
}

@TechReport{Sang92,
  author = 	 "Janche Sang and K. Chung and Vernon Rego",
  title = 	 "Si:  A Simulation Package based on Lightweight
		  Processes",
  institution =  "Purdue University, Department of Computer Sciences",
  year = 	 "1992",
  number =	 "92-085",
  month =	 "Nov",
  abstract =	 ""
}

@TechReport{Sang93a,
  author = 	 "Janche Sang and Felipe Knop and Vernon Rego",
  title = 	 "Design and Implementation of a Threads library",
  institution =  "Purdue University, Department of Computer Sciences",
  year = 	 "1993",
  month =	 "Jul",
  number =	 "93-043",
  abstract =	 ""
}

@TechReport{Sang93b,
  author = 	 "Janche Sang and Vernon Rego",
  title = 	 "Efficient Implementation of Thread Migration on
		  Distributed-Memory Multi-processors",
  institution =  "Purdue University, Department of Computer Sciences",
  year = 	 "1993",
  number =	 "93-065",
  month =	 "Oct",
  note =	 "",
  abstract =	 ""
}

@InProceedings{Sang94,
  author = 	 "Janche Sang and Geoffrey W. Peters and Vernon Rego",
  title = 	 "Thread Migration on Heterogeneous Systems via
		  Compile-Time Transformation",
  pages =	 "??--??",
  booktitle =	 "Proceedings of the International Conference on
		  Parallel and Distributed Systems (ICPADS'94)",
  year =	 "1994",
  abstract =	 "This paper describes a technique to provide
		  multithreading in an enhanced C language. In
		  contrast to the traditional design of a thread
		  library, which usually utilizes a few lines of
		  assembly code to effect context-switching between
		  threads, the technique we use is based on
		  compile-time program transformations and a run-time
		  library. Since this approach transforms a thread's
		  physical states into logical forms, thread migration
		  in a heterogeneous distributed environment becomes
		  practically feasible. Performance measurements of
		  the current implementation are reported. "
}

@InProceedings{Sato92,
  author = 	 "Mitsuhisa Sato and Yuetzu Kodama and Shuichi Sakai
		  and Yoshinori Yamaguchi and Yasuhito Komura",
  title = 	 "Thread-based programming for the EM-4 hybrid
		  dataflow machine",
  pages =	 "146--155",
  booktitle =	 isca19,
  year =	 "1992",
  address =	 "Gold Coast, Australia",
  month =	 "May",
  note =	 "Published in " # canews # "Vol.20, No.2, May 1992",
  abstract =	 "The paper presents a thread-based programming model
		  for the EM-4 hybrid dataflow machine, where
		  parallelism and synchronization among threads of
		  sequential execution are described explicitly by the
		  programmer. Although EM-4 was originally designed as
		  a dataflow machine, the paper demonstrates that it
		  provides effective architectural support for a
		  variety of programming styles, including message
		  passing and distributed data sharing in imperative
		  languages. The approach allows the programmer to
		  control the parallelism and maintain data locality
		  explicitly to achieve high performance. EM-4 can be
		  thought of as a multi-threaded architecture that can
		  exploit both von Neumann and dataflow compiling
		  technology. Thread-based programming provides the
		  first step to explore better programming/compiling
		  technology for a hybrid dataflow machine as well as
		  EM-4."
}

@InProceedings{Sato94,
  author = 	 "Mitsuhisa Sato and Yuetsu Kodama and Shuichi Sakai
		  and Yoshinori Yamaguchi",
  title = 	 "Experience with executing shared memory programs
		  using fine-grain communication and multithreading in
		  EM-4",
  pages =	 "630--636",
  booktitle =	 ipps8,
  year =	 "1994",
  address =	 "Cancun, Mexico",
  month =	 "Apr",
  abstract =	 "We present our experience and results obtained from
		  executing shared memory application programs using
		  fine-grain remote  memory access communication and
		  multithreading in the EM-4 multiprocessor. The EM-4
		  is a distributed memory multiprocessor which has a
		  dataflow mechanism. The dataflow mechanism enables a
		  fine-grain communication packet through the network
		  to invoke the thread of control dynamically with
		  very small overhead and is extended to access remote
		  memory in different processors. We hide the remote
		  memory access latencies with multithreading. The
		  benchmark results show that shared memory
		  applications achieve reasonable speedup with four to
		  eight threads in the EM-4 prototype. We found that
		  aggressive multithreading can negatively affect its
		  network interface and increase the network
		  contention. We also describe the EM-4 parallel
		  programming language called EM-C, which provides the
		  notion of a global address space and parallel
		  constructs for exploiting medium-grain parallelism
		  to tolerate several remote operation latencies."
}

@InProceedings{Saxena93,
  author = 	 "Sunil Saxena and J. Kent Peacock and Fred Yang and
		 Vijaya Verma and Mohan Krishan",
  title = 	 "Pitfalls in multithreading {SVR4 STREAMS} and other
		  weightless processes",
  pages =	 "85--96",
  booktitle =	 usenixw93,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "Jan",
  abstract =	 "As part of the effort of creating a multiprocessor
		  version of System V Release 4, the Intel
		  Multiprocessor Consortium attempted to multithread
		  the kernel STREAMS subsystem. STREAMS are a System V
		  facility which provides a message-based
		  communications framework, primarily for use in
		  providing pipe-like configurability for character
		  devices. This effort represents an interesting case
		  study for the type of difficulties encountered in
		  multithreading a complex subsystem. In particular, a
		  very fine-grained multithreading strategy was tried
		  first and found to have undesirable deadlock,
		  performance and stability characteristics.
		  Subsequent versions allowed less apparent
		  parallelism, but actually improved all of these
		  properties. The root cause of many of the problems
		  encountered was the existence of 'weightless
		  processes', that is, control threads which do not
		  have their own processor stacks. Examples include
		  interrupts, timeouts and STREAMS processing. The
		  major drawback to weightless processes is their
		  inability to suspend execution to wait for an event
		  or resource, thus making them susceptible to
		  deadlock. A number of examples of weightless process
		  deadlocks are explored to illustrate the
		  disadvantages of this approach, particularly in a
		  multiprocessor system."
}

@InProceedings{Schauser91a,
  author =       "Klaus Erik Schauser and David E. Culler and
		  Thorsten von Eicken",
  title =        "Compiler-Controlled Multithreading for Lenient
		  Parallel Languages",
  booktitle =    "Functional Programming Languages and Computer
		  Architecture. 5th ACM Conference Proceedings",
  address =      "Harvard, MA, USA",
  year =         "1991",
  series =       "Lecture Notes in Computer Science 523",
  publisher =    "Springer-Verlag",
  pages =        "50--71",
  note   =       "Also as Tech report UCB-CSD-91-640, University of
		  California, Berkeley. Computer Science Division",
  note2 =	 "HAR",
  available  =   "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-91-640",
  available2=   "ftp://ftp.cs.berkeley.edu/ucb/TAM/fpca91.ps",
  abstract =     "Tolerance to communication latency and inexpensive
		  synchronization are critical for general-purpose
		  computing on large multiprocessors. Fast dynamic
		  scheduling is required for powerful non-strict
		  parallel languages. However, machines that support
		  rapid switching between multiple execution threads
		  remain a design challenge. This paper explores how
		  multithreaded execution can be addressed as a
		  compilation problem, to achieve switching rates
		  approaching what hardware mechanisms might
		  provide. Compiler-controlled multithreading is
		  examined through compilation of a lenient parallel
		  language, Id90, for a threaded abstract machine,
		  TAM. A key feature of TAM is that synchronization is
		  explicit and occurs only at the start of a thread,
		  so that a simple model can be applied. A scheduling
		  hierarchy allows the compiler to schedule logically
		  related threads closely together in time and to use
		  registers across threads. Remote communication is
		  via message sends and split-phase memory
		  accesses. Messages and memory replies are received
		  by compiler-generated message handlers which rapidly
		  integrate these events with thread scheduling. To
		  compile Id90 for TAM, we employ a new parallel
		  intermediate form, dual-graphs, with distinct
		  control and data arcs. This provides a clean
		  framework for partitioning the program into threads,
		  scheduling threads, and managing registers under
		  asynchronous execution. The compilation process is
		  described and preliminary measurements of its
		  effectiveness are discussed. Dynamic execution
		  measurements are obtained via a second compilation
		  step, which translates TAM into native code for
		  existing machines with instrumentation
		  incorporated. These measurements show that the cost
		  of compiler-controlled multithreading is within a
		  small factor of the cost of control flow in
		  sequential languages."
}

@MastersThesis{Schauser91b,
  author =       "Klaus Erik Schauser",
  title =        "Compiling Dataflow into Threads : Efficient
		  Compiler-Controlled Multithreading for Lenient
		  Parallel Languages",
  year  =        "1991",
  month =        "Jul",
  school =       "University of California - Berkeley, Department of
		  Computer Science",
  note =	 "71 Pages. Also as Tech report UCB-CSD-91-644",
  available  =   "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-91-644",
  abstract =     "Powerful non-strict parallel languages require fast
		  dynamic scheduling. This thesis explores how the
		  need for multithreaded execution can be addressed
		  ada compilation problem, to achieve switching rates
		  approaching what hardware mechanisms might provide.
		  Compiler-controlled multithreading is examined
		  through compilation of a lenient parallel language,
		  for a threaded abstract machine, TAM. A key feature
		  of TAM is that synchronization is explicit and
		  occurs only at the start of a thread, so that a
		  simple cost model can be applied. A scheduling
		  hierarchy allows the compiler to schedule logically
		  related threads closely together in time and to use
		  registers across threads.  Remote communication is
		  via message sends and split-phase memory accesses.
		  Messages and memory  replies are received by
		  compiler-generated message handlers which rapidly
		  integrate these events with thread scheduling. To
		  compile ILD90 for TAM, we employ a  new parallel
		  intermediate form, dual-graphs, with distinct
		  control and data arcs. This provides a clean clean
		  framework for partitioning the program into threads,
		  scheduling threads, and managing registers under
		  asynchronous execution. The compilation process is
		  described and preliminary measurements of the
		  effectiveness of the approach are discussed.
		  Previous to this work, execution of Id90 programs
		  was limited to specialized architectures or dataflow
		  graph interpreters. By compiling via TAM, we have
		  achieved more than two orders of magnitude
		  performance improvement over graph interpreters on
		  conventional machines, making this Id90
		  implementation competitive with machines supporting
		  dynamic instruction scheduling in hardware. Timing
		  measurements show that our Id90 implementation on a
		  standard RISC can achieve a performance close to
		  Id90 on one processor of the recent dataflow machine
		  Monsoon. It can be seen that the TAM partitioning
		  presented in this thesis reduces the control
		  overhead substantially and that more aggressive
		  partitioning would yield modest additional benefit.
		  There is, however, considerable room for improvement
		  in sceduling and register management."
}

@PhdThesis{Schauser94,
  author = 	 "Klaus Erik Schauser",
  title = 	 "Compiling Lenient Languages for Parallel
		  Asynchronous Execution",
  school = 	 "University of California at Berkeley, Department of
		  Computer Science",
  year = 	 "1994",
  note =	 "234 pages. As Tech report CSD-94-832",
  available  =   "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-94-832",
  abstract =	 "High-level programming languages and exotic
		  architectures have often been developed together,
		  because the languages seem to require complex
		  mechanisms realized by specialized hard ware. The
		  implicitly parallel language Id and dataflow
		  architectures are a prime example. The language
		  allows an elegant formulation of a broad class of
		  problems while exposing substantial parallelism,
		  however, its non-strict semantics require fine-grain
		  dynamic scheduling and synchronization making an
		  efficient implementation on conventional parallel
		  machines challenging.  This thesis presents novel
		  compilation techniques for implicitly parallel
		  languages, focusing on techniques addressing dynamic
		  scheduling. It shows that it is possible to
		  implement the lenient functional language Id
		  efficiently by partitioning the program into regions
		  that can be scheduled statically as sequential
		  threads, and realizing the remaining dynamic
		  scheduling through compiler- controlled
		  multithreading. Partitioning the program into
		  sequential threads requires substantial compiler
		  analysis because the evaluation order is not
		  specified by the programmer.   The main contribution
		  of the thesis is the development and implementation
		  of powerful thread partitioning algorithms. In
		  addition, a new theoretical framework is formulated
		  for proving the correctness of the partitioning
		  algorithms. The thesis presents a new basic block
		  partitioning algorithm, separation constraint
		  partitioning, which groups nodes in situations where
		  previous algorithms failed. It presents a new
		  solution for interprocedural partitioning which
		  works in the presence of recursion. All of the
		  partitioning algorithms have been implemented and
		  form the basis of a compiler of Id for workstations
		  and several parallel machines. This execution
		  vehicle is used to quantify the effectiveness of the
		  different partitioning schemes on whole
		  applications. The experimental data, collected from
		  our implementation of Id for workstations and the
		  CM-5 multiprocessor, shows the effectiveness of the
		  partitioning algorithm and compiler-controlled
		  multithreading."
}

@TechReport{Schauser95,
  author = 	 "Klaus E. Schauser and David E. Culler and Seth C.
		  Goldstein",
  title = 	 "Separation Constraint Partitioning - {A} New
		  Algorithm for Partitioning Non-strict Programs into
		  Sequential Threads",
  institution =  "University of California at Santa Barbara, Computer
		  Science Department",
  year = 	 "1995",
  number =	 "TRCS95-01",
  month =	 "Jan",
  url =     "http://www.cs.ucsb.edu/TRs/techreports/TRCS95-01.ps",
  abstract =	 "In this paper we present substantially improved
		  thread partitioning algorithms for modern implicitly
		  parallel languages. We present a new block
		  partitioning algorithm, separation constraint
		  partitioning, which is both more powerful and more
		  flexible than previous algorithms. Our algorithm is
		  guaranteed to derive maximal threads. We present a
		  theoretical framework for proving the correctness of
		  our partitioning approach, and we show how
		  separation constraint partitioning makes
		  interprocedural partitioning viable. We have
		  implemented the partitioning algorithms in an Id90
		  compiler for workstations and parallel machines.
		  Using this experimental platform, we quantify the
		  effectiveness of different partitioning schemes on
		  whole applications."
}

@InProceedings{Schmidtmann93,
  author = 	 "Carl Schmidtmann and Michael Tao and Steven Watt",
  title = 	 "Design and implementation of a multi-thread {Xlib}",
  pages =	 "193--203",
  booktitle =	 usenixw93,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "Jan",
  note2 =	 "HAR",
  url = 	 "TBD",
  abstract =	 "In the MIT X Window System's library Version 11
		  Release 5 (Xlib) there is minimal support for
		  multi-threaded applications. Programmers writing
		  multi-threaded programs using Xlib are required to
		  provide locking or designate a single thread to
		  handle many of the calls to X functions in their
		  programs. The authors describe the design and
		  implementation of an upgraded version of Xlib that
		  provides more support for multi-threaded
		  applications. Their goals are to make as few changes
		  to the Application Programming Interface as
		  possible, making the locking invisible to the
		  programmer using the library, and maintain the
		  current portability and performance of the library.
		  This library was implemented on Digital Equipment
		  Corporation's version of OSF/1 using the Pthreads
		  library and Xerox Corporation's Cedar environment."
}

@InProceedings{Scholz94,
  author = 	 "Thomas Scholz and Michael Schaefers",
  title = 	 "MULTI WINDOWS: A Dynamic Register Array Concept for
		  High-Performance RISC Processors",
  pages =	 "??--??",
  booktitle =	 euromicro94,
  year =	 "1994",
  month =	 "",
  abstract =	 "Threaded Windows utilize a dynamic concept for
		  handling a very large number of general purpose
		  registers. This concept enables fast context
		  switches and a short interrupt latency. It is well
		  suited for real time systems. Unfortunately, there
		  are some disadvantages too. To avoid most of the
		  disadvantages of Threaded Windows, we started the
		  development of the Multi Windows. In Multi Windows,
		  the data structures were simplified and improved.
		  The exception routines are less complex and faster.
		  Both concepts will be discussed in this article."
}

@InProceedings{Scholz95,
  author = 	 "Thomas Scholz and Michale Schaefers",
  title = 	 "An Improved Dynamic Register Array Concept for
		  High-Performance RISC Processors",
  volume =	 "?",
  pages =	 "??--??",
  booktitle =	 hicss28,
  year =	 "1995",
  month =	 "Jan",
  abstract =	 "To avoid RISC processors accessing the external
		  memory, an  increased number of processor registers
		  is desirable. However,  sophisticated concepts are
		  needed for the handling of large amounts of
		  registers.  Multi Windows are an improved version of
		  Threaded Windows, the  first dynamic register array
		  concept. Both utilize dynamic  register allocation
		  for handling a very large number of general  purpose
		  registers. This concept enables fast context
		  switches and  a short interrupt latency, which makes
		  it suitable for real time  systems. In Multi
		  Windows, the data structures were simplified and
		  improved. Exception routines are less complex and
		  faster. Both concepts will be discussed in this
		  article."
}

@Article{Schwan91,
  author = 	 "Karsten Schwan and Hongyi Zhou and Ahmed Gheith",
  title = 	 "Real-time threads",
  journal =	 acmosr,
  year =	 "1991",
  volume =	 "25",
  number =	 "4",
  pages =	 "35--46",
  month =	 "Oct",
  abstract =	 "Portability is an important attribute of real-time
		  operating systems because their target hardware
		  environments routinely vary from special purpose
		  processors to parallel machines to distributed
		  execution environments. The paper addresses the
		  issue of operating systems portability by
		  development of a real-time threads package based on
		  the Mach cthreads interface. Real-time threads have
		  been implemented on standard Unix platforms and on a
		  32-node BBN Butterfly multiprocessor. In contrast to
		  cthreads, the schedulability of real-time threads
		  may be determined dynamically at the time of threads
		  creation. In addition, any scheduling guarantees
		  made at thread creation time are maintained when
		  threads communicate or cooperate using the package's
		  other primitives."
}

@Article{Schwan92a,
  author = 	 "Karsten Schwan and Hongyi Zhou",
  title = 	 "Dynamic scheduling of hard real-time tasks and
		  real-time threads",
  journal =	 ieeetse,
  year =	 "1992",
  volume =	 "18",
  number =	 "8",
  pages =	 "736--748",
  month =	 "Aug",
  note =	 "Also published in Real Time Programming, Proceedings
		  of the IFAC/IFIP Workshop, pp.13--18, May 1991",
  abstract =	 "The authors investigate the dynamic scheduling of
		  tasks with well-defined timing constraints. They
		  present a dynamic uniprocessor scheduling algorithm
		  with an O(n log n) worst-case complexity. The
		  preemptive scheduling performed by the algorithm is
		  shown to be of higher efficiency than that of other
		  known algorithms. Furthermore, tasks may be related
		  by precedence constraints, and they may have
		  arbitrary deadlines and start times (which need not
		  equal their arrival times). An experimental
		  evaluation of the algorithm compares its average
		  case behavior to the worst case. An analytic model
		  used for explanation of the experimental results is
		  validated with actual system measurements. The
		  dynamic scheduling algorithm is the basis of a
		  real-time multiprocessor operating system kernel
		  developed in conjunction with this research.
		  Specifically, this algorithm is used at the lowest,
		  threads-based layer of the kernel whenever threads
		  are created."
}

@Article{Schwan92b,
  author = 	 "Karsten Schwan and Hongyi Zhou and Ahmed Gheith",
  title = 	 "Multiprocessor real-time threads",
  journal =	 acmosr,
  year =	 "1992",
  volume =	 "26",
  number =	 "1",
  pages =	 "54--65",
  month =	 "Jan",
  abstract =	 "Portability is an important attribute of real-time
		  operating systems because their target hardware
		  environments routinely vary from special purpose
		  processors to parallel machines to distributed
		  execution environments. The paper, addresses the
		  issue of operating systems portability by
		  development of a real-time threads package based on
		  the Mach cthreads interface. Real-time threads have
		  been implemented on standard Unix platforms and on a
		  32-node BBN Butterfly multiprocessor. In contrast to
		  cthreads, the schedulability of real-time threads
		  may be determined dynamically at the time of threads
		  creation. In addition, any scheduling guarantees
		  made at thread creation time are maintained when
		  threads communicate or cooperate using the package's
		  other primitives."
}

@InProceedings{Scipioni92,
  author = 	 "B. Scipioni and D. Liu and T. Song",
  title = 	 "{SISSY:} a multi-threaded, networked, object-oriented
		  databased example",
  editor =	 "C. Verkerk and W. Wojcik",
  pages =	 "564--567",
  booktitle =	 "Proceedings of the International Conference on
		  Computing in High Energy Physics '92.",
  year =	 "1992",
  publisher =	 "{CERN}",
  address =	 "Annecy, France",
  month =	 "Sep",
  abstract =	 "The Systems Integration Support SYstem (SISSY) is
		  presented and its capabilities and techniques are
		  discussed. It is fully automated data collection and
		  analysis system supporting the SSCL's systems
		  analysis of the Physics Detector and Simulation
		  Facility (PDSF). SISSY itself is a paradigm of
		  effective computing on the PDSF. It uses home-grown
		  code (C++), network programming (RPC,SNMP),
		  relational (SYBASE) and object-oriented
		  (ObjectStore) DBMSs, UNIX operating system services
		  (IRIX threads, cron, system utilities, shell
		  scripts, etc.), and third party software
		  applications (NetCentral Station, Wingz, DataLink,
		  Natural Language) acting as a  single application to
		  monitor and analyze the PDSF."
}

@TechReport{Scott91,
  author =       "Michael L. Scott",
  title =        "The Lynx Distributed Programming Language:
		  Motivation, Design, and Experience",
  institution =  "University of Rochester, Department of Computer
		  Science",
  year =         "199?",
  number =       "TR 308",
  note =         "(COMPLANG, 16, 3/4, 1991, 209-233)CHECK THIS",
  url =     "ftp://ftp.cs.rochester.edu/pub/systems_papers/91.Comp_Lang.Lynx.ps.Z",
  abstract =     "A programming language has clear advantages over a
		  library package for communication between processes
		  in a distributed environment. Unfortunately, most
		  existing distributed languages are better suited to
		  communication between the processes of a single
		  application than they are to communication between
		  processes that are developed independently. Such
		  independent development is characteristic both of
		  the systems software for multicomputers and of the
		  applications software for geographically-distributed
		  networks. Lynx is a message-based language designed
		  to support both application and system software in a
		  single conceptual framework. It extends the
		  advantages of high-level communication to processes
		  designed in isolation, and compiled and placed in
		  operation without knowledge of their peers. A
		  virtual-circuit abstraction called the link supports
		  changes in communication topology at run time, under
		  user control. A lightweight thread mechanism is
		  integrated with message passing in a way that makes
		  it easy for server processes to maintain context for
		  interleaved conversations with an arbitrary number
		  of clients. Experience with Lynx under two very
		  different implementations has yielded important
		  insights into the interface between a language
		  run-time package and the underlying operating
		  system, and has helped to identify the inherent
		  costs of message-passing systems."
}

@PhdThesis{Serrano94a,
  author = 	 "Mauricio J. Serrano",
  title = 	 "Performance tradeoffs in multistreamed superscalar
		  architectures",
  school = 	 "University of California at Santa Barbara,
		  Department of Electrical and Computer Engineering",
  year = 	 "1994",
  month =	 "Mar",
  abstract =	 "Superscalar processor employ multiple functional
		  unit designs that can dispatch several instructions
		  every cycle. Two factors limiting the number of
		  instructions dispatched per cycle are: 1) the number
		  of functional units available (hardware), and 2) the
		  amount of parallelism in the workload (software).
		  While the number of functional units determines the
		  peak throughput of a processor, the
		  instruction-level parallelism limits the actual
		  performance obtained. Data dependencies and control
		  breaks constrain instruction-level parallelism
		  resulting in a sustained system performance that is
		  well below the peak. The ability to execute
		  simultaneously multiple instruction streams,
		  referred to as multistreaming,  significantly
		  increases the number of independent  instructions
		  that could be issued in a cycle. A multistreamed,
		  superscalar processor can dispatch instructions from
		  multiple streams simultaneously. Each stream context
		  is stored internally. The processor adjust the
		  scheduling policy as the workload changes to
		  maximize throughput. We developed an analytic model
		  to estimate the overall performance of these
		  architectures. The model uses simple workload and
		  architectural descriptions that are obtained using
		  commonly available tools. The model produces
		  instructions executed per cycle (IPC) as the number
		  of streams is varied.  We present a distributed
		  architecture with several functional units that
		  minimizes the number of global interconnections. A
		  novel instruction issue mechanism is discussed with
		  the provision of precise interrupts and branch
		  squashing. Finally, simulation results with data
		  caches are presented."
}

@InProceedings{Serrano94b,
  author = 	 "Maurico J. Serrano and Wayne Yamamoto and Roger C.
		  Wood and Mario Nemirovsky",
  title = 	 "A Model for Performance Estimation in a
		  Multistreamed Superscalar Processor",
  pages =	 "353--368",
  booktitle =	 "Proceedings of 7th International Conference on
		  Modeling Techniques and Tools for Computer
		  Performance Evaluation",
  publisher =	 "Springer-Verlag",
  year =	 "1994",
  month =	 "May",
  abstract =	 "The current trend is integrating more hardware
		  functional units within the superscalar processor.
		  However, the functional units are not fully utilized
		  due to the inherent limit of instruction-level
		  parallelism in a single instruction stream. The use
		  of simultaneous execution of instructions from
		  multiple streams, referred to as multistreaming, can
		  increase the number of instructions dispatched per
		  cycle by providing more ready-to-issue instructions.
		  We present an analytical modeling technique to
		  evaluate the effect of dynamically interleaving
		  additional instruction streams within superscalar
		  architectures. Estimates of the instructions
		  executed per cycle (IPC) are calculated given simple
		  descriptions of the workload and hardware. To
		  validate this technique, estimates obtained from the
		  model for several benchmarks are compared against
		  results from a hardware simulator."
}

@InProceedings{Serverance95,
  author = 	 "Charles Serverance and Richard J. Enbody and Brad
		  Funkhouser and Steve Wallach",
  title = 	 "Automatic Self-Allocating Threads (ASAT) on the
		  Convex Exemplar",
  volume =	 "I",
  pages =	 "24--31",
  booktitle =	 icpp95,
  year =	 "1995",
  month =	 "Aug",
  note =         "Also as Tech report MSU CPS-94-17, Michigan State
		  University, Department of Computer Science",
  url = 	 "http://clunix.cl.msu.edu:80/~crs/papers/asat_ex/",
  abstract =	 "While parallel processing systems have an advantage
		  over traditional supercomputers in
		  price/performance, traditional supercomputers retain
		  a significant advantage over parallel processing
		  systems in the area of flexibility. Traditional
		  supercomputers can easily handle a mix
		  ofinteractive, batch, scalar, vector, parallel, and
		  large memory jobs simultaneously while maintaining
		  high utilization. Often parallel processing systems
		  do not effectively support dynamic sharing of the
		  resources of a system which results in low overall
		  utilization given a mix of jobs. This paper
		  describes the implementation of thread balancing
		  software intended to allow the sharing of a large
		  parallel processing system under a variety of load
		  conditions. The system under consideration for this
		  study is the Convex Exemplar scalable,
		  parallel-processing system. While this paper is
		  focused on the performance of this technique on the
		  Convex Exemplar, this is a gereral technique which
		  should prove to be useful on many shared memory
		  parallel processors."
}

@InProceedings{Shankaar95,
  author = 	 "Banu Shankar and Lucas Roh and A.P. Willem Bohm and
		  Walid Najjar",
  title = 	 "Control of Parallelism in Multithreading Code",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "Jun",
  abstract =	 ""
}

@InProceedings{Shapiro92,
  author = 	 "Marc Shapiro and Mesaac Makpangou",
  title = 	 "Distributed abstractions, lightweight references",
  pages =	 "263--267",
  booktitle =	 "Proceedings of the USENIX Workshop on Micro-Kernels
		  and Other Kernel Architectures",
  year =	 "1992",
  address =	 "Seattle, WA, USA",
  month =	 "Apr",
  note2 =	 "HAR",
  abstract =	 "The SOR group at INRIA has been researching OS
		  support for objects. This research has identified a
		  number of areas where the current OS design should
		  be amended. Existing microkernels support resources
		  such as protection domains, threads, regions of VM
		  backed by external mappers, and communication
		  endpoints. The paper discusses more specific
		  adaptations of the microkernel technology: a layer
		  of common distributed abstractions, above the
		  kernel, and a lightweight, uniform garbage-collected
		  reference mechanism."
}

@Article{Shar74,
  author = 	 "Leonard E. Shar and Edward S. Davidson",
  title = 	 "A multiminiprocessor system implemented through
		  pipelining",
  journal =	 ieeecomp,
  year =	 "1974",
  volume =	 "7",
  number =	 "2",
  pages =	 "42--51",
  note2 =	 "HAR",
  abstract =	 "This paper considers choosing the appropriate
		  architecture for a multiminiprocessor system and
		  illustrates some inherent cost advantages of
		  configuring a system which appears to the users as a
		  multiprocessor but is infact a single pipelined
		  processor. The thesis of this paper is that altough
		  pipelining and parallelism may be used to create a
		  virtual multiprocessor at substantial savings in
		  cost over using several conventional processors
		  while providing the same throughput. Pipelining is
		  thus advicated here as an attractive architecture
		  for much smaller computer systems than those
		  typically associated with pipelining"
}

@Unpublished{Sharma89,
  author = 	 "M. Sharma and Rishiyur S. Nikhil",
  title = 	 "PRISC-1: a multithreaded RISC architecture",
  institution =	 "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  note = 	 "Working paper",
  year =	 "1989",
  month =	 "Nov",
  abstract =	 ""
}

@InProceedings{Shaw92,
  author = 	 "Andy Shaw and Yuetsu Kodama and M. Sato and Shuichi
		  Sakai and Yoshinori Yamaguchi",
  title = 	 "Performance of data-parallel primitives on the EM-4
		  dataflow parallel supercomputer",
  pages =	 "302--309",
  booktitle =	 "Fourth Symposium on the Frontiers of Massively
		  Parallel Computation",
  year =	 "1992",
  month =	 "Oct",
  abstract =	 "The authors have implemented seven data-parallel
		  primitives on the hybrid dataflow/von Neumann
		  parallel computer EM-4. To evaluate the performance
		  of these primitives, the authors compare them to
		  identical primitives running on a CM-200 SIMD
		  (single-instruction multiple-data) parallel
		  computer. For integer arithmetic element-wise
		  operations, EM-4 is faster than the CM-200 when two
		  or more operations can be combined. For
		  communications operations, EM-4 has significantly
		  higher performance. EM-4's distinguishing feature in
		  running data-parallel codes is its exceptional
		  communications performance in terms of network
		  bandwidth and latency, and processor/network
		  interface. Additional special-purpose hardware for
		  barrier synchronization and scan-like operations is
		  not necessary. Dataflow-style token synchronization
		  is helpful, but not necessary in implementing
		  data-parallel primitives."
}

@TechReport{Sheppard87,
  author =       "A. Sheppard",
  title =        "Multiple Thread Debugging Considerations",
  type =         "Internal Document",
  institution =  "CONVEX",
  month =        apr,
  year =         "1987",
  abstract =	 ""
}

@InProceedings{Shetler91,
  author = 	 "Joy Shetler and Steven E. Butner",
  title = 	 "Multiple stream execution on the DART processor",
  volume =	 "I",
  pages =	 "92--96",
  booktitle =	 icpp91,
  year =	 "1991",
  month =	 "Aug",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Sheu94,
  author = 	 "Jang-Ping Sheu and Chih-Yung Chang",
  title = 	 "Extracting Multi-thread with Data Localities for
		  Vector Computers",
  pages =	 "??--??",
  booktitle =	 "Proceedings of the International Conference on
		  Parallel and Distributed Systems (ICPADS'94)",
  year =	 "1994",
  abstract =	 "In this paper, we propose a source-to-source
		  compilation strategy to partition vectorized loop
		  programs into multi-thread execution form. Each
		  partitioned thread consists of instances of
		  statements with localities in vector registers. The
		  multi-threading scheme gives a novel combination of
		  loop unrolling, statement instances reordering,
		  index shifting, vector register reuse exploiting,
		  and multi-threading. Experimental results show that
		  our multi-threading scheme assists vector compiler
		  of Convex C38 series to reduce the number of memory
		  accesses and the number of synchronizations among
		  CPUs and usually obtains a better performance. "
}

@Article{Shimizu89,
  author = 	 "Kentaro Shimizu and Eiichi Goto and Shuichi
		  Ichikawa",
  title = 	 "CPC (Cyclic Pipeline Computer) - An architecture
		  suitedd for Josephson and pipelined memory
		  machines",
  journal =	 ieeetc,
  year =	 "1989",
  volume =	 "38",
  number =	 "6",
  pages =	 "825--832",
  month =	 "Jun",
  note =	 "As Technical report 86-19. University of Tokyo. Faculty of
		  Science. Dept. of Information Science",
  abstract =	 "Describes a novel computer architecture, called a
		  cyclic pipeline computer (CPC), which is especially
		  suited for Josephson technologies. Since each
		  Josephson logic device acts as a latch, it is
		  possible to use high-pitch and shallow logic
		  pipelining without any increase in delay time and
		  cost. Hence, both the processor and the main memory
		  can be built from the Josephson devices and can be
		  pipelined with the same pipeline pitch time. The CPC
		  supports multiple instruction/multiple data stream
		  (MIMD) by time-sharing the processor and the main
		  memory among multiple instruction streams. In
		  addition, it employs advanced control to speed up
		  the computation for each instruction stream."
}

@Article{Shu94,
  author = 	 "Wei W. Shu",
  title = 	 "Runtime Support for User-Level Ultra Lightweight
		  Threads on Distributed Memory Computers",
  journal =	 jsup,
  year =	 "1995",
  volume =	 "9",
  number =	 "1",
  pages =	 "91--104",
  note2 =	 "HAR",
  url = 	 "http://www.cs.buffalo.edu/pub/WWW/faculty/shu/psfile/uthread.ps",
  abstract =	 "Ultra-lightweight (uThread) is a library package
		  designed and optimized for user-level management of
		  parallelism in a single application program running
		  on distributed memory computers. Existing process
		  management systems incur an unnecessarily high cost
		  when used for the type of parallelism exploited
		  within an application. By reducing the overhead of
		  ownership protection and frequent context switches.,
		  uThread encourages both simplicity and
		  performance. In addition, uThread provides various
		  scheduling support to balance the system load. The
		  uThread package reduces the cost of parallelism
		  management to nearly the lower bound. This package
		  has been succesfully running on most distributed
		  memory computers"
}

@InProceedings{Shu95a,
  author = 	 "Wei W. Shu",
  title = 	 "Runtime Support for User-Level Ultra Lightweight
		  Threads on Massively Parallel Distributed Memory
		  Machines",
  pages =	 "??--??",
  booktitle =	 "The Fifth Symposium on the Frontiers of Massively
		  Parallel Computation",
  year =	 "1995",
  month =	 "Feb",
  url = 	 "http://www.cs.buffalo.edu/pub/WWW/faculty/shu/psfile/uthread.frontier.ps",
  abstract =	 "Ultra-lightweight Thread (uThread) is a library
		  package designed and optimized for user-level
		  management of parallelism in a single application
		  program running on distributed memory computers.
		  Existing process management systems incur an
		  unnecessarily high cost when used for the type of
		  parallelism exploited within an application. By
		  reducing the overhead of ownership protection and
		  frequent context switches, uThread encourages both
		  simplicity and performance. In addition, uThread
		  provides various scheduling support to balance the
		  system load. The uThread package reduces the cost of
		  parallelism management to nearly the lower bound.
		  This package has been successfully running on most
		  distributed memory computers, such as Intel
		  iPSC/860, Touchstone Delta, NCUBE, and TMC CM-5."
}

@Article{Shu95b,
  author = 	 "Wei W. Shu and and Min-You Wu",
  title = 	 "Asynchronous problems on SIMD parallel computers",
  journal =      ieeepds,
  year =         "1995",
  volume =       "??",
  number =       "??",
  pages =        "??",
  url =     "http://www.cs.buffalo.edu/pub/WWW/faculty/shu/psfile/simd.trans.ps",
  note =	 "Also as Tech-report 93-11, State University of New York at
		  Buffalo. Dept. of Computer Science, Apr.1993 ",
  abstract =	 "One of the essential problems in parallel computing
		  is: can SIMD machines handle asynchronous problems?
		  This is a difficult, unsolved problem because of the
		  mismatch between asynchronous problems and SIMD
		  architectures. We propose a solution to let SIMD
		  machines handle general asynchronous problems. Our
		  approach is to implement a runtime support system
		  which can run MIMD-like software on SIMD hardware.
		  The runtime support system, named P kernel, is
		  thread-based. There are two major advantages of the
		  thread-based model. First, for application problems
		  with irregular and/or unpredictable features,
		  automatic scheduling can move some threads from
		  overloaded processors to underloaded
		  processors.Second and more important, the
		  granularity of threads can be controlled to reduce
		  system overhead. The P kernel is also able to handle
		  bookkeeping and message management, as well as to
		  make these low-level tasks transparent to users.
		  Substantial performance has been obtained on CM-2 and
		  CM-5."
}

@PhdThesis{Simpson88,
  author = 	 "Richard O. Simpson",
  title = 	 "METRIC context unit architecture",
  school = 	 "University of Texas at Austin, Department of
		  Computer and Electrical Engineering",
  year = 	 "1988",
  month =	 "May",
  note2 =	 "HAR",
  abstract =	 "Metric is an architecture for a simple but powerful
		  reduced instruction set computer (RISC). Its speed
		  comes from the simultaneous processing of several
		  instruction streams, with instructions from the
		  various streams being dispatched into METRICS
		  execution pipeline as they become available for
		  execution. The pipeline is thus kept full, with a
		  mix of instruction for several contexts in execution
		  at the same time. True parallel programming is
		  supported within a single execution uniut, the
		  Metric Context unit. Metrics architecture provides
		  for expansion through the addition of multiple
		  context units and of specialized functional
		  units. The architecture thus spans a range of size
		  and performance from a single chip microcomputer up
		  through large and powerful multiprocessors. This
		  research concentrates on the specification of the
		  METRIC context unit at the architectural
		  level. Performance tradeoffs made during METRICS
		  design are discussed, and projections of METRICS
		  performance are made based on simulation studies."
}

@InProceedings{Sites79,
  author = 	 "Richard L. Sites",
  title = 	 "How to use 1000 registers",
  pages =	 "527--532",
  booktitle =	 "Proceedings of 1st Caltech Conference on VLSI",
  year =	 "1979",
  organization = "Caltech CS dept",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Sloman95,
  author = 	 "Ben Sloman and Tom Lake",
  title = 	 "Featherweight Threads and ANDF Compilation of
		  Concurrency",
  pages =	 "??--??",
  booktitle =	 "Proceedings of EURO-PAR'95",
  year =	 "1995",
  month =	 "Aug",
  abstract =	 "We present an intermediate representation called
		  ThreadTDF, a component of the Parallel TDF system
		  for compiling distributed concurrent programs to
		  shared and distributed memory multiprocessors.
		  ThreadTDF is a parallel extension of the TDF
		  architecture neutral distribution format (ANDF) for
		  sequential programs. ThreadTDF provides {\em
		  featherweight} thread mechanisms for explicitly
		  scheduling dynamic fine-grain concurrent
		  computations within procedures (and more generally
		  within static local scopes). Communication between
		  address spaces is supported by remote service
		  request mechanisms based upon asynchronous
		  activation of remote threads and synchronous remote
		  procedure calls. In ThreadTDF variable lifetimes
		  bound the lifetimes of featherweight threads
		  declared in their scope. We show how a compiler uses
		  thread lifetime information to integrate resource
		  allocation and communication with thread scheduling
		  for efficient intraprocedural concurrency. Initial
		  performance results are given for the SPARC
		  processor."
}

@InProceedings{Smirni95,
  author = 	 "Evgenia Smirni and Cynthia A. Childens and Emilia
		  Rosti and Larry W. Dowdy", 
  title = 	 "Thread Placement on the Intel Paragon : Modeling and
		  Experimentation", 
  pages =	 "??--??",
  booktitle =	 "Proceedings of the International Workshop on
		  Modeling, Analysis, and Simulation of Computer and
		  Telecommunication Systems (MASCOTS '95)",
  year =	 "1995",
  month =	 "Jan",
  url = 	 "http://www.vuse.vanderbilt.edu/~peg/publications/mascots95.ps",
  abstract =	 "In multicomputer architectures where communication
		  latency is distance independent, thread placement is
		  expected to have a limited impact on an
		  application's performance. In this paper, the impact
		  of thread placement on application performance is
		  demonstrated on a wormhole routed multicomputer, the
		  Intel Paragon. A communication intensive synthetic
		  workload is used to ``stress test'' the effects of
		  contention on communication latency induced by
		  thread placement. It is shown by means of
		  experimentation and modeling that appropriate thread
		  placement patterns minimizing contention in the
		  system's interconnection network improve
		  performance. The analytic model and the experimental
		  observations are in good agreement."
}

@InProceedings{Smith78,
  author = 	 "Burton J. Smith",
  title = 	 "A pipelined, shared resource {MIMD} computer",
  pages =	 "6--8",
  booktitle =	 icpp78,
  year =	 "1978",
  note2 =	 "HAR",
  abstract =	 "The HEP computer system currently being implemented
		  by Denelcor Inc, under contract to the US army
		  ballistic research labv is an MIMD machine of the
		  shared resource type as defined by Flynn. In this
		  type of organization, skeleton processors compete
		  for executionn resources in either space or time. In
		  the HEP processor, spatial switching occurs between
		  two queues of processes, one of these controls
		  program memory, register memory, and the functional
		  memory while the other controls data
		  memory. Multiplke processors and data memories may
		  be interconnected via a pipelined switch and any
		  register memory or data memory location may be used
		  to syncrhonize two processes on a producer-consumer
		  basis."
}

@Article{Smith81,
  author = 	 "Burton J. Smith",
  title = 	 "Architecture and applications of the HEP
		  multiprocessor computer system",
  journal =	 "SPIE Real-Time Signal Processing IV",
  year =	 "1981",
  volume =	 "298",
  pages =	 "241--248",
  note2 =	 "HAR",
  abstract =	 "The HEP computer system is a large scale scientific
		  parallel computer emplying shared resource MIMD
		  architecture. The ahardware and software facilities
		  provided by the system are described, and techniques
		  found to be useful in programming the system are
		  also discussed"
}

@InBook{Smith84,
  author = 	 "Burton J. Smith",
  title = 	 "High-Speed Computation",
  chapter = 	 "Latency and {HEP}",
  publisher =	 "Springer-Verlag",
  year =	 "1984",
  pages =	 "139--144",
  volume =	 "F7",
  series =	 "NATO ASI Series F",
  note2 =	 "HAR",
  abstract =	 "The HEP parallel computer system is able to overcome
		  most of the undesirable effects of latency by
		  pipelining, but without the usual consequences. The
		  reasons for HEPs latency tolerance may be found in a
		  few unusual features of the HEP architecture, one of
		  which seems contradictory; the HEP has very low
		  latency process synchronization and communication
		  facilities compared to other MIMD machines. These
		  features allow a high degree of parallelism to be
		  sustained"
}

@InCollection{Smith85,
  author = 	 "Burton J. Smith",
  title = 	 "The architecture of HEP",
  booktitle = 	 "Parallel MIMD computation: HEP supercomputer and its
		  application",
  year =	 "1985",
  editor =	 "J.S. Kowalik",
  pages =	 "41-55",
  publisher = 	 "MIT Press",
  abstract =	 ""
}

@InProceedings{Smith92,
  author =       "Gail A. Alverson and Robert Alverson and David
		  Callahan and Brian Koblenz and Allan Porterfield and
		  Burton J. Smith",
  title =        "Exploiting Heterogeneous Parallelism on a
		  Multi-Threaded Multiprocessor",
  year =         "1992",
  month =        jul,
  booktitle =    "6th ACM International Conference on Supercomputing",
  pages =        "188--197",
  address =      "Washington, D.C.",
  note2 =	 "HAR",
  url = 	 "ftp://www.net-serve.com/tera/design.ps.gz",
  abstract =	 ""
}

@MastersThesis{Snyder92,
  author = 	 "Jeffrey Snyder",
  title = 	 "Fast context switches",
  school = 	 "Florida State University, Department of Computer
		  Science",
  year = 	 "1992",
  month =	 "??",
  abstract =	 ""
}

@Article{Snyder93,
  author = 	 "Jeffrey S. Snyder and David B. Whalley and Theodore
		  P. Baker",
  title = 	 "Fast Context switches: Compiler and architectural
		  support for preemptive scheduling",
  journal =	 "Microprocessors and Microsystems",
  year =	 "1995",
  pages =	 "35--42",
  month =	 "Feb",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.cs.fsu.edu/pub/whalley/papers/mnm94.ps.Z",
  abstract =	 "This paper addresses the possibility of reducing
		  the overhead due to preemptive context switching in
		  real-time systems that use preemptive
		  scheduling. The method introduced in this paper
		  attempts to avoid saving and restoring registers by
		  performing context switches at points in the program
		  where only a small subset of the registers are live
		  . When context switches occur frequently , which ius
		  the case in some real-time systems,m performing
		  context switches at fast context switch points is
		  found to reduce the total number of memory
		  references. A new technique know as register
		  remapping is introduced which increases the number
		  of these fast context switch points withouth
		  degrading the efficiency of the code"
}

@InProceedings{Sohn95,
  author = 	 "Andrew Sohn and Chinhyun Kim and Mitsuhisa Sato",
  title = 	 "Multithreading with the EM-4 Distributed-Memory
		  Multiprocessor",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "Jun",
  abstract =	 ""
}

@TechReport{Solworth85,
  author = 	 "Jon Solworth and Alexandru Nicolau",
  title = 	 "Microflow: A fine-grain parallel processing
		  approach",
  institution =  "Cornell University, Department of Computer Science",
  year = 	 "1985",
  number =	 "TR85-710",
  month =	 "Nov",
  note2 =	 "HAR",
  url = 	 "http://cs-tr.cs.cornell.edu:80/TR/CORNELLCS:TR85-710",
  abstract =	 ""
}

@InProceedings{Solworth88,
  author = 	 "Jon A. Solworth",
  title = 	 "The Microflow architecture",
  volume =	 "I",
  pages =	 "113--117",
  booktitle =	 icpp88,
  year =	 "1988",
  address =	 "University Park, PA, USA",
  month =	 "Aug",
  abstract =	 "A multiple-instruction, multiple-data (MIMD)
		  architecture dubbed Microflow, which combines
		  very-low-cost communication and synchronization with
		  the latency avoidance techniques of uniprocessor
		  architectures, is presented. The communication and
		  synchronization are implemented with extremely fast
		  message passing by having targets of messages be
		  general-purpose registers. Communication between
		  adjacent nodes can be accomplished in the time it
		  takes to execute one instruction. A Microflow
		  processor contains multiple windows, each containing
		  a context. This mechanism enables high-performance
		  servers to be constructed in software while enabling
		  the server to have high priority and low overhead.
		  The message-passing elements integrate smoothly with
		  RISC (reduced-instruction-set computer) or even
		  moderately horizontal instruction set, enabling
		  Microflow to perform well even on those parts of the
		  code which do not parallelize well. "
}

@InProceedings{Sommer92,
  author = 	 "Jeppe Sommer",
  title = 	 "The DaCapo project: distributed, active,
		  communicating, persistent objects",
  editor =	 "Luis Felipe Cabrera and Eric Jul",
  pages =	 "129--132",
  booktitle =	 "Proceedings of the Second International Workshop on
		  Object Orientation in Operating systems",
  year =	 "1992",
  address =	 "Dourdan, France",
  month =	 "Sep",
  abstract =	 "DeCapo unifies state, processing thread,
		  synchronization, distribution, persistence, and
		  mobility into the concept of an object. This is
		  reflected both in the DaCapo programming language
		  and the DaCapo system. By doing so, the authors have
		  achieved a simple, yet powerful object model for
		  object-oriented distributed software systems."
}

@MastersThesis{Soundarajarajan92a,
  author = 	 "Vijayaraghavan Soundarajarajan",
  title = 	 "Dribble-back registers: A technique for latency
		  tolerance in multiprocessors",
  school = 	 "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  month =	 "Jun",
  note2 =	 "HAR",
  url =  	 "TBD",
  abstract = 	 ""
}

@TechReport{Soundararajan92b,
  author = 	 "Vijayaraghavan Soundararajan and Anant Agarwal",
  title = 	 "Dribbling Registers: A Mechanism for Reducing
		  Context Switch Latency in Large-Scale
		  Multiprocessors.",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  for Computer Science",
  year = 	 "1992",
  number =	 "MIT/LCS/TM-474",
  month =	 "Jun",
  note = 	 "21 pages",
  note2 =	 "HAR",
  url =  	 "ftp://cag.lcs.mit.edu/pub/papers/dribbling.ps.Z",
  abstract = 	 "As parallel machines grow in scale and complexity,
		  latency tolerance of synchronization faults and
		  remote memory accesses becomes increasingly
		  important. One method for tolerating this latency is
		  by multithreading the processor and rapidly contact
		  switching between these threads. Fast context
		  switching is most effective when the latencies being
		  tolerated are short compared to the total run
		  lengths of all the resident threads. If this
		  condition is not met, it may become necessary to
		  expend processor cycles to unload a blocked thread
		  and load in a new one. This thesis presents the
		  dribble-in, dribble-out register file, which
		  facilitates fast context switching and the ability
		  to hide the latency of loading and unloading context
		  state. Through an analytical model and a simulation
		  framework, we show that the dribble-in, dribble-out
		  register file compares favorably against existing
		  designs."
}

@TechReport{Spertus90,
  author = 	 "Ellen Spertus",
  title = 	 "Dataflow computation for the J-Machine",
  institution =  "Massachusetts Institute of Technology Artificial
		  Intelligence Lab",
  year = 	 "1990",
  number =	 "Ai-Report 1233",
  note =	 "B.S. Thesis",
  abstract =	 "The dataflow model of computation exposes and
		  exploits parallelism in programs without requiring
		  programmer annotation; however, instruction-level
		  dataflow is too fine- grained to be efficient on
		  general-purpose processors. A popular solution is to
		  develop a 'hybrid' model of computation where
		  regions of dataflow graphs are combined into
		  sequential blocks of code. I have implemented such a
		  system to allow the J-Machine to run Id programs,
		  leaving exposed a high amount of parallelism -- such
		  as among loop iterations. I describe this system and
		  provide an analysis of its strengths and weaknesses
		  and those of the J-Machine, along with ideas for
		  improvement."
}

@TechReport{Spertus94,
  author = 	 "Ellen Spertus and William J. Dally",
  title = 	 "Experiments with dataflow on a general purpose
		  parallel computer",
  institution =  "Massachusetts Institute of Technology, Artificial
		  Intelligence Laboratory",
  number =	 "A.I. Memo No.1272,  ",
  year =	 "1994",
  note2 =	 "HAR",
  url =  	 "ftp://ftp.ai.mit.edu/pub/cva/ai-tr1272.ps.Z",
  abstract =	 "The MIT J-Machine [2], a massively-parallel
		  computer, is an experiment in providing
		  general-purpose mechanisms for communication,
		  synchronization, and naming that will support a wide
		  variety of parallel models of computuation. Having
		  universal mechanisms allows the separation of issues
		  of language design and machine organization [4]. We
		  have developed two experimental dataflow programming
		  systems for the J-Machine. For the first system, we
		  adapted Papadopoulos' explicit token store [12] to
		  implement static and then dynamic dataflow. Each
		  node in a dataflow graph is expanded into a sequence
		  of code, each of which is scheduled individually at
		  runtime.For a later system, we made use of
		  Iannucci's hybrid execution model [10] to combine
		  several dataflow graph nodes into a single sequence,
		  decreasing scheduling overhead. By combining the
		  strengths of the two systems, it is possible to
		  produce a system with competitive performance. We
		  have demonstrated the feasiblity of efficiently
		  executing dataflow programs on a general-purpose
		  parallel computer."
}

@TechReport{Squillante94,
  author = 	 "Mark S. Squillante",
  title = 	 "Analytic Modeling of Processor Utilization in
		  Multithreaded Processor Architectures",
  institution =  "IBM Research division, T.J.Watson Research center",
  year = 	 "1994",
  number =	 "RC 19543",
  month =	 "Apr",
  note2 =	 "HAR",
  url = 	 "http://www.watson.ibm.com:8080/main-cgi-bin/search_paper.pl/entry_ids=131",
  abstract =	 "In this paper, we develop an analytic model of
		  processor utilization in multithreaded processor
		  architectures that supports both serial and parallel
		  processing of memory requests. The system is modeled
		  as a finite, continuous-time Markov chain whose
		  solution can be obtained efficiently. Although it
		  applies more generally, our modeling approach
		  supports an important class of probability
		  distributions that can be used to approximate the
		  distributions of interest with sufficient accuracy
		  in most practical cases. This results in an
		  efficient andaccurate model across a wide variety of
		  system environments."
}

@InProceedings{Staley86a,
  author = 	 "Clinton A. Staley and Steven F. Butner",
  title = 	 "A feasability study and simulation of the
		  circulating context multiprocessor",
  pages =	 "455--462",
  booktitle =	 icpp86,
  year =	 "1986",
  note2 =	 "HAR",
  abstract =	 "This paper discusses the design and preliminary
		  performance evaluation of the Circulating Context
		  Multiprocessor (CCMP) - a novel form of tightly
		  coupled multiprocessor that combines significant
		  positive features of modern computer structures and
		  organizations while avoiding many of the negative
		  ones. The CCMP consists of banks of special-purpose
		  processors joined by full interconnection nets with
		  queue-buffering. Each bank contains processors which
		  perform one portion of the traditional Von-Neumann
		  cycle. Processes are represented by packets of
		  information circulating among these banks and being
		  modified by them. Many processes are active at once,
		  being executed in an instruction interleaved
		  fashion. The structure inherently supports load
		  balancing, high degrees of pipelining, efficient
		  context switching, and modular reconfiguration. It
		  appears to the software designer, however, to be
		  simply a parallel computer with many Von Neumann
		  processors sharing a memory."
}

@PhdThesis{Staley86b,
  author = 	 "Clinton A. Staley",
  title = 	 "Design and analysis of the CCMP: A highly expandable
		  shared memory parallel computer",
  school = 	 "University of California at Santa Barbara,
		  Department of Electrical and Computer Engineering",
  year = 	 "1986",
  month =	 "Aug",
  abstract =	 ""
}

@InProceedings{Steele90,
  author = 	 "Guy L. Jr. Steele",
  title = 	 "Making asynchronous parallelism safe for the world",
  pages =	 "218--231",
  booktitle =	 "Conference Record of the Seventeenth Annual ACM
		  Symposium on Principles of Programming Languages",
  year =	 "1990",
  address =	 "San Francisco, CA, USA",
  month =	 "Jan",
  abstract =	 "A programming model is needed that combines the
		  advantages of the synchronous and asynchronous
		  parallel styles. The author proposes a programming
		  model with the benefits of both styles. He allows
		  asynchronous threads of control but restricts
		  shared-memory accesses and other side effects so as
		  to prevent the behavior of the program from
		  depending on any accidents of execution order that
		  can arise from the indeterminacy of the asynchronous
		  process model. These restrictions may be enforced
		  either dynamically (at run time) or statically (at
		  compile time). The author concentrates on dynamic
		  enforcement and exhibits an implementation of a
		  parallel dialect of Scheme based on these ideas. He
		  also speculates on a design for a programming
		  language using static enforcement. This parallel
		  programming model does not support all styles of
		  parallel programming, but it can support a large
		  class of interesting algorithms with considerably
		  greater efficiency (in some cases) than a strict
		  SIMD approach and considerably greater safety (in
		  all cases) than a full-blown MIMD approach."
}

@InProceedings{Stein92,
  author =       "Dan Stein and Devang Shah",
  title =        "Implementing Lightweight Threads",
  booktitle =    usenixs92,
  pages =        "1--10",
  publisher =    "USENIX",
  address =      "San Antonio, TX",
  year =         "1992",
  note2 =	 "HAR",
  url = 	 "http://www.sun.com/sunsoft/Developer-products/sig/threads/papers/impl_threads.ps",
  abstract =	 "We describe an implementation of a threads library
		  that provides extremely lightweight threads within a
		  single UNIX process while allowing fully concurrent
		  access to system resources. The threads are
		  lightweight enough so that they can be created
		  quickly, there can be thousands present, and
		  synchronization can be accomplished rapidly. These
		  goals are achieved by providing user threads which
		  multiplex on a pool of kernel-supported threads of
		  control. This pool is managed by the library and
		  will automatically grow or shrink as required to
		  ensure that the process will make progress while not
		  using an excessive amount of kernel resources. The
		  programmer can also tune the relationship between
		  threads and kernel threads of control. This paper
		  focuses on scheduling and synchronizing user threads
		  and their interaction with UNIX signals in a
		  multiplexing threads library"
}

@Article{Steven89,
  author = 	 "Gordon B. Steven and S. M. Gray and Rod G. Adams",
  title = 	 "HARP: A parallel pipelined RISC processor",
  journal =	 mm,
  year =	 "1989",
  volume =	 "13",
  number =	 "9",
  pages =	 "579--587",
  month =	 "nov",
  abstract =	 "HARP (the Hatfield RISC processor) is a reduced
		  instruction set processor being developed at
		  Hatfield Polytechnic, UK. The major aim of the HARP
		  project is to develop a RISC processor capable of a
		  sustained instruction execution rate in excess of
		  one instruction per cycle. Investigations to date
		  support the hypothesis that this goal can be
		  achieved by the development of an integrated
		  processor-compiler pair in which the processor is
		  specifically designed to support low-level
		  parallelism identified by the compiler. This paper
		  describes the HARP architectural model and discusses
		  those features which support parallel instruction
		  execution. Parallelism is provided in the hardware
		  by multiple instruction pipelines which execute
		  independent RISC-like instructions simultaneously.
		  The principal techniques employed to exploit the
		  available parallelism are efficient pipelining,
		  register bypassing, optional register writeback and
		  conditional execution of instructions. Examples are
		  given which illustrate the effectiveness of these
		  techniques in increasing the performance of HARP."
}

@PhdThesis{Stiemerling91,
  author = 	 "Thomas R. Stiemerling",
  title = 	 "Design and simulation of an MIMD shared memory
		  multiprocessor with interleaved instruction
		  streams",
  school = 	 "University of Edinburgh, UK, Department of Computer
		  Science",
  year = 	 "1991",
  note =	 "As tech report CST-85-91,
		  1991",
  abstract =	 ""
}

@InProceedings{Sundaresan95,
  author = 	 "Neelakantan Sundaresan and D. Gannon",
  title = 	 "A Thread-Model for Supporting Task-and Data-
		  Parallelism in Object-Oriented Languages",
  volume =	 "II",
  pages =	 "45--59",
  booktitle =	 icpp95,
  year =	 "1995",
  month =	 "Aug",
  abstract =	 ""
}

@Article{Tamir83,
  author = 	 "Yuval Tamir and Carlo H. Sequin",
  title = 	 "Strategies for managing the register file in RISC",
  journal =	 tocs,
  year =	 "1983",
  volume =	 "32",
  number =	 "11",
  pages =	 "977--988",
  note2 =	 "HAR",
  abstract =	 "The RISC architecture attempts to achieve high
		  performance without resorting to complex
		  instructions and irregular pipelining schemes. One
		  of the novel features of this architecture is a
		  large register file which is used to minimize the
		  overhead involved in procedure calls and
		  returns. This paper investigates several strategies
		  for managing this register file. The costs of
		  practical strategies are compared with a lower bound
		  on this management overhead obtained from a
		  theoretical optimal strategy for several register
		  file sizes."
}

@InProceedings{Taniguchi93,
  author = 	 "Rin-Ichiro Taniguchi and Tetsuo Kawano and Makoto
		  Amamiya",
  title = 	 "A distributed-memory multi-thread multiprocessor
		  architecture for computer vision and image
		  processing: optimized version of {AMP}",
  pages =	 "151--160",
  booktitle =	 hicss26,
  volume =	 "1",
  year =	 "1993",
  address =	 "Wailea, HI, USA",
  month =	 "Jan",
  abstract =	 "Presents a massively parallel machine for image
		  processing and computer vision, designed to improve
		  the AMP (Autonomous Multiprocessor), a previously
		  designed pure-dataflow-based multiprocessor system
		  for image processing, and to make it faster and more
		  efficient for image processing and computer vision
		  tasks. In the basic design of the AMP there was room
		  for improvement, and, therefore, the authors have
		  recently begun to redesign an improved version of
		  the AMP. The key point of the improvement is to
		  increase the efficiency of execution, especially by
		  optimizing its token matching mechanism, which is
		  indispensable for dataflow-based processors, and the
		  flexibility in its resource management mechanism.
		  The authors first discuss the defects of the
		  previous image processors, then give an overview of
		  the original AMP design. The methodology of its
		  optimization, and the improved system design are
		  presented."
}

@InProceedings{Taura94,
  author = 	 "Kenjiro Taura and Satoshi Matsuoka and Akinori
		  Yonezawa",
  title = 	 "{\sl StackThreads}: An abstract machine for
		  scheduling fine-grain threads on stock cpu",
  pages = 	 "??-??",
  booktitle =    "Joint Symposium on Parallel Processing",
  year = 	 "1994",
  month = 	 "??",
  note2 =	 "HAR",
  url =          "ftp://camille.is.s.u-tokyo.jp/pub/papers/jspp94-stackthreads-a4.ps.Z",
  abstract =     ""
}

@TechReport{Tevanian87,
  author = 	 "Avadis Jr. Tevanian and Richard F. Rashid and David
		  B. Golub and David L. Black and Eric Cooper and
		  Michael W. Young",
  title = 	 "Mach Threads and the Unix Kernel: The Battle for
		  Control",
  institution =  "Carnegie Mellon University, School of Computer
		  Science",
  year = 	 "1987",
  number =	 "CMU-CS-87-149",
  month =	 "Aug",
  note2 =	 "HAR",
  url =  	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/published/threads87.ps",
  abstract =	 "This paper examines a kernel implemented lightweight
		  process mechanism built for the Mach operating
		  system. The pros and cons of such a mechanism are
		  discussed along with the problems encountered during
		  its implementation."
}


@InProceedings{Thekkath93,
  author = 	 "Radhika Thekkath and Susan J. Eggers",
  title = 	 "Impact of Sharing-Based Thread Placement on
		  Multithreaded Architecture",
  booktitle   =   isca21,
  year = 	 "1994",
  month = 	 "Apr",
  pages = 	 "176--186",
  note = 	 "Also as Tech Report UW-CSE-93-10-04, University of
		  Washington, Department of Computer Science and Engineering",
  note2 =	 "HAR",
  url =  	 "ftp://ftp.cs.washington.edu/tr/1993/10/UW-CSE-93-10-04.PS.Z",
  abstract = 	 "Multithreaded architectures context switch between
		  instruction streams to hide memory access latency.
		  Although this improves processor  utilization, it
		  can increase cache interference and degrade overall
		  performance. One technique to reduce the
		  interconnect traffic is to co-locate threads that
		  share data on the same processor. Multiple  threads
		  sharing in the cache should reduce compulsory and
		  invalidation  misses, thereby improving execution
		  time. To test this hypothesis, we compared a variety
		  of thread placement  algorithms via trace-driven
		  simulation of fourteen coarse- and  medium-grain
		  parallel applications on several multithreaded
		  architectures. Our results contradict the
		  hypothesis. Rather than decreasing,  compulsory and
		  invalidation misses remained nearly constant across
		  all placement algorithms, for all processor
		  configurations, even  with an infinite cache. That
		  is, sharing-based placement had no  (positive)
		  effect on execution time. Instead, load balancing
		  was the  critical factor that affected performance.
		  Our results were due to one  or both of the
		  following reasons: (1) the sequential and uniform
		  access of shared data by the application's threads
		  and (2) the  insignificant number of data references
		  that require interconnect  access, relative to the
		  total number of instructions."
}

@InProceedings{Thekkath94,
  author = 	 "Radhika Thekkath and Susan J. Eggers",
  title = 	 "The Effectiveness of Multiple Hardware Contexts",
  pages =	 "328--337",
  booktitle =	 asplos6,
  year =	 "1994",
  month =	 "Oct",
  note2 =	 "HAR",
  url = 	 "http://www.cs.washington.edu/research/arch/eff-mt.html",
  abstract =	 "Multithreaded processors are used to tolerate long
		  memory latencies. By executing threads loaded in
		  multiple hardware contexts, an otherwise idle
		  processor can keep busy, thus increasing its
		  utilization. However, the larger size of a
		  multi-thread working set can have a negative effect
		  on cache conflict misses. In this paper we evaluate
		  the two phenomena together, examining their combined
		  effect on execution time. The usefulness of multiple
		  hardware contexts depends on: program data locality,
		  cache organization and degree of multiprocessing.
		  Multiple hardware contexts are most effective on
		  programs that have been optimized for data locality.
		  For these programs, execution time dropped with
		  increasing contexts, over widely varying
		  architectures. With unoptimized applications,
		  multiple contexts had limited value.%when latencies
		  were only The best performance was seen with only
		  two contexts, and only on uniprocessors and small
		  multiprocessors. The behavior of the unoptimized
		  applications changed more noticeably with variations
		  in cache associativity and cache hierarchy, unlike
		  the optimized programs. As a mechanism for
		  exploiting program parallelism, an additional
		  processor is clearly better than another context.
		  However, there were many configurations for which
		  the addition of a few hardware contexts brought as
		  much or greater performance than a larger
		  multiprocessor with fewer than the optimal number of
		  contexts. "
}

@InProceedings{Thistle88,
  author = 	 "Mark Thistle and Burton J. Smith",
  title = 	 "A processor architecture for Horizon",
  pages =	 "35--41",
  booktitle =	 "Proceedings. Supercomputing '88",
  year =	 "1988",
  note2 =	 "HAR",
  abstract =	 "Horizon is a scalable shared-memory
		  multiple-instruction-stream-multiple-data-stream
		  computer, currently under study, composed of a few
		  hundred identical scalar processors and a comparable
		  number of memories, sparsely embedded in a
		  three-dimensional nearest-neighbor network. Each
		  processor has a horizontal instruction set that can
		  perform up to three floating-point operations per
		  cycle without resorting to vector operations and
		  will be capable performing at a rate of several
		  hundred MFLOPS (millions of floating-point
		  operations per second), to achieve an overall system
		  performance target of 100 GFLOPS (billion of FLOPS).
		  The architecture of the processor in the Horizon
		  system is described."
}

@TechReport{Thistle91,
  author = 	 "Mark R. Thistle and Thomas L. Sterling",
  title = 	 "A fine grain MIMD system with hybrid
		  event-driven/dataflow synchronization for
		  bit-oriented computation",
  institution =  "Supercomputing Research Center : IDA",
  year = 	 "1991",
  number =	 "SRC-TR-91-038",
  month =	 "Aug",
  abstract =	 "The relationship between event-driven and
		  data-driven synchronization is explored in the
		  framework of an operational system and real world
		  application programs. Hardware support for fine
		  grain event- driven synchronization is coupled with
		  software supported data-driven synchronization to
		  produce a hybrid synchronization model that permits
		  efficient imperative program execution. The Zycad
		  System Development Engine provides the event-based
		  parallel execution engine on which is structured a
		  static dataflow programming model. Example problems
		  on an actual operational hybrid system reveal that
		  high efficiency is possible by exploiting
		  event-driven execution through a data-driven model
		  with low average temporal overhead."
}

@InProceedings{Thornton64,
  author = 	 "James E. Thornton",
  title = 	 "Parallel operation in the control data 6600",
  pages =	 "33--41",
  booktitle =	 "AFIPS 1964 Fall joint computer conference",
  year =	 "1965",
  publisher =	 "Spartan Books",
  note =	 "Reprinted in Computer Structures: Principles and
		  Examples, chapter 43, McGraw-Hill 1982",
  note2 =	 "HAR",
  abstract =	 "Describes architectural and implementation details
		  of the Control-DATA CDC6600 computer. Known as the
		  first computer to use multithreaded techniques"
}

@InProceedings{Toda91,
  author = 	 "Kenji Toda and Kenji Nishida and Yoshinobu Uchibori and
		  Shuichi Sakai and Toshino Shimada",
  title = 	 "Parallel multi-context architecture with high-speed
		  synchronization mechanism",
  pages =	 "336--343",
  booktitle =	 ipps5,
  year =	 "1991",
  address =	 "Anaheim, CA, USA",
  month =	 "Apr",
  abstract =	 "Current interest in parallel processing architecture
		  is focused on compatibility of extracting
		  parallelism and improving processor utilisation. The
		  authors propose a new parallel processing
		  architecture called CODA which can attain a high
		  processor utilization while extracting parallelism
		  effectively. CODA is based on single-thread pipeline
		  architecture with advanced instruction fetch, which
		  uses processors efficiently. Synchronization can be
		  performed implicitly at a register reading to
		  provide high-speed fine-grain synchronization
		  effectively. CODA also has a hardware multi-context
		  support which reduces the cost of context switch
		  caused by synchronization. Synchronization and
		  packet communication ability are effectively
		  integrated into an execution pipeline by an
		  instruction insertion mechanism."
}

@InProceedings{Tokuda89,
  author = 	 "Hideyuki Tokuda and Cliff W. Mercer and Y. Ishikawa
		  and T. E. Marchok",
  title = 	 "Priority inversions in real-time communication",
  pages =	 "??---??",
  booktitle =	 "Proceedings. Real Time Systems Symposium",
  year =	 "1989",
  address =	 "Santa Monica, CA, USA",
  month =	 "Dec",
  abstract =	 "The priority-inversion problems in real-time
		  communication are addressed, and solutions developed
		  for the ARTS distributed real-time operating system
		  are presented. The performance results of the
		  multi-thread-based protocol implementation are
		  compared with those of other implementation schemes,
		  and the schedulability is analyzed. Experimental
		  results indicate that the multi-thread-based
		  protocol implementation could eliminate potential
		  priority-inversion problems and also demonstrate the
		  same schedulability as the softint implementation
		  scheme in spite of about 10% additional
		  implementation overhead."
}

@TechReport{Tolmach92,
  author =       "Andrew P. Tolmach and J. Gregory Morrisett",
  title =        "A Portable Multiprocessor Interface for Standard
		  {ML} of New Jersey",
  institution =  "Princeton University",
  number =       "TR-376-92",
  pages =        "31",
  month =        jun,
  year =         "1992",
  url =     "http://www.cs.princeton.edu:80/TR/PRINCETONCS:TR-376-92",
  abstract =     "We have designed a portable interface between
		  shared--memory multiprocessors and Standard ML of
		  New Jersey. The interface is based on the
		  conventional kernel thread model and provides
		  facilities that can be used to implement user--level
		  thread packages. The interface supports
		  experimentation with different thread scheduling
		  policies and synchronization constructs. It has been
		  ported to three different multiprocessors and used
		  to construct a general purpose, user--level thread
		  package. In this paper, we discuss the interface and
		  its implementation and performance, with emphasis on
		  the Silicon Graphics 4D/380S multiprocessor."
}

@InProceedings{Tomita92,
  author =       "Tetsuo Hironaka and Takashi Hashimoto and Keizo
		  Okazaki and Kazuaki Murakami and Shinji Tomita",
  title =        "Benchmarking a Vector-Processor Prototype Based on
		  Multithread Streaming/{FIFO} Vector ({MSFV})
		  Architecture",
  year =         "1992",
  month =        jul,
  booktitle =    "6th ACM International Conference on Supercomputing",
  pages =        "272--281",
  address =      "Washington, D.C.",
  note2 =	 "HAR",
  abstract =	 ""
}

@TechReport{Topham87,
  author = 	 "Nigel P. Topham and Amos R. Omondi and Roland N.
		  Ibbett",
  title = 	 "On the Design and Performance of Pipelined
		  Architectures",
  institution =  "University of North Carolina at Chapel Hill,
		  Department of Computer Science",
  year = 	 "1987",
  number =	 "TR87-022",
  note =	 "Also appears in Journal of Supercomputing, Vol.1,
		  No.4, pp.353--393 1988. Also Tech report at University of
		  Edinburgh. Dept. of Computer Science., CSR-247-87",
  abstract =	 "Pipelining is a widely used technique for
		  implementing  architectures which have inherent
		  temporal parallelism  when there is an operational
		  requirement for high  throughput.  Many variations
		  on the basic theme have  been proposed, with varying
		  degrees of success.  The  aims of this paper are
		  twofold.  The first is to present a  critical review
		  of conventional pipelined architectures,  and put
		  some well known problems in sharp relief.  It is
		  argued that conventional pipelined architectures
		  have  underlying limitations which can only be dealt
		  with by  adopting a different view of pipelining.
		  These limitations  are explained in terms of
		  discontinuities in the flow of  instructions and
		  data, and representative machines are  examined in
		  support of this argument.  The second aim is  to
		  introduce an alternative theory of pipelining, which
		  we  call Context Flow, and show how it can be used
		  to  construct efficient parallel systems."
}

@Article{Topham88,
  author =       "Nigel P. Topham and Amos R. Omondi and Roland N.
		  Ibbett",
  title =        "Context Flow: An Alternative to Conventional
		  Pipelined Architectures",
  journal =      jsup,
  year =         "1988",
  pages =        "29--53",
  volume =       "2",
  number =       "1",
  month =        sep,
  note2 =        "HAR",
  abstract =	 "In an earlier paper (Topham, Omondi, and Ibbett
		  1988) the authors reviewed conventional approaches
		  to the design of conventional pipelined
		  architectures and concluded that the underlying
		  limitations of these techniques necessitate radical
		  alternatives. This paper discusses one such
		  alternative, micromultiprogramming, and examines
		  several implementation proposals based on this
		  approach. The weaknesses of these proposals are
		  highlighted, and it is argued that they can be
		  eliminated within the unifying theory of context
		  flow architectures. Implementations of uniprocessor
		  and multiprocessor context flow architectures are
		  considered."
}

@InProceedings{Traub89,
  author =       "Kenneth R. Traub",
  title =        "Compilation as Partitioning: {A} New Approach to
		  Compiling Non-Strict Functional Languages",
  booktitle =    "Proceedings of the Conference on Functional
		  Programming Languages and Computer Architecture '89,
		  Imperial College, London",
  pages =        "75--88",
  publisher =    "ACM",
  address =      "New York, NY",
  year =         "1989",
  abstract =     "In non-strict functional languages, a data structure
		  may be read before all its components are written,
		  and a function may return a value before finishing
		  all its computation or even before all its arguments
		  have been evaluated. Such flexibility gives
		  expressive power to the programmer, but makes life
		  difficult for the compiler because it may not be
		  possible to totally order instructions at compile
		  time; the correct order can very dramatically with
		  the input data. Consequently, the compiler must
		  break the program into sequential fragments, or
		  threads, whose relative ordering is determined at
		  run time. Good compilers employ strictness analysis
		  and other techniques to make threads as large as
		  possible, to minimize run-time overhead. While
		  partitioning a program into sequential threads is a
		  crucial issue, existing compilers treat it as a
		  byproduct of applying some other methodology, such
		  as performing Henderson's force/delay
		  transformations or generating intermediate code for
		  an abstract graph reduction machine (e.g., the
		  G-machine or Tim). In this paper, we present a view
		  of functional language compilation that takes
		  partitioning a function into sequential threads as
		  the first order of business. The resulting framework
		  cleanly separates issues of partitioning, of thread
		  scheduling, of environments, and of data type
		  representation (including first-class
		  functions). Our method sidesteps both the
		  force/delay transformation and abstract machines,
		  going directly from source code to sequential
		  three-address code. Nevertheless, nearly all of the
		  optimizations proposed for existing approaches are
		  easily expressed. We consider two non-strict
		  evaluation rules: the familiar lazy evaluation, and
		  lenient evaluation as found in the language Id. We
		  also consider both uniprocessor and parallel
		  processor scheduling policies."
}

@InProceedings{Traub91,
  author = 	 "Kenneth R. Traub and Gregory M. Papadopoulos and
		  Michael J. Beckerle and James E. Hicks",
  title = 	 "Overview of the Monsoon Project",
  booktitle =	 iccd91,
  year =	 "1991",
  pages =	 "150--155",
  note =	 "Also as CSG-Memo-338, Massachussets Institute of
		  Technology, Laboratory for Computer Science",
  note2 =	 "HAR",
  available=     "ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-338.ps.gz",
  abstract =	 "Monsoon is an experimental multi-threaded
		  multi-processor targeted to large-scale, general
		  purpose scientific and symbolic computations. In
		  particular, Monsoon is designed for the efficient
		  execution of code compiled from Id, a high level,
		  implicitly parallel declarative language [6].
		  Monsoon is a product of a multiyear cooperative
		  research and development program between the
		  Massachusetts Institute of Technology and Motorola,
		  Inc., which, in turn, is an outgrowth of over ten
		  years of research in dynamic dataflow architectures
		  and languages conducted at MIT. The intent of this
		  paper, and of its sequels [5, 4], is to provide an
		  overall view of the Monsoon hardware and software
		  system architecture."
}

@InProceedings{Traub91a,
  author =       "Kenneth R. Traub",
  title =        "Multi-thread Code Generation for Dataflow
		  Architectures from Non-Strict Programs", 
  booktitle =    "Proceedings of the 1991 Conference on Functional
		  Programming Languages and Computer Architecture", 
  pages =        "73--101",
  publisher =    "Springer-Verlag",
  series =       "Lecture Notes in Computer Science 523",
  address =      "Cambridge, MA, USA",
  year =         "1991",
  abstract =     "This paper presents a new style of code generation
		  for dataflow architectures, based on a view of such
		  architectures as general multi-threaded von Neumann
		  machines. Whereas the traditional picture of
		  dataflow object code consists of tokens flowing
		  along the arcs of a dataflow graph, the
		  multi-threaded style treats a linear sequence of
		  dataflow instructions as a sequential thread. Within
		  a thread, data is passed by imperatively reading and
		  writing an activation frame associated with a
		  procedure invocation, just as in conventional
		  architectures. Also, advanced dataflow architectures
		  like Monsoon provide specific support for sequential
		  threads, in the form of general registers for
		  operand storage within a thread. Between threads,
		  values are also communicated via the activation
		  frame. The generation and synchronization of tokens,
		  which in traditional dataflow is the primary means
		  of communicating data, is relegated haere to a
		  purely control flow role, spawning new threads and
		  gating their initiation, but themselves carrying no
		  data. Our results show that in many cases, the
		  multi-threaded style of code generation results in
		  fewer total machine cycles to executed a given
		  program as compared to the traditional dataflow
		  style. Surprisingly, this remains true even if the
		  general registers are not employed. The reason the
		  multi-threaded style is so successful is that by
		  separating data flow from control flow, redundant
		  control flow can be eliminated. In other words,
		  fewer forking (token creation) and joining (token
		  matching) operations are required. Central to
		  eliminating redundant control flow is the compiler's
		  ability to discover sequential threads that may be
		  scheduled at compile time. While this is inherently
		  difficult when starting from a non-strict language
		  such as Id, even small threads have a beneficial
		  effect. The most dramatic performance improvement
		  comes about in compiling Id loop, where the language
		  semantics specifies a greater degree of strictness,
		  resulting in larger threads. We demonstrate the
		  comparative performance between the multi-thread and
		  traditional dataflow styles of code generation
		  through empirical observations on the Monsoon
		  hardware, and also through a detailed analytic
		  examination of the code generated under each
		  paradigm."
}

@TechReport{Traub92a,
  author = 	 "Kenneth R. Traub and Gregory M. Papadopoulos and J.
		  Young",
  title = 	 "Monsoon macroarchitecture reference manual ver.003",
  institution =  "Cambridge Reseach Center, Motorola Inc.",
  year = 	 "1992",
  number =	 "MCRC-TR-23",
  month =	 "Jan",
  abstract =	 ""
}

@InProceedings{Traub92b,
  author = 	 "Kenneth R. Traub and David E. Culler and Klaus E.
		  Schauser",
  title = 	 "Global Analysis for Partitioning Non-Strict Programs
		  into Sequential Threads",
  pages =	 "324--334",
  booktitle =	 "Proceedings of the ACM Conference on LISP and
		  Functional Programming",
  year =	 "1992",
  address =	 "San Francisco, CA",
  month =	 "Jun",
  note2 =	 "HAR",
  available=     "ftp://ftp.cs.berkeley.edu/ucb/TAM/lfp92.ps",
  abstract =	 "We present a new solution to the problem of
		  compiling an eager, non-strict language into
		  multiple sequential threads. The solution is
		  described using an intermediate program form
		  developed for the programming language Id (R.S.
		  Nikhil, 1990), a functional language extended with
		  I-structures (Arvind et al., 1986) and M-structures
		  (P.S. Barth et al., 1991). A similar intermediate
		  form has also been suggested for imperative
		  languages (M. Beck, K. Pingali, 1989), as a way of
		  exposing parallelism in those languages. With
		  suitable restrictions, we believe the method is also
		  appropriate for lazy, purely functional languages
		  such as Haskell. Throughout the paper, a thread will
		  mean a subset of the instructions comprising a
		  procedure body."
}

@Article{Treleaven82,
  author = 	 "Philip Treleaven and Richard P. Hopkins and Paul
		  W. Rautenbach",
  title = 	 "Combining data flow and control flow computing",
  journal =	 "Computer Journal",
  year =	 "1982",
  volume =	 "25",
  number =	 "2",
  pages =	 "207--217",
  month =	 "Feb",
  note =	 "Reproduced in ``Selected Reprints on Dataflow and
		  Reduction Architectures'' ed. S. S. Thakkar, IEEE,
		  1987, pp. 355-365.",
  note2 =	 "HAR",
  abstract =	 ""
}

@InProceedings{Trembaly95,
  author = 	 "Marc Trembaly and Bill Joy and Ken Shin",
  title = 	 "A Three Dimensional Register File for Superscalar
		  Processors",
  volume =	 "?",
  pages =	 "??--??",
  booktitle =	 hicss28,
  year =	 "1995",
  abstract =	 "The register file is a key datapath component of a
		  superscalar microprocessor. Its access time is
		  critical since it can impact cycle time. Its size
		  can easily become a problem: superscalar
		  microprocessors have a large number of ports
		  (typically 10 for a three-scalar machine) and the
		  size is quadratic in the number of ports. The ``3-D
		  Register File'' uses the area inherently consumed by
		  the metal wires used for the word and bit lines for
		  each cell the hide N sets of registers. Each set is
		  logically a plane in the third dimension. The
		  ability to access multiple planes can be used for
		  register windows or for extra register sets for real
		  time tasks or microtask switching. The data array of
		  a 3-D eight-window 10 ported register file is six
		  times smaller than a flat register file. Access time
		  is sped up by shortening bus lines and by sharing a
		  large buffer between bit cells. The 3-D register
		  file has been implemented on two high performance
		  superscalar processors and early silicon confirms
		  our simulations."
}

@InProceedings{Tremblay87,
  author = 	 "Marc Tremblay and Miquel Huguet",
  title = 	 "VLSI implementation of a Shift-register file",
  volume =	 "1",
  pages =	 "112--121",
  booktitle =	 hicss20,
  year =	 "1987",
  month =	 "Jan",
  abstract =	 ""
}

@Article{Tsunedomi91,
  author = 	 "K. Tsunedomi and Akira Fukuda and Kazuaki Murakami and
		  Shinji Tomita",
  title = 	 "A message-pool-based parallel operating system for
		  the Kyushu University reconfigurable parallel
		  processor-parallel creation of multiple threads",
  journal =	 "Journal of Information Processing",
  year =	 "1991",
  volume =	 "14",
  number =	 "4",
  pages =	 "423--432",
  abstract =	 "The Kyushu University reconfigurable parallel
		  processor system under development is a MIMD-type
		  multiprocessor which consists of 128 processing
		  elements, interconnected by a full (128*128)
		  crossbar network. Reconfigurable memory architecture
		  employed by the system allows the system to be
		  configured as either a shared-memory TCMP, a
		  message-passing LCMP, or a hybrid of the two. A
		  parallel operating system under development is for
		  the shared-memory TCMP, and aims at extracting
		  various kinds of parallelism of the operating system
		  itself to provide high-performance. To exploit the
		  parallelism, the operating system is constructed by
		  using a message-pool mechanism. A typical example of
		  the parallelism is the parallel creation of multiple
		  threads. The paper proposes four schemes for the
		  parallel creation; the simple parallel scheme, the
		  parallel template scheme, the chunk scheme, and the
		  combination scheme. Simulation results show that the
		  chunk scheme is the most desirable."
}

@InProceedings{Tullsen95,
  author = 	 "Dean M. Tullsen and Susan Eggers and Henry M.
		  Levy",
  title = 	 "Simultaneous Multithreading: Maximizing On-Chip
		  Parallelism",
  pages =	 "??--??",
  booktitle =	  isca22,
  year =	 "1995",
  month =	 "Jun",
  note2 =	 "HAR",
  url = 	 "http://www.cs.washington.edu/research/arch/mult-sim.html",
  abstract =	 "This paper examines simultaneous multithreading, a
		  technique permitting several independent threads to
		  issue instructions to a superscalar's multiple
		  functional units in a single cycle. We present
		  several models of simultaneous multithreading and
		  compare them with alternative organizations: a wide
		  superscalar, a fine-grain multithreaded processor,
		  and single-chip, multiple-issue multiprocessing
		  architectures. Our results show that both
		  (single-threaded) superscalar and fine-grain
		  multithreaded architectures are limited in their
		  ability to utilize the resources of a wide-issue
		  processor. Simultaneous multithreading has the
		  potential to achieve 4 times the throughput of a
		  superscalar, and double that of fine-grain
		  multithreading. We evaluate several cache
		  configurations made possible by this type of
		  organization and evaluate tradeoffs between them. We
		  also show that simultaneous multithreading is an
		  attractive alternative to single-chip
		  multiprocessors; simultaneous multithreaded
		  processors with a variety of organizations
		  outperform corresponding conventional
		  multiprocessors with similar execution resources.
		  While simultaneous multithreading has excellent
		  potential to increase processor utilization, it can
		  add substantial complexity to the design. We examine
		  many of these complexities and evaluate alternative
		  organizations in the design space."
}

@InProceedings{Tyson92,
  author = 	 "Gary Tyson and Matthew Farrens and Andrew R.
		  Pleszkun",
  title = 	 "MISC: a multiple instruction stream computer",
  pages =	 "193--196",
  booktitle =	 isma25,
  year =	 "1992",
  month =	 "Dec",
  note2 =	 "HAR",
  url = 	 "http://american.cs.ucdavis.edu/publications/micro25a.ps",
  abstract =	 "This paper describes a single chip multiple
		  instruction stream computer (MISC) capable of
		  extracting instruction level parallelism from a
		  broad spectrum of programs. The MISC architecture
		  uses multiple asyncrhonous processing elements to
		  separate a program into streams that can be executed
		  in parallel, and integrated a conflict-free message
		  passing system into the lowest level of the
		  processor design to facilitate low latency
		  intra-MISC communication. This approach allows for
		  increased machine parallelism with minimal code
		  expansion, and provides an alternative approach to
		  single instruction stream multi-issue machines such
		  as Sueprscalar and VLIW"
}

@InProceedings{Ungerer92,
  author = 	 "Theo Ungerer and Eberhard Zehendner",
  title = 	 "Threads and subinstruction level parallelism in a
		  dataflow architecture",
  editor =	 "L. Bouge et al",
  pages =	 "731--736",
  booktitle =	 "Parallel Processing: CONPAR 92-VAPP V. Second Joint
		  International Conference on Vector and Parallel Processing",
  year =	 "1992",
  month =	 "Sep",
  series =       "Lecture Notes in Computer Science 634",
  publisher =	 "Springer-Verlag",
  address =	 "Lyon, France",
  abstract =	 "Presents a dataflow architecture that utilizes task
		  level parallelism by the architectural structure of a
		  distributed memory multiprocessor, instruction level
		  parallelism by a token-passing computation scheme,
		  and subinstruction level parallelism by SIMD
		  evaluation of complex machine instructions.
		  Sequential threads of data instructions are compiled
		  to dataflow macro actors and executed consecutively
		  using registers."
}

@InProceedings{Uvieghara90,
  author = 	 "G.A. Uvieghara and Y. Nakagome and D.K. Jeong and
		  D.A. Hodges",
  title = 	 "An on-chip smart memory for a data-flow CPU",
  pages =	 "84--94",
  booktitle =	 "IEEE VLSI Circuits Symposium",
  year =	 "1989",
  address =	 "Kyoto, Japan",
  month =	 "May",
  note =	 "Published in IEEE Journal of Solid-State Circuits
		  Vol.25, No.1, pp.84--94, Feb.1990",
  abstract =	 " Register Alias Table (RAT) is a smart memory that
		  is embedded in HPSm (High-Performance Substrate), a
		  Berkeley data-flow CPU. It is a multiport memory
		  that has content addressability and support for
		  branch prediction and exception handling, in
		  addition to conventional read and write
		  operations. An experimental 1240-b smart memory chip
		  is implemented in a 1.6- mu m double-metal scalable
		  CMOS process. This memory performs 15 operations
		  within a cycle time of 100 ns, has 34658
		  transistors, occupies an area of 3.8 mm*5.2 mm, and
		  dissipates 0.51 W."
}

@InProceedings{Vasell94,
  author = 	 "Jesper Vasell",
  title = 	 "A Fine-Grain Threaded Abstract Machine",
  booktitle =	 pact94,
  year = 	 "1994",
  pages = 	 "15--24",
  note =	 "Also as Tech report TR-176,Jan.1994, Chalmers University of
		  Technology, Department of Computer Engineering",
  note2 =	 "HAR",
  url = 	 "ftp://www.ce.chalmers.se/pub/cal/reports/TR-176.ps.Z",
  abstract =	 "This paper presents an execution model, called S-TAM
		  which supports efficient explotation of fine-grain
		  parallelism. It uses a mix of static scheduling
		  performed by the compiler, and dynamic scheduling
		  based on very simple scheduling mechanisms in order
		  not to compromise the granularity"
}

@TechReport{Velde93a,
  author = 	 "Wim Vande Velde and Johan Opsommer and Erik H.
		  D'Hollander",
  title = 	 "Realisation of Microkernel Thread Schedulers",
  institution =  "University of Ghent, Belgium, Department of
		  Electronics and Information Systems",
  year = 	 "1993",
  number =	 "dg93-6",
  month =	 "Jan",
  url = "ftp://ftp.elis.rug.ac.be/pub/parallel/techreports/dg93_6.ps.Z",
  abstract =	 "This paper examines the performance implications of
		  several  algorithms for thread (``lightweight''
		  process) management in shared-memory
		  multiprocessors.  The algorithms are analysed and
		  tested on the Virtual Processor System (VPS).  Small
		  differences  in thread management are shown to have
		  significant performance  impact for fine-grained
		  parallel applications. These different  ways of
		  thread management result in the implementation of
		  different kernels"
}

@InProceedings{Velde93b,
  author = 	 "Wim Vande Velde and Johan Opsommer and Erik H.
		  D'Hollander",
  title = 	 "Performance modeling of microkernel thread
		  schedulers for shared memory multiprocessors",
  pages =	 "736--739",
  booktitle =	 parle93,
  year =	 "1993",
  month =	 "Jun",
  series =	 "Lecture Notes in Computer Science ??",
  address =	 "Munich, Germany",
  note2 =	 "HAR",
  url = 	 "ftp://ftp.elis.rug.ac.be/pub/parallel/parle93_kernel.ps.Z",
  abstract =	 "The scheduling policy of a microkernel significantly
		  affects the parallel execution of fine-grained
		  programs. Several thread management alternatives are
		  implemented on a shared memory system with 6
		  processors. The speedup and execution behaviour is
		  monitored for programs with a varying degree of
		  granularity and parallelism. For each scheduling
		  policy a suitable queueing network is developed and
		  identified with the observed execution using the
		  QNAP2 queueing network software package. Because of
		  the close agreement between the predicted and the
		  observed behaviour, the models allow to compare the
		  scheduling policies, pin-point the bottlenecks in
		  the algorithm and the shared data structures, and
		  improve the scheduling discipline significantly. "
}

@TechReport{Velde93c,
  author = 	 "Wim Vande Velde and Johan Opsommer and Erik H.
		  D'Hollander",
  title = 	 "Implementation and Evaluation of Thread Schedulers
		  for Shared Memory Multiprocessors",
  institution =  "University of Ghent, Belgium, Department of
		  Electronics and Information Systems",
  year = 	 "1993",
  number =	 "dg93-4",
  month =	 "Jan",
  note2 =	 "HAR",
  url = "ftp://ftp.elis.rug.ac.be/pub/parallel/techreports/dg93_4.ps.Z",
  abstract =	 "The scheduling policy of a microkernel significantly
		  affects the parallel execution of fine-grained
		  programs. In this study several thread management
		  alternatives were implemented on a shared memory
		  system with 6 processors. The speedup and execution
		  behaviour was monitored for programs with a varying
		  degree of granularity and parallelism. Then for each
		  scheduling policy a suitable queueing network was
		  developed and identified with the observed execution
		  using the QNAP2 queueing network software package.
		  Because of the close agreement between the predicted
		  and the observed behaviour, the models allowed to
		  compare the scheduling policies, pin-point the
		  bottlenecks in the algorithm and the shared data
		  structures, and improve the scheduling discipline
		  significantly."
}

@TechReport{Vuong-adlerberg89,
  author = 	 "Ingmar Vuong-Adlerberg",
  title = 	 "Cache for Multi-Threaded processors on a
		  Split Transaction Bus",
  institution =  "Massachusetts Institute of Technology, Laboratory
		  of Computer Science",
  year = 	 "1989",
  number =	 "MIT/LCS/TR-466",
  month =	 "Nov",
  note =	 "30 pages",
  abstract = 	 "A multi-threaded processor has several sets of
		  registers, and therefore can keep several tasks in a
		  state of being ready to run. This ability to combine
		  several independent instruction streams prevents
		  such a processor from getting systematically blocked
		  upon every cache miss involving a long memory
		  access. Unfortunately, upon a cache miss,
		  conventional caches remain unavailable for further
		  processor requests until the cache miss is
		  completely processed. This of course defeats the
		  purpose of this kind of architecture, since memory
		  accesses performed by the other threads might hit in
		  the cache and therefore succeed. Instead, the
		  processor stays idle. This article describes a cache
		  architecture capable of servicing processor requests
		  even while a memory access is currently being
		  performed. For further efficiency reasons, this
		  cache communicates with the memory via a split
		  transaction bus. These two features increase
		  substantially the amount of state information to be
		  kept along with each cache entry, making the cache
		  automaton and protocol quite complicated. We detail
		  the kind of consistency provided by our cache, along
		  with a proof of its validity. As very little
		  theoretical support exists for this kind of proof,
		  we also present a formalism that we developed in the
		  course of this project, and which is suitable for
		  expressing statements of consistency."
}

@TechReport{Wagner91,
  author = 	 "David B. Wagner",
  title = 	 "Awesime/Cthreads library user's manual",
  institution =  "University of Colorado, Boulder, Dept. of Computer
		  Science",
  year = 	 "1991",
  number =	 "CU-CS-567-91",
  month =	 "Jan",
  abstract =	 "The Awesime/Cthreads library is a software package
		  that provides a Mach Cthreads [1] interface on top
		  of the Awesime thread library. The library allows
		  Cthreads programs to be written and debugged on
		  machines that do not support the Mach operating
		  system. Such programs typically run in the Mach
		  environment with no changes required. The underlying
		  runtime system, Awesime [2], is very portable and
		  currently runs on the DECstation, Sparc (including
		  multiprocessors such as the Solbourne), Sequent, and
		  Encore architectures. The Cthreads interface is
		  simple and is ideal for educational use. It has been
		  used as the implementation environment for
		  programming assignments in CSCI 5573, a
		  graduate-level operating systems course at the
		  University of Colorado.It should also be suitable
		  for an undergraduate-level operating systems course,
		  or a course in parallel programming."
}

@InProceedings{Wagner94,
  author = 	 "T. D. Wagner and Evgenia Smirni and Amy W. Apon and
		  Manish Madhukar and Larry W. Dowdy",
  title = 	 "The effects of thread placement on the KSR1",
  pages =	 "618--624",
  booktitle =	 ipps8,
  year =	 "1994",
  address =	 "Cancun, Mexico",
  month =	 "Apr",
  url = 	 "http://www.vuse.vanderbilt.edu/~peg/publications/IPPS94.ps",
  abstract =	 "This paper describes a effects of thread placement
		  on memory access times measurement study on the
		  Kendall Square KSR1 multiprocessor. The KSR1 uses a
		  conventional shared memory programming model in a
		  distributed memory architecture based on a ring of
		  rings of 64-bit superscalar microprocessors. Memory
		  consists of local cache memories attached to each
		  processor and is managed in a cache-only memory
		  architecture (COMA) fashion. Experiments run on the
		  KSR1 across a variety of thread configurations show
		  that shared memory access is accelerated through
		  strategic placement of threads which share data. The
		  experiments 'stress test' the automatic prefetching
		  feature of the hardware. Strategies to keep the KSR1
		  memory access times nearly constant even when the
		  number of participating threads increases are
		  proposed."
}

@InProceedings{Waldspurger93,
  author = 	 "Carl A. Waldspurger and William E. Weihl",
  title = 	 "Register relocation: flexible contexts for
		  multithreading",
  pages =	 "120--130",
  booktitle =	 isca20,
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "May",
  note =	 "Published in " # canews # " Vol.21, No.2, May 1993",
  note2 =	 "HAR",
  abstract =	 "Multithreading is an important technique that
		  improves processor utilization by allowing
		  computation to be overlapped with the long latency
		  operations that commonly occur in multiprocessor
		  systems. The paper presents register relocation, a
		  new mechanism that efficiently supports flexible
		  partitioning of the register file into variable-size
		  contexts with minimal hardware support. Since the
		  number of registers required by thread contexts
		  varies, this flexibility permits a better
		  utilization of scarce registers, allowing more
		  contexts to be resident, which in turn allows
		  applications to tolerate shorter run lengths and
		  longer latencies. Experiments show that compared to
		  fixed-size hardware contexts, register relocation
		  can improve processor utilization by a factor of two
		  for many workloads."
}

@Unpublished{Walmer88,
  author = 	 "Linda R. Walmer and Mary R. Thompson",
  title = 	 "A Programmer's Guide to the Mach User Environment",
  note = 	 "School of Computer Science, Carnegie Mellon University",
  year =	 "1988",
  month =	 "Feb",
  note2 =	 "HAR",
  url = 	 "ftp://mach.cs.cmu.edu/afs/cs/project/mach/public/doc/upublished/machuse.ps",
  abstract =	 "This document is one of two tutorials designed to
		  teach basic Mach programming skills. This manual
		  demonstrates the use of the C Threads library
		  primitives in writing a multithreaded program and
		  the use of the Mach Interface Generator (MIG) to
		  generate remote procedure calls for interprocess
		  communication. The reader should be familiar with
		  the basic Mach abstractions of ports, messages,
		  virtual memory, tasks and threads which the
		  introduction to the companion document, A
		  Programmer's Guide to the Mach System Calls,
		  explains. "
}

@InProceedings{Weber89,
  author = 	 "Wolf-Dietrich Weber and Anoop Gupta",
  title = 	 "Exploring the benefits of multiple hardware contexts
		  in a multiprocessor architecture: preliminary
		  results",
  pages =	 "273--280",
  booktitle =	 isca16,
  year =	 "1989",
  address =	 "Jerusalem, Israel",
  month =	 "May",
  note2 =	 "HAR",
  abstract =	 "The authors explore the extent to which multiple
		  hardware contexts per processor can help to mitigate
		  the negative effects of high latency. In particular,
		  they evaluate the performance of a directory-based
		  cache coherent multiprocessor using memory reference
		  traces obtained from three parallel applications.
		  The authors explore the case where there are a small
		  fixed number (2-4) of hardware contexts per
		  processor and the context switch overhead is low. In
		  contrast to previously proposed approaches, they
		  also use a very simple context switch criterion,
		  namely a cache miss or a write-hit to shared data.
		  The results show that the effectiveness of multiple
		  contexts depends on the nature of the applications,
		  the context switch overhead, and inherent latency of
		  the machine architecture. Given reasonably low
		  overhead hardware context switches, the authors show
		  that two or four contexts can achieve substantial
		  performance gains over a single context. For one
		  application, the processor utilization increased by
		  about 46% with two contexts and by about 80% with
		  four contexts."
}

@InProceedings{Weiser89,
  author = 	 "Mark Weiser and Allan Demers and Carl Hauser",
  title = 	 "The Portable Common Runtime Approach to
		  interoperability",
  booktitle =	 sosp13,
  year =	 "1989",
  pages =	 "114--122",
  month =	 "Dec",
  note =	 "Published in " # acmosr # " Vol.23, No.5",
  url = 	 "ftp://ftp.parc.xerox.com/pub/pcr/PCRSOSP.ps.Z",
  abstract =	 "The authors have built the Portable Common Runtime
		  (PCR), a language-independent and
		  operating-system-independent base for modern
		  languages. PCR offers four interrelated facilities:
		  storage management (including universal garbage
		  collection), symbol binding (including static and
		  dynamic linking and loading), threads (lightweight
		  processes), and low-level I/O (including network
		  sockets). PCR is 'common' because these facilities
		  simultaneously support programs in several
		  languages. PCR supports C, Cedar, Scheme, and
		  CommonLisp intercalling and runs pre-existing C and
		  CommonLisp (Kyoto) binaries. PCR is 'portable'
		  because it uses only a small set of operating system
		  features. The PCR source code is available for use
		  by other researchers and developers."
}

@InProceedings{Wheat94,
  author = 	 "Stephen R. Wheat and Arthur B. Maccabe and Rolf
		  Riesen and David W. van Dresser and T. Mack Stallcup",
  title = 	 "{PUMA}: an operating system for massively parallel
		  systems",
  volume =	 "II",
  pages =	 "56--65",
  booktitle =	 hicss27,
  year =	 "1994",
  address =	 "Wailea, HI, USA",
  month =	 "Jan",
  url =     "ftp://www.cs.sandia.gov/pub/sunmos/papers/hicss.ps.Z",
  abstract =	 "This paper presents an overview of PUMA
		  (Performance-oriented, User-managed Messaging
		  Architecture), a message passing kernel. Message
		  passing in PUMA is based an portals-an opening in
		  the address space of an application process. Once an
		  application process has established a portal, other
		  processes can write values into the portal using a
		  simple send operation. Because messages are written
		  directly into the address space of the receiving
		  process, there is no need to buffer messages in the
		  PUMA kernel and later copy them into the
		  applications address space. PUMA consists of two
		  components: the quintessential kernel (Q-Kernel) and
		  the process control thread (PCT). While the PCT
		  provides management decisions, the Q-Kernel controls
		  access and implements the policies specified by the PCT."
}

@InProceedings{Wing92,
  author = 	 "Jeannette M. Wing and Manuel Faehndrich and J. Gregory
		  Morrisett and Scott Nettles",
  title = 	 "Extensions to Standard {ML} to Support Transactions",
  year = 	 "1992",
  pages =	 "104--118",
  booktitle =    "1992 ACM Workshop on ML and its Applications",
  note =         "Also as tech report CMU-CS-92-132 Carnegie Mellon
		  University, School of Computer Science 1992",
  url =  	 "http://rose.mercury.acs.cmu.edu:1082/TR/CMU:CS-92-132",
  abstract = 	 "A transaction is a control abstraction that lets
		  programmers treat a sequence of operations as an
		  atomic (``all-or-nothing'') unit. This paper describes
		  our progress on on-going work to extend {SML} with
		  transactions. What is novel about our work on
		  transactions is support for multi-threaded
		  concurrent transactions. We use {SML's} modules
		  facility to reflect explicitly orthogonal concepts
		  heretofore inseparable in other transaction-based
		  programming languages."
}

@TechReport{Wright92,
  author = 	 "R.N. Wright",
  title = 	 "Library for synthetic multithread trace generation",
  institution =  "Hewlett-Packard Lab",
  year = 	 "1992",
  number =	 "HPL-92-108",
  address =	 "Palo Alto, CA, USA",
  abstract =	 "Gathering a large and varied set of disk traces is
		  often difficult. Furthermore, gathered disk traces
		  are not necessarily representative of the full space
		  of possible traces and may be hard to scale. The
		  author presents a method for generating synthetic
		  disk traces that allows for better coverage and
		  control of an enormous trace space. To more closely
		  model the real world, the traces consist of multiple
		  concurrent threads, each corresponding to I/O
		  requests made by a single process."
}

@InProceedings{Yamaguchi89,
  author = 	 "Yoshinori Yamaguchi and Shuichi Sakai and Kei Hiraki and
		  Yuetsu Kodama and Tositsugu Yuba",
  title = 	 "An architectural design of a highly parallel
		  dataflow machine",
  pages =	 "1155--1160",
  booktitle =	 "Proceedings of the IFIP 11th World Computer
		  Congress, Information Processing 89.",
  year =	 "1989",
  month =	 "Aug",
  abstract =	 "Presents an architectural design of the highly
		  parallel dataflow machine EM-4. The design principle
		  of the EM-4 is to construct a highly parallel
		  machine with high performance using compact
		  architecture by reducing the several defects of
		  dataflow machines. The special design features of
		  the EM-4 are: introducing a more sophisticated
		  dataflow model named the strongly connected arc
		  model; a direct matching scheme; a RISC based
		  processing element; a versatile processor connected
		  network; an efficient load balancing mechanism. The
		  prototype of the EM-4 is under developing. It
		  consists of 80 processing elements. The processing
		  element of the prototype is specially designed by
		  gate array on a single chip which has about fifty
		  thousand gates. The architecture of the EM-4
		  prototype is also presented."
}

@Article{Yamaguchi91,
  author = 	 "Yoshinori Yamaguchi and Shuichi Sakai and Yuetzu
		  Kodama ",
  title = 	 "Synchronization mechanisms of a highly parallel
		  dataflow machine EM-4",
  journal =	 "IEICE Transactions",
  year =	 "1991",
  volume =	 "E74",
  number =	 "1",
  pages =	 "204--213",
  month =	 "Jan",
  abstract =	 "Presents the synchronization mechanisms of the
		  highly parallel dataflow machine EM-4 with some
		  results of measurement. First, various
		  synchronization mechanisms of parallel computers are
		  surveyed and compared, including dataflow
		  synchronization. Then, the fundamental
		  synchronization mechanisms of the EM-4 are shown,
		  examining the reason why they are adopted. There are
		  three types of synchronizations: (1) strongly
		  connected instruction sequencing, (2) instruction
		  level direct matching, and (3) function level
		  synchronization. These mechanisms are preliminary
		  evaluated on the EM-4 prototype, and the results are
		  reported and analyzed. Next, synchronization
		  mechanisms for resource managements are described."
}

@InProceedings{Yamaguchi92,
  author = 	 "Yoshinori Yamaguchi and Shuichi Sakai and Yuetsu
		  Kodama",
  title = 	 "High performance synchronization mechanisms for
		  general purpose parallel computers",
  pages =	 "160--168",
  booktitle =	 "Proceedings of a JSPS Seminar. Parallel Programming
		  Systems",
  year =	 "1992",
  month =	 "May",
  abstract =	 "The synchronization mechanisms of the dataflow
		  machine EM-4 are described. The authors invented
		  efficient synchronization mechanisms by totally
		  reconsidering the defects of conventional parallel
		  computers, including dataflow machines. The design
		  of EM-4 shows the following novel schemes to
		  actually implement the fundamental synchronizations
		  efficiently: strongly connected arc data-flow model;
		  and direct matching scheme. As a result, EM-4 can
		  support low cost synchronizations such as dataflow
		  synchronization at the dataflow node level, register
		  based synchronization within the strongly connected
		  block level, and dynamic synchronizations at the
		  functional level. The authors measure the total
		  synchronization cost of EM-4 prototype by showing
		  the total ability of communication and
		  synchronization, and the effect of the strongly
		  connected arc model. They found that the
		  synchronization cost of the EM-4 prototype is small
		  enough to make all the PEs totally busy for almost
		  the whole execution time."
}

@InProceedings{Yamamoto94,
  author = 	 "Wayne Yamamoto and  Mauricio J. Serrano and  Adam R.
		  Talcott and Roger C. Wood and Mario Nemirovsky",
  title = 	 "Performance Estimation of Multistreamed, Superscalar
		  Processors",
  pages =	 "194--204",
  volume =	 "1",
  booktitle =	 hicss27,
  year =	 "1994",
  month =	 "Jan",
  abstract =	 "Multistreamed processors can significantly improve
		  processor throughput by allowing interleaved
		  execution of instructions from multiple instruction
		  streams. In this paper, we present an analytical
		  modeling technique to evaluate the effect of
		  dynamically interleaving additional instruction
		  streams within superscalar architectures. Using this
		  technique, estimates of the instructions executed
		  per cycle (IPC) for a processor architecture are
		  quickly calculated given simple descriptions of the
		  workload and hardware characteristics. To validate
		  this technique, estimates of the SPEC89 benchmark
		  suite obtained from the model are compared to
		  results from a hardware simulator. Our results show
		  the technique produces accurate estimates with an
		  average deviation of 4% from the simulation results.
		  Finally, we demonstrate that as the number of
		  functional units increases, multistreaming is an
		  effective technique to exploit these additional
		  resources."
}

@InProceedings{Yamamoto95,
  author = 	 "Wayne Yamamoto and Mario Nemirovsky",
  title = 	 "Increasing Superscalar Performance Through
		  Multistreaming",
  pages =	 "??--??",
  booktitle =	 pact95,
  year =	 "1995",
  month =	 "June",
  abstract =	 ""
}

@InProceedings{Yasugi94,
  author = 	 "Masahiro Yasugi and Satoshi Matsuoka and Akinori
		  Yonezawa",
  title = 	 "The plan-do style compilation technique for eager
		  data transfer in thread-base execution.",
  pages = 	 "57-66",
  booktitle =    pact94,
  year = 	 "1994",
  organization = "IFIP WG10.3",
  month = 	 "Aug",
  url =     "ftp://camille.is.s.u-tokyo.ac.jp/pub/papers/yasugiPACT94-a4.ps.Z",
  abstract =	 "Plan-do compilation technique is a new, advanced
		  compilation framework for eager data transfer on
		  distributed-memory parallel architectures. The
		  technique is especially effective for a recent breed
		  of low-latency architectures by realizing a
		  high-throughput low-latency communication scheme,
		  pipelined sends. The compilation of high-level,
		  plan-do style code into low-level, eager data
		  transfer code is achieved via straightforward
		  application of a set of translation rules.
		  Preliminary low-level benchmark results on a real
		  parallel architecture, EM-4, exhibit good speedups."
}

@PhdThesis{Yeh82,
  author = 	 "Chi-Chung Yeh",
  title = 	 "Shared cache organization for multiple-stream
		  computer",
  school = 	 "University of Illinois at Champaign, Urbana,
		  Coordinated Science Lab.",
  year = 	 "1981",
  note =	 "As Tech report CSG-1, Center for Reliable and
		  High-Performance Computing",
  abstract =	 ""
}

@Article{Yeh83,
  author = 	 "Chi-Chung Yeh and Janak H. Patel and Edward
		  S. Davidson",
  title = 	 "Shared cache for multiple-stream computer systems",
  journal =	 ieeetc,
  year =	 "1983",
  volume =	 "C-32",
  number =	 "1",
  pages =	 "38--47",
  month =	 "Jan",
  note2 =	 "HAR",
  abstract =	 ""
}

@Article{Young-myers93a,
  author =       "Helene Young-Myers and Louiqa Raschid",
  title =        "An Experimental Study of Three Dataflow Paradigms in
		  Multithreaded Database Transitive Closure Algorithms
		  on Shared Memory Multiprocessors",
  journal =      jpal,
  pages =        "371--389",
  volume =       "18",
  number =       "3",
  month =        jul,
  year =         "1993",
  note =	 "Also as Tech report University of Maryland at
		  College Park, CS-TR-3060, UMIACS-TR-93-33",
  abstract =	 "This paper describes an experimental study of three
		  dataflow paradigms, namely, no dataflow, pipelined
		  dataflow, and network dataflow, in multithreaded
		  database transitive closure algorithms on shared
		  memory multiprocessors. This study shows that
		  dataflow paradigm directly influences performance
		  parameters such as the amount of interthread
		  communication, how data are partitioned among the
		  threads, whether access to each page of data is
		  exclusive or shared, whether locks are needed for
		  concurrency control, and how calculation termination
		  is detected. The algorithm designed with no dataflow
		  outperforms the algorithms with
		  dataflow.Approximately linear speedup is achieved by
		  the no dataflow algorithm with sufficient workload
		  and primary memory. An exclusive access working set
		  model and a shared access working set model describe
		  the interactions between two or more threads'
		  working sets when access to eachpage of data is
		  exclusive or shared among the threads, respectively.
		  These models are experimentally verified."
}

@InProceedings{Young-myers93b,
  author = 	 "Helene Young-Myers and Louiqa Raschid",
  title = 	 "Transitive closure: an experimental case study of
		  three multithreaded database algorithms on a shared
		  memory multiprocessor",
  pages =	 "255--259",
  booktitle =	 "Proceedings of the Second International Conference
		  on Parallel and Distributed Information Systems",
  year =	 "1993",
  address =	 "San Diego, CA, USA",
  month =	 "Jan",
  abstract =	 "An experimental performance evaluation of three
		  multithreaded database transitive closure algorithms
		  on a shared memory multiprocessor is described. The
		  algorithms are differentiated on the basis of the
		  dataflow patterns among the threads, namely no
		  dataflow, pipelined dataflow, and network dataflow.
		  Close to linear speedup is achieved by the no
		  dataflow algorithm when there is sufficiently large
		  workload, and sufficient primary memory for
		  processing. Even when communication is through
		  shared memory, the no dataflow algorithm performs
		  significantly better than the algorithms with
		  dataflow. When access to each package of memory is
		  shared by multiple threads, the number of threads
		  significantly affects the performance of the
		  algorithms in limited primary memory. When each page
		  is exclusively accessed by a single thread,
		  performance remains constant when the number of
		  threads is varied."
}

@InProceedings{Zahorjan88,
  author = 	 "John Zahorjan and Edward D. Lazowska and Derek L.
		  Eager",
  title = 	 "Spinning versus blocking in parallel systems with
		  uncertainty",
  editor =	 "T. Hasegawa and H. Takagi and Y. Takahashi",
  pages =	 "455--472",
  booktitle =	 "Performance of Distributed and Parallel Systems.
		  Proceedings of the IFIP TC 7/WG 7.3 International
		  Seminar",
  year =	 "1988",
  publisher =	 "North-Holland, Amsterdam, Netherlands",
  address =	 "Kyoto, Japan",
  month =	 "Dec",
  abstract =	 "In waiting for an event on a parallel machine, a
		  thread of control may either spin (busy wait) or
		  block (relinquish the processor). The appropriate
		  mechanism depends on the relationship of the
		  expected spin time to the context switch time on
		  that machine. If the programmer has accurate
		  information about the behavior of an application,
		  the choice between spinning and blocking can be made
		  relatively easily. This might be the case, for
		  instance, when a parallel machine is dedicated to a
		  single, well understood application. However, in the
		  presence of uncertainty, the choice of mechanism is
		  more difficult. The authors examine the choice
		  between spinning and blocking in environments
		  characterized by two kinds of uncertainty:
		  multiprogramming, where the applications programmer
		  does not have control over which threads are running
		  at any point in time, and data-dependent programs,
		  where expected running times can depend heavily on
		  input data. They compare the loss incurred by
		  spinning in these two environments to that in
		  systems running a single, 'well-behaved'
		  application. The goal is to determine how
		  multiprogramming and data-dependent behavior affect
		  expected spin time, and so complicate the job of
		  selecting the right mechanism. They examine the
		  base, multiprogrammed, and data-dependent
		  environments for two different situations: lock
		  acquisition for mutual exclusion and for barrier
		  synchronization. Using simple analytic models they
		  conclude that for the case of lock acquisition
		  neither multiprogramming nor data-dependent behavior
		  significantly increase the expected spin time, and
		  thus do not complicate the choice of mechanism.
		  However, for barrier synchronization both kinds of
		  uncertainty lead to sharply increased spin times,
		  and thus must be taken into consideration when
		  choosing between spinning and blocking."
}

@Article{Zahorjan91,
  author = 	 "John Zahorjan and Edward D. Lazowska and Derek L.
		  Eager",
  title = 	 "The effect of scheduling discipline on spin overhead
		  in shared memory parallel systems",
  journal =	 ieeepds,
  year =	 "1991",
  volume =	 "2",
  number =	 "2",
  pages =	 "180--198",
  month =	 "Apr",
  abstract =	 "Spinning, or busy waiting, is commonly employed in
		  parallel processors when threads of execution must
		  wait for some event, such as synchronization with
		  another thread. Because spinning is purely overhead,
		  it is detrimental to both user response time and
		  system throughput. The effects of two environmental
		  factors, multiprogramming and data-dependent
		  execution times, on spinning are discussed, and it
		  is shown how the choice of scheduling discipline can
		  be used to reduce the amount of spinning in each
		  case."
}

@InProceedings{Zehendner92,
  author = 	 "Eberhard Zehendner and Theo Ungerer",
  title = 	 "A Large-Grain Data Flow Architecture Utilizing
		  Multiple Levels of Parallelism.",
  pages =	 "23--28",
  booktitle =	 "Proc. CompEuro '92,",
  year =	 "1992",
  address =	 "Den Haag, Netherlands",
  month =	 "Mai",
  abstract =	 ""
}

@InProceedings{Zhou92,
  author = 	 "Hongyi Zhou and Karsten Schwan and Ahmed Gheith",
  title = 	 "Dynamic synchronization of real-time threads for
		  multiprocessor systems",
  pages =	 "93--107",
  booktitle =	 "Proceedings of 3rd symposium on Experiences with
		  Distributed and Multiprocessor systems (SEDMS III)",
  year =	 "1992",
  publisher =	 "USENIX Assoc.",
  address =	 "Newport Beach, CA, USA",
  month =	 "Mar",
  abstract =	 "The authors study mutual exclusion and
		  synchronization for dynamic hard real-time
		  multiprocessor applications. As with any dynamic
		  parallel program, a dynamic real-time application's
		  execution can result in on-line creation of
		  additional tasks, and the creation of such
		  time-constrained tasks cannot be predicted or
		  accounted for prior to program execution. The
		  research results presented concern task
		  synchronization such that guarantees can be made
		  regarding the synchronized tasks' timing
		  constraints. Such guarantees cannot be made without
		  performing on-line schedulability analysis and
		  on-line analysis concerning the maximum time that a
		  task will wait for some resource being acquired with
		  a synchronization primitive. A real-time locking
		  scheme is presented that prevents deadlocks and
		  ensures time-bounded mutual exclusion. The maximum
		  waiting time for a task attempting to acquire a
		  resource is computed with an O(1) algorithm."
}

@InProceedings{Zimmermann95,
  author = 	 "Chris Zimmermann and Vinny Cahill",
  title = 	 "{Roo}: {A} Framework for Real-Time Threads",
  booktitle =	 "Proceedings of the Workshop on Parallel and
		  Distributed Real-Time Systems",
  pages =	 "??--??",
  year =	 "1995",
  url = 	 "ftp://ftp.dsg.cs.tcd.ie/pub/doc/TCD-CS-95-10.ps.gz",
  note =         "Also technical report TCD-CS-95-10, Dept. of
		  Computer Science, Trinity College Dublin.",
  abstract =	 "Traditional object-oriented real-time systems are
		  often limited in that they provide only one approach
		  to real-time object support. Taking the increasing
		  demand for flexible and extensible object support
		  environments into account, we discuss the design and
		  implementation of a small object-oriented real-time
		  executive based on a sub-framework which we call
		  Roo. Roo is a component of the Tigger framework (our
		  proposal for an extensible object support operating
		  system) and is intended to support different object
		  models providing soft real-time behaviour. Roo
		  provides support for different mechanisms and
		  policies for real-time thread management, scheduling
		  and synchronization. In this it serves as a basis
		  for other components of the Tigger framework."
}

@TechReport{Zorn88,
  author = 	 "Benjamin G. Zorn and Paul N. Hilfinger and Kinsom Ho
		  and James R. Larus and Luigi Semenzato",
  title = 	 "Features for Multiprocessing in {SPUR} Lisp",
  institution =  "University of California Berkeley, Department of
		  Computer Science",
  year = 	 "1988",
  number = 	 "UCB-CSD-88-406",
  month = 	 "Mar",
  url =    "ftp://tr-ftp.cs.berkeley.edu/pub/tech-reports/csd/csd-88-406",
  abstract =     "This paper describes simple extensions to Common
		  Lisp for concurrent computation on multiprocessors.
		  Functions for process creation, communication, and
		  synchronization are described. Multiple threads of
		  control are created with process objects.
		  Communication and synchronization are managed using
		  mailboxes. Signals provide asynchronous
		  communication between processes. SPUR Lisp includes
		  future and delay values, which were first introduced
		  in Multilisp [6]. These features provide a flexible
		  and efficient basis on which higher-level
		  multiprocessing abstractions can be implemented and
		  studied"
}

@InProceedings{Zorn89,
  author = 	 "Benjamin Zorn and Kinsom Ho and James R. Larus and
		  Luigi Semenzato and Paul N. Hilfinger",
  title = 	 "Lisp extensions for multiprocessing",
  volume =	 "II",
  pages =	 "761--770",
  booktitle =	 hicss22,
  year =	 "1989",
  publisher =	 "IEEE Comput. Soc. Press",
  address =	 "Hawaii",
  month =	 "Jan",
  abstract =	 "Extensions to Common Lisp for concurrent computation
		  on multiprocessors are discussed. Functions for
		  process creation, communication, and synchronization
		  are described. Process objects create multiple
		  threads of control. Processes are lightweight so
		  that programmers can use them to take advantage of
		  fine-grained parallelism. Communication and
		  synchronization are managed with mailboxes. Signals
		  allow processes to communicate using asynchronous
		  interrupts. These constructs are used to implement
		  several higher-level multiprocessing abstractions.
		  These include structured processes, a parallel tree
		  search, and dataflow computation."
}

@Book{mthread94,
  title = 	 "Multithreaded computer architecture : a summary of
		  the state of the art",
  publisher = 	 "Kluwer Academic",
  year = 	 "1994",
  editor =	 "Robert A. Iannucci and Guang R. Gao and Robert H.
		  Halstead and Burton J. Smith",
  note =	 "ISBN: 0792394771",
  abstract =	 "Multithreaded computer architecture has emerged as
		  one of the most promising and exciting avenues for
		  the exploitation of parallelism. This new field
		  represents the confluence of several independent
		  research directions which have united over a common
		  set of issues and techniques. Multithreading draws
		  on recent advances in dataflow, RISC, compiling for
		  fine-grained parallel execution, and dynamic
		  resource management. It offers the hope of dramatic
		  performance increases through parallel execution for
		  a broad spectrum of significant applications based
		  on extensions to `traditional' approaches.
		  Multithreaded Computer Architecture is divided into
		  four parts, reflecting four major perspectives on
		  the topic. Part I provides the reader with basic
		  background information, definitions, and surveys of
		  work which have in one way or another been pivotal
		  in defining and shaping multithreading as an
		  architectural discipline. Part II examines key
		  elements of multithreading, highlighting the
		  fundamental nature of latency and synchronization.
		  This section presents clever techniques for hiding
		  latency and supporting large synchronization name
		  spaces. Part III looks at three major multithreaded
		  systems, considering issues of machine organization
		  and compilation strategy. Part IV concludes the
		  volume with an analysis of multithreaded
		  architectures, showcasing methodologies and actual
		  measurements.  Multithreaded Computer Architecture:
		  A Summary of the State of the Art is an excellent
		  reference source and may be used as a text for
		  advanced courses on the subject."
}

@misc{sunos5-94,
  author =       "??",
  title =	 "Solaris SunOS 5.0 multithreaded architecture",
  howpublished = "White paper",
  year =	 "199?",
  url =  	 "http://www.sun.com/sunsoft/Developer-products/sig/threads/papers/solaris_whitepaper.ps",
  abstract =	 ""
}

@manual{sunoslwt88,
  title   =      "SUN OS Reference Manual",
  organization=  "SUN Microsystems",
  year     =     "9 May 1988",
  abstract =     "Contains manual for lightweight process system for
		  SunOS 4.x"
}



