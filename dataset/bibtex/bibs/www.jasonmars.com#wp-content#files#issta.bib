@inproceedings{Godefroid:2010:PMS:1831708.1831710,
 author = {Godefroid, Patrice and Kinder, Johannes},
 title = {Proving memory safety of floating-point computations by combining static and dynamic program analysis},
 abstract = {Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buffer overflows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use floating-point instructions available on modern processors, yet existing whitebox fuzzers and SMT constraint solvers do not handle floating-point arithmetic. Are there new security vulnerabilities lurking in floating-point code? A naive solution would be to extend symbolic execution to floating-point (FP) instructions (months of work), extend SMT solvers to reason about FP constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that FP code should only perform memory safe data-processing of the "payload" of an image or video file, while the non-FP part of the application should deal with buffer allocations and memory address computations, with only the latter being prone to buffer overflows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive "may" static analysis of FP instructions with (2) a high-precision whole-program path-sensitive "must" dynamic analysis of non-FP instructions. The aim of this combination is to prove memory safety of the FP part of each execution and a form of non-interference between the FP part and the non-FP part with respect to memory address computations. We have implemented our approach using two existing tools for, respectively, static and dynamic x86 binary analysis. We present preliminary results of experiments with standard JPEG, GIF and ANI Windows parsers. For a given test suite of diverse input files, our mixed static/dynamic analysis is able to prove memory safety of FP code in those parsers for a small upfront static analysis cost and a marginal runtime expense compared to regular dynamic symbolic execution.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831710},
 doi = {http://doi.acm.org/10.1145/1831708.1831710},
 acmid = {1831710},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {program verification, static and dynamic program analysis},
} 

@inproceedings{Bao:2010:SCD:1831708.1831711,
 author = {Bao, Tao and Zheng, Yunhui and Lin, Zhiqiang and Zhang, Xiangyu and Xu, Dongyan},
 title = {Strict control dependence and its effect on dynamic information flow analyses},
 abstract = {Program control dependence has substantial impact on applications such as dynamic information flow tracking and data lineage tracing (a technique tracking the set of inputs that affects individual outputs). Without considering control dependence, information can leak via implicit channels without being tracked; important inputs may be absent from output lineage. However, considering control dependence may lead to a large volume of false alarms in information flow tracking or undesirably large lineage sets. We identify a special type of control dependence called strict control dependence (SCD). The nature of SCDs highly resembles that of data dependences, reflecting strong correlations between statements and hence should be considered the same way as data dependences in various applications. We formally define the semantics. We also describe a cost-effective design that allows tracing only strict control dependence. Our empirical evaluation shows that the proposed technique has very low overhead and it greatly improves the effectiveness of lineage tracing and taint analysis.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831711},
 doi = {http://doi.acm.org/10.1145/1831708.1831711},
 acmid = {1831711},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data dependence, dynamic information flow, strict control dependence, taint analysis},
} 

@inproceedings{Galeotti:2010:AIE:1831708.1831712,
 author = {Galeotti, Juan Pablo and Rosner, Nicol\'{a}s and L\'{o}pez Pombo, Carlos Gustavo and Frias, Marcelo Fabian},
 title = {Analysis of invariants for efficient bounded verification},
 abstract = {SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the error is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this article we present TACO, a prototype tool which implements a novel, general and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate that allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading in the experiments we have carried out to an improvement on the efficiency of the analysis of orders of magnitude, compared to the non-instrumented SAT-based analysis. We show that, in some cases, our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking or SMT-solving.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831712},
 doi = {http://doi.acm.org/10.1145/1831708.1831712},
 acmid = {1831712},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alloy, dynalloy, kodkod, sat-based code analysis, static analysis},
} 

@inproceedings{Carbin:2010:AIC:1831708.1831713,
 author = {Carbin, Michael and Rinard, Martin C.},
 title = {Automatically identifying critical input regions and code in applications},
 abstract = {Applications that process complex inputs often react in different ways to changes in different regions of the input. Small changes to forgiving regions induce correspondingly small changes in the behavior and output. Small changes to critical regions, on the other hand, can induce disproportionally large changes in the behavior or output. Identifying the critical and forgiving regions in the input and the corresponding critical and forgiving regions of code is directly relevant to many software engineering tasks. We present a system, <b>Snap</b>, for automatically grouping related input bytes into fields and classifying each field and corresponding regions of code as critical or forgiving. Given an application and one or more inputs, <b>Snap</b> uses targeted input fuzzing in combination with dynamic execution and influence tracing to classify regions of input fields and code as critical or forgiving. Our experimental evaluation shows that Snap makes classifications with close to perfect precision (99\%) and very good recall (between 99\% and 73\%, depending on the application).},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831713},
 doi = {http://doi.acm.org/10.1145/1831708.1831713},
 acmid = {1831713},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical code, critical input, forgiving code, forgiving input},
} 

@inproceedings{Artzi:2010:DTG:1831708.1831715,
 author = {Artzi, Shay and Dolby, Julian and Tip, Frank and Pistoia, Marco},
 title = {Directed test generation for effective fault localization},
 abstract = {Fault-localization techniques that apply statistical analyses to execution data gathered from multiple tests are quite effective when a large test suite is available. However, if no test suite is available, what is the best approach to generate one? This paper investigates the fault-localization effectiveness of test suites generated according to several test-generation techniques based on combined concrete and symbolic (concolic) execution. We evaluate these techniques by applying the Ochiai</i> fault-localization technique to generated test suites in order to localize 35 faults in four PHP Web applications. Our results show that the test-generation techniques under consideration produce test suites with similar high fault-localization effectiveness, when given a large time budget. However, a new, "directed" test-generation technique, which aims to maximize the similarity between the path constraints of the generated tests and those of faulty executions, reaches this level of effectiveness with much smaller test suites. On average, when compared to test generation based on standard concolic execution techniques that aims to maximize code coverage, the new directed technique preserves fault-localization effectiveness while reducing test-suite size by 86.1\% and test-suite generation time by 88.6\%.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831715},
 doi = {http://doi.acm.org/10.1145/1831708.1831715},
 acmid = {1831715},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, concolic testing, testing web applications},
} 

@inproceedings{Wei:2010:AFP:1831708.1831716,
 author = {Wei, Yi and Pei, Yu and Furia, Carlo A. and Silva, Lucas S. and Buchholz, Stefan and Meyer, Bertrand and Zeller, Andreas},
 title = {Automated fixing of programs with contracts},
 abstract = {In program debugging, finding a failing run is only the first step; what about correcting the fault? Can we automate the second task as well as the first? The AutoFix-E tool automatically generates and validates fixes for software faults. The key insights behind AutoFix-E are to rely on contracts present in the software to ensure that the proposed fixes are semantically sound, and on state diagrams using an abstract notion of state based on the boolean queries of a class. Out of 42 faults found by an automatic testing tool in two widely used Eiffel libraries, AutoFix-E proposes successful fixes for 16 faults. Submitting some of these faults to experts shows that several of the proposed fixes are identical or close to fixes proposed by humans.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831716},
 doi = {http://doi.acm.org/10.1145/1831708.1831716},
 acmid = {1831716},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic debugging, automatic fixing, dynamic invariants, program synthesis},
} 

@inproceedings{Baah:2010:CIS:1831708.1831717,
 author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
 title = {Causal inference for statistical fault localization},
 abstract = {This paper investigates the application of causal inference</i> methodology for observational studies to software fault localization based on test outcomes and profiles. This methodology combines statistical techniques for counterfactual inference with causal graphical models to obtain causal-effect estimates that are not subject to severe confounding bias. The methodology applies Pearl's Back-Door Criterion to program dependence graphs to justify a linear model for estimating the causal effect of covering a given statement on the occurrence of failures. The paper also presents the analysis of several proposed-fault localization metrics and their relationships to our causal estimator. Finally, the paper presents empirical results demonstrating that our model significantly improves the effectiveness of fault localization.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831717},
 doi = {http://doi.acm.org/10.1145/1831708.1831717},
 acmid = {1831717},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {causal inference, debugging, fault localization, potential outcome model, program analysis},
} 

@inproceedings{Dallmeier:2010:GTC:1831708.1831719,
 author = {Dallmeier, Valentin and Knopp, Nikolai and Mallon, Christoph and Hack, Sebastian and Zeller, Andreas},
 title = {Generating test cases for specification mining},
 abstract = {Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining--a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives, and significantly fewer false positives than the initial models.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831719},
 doi = {http://doi.acm.org/10.1145/1831708.1831719},
 acmid = {1831719},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {specification mining, test case generation, typestate analysis},
} 

@inproceedings{Dobolyi:2010:MCW:1831708.1831720,
 author = {Dobolyi, Kinga and Weimer, Westley},
 title = {Modeling consumer-perceived web application fault severities for testing},
 abstract = {Despite the growing usage of web applications, extreme resource constraints during their development frequently leave them inadequately tested. Because testing may be perceived as having a low return on investment for web applications, we believe that providing a consumer-perceived fault severity model could allow developers to prioritize faults according to their likelihood of impacting consumer retention, encouraging web application developers to test more effectively. In a study involving 386 humans and 800 web application faults, we observe that an arbitrary human judgment of fault severity is unreliable. We thus present two models of fault severity that outperform individual humans in terms of correctly predicting the average consumer-perceived severity of web application faults. Our first model uses human annotations of fault surface features, and is 87\% accurate at identifying low-priority, non-severe faults. We also present a fully automated conservative model that correctly identifies 55\% of non-severe faults without missing any severe faults. Both models outperform humans at flagging severe faults, and can substitute or reinforce humans by prioritizing faults encountered in web application development and testing.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1831708.1831720},
 doi = {http://doi.acm.org/10.1145/1831708.1831720},
 acmid = {1831720},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault, severity, web application},
} 

@inproceedings{Kwon:2010:ADU:1831708.1831722,
 author = {Kwon, Taeho and Su, Zhendong},
 title = {Automatic detection of unsafe component loadings},
 abstract = {Dynamic loading of software components (e.g., libraries or modules) is a widely used mechanism for improved system modularity and flexibility. Correct component resolution is critical for reliable and secure software execution, however, programming mistakes may lead to unintended or even malicious components to be resolved and loaded. In particular, dynamic loading can be hijacked</i> by placing an arbitrary file with the specified name in a directory searched before resolving the target component. Although this issue has been known for quite some time, it was not considered serious because exploiting it requires access to the local file system on the vulnerable host. Recently such vulnerabilities started to receive considerable attention as their remote exploitation became realistic; it is now important to detect and fix these vulnerabilities. In this paper, we present the first automated</i> technique to detect vulnerable and unsafe dynamic component loadings. Our analysis has two phases: 1) apply dynamic binary instrumentation to collect runtime information on component loading (online phase</i>); and 2) analyze the collected information to detect vulnerable component loadings (offline phase</i>). For evaluation, we implemented our technique to detect vulnerable and unsafe DLL loadings in popular Microsoft Windows software. Our results show that unsafe DLL loading is prevalent and can lead to serious security threats. Our tool detected more than 1,700 unsafe DLL loadings in 28 widely used software and discovered serious attack vectors for remote code execution. Microsoft has opened a Microsoft Security Response Center (MSRC) case on our reported issues and is working with us and other affected software vendors to develop necessary patches.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {107--118},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831722},
 doi = {http://doi.acm.org/10.1145/1831708.1831722},
 acmid = {1831722},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, unsafe component loading},
} 

@inproceedings{Gruska:2010:LPL:1831708.1831723,
 author = {Gruska, Natalie and Wasylkowski, Andrzej and Zeller, Andreas},
 title = {Learning from 6,000 projects: lightweight cross-project anomaly detection},
 abstract = {Real production code contains lots of knowledge - on the domain, on the architecture, and on the environment. How can we leverage this knowledge in new projects? Using a novel lightweight source code parser, we have mined more than 6,000 open source Linux projects (totaling 200,000,000 lines of code) to obtain 16,000,000 temporal properties</i> reflecting normal interface usage. New projects can be checked against these rules to detect anomalies</i> - that is, code that deviates from the wisdom of the crowds. In a sample of 20 projects, ~25\% of the top-ranked anomalies uncovered actual code smells or defects.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {119--130},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831723},
 doi = {http://doi.acm.org/10.1145/1831708.1831723},
 acmid = {1831723},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal concept analysis, language independent parsing, lightweight parsing, mining specifications, temporal properties},
} 

@inproceedings{Tang:2010:PNC:1831708.1831724,
 author = {Tang, Enyi and Barr, Earl and Li, Xuandong and Su, Zhendong},
 title = {Perturbing numerical calculations for statistical analysis of floating-point program (in)stability},
 abstract = {Writing reliable software is difficult. It becomes even more difficult when writing scientific software involving floating-point numbers. Computers provide numbers with limited precision; when confronted with a real whose precision exceeds that limit, they introduce approximation and error. Numerical analysts have developed sophisticated mathematical techniques for performing error and stability analysis of numerical algorithms</i>. However, these are generally not accessible to application programmers or scientists who often do not have in-depth training in numerical analysis and who thus need more automated techniques to analyze their code. In this paper, we develop a novel, practical technique to help application programmers (or even numerical experts) obtain high-level information regarding the numerical stability and accuracy of their code. Our main insight is that by systematically altering (or perturbing</i>) the underlying numerical calculation, we can uncover potential pitfalls in the numerical code. We propose two complementary perturbations to statistically measure numerical stability: value perturbation</i> and expression perturbation</i>. Value perturbation dynamically replaces the least significant bits of each floating-point value, including intermediate values, with random bits to statistically induce numerical error in the code. Expression perturbation statically changes the numerical expressions in the user program to mathematically equivalent (in the reals, likely not in floating-point numbers), but syntactically different forms. We then compare the executions of these "equivalent" forms to help discover and remedy potential instabilities. Value perturbation can overstate error, while expression perturbation is relatively conservative, so we use value perturbation to generate candidates for expression perturbation. We have implemented our technique, and evaluation results on various programs from the literature and the GNU Scientific Library (GSL) show that our technique is effective and offers a practical alternative for understanding numerical stability in scientific software.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {131--142},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831724},
 doi = {http://doi.acm.org/10.1145/1831708.1831724},
 acmid = {1831724},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {floating-point, numerical code, perturbation, stability, testing},
} 

@inproceedings{Jiang:2010:AAL:1831708.1831726,
 author = {Jiang, Zhen Ming},
 title = {Automated analysis of load testing results},
 abstract = {Many software systems must be load tested to ensure that they can scale up while maintaining functional and performance requirements. Current industrial practices for checking the results of a load test remain ad hoc, involving high level checks. Few research efforts are devoted to the automated analysis of load testing results, mainly due to the limited access to large scale systems for use as case studies. Automated and systematic load testing analysis is going to be much needed, as many services have been offered online to an increasing number of users. This dissertation proposes automated approaches to detect functional and performance problems in a load test by mining the recorded load testing data (execution logs and performance metrics). Case studies show that our approaches scale well to large enterprise systems and output high precision results that help analysts detect load testing problems.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {143--146},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1831708.1831726},
 doi = {http://doi.acm.org/10.1145/1831708.1831726},
 acmid = {1831726},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, load testing, software mining},
} 

@inproceedings{Fraser:2010:MGU:1831708.1831728,
 author = {Fraser, Gordon and Zeller, Andreas},
 title = {Mutation-driven generation of unit tests and oracles},
 abstract = {To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a non-detected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized towards finding defects rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on two open source libraries, our muTest prototype generates test suites that find significantly more seeded defects than the original manually written test suites.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {147--158},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831728},
 doi = {http://doi.acm.org/10.1145/1831708.1831728},
 acmid = {1831728},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertions, mutation analysis, mutation testing, search based testing, test case generation, test oracles, unit testing},
} 

@inproceedings{Jaygarl:2010:OOC:1831708.1831729,
 author = {Jaygarl, Hojun and Kim, Sunghun and Xie, Tao and Chang, Carl K.},
 title = {OCAT: object capture-based automated testing},
 abstract = {Testing object-oriented (OO) software is critical because OO languages are commonly used in developing modern software systems. In testing OO software, one important and yet challenging problem is to generate desirable object instances for receivers and arguments to achieve high code coverage, such as branch coverage, or find bugs. Our initial empirical findings show that coverage of nearly half of the difficult-to-cover branches that a state-of-the-art test-generation tool cannot cover requires desirable object instances that the tool fails to generate. Generating desirable object instances has been a significant challenge for automated test-generation tools, partly because the search space for such desirable object instances is huge, no matter whether these tools compose method sequences to produce object instances or directly construct object instances. To address this significant challenge, we propose a novel approach called Object Capture based Automated Testing (OCAT). OCAT captures object instances dynamically from program executions (e.g., ones from system testing or real use). These captured objects assist an existing automated test-generation tool, such as a random testing tool, to achieve higher code coverage. Afterwards, OCAT mutates collected instances, based on observed not-covered branches. We evaluated OCAT on three open source projects, and our empirical results show that OCAT helps a state-of-the-art random testing tool, Randoop, to achieve high branch coverage: on average 68.5\%, with 25.5\% improved from only 43.0\% achieved by Randoop alone.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831729},
 doi = {http://doi.acm.org/10.1145/1831708.1831729},
 acmid = {1831729},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, object capturing, object generation, object mutation},
} 

@inproceedings{Martignoni:2010:TSV:1831708.1831730,
 author = {Martignoni, Lorenzo and Paleari, Roberto and Fresi Roglia, Giampaolo and Bruschi, Danilo},
 title = {Testing system virtual machines},
 abstract = {Virtual machines offer the ability to partition the resources of a physical system and to create isolated execution environments. The development of virtual machines is a very challenging task. This is particularly true for system virtual machines, since they run an operating system and must replicate in every detail the incredibly complex environment it requires. Nowadays, system virtual machines are the key component of many critical architectures. However, only little effort has been invested to test if the environment they provide is semantically equivalent to the environment found on real machines. In this paper we present a methodology specific for testing system virtual machines. This methodology is based on protocol-specific fuzzing and differential analysis, and consists in forcing a virtual machine and the corresponding physical machine to execute specially crafted snippets of user- and system-mode code and in comparing their behaviors. We have developed a prototype, codenamed <b>KEmuFuzzer</b>, that implements our methodology for the Intel x86 architecture and used it to test four state-of-the-art virtual machines: BOCHS, QEMU, VirtualBox and VMware. We discovered defects in all of them.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831730},
 doi = {http://doi.acm.org/10.1145/1831708.1831730},
 acmid = {1831730},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, emulation, fuzzing, software testing, virtualization},
} 

@inproceedings{Staats:2010:PSE:1831708.1831732,
 author = {Staats, Matt and P\v{a}s\v{a}reanu, Corina},
 title = {Parallel symbolic execution for structural test generation},
 abstract = {Symbolic execution is a popular technique for automatically generating test cases achieving high structural coverage. Symbolic execution suffers from scalability issues since the number of symbolic paths that need to be explored is very large (or even infinite) for most realistic programs. To address this problem, we propose a technique, Simple Static Partitioning</i>, for parallelizing symbolic execution. The technique uses a set of pre-conditions to partition the symbolic execution tree, allowing us to effectively distribute symbolic execution and decrease the time needed to explore the symbolic execution tree. The proposed technique requires little communication between parallel instances and is designed to work with a variety of architectures, ranging from fast multi-core machines to cloud or grid computing environments. We implement our technique in the Java PathFinder verification tool-set and evaluate it on six case studies with respect to the performance improvement when exploring a finite symbolic execution tree and performing automatic test generation. We demonstrate speedup in both the analysis time over finite symbolic execution trees and in the time required to generate tests relative to sequential execution, with a maximum analysis time speedup of 90x observed using 128 workers and a maximum test generation speedup of 70x observed using 64 workers.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831732},
 doi = {http://doi.acm.org/10.1145/1831708.1831732},
 acmid = {1831732},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java pathfinder, parallel, symbolic execution},
} 

@inproceedings{Santelices:2010:EPD:1831708.1831733,
 author = {Santelices, Raul and Harrold, Mary Jean},
 title = {Exploiting program dependencies for scalable multiple-path symbolic execution},
 abstract = {This paper presents a new technique, called Symbolic Program Decomposition</i> (or SPD), for symbolic execution of multiple paths that is more scalable than existing techniques, which symbolically execute control-flow paths individually. SPD exploits control and data dependencies to avoid analyzing unnecessary combinations of subpaths. SPD can also compute an over-approximation of symbolic execution by abstracting away symbolic subterms arbitrarily, to further scale the analysis at the cost of precision. The paper also presents our implementation and empirical evaluation showing that SPD can achieve savings of orders of magnitude in the path-exploration costs of multiple-path symbolic execution. Finally, the paper presents a study that examines the use of SPD for a particular application: change analysis for test-suite augmentation.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831733},
 doi = {http://doi.acm.org/10.1145/1831708.1831733},
 acmid = {1831733},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {change analysis, control dependence, data dependence, invariant detection, modular analysis, path condition, path family, path sensitive analysis, program analysis, symbolic execution, test generation, test suite augmentation, verification},
} 

@inproceedings{Daniel:2010:TRU:1831708.1831734,
 author = {Daniel, Brett and Gvero, Tihomir and Marinov, Darko},
 title = {On test repair using symbolic execution},
 abstract = {When developers change a program, regression tests can fail not only due to faults in the program but also due to out-of-date test code that does not reflect the desired behavior of the program. When this occurs, it is necessary to repair test code such that the tests pass. Repairing tests manually is difficult and time consuming. We recently developed ReAssert, a tool that can automatically repair broken unit tests, but only if they lack complex control flow or operations on expected values. This paper introduces symbolic test repair</i>, a technique based on symbolic execution, which can overcome some of ReAssert's limitations. We reproduce experiments from earlier work and find that symbolic test repair improves upon previously reported results both quantitatively and qualitatively. We also perform new experiments which confirm the benefits of symbolic test repair and also show surprising similarities in test failures for open-source Java and .NET programs. Our experiments use Pex, a powerful symbolic execution engine for .NET, and we find that Pex provides over half of the repairs possible from the theoretically ideal symbolic test repair.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831734},
 doi = {http://doi.acm.org/10.1145/1831708.1831734},
 acmid = {1831734},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pex, reassert, symbolic execution, symbolic test repair, test repair},
} 

@inproceedings{Arcuri:2010:FAE:1831708.1831736,
 author = {Arcuri, Andrea and Iqbal, Muhammad Zohaib and Briand, Lionel},
 title = {Formal analysis of the effectiveness and predictability of random testing},
 abstract = {There has been a lot of work to shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications appear in the literature. Although it is not going to solve all possible testing problems, random testing is an essential tool in the hands of software testers. In this paper, we address general questions about random testing, such as how long random testing needs on average to achieve testing targets (e.g., coverage), how does it scale and how likely is it to yield similar results if we re-run random testing on the same testing problem. Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Our formal results can be applied to most types of software and testing criteria. Simulations are carried out to provide further support to our formal results. The obtained results are then used to assess the validity of empirical analyses reported in the literature. Results show that random testing is more effective and predictable than previously thought.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831736},
 doi = {http://doi.acm.org/10.1145/1831708.1831736},
 acmid = {1831736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coupon collector, predictability, random testing, schur function, theory},
} 

@inproceedings{Kettunen:2010:SAT:1831708.1831737,
 author = {Kettunen, Vesa and Kasurinen, Jussi and Taipale, Ossi and Smolander, Kari},
 title = {A study on agility and testing processes in software organizations},
 abstract = {In this paper, we studied the differences in testing activities between software organizations which apply agile development methods and organizations which take the traditional plan-driven approach. Our focus was on the concepts which allow the software organization to successfully apply agile development methods or plan-driven methods. We also observed the test process enhancements and hindrances, which originate in the selected development method. We interviewed 12 organizations, which were selected to represent different polar types of software production. The interviews were tape-recorded and transcribed for further analysis. The study yielded hypotheses which were derived by applying the qualitative grounded theory method. The results indicated that in practice, agile methods can improve the position of testing through the early involvement of testing activities in development, and also have a positive influence on end-product satisfaction. By applying these results, organizations can improve their processes and avoid pitfalls when transitioning to agile methods.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1831708.1831737},
 doi = {http://doi.acm.org/10.1145/1831708.1831737},
 acmid = {1831737},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {agile development, case study, empirical study, test process},
} 

@inproceedings{Ayewah:2010:GFF:1831708.1831738,
 author = {Ayewah, Nathaniel and Pugh, William},
 title = {The Google FindBugs fixit},
 abstract = {In May 2009, Google conducted a company wide FindBugs "fixit". Hundreds of engineers reviewed thousands of FindBugs warnings, and fixed or filed reports against many of them. In this paper, we discuss the lessons learned from this exercise, and analyze the resulting dataset, which contains data about how warnings in each bug pattern were classified. Significantly, we observed that even though most issues were flagged for fixing, few appeared to be causing any serious problems in production. This suggests that most interesting software quality problems were eventually found and fixed without FindBugs, but FindBugs could have found these problems early, when they are cheap to remediate. We compared this observation to bug trends observed in code snapshots from student projects. The full dataset from the Google fixit, with confidential details encrypted, will be published along with this paper.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831738},
 doi = {http://doi.acm.org/10.1145/1831708.1831738},
 acmid = {1831738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug patterns, bugs, false positives, findbugs, java, software defects, software quality, static analysis},
} 

@inproceedings{Weeratunge:2010:ACB:1831708.1831740,
 author = {Weeratunge, Dasarath and Zhang, Xiangyu and Sumner, William N. and Jagannathan, Suresh},
 title = {Analyzing concurrency bugs using dual slicing},
 abstract = {Recently, there has been much interest in developing analyzes to detect concurrency bugs that arise because of data races, atomicity violations, execution omission, etc. However, determining whether reported bugs are in fact real, and understanding how these bugs lead to incorrect behavior, remains a labor-intensive process. This paper proposes a novel dynamic analysis that automatically produces the causal path of a concurrent failure leading from the root cause to the failure. Given two schedules, one inducing the failure and the other not, our technique collects traces of the two executions, and compares them to identify salient differences. The causal relation between the differences is disclosed by leveraging a novel slicing algorithm called dual slicing</i> that slices both executions alternatively and iteratively, producing a slice containing trace differences from both runs. Our experiments show that dual slices tend to be very small, often an order of magnitude or more smaller than the corresponding dynamic slices; more importantly, they enable precise analysis of real concurrency bugs for large programs, with reasonable overhead.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1831708.1831740},
 doi = {http://doi.acm.org/10.1145/1831708.1831740},
 acmid = {1831740},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, dual slicing, execution indexing},
} 

@inproceedings{Paleari:2010:NDD:1831708.1831741,
 author = {Paleari, Roberto and Martignoni, Lorenzo and Fresi Roglia, Giampaolo and Bruschi, Danilo},
 title = {N-version disassembly: differential testing of x86 disassemblers},
 abstract = {The output of a disassembler is used for many different purposes (e.g., debugging and reverse engineering). Therefore, disassemblers represent the first link of a long chain of stages on which any high-level analysis of machine code depends upon. In this paper we demonstrate that many disassemblers fail to decode certain instructions and thus that the first link of the chain is very weak. We present a methodology, called N-version disassembly</i>, to verify the correctness of disassemblers, based on differential analysis. Given a set of n</i> - 1 disassemblers, we use them to decode fragments of machine code and we compare their output against each other. To further corroborate the output of these disassemblers, we developed a special instruction decoder, the n<sup>th</sup></i>, that delegates the decoding to the CPU, the ideal decoder. We tested eight of the most popular disassemblers for Intel x86, and found bugs in each of them.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {265--274},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1831708.1831741},
 doi = {http://doi.acm.org/10.1145/1831708.1831741},
 acmid = {1831741},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, differential testing, software testing},
} 

@inproceedings{Ostrand:2010:SFP:1831708.1831743,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
 title = {Software fault prediction tool},
 abstract = {We have developed an interactive tool that predicts fault likelihood for the individual files of successive releases of large, long-lived, multi-developer software systems. Predictions are the result of a two-stage process: first, the extraction of current and historical properties of the system, and second, application of a negative binomial regression model to the extracted data. The prediction model is presented to the user as a GUI-based tool that requires minimal input from the user, and delivers its output as an ordered list of the system's files together with an expected percent of faults each file will have in the release about to undergo system test. The predictions can be used to prioritize testing efforts, to plan code or design reviews, to allocate human and computer resources, and to decide if files should be rewritten.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {275--278},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1831708.1831743},
 doi = {http://doi.acm.org/10.1145/1831708.1831743},
 acmid = {1831743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect, fault, gui tool, negative binomial, prediction},
} 

@inproceedings{Romero-Mariona:2010:AAS:1831708.1831744,
 author = {Romero-Mariona, Jose and Ziv, Hadar and Richardson, Debra},
 title = {ASSURE: automated support for secure and usable requirements engineering},
 abstract = {Proper testing is an essential and critical part of any development effort. However, software testing is a complex undertaking, especially in the midst of today's security threats. Hackers, social engineering scams, and unaware users, are just a few potential threats that developers must consider not only during development, but more importantly during testing. There are significant reputation and financial losses related to security aspects that could have been addressed during requirements specification. While a variety of approaches to security requirements specification have been proposed, there is a tangible lack in the support that they offer during testing. In this paper we describe the tool support of a new security requirements engineering technique called SURE-Secure and Usable Requirements Engineering. ASSURE - Automated Support for Secure and Usable Requirements Engineering -, is a system developed to aid in the mapping of security requirements into testing artifacts. This support goes beyond mapping and aids also in the management of users and projects.},
 booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
 series = {ISSTA '10},
 year = {2010},
 isbn = {978-1-60558-823-0},
 location = {Trento, Italy},
 pages = {279--282},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1831708.1831744},
 doi = {http://doi.acm.org/10.1145/1831708.1831744},
 acmid = {1831744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {security requirements, security requirements-based testing, usable requirements},
} 

@inproceedings{Bravenboer:2009:EAP:1572272.1572274,
 author = {Bravenboer, Martin and Smaragdakis, Yannis},
 title = {Exception analysis and points-to analysis: better together},
 abstract = {Exception analysis and points-to analysis are typically done in complete separation. Past algorithms for precise exception analysis (e.g., pairing throw clauses with catch statements) use pre-computed points-to information. Past points-to analyses either unsoundly ignore exceptions, or conservatively compute a crude approximation of exception throwing (e.g., considering an exception throw as an assignment to a global variable, accessible from any catch clause). We show that this separation results in significant slowdowns or vast imprecision. The two kinds of analyses are interdependent: neither can be performed accurately without the other. The interdependency leads us to propose a joint handling for performance and precision. We show that our exception analysis is expressible highly elegantly in a declarative form, and can apply to points-to analyses of varying precision. In fact, our specification of exception analysis is "fully precise", as it models closely the Java exception handling semantics. The necessary approximation is provided only through whichever abstractions are used for contexts and objects in the base points-to analysis. Our combined approach achieves similar precision relative to exceptions (exception-catch links) as the best past precise exception analysis, with a runtime of seconds instead of tens of minutes. At the same time, our analysis achieves much higher precision of points-to information (an average of half as many values for each reachable variable for most of the DaCapo benchmarks) than points-to analyses that treat exceptions conservatively, all at a fraction of the execution time.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572274},
 doi = {http://doi.acm.org/10.1145/1572272.1572274},
 acmid = {1572274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-sensitive, exception handling, points-to analysis, precision},
} 

@inproceedings{Stengel:2009:ASC:1572272.1572275,
 author = {Stengel, Zachary and Bultan, Tevfik},
 title = {Analyzing singularity channel contracts},
 abstract = {This paper presents techniques for analyzing channel contract specifications in Microsoft Research's Singularity operating system. A channel contract is a state machine that specifies the allowable interactions between a server and a client through an asynchronous communication channel. We show that, contrary to what is claimed in the Singularity documentation, processes that faithfully follow a channel contract can deadlock. We present a realizability analysis that can be used to identify channel contracts with problems. Our realizability analysis also leads to an efficient verification approach where properties about the interaction behavior can be verified without modeling the contents of communication channels. We analyzed more than 90 channel contracts from the Singularity code distribution and documentation. Only two contracts failed our realizability condition and these two contracts allow deadlocks. Our experimental results demonstrate that realizability analysis and verification of channel contracts can be done efficiently using our approach.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572275},
 doi = {http://doi.acm.org/10.1145/1572272.1572275},
 acmid = {1572275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous communication, conversations, realizability},
} 

@inproceedings{Gorg:2009:ISD:1572272.1572276,
 author = {G\"{o}rg, Martin Th and Zhao, Jianjun},
 title = {Identifying semantic differences in AspectJ programs},
 abstract = {Program differencing is a common means of software debugging. Although many differencing algorithms have been proposed for procedural and object-oriented languages like C and Java, there is no differencing algorithm for aspect-oriented languages so far. In this paper we propose an approach for difference analysis of aspect-oriented programs. The proposed algorithm contains a novel way of matching two versions of a module of which the signature has been modified. For this, we also work out a set of well defined signatures for the new elements in the AspectJ language. In accordance with these signatures, and with those existent for elements of the Java language, we investigate a set of signature patterns to be used with the module matching algorithm. Furthermore, we demonstrate successful application of a node-by-node comparison algorithm originally developed for object-oriented programs. Using a tool which implements our algorithms, we set up and evaluate a set of test cases. The results demonstrate the effectiveness of our approach for a large subset of the AspectJ language.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572276},
 doi = {http://doi.acm.org/10.1145/1572272.1572276},
 acmid = {1572276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aop, aspectj, difference analysis, disjunctive matching, semantic analysis, static analysis},
} 

@inproceedings{Adler:2009:ACC:1572272.1572278,
 author = {Adler, Yoram and Farchi, Eitan and Klausner, Moshe and Pelleg, Dan and Raz, Orna and Shochat, Moran and Ur, Shmuel and Zlotnick, Aviad},
 title = {Advanced code coverage analysis using substring holes},
 abstract = {Code coverage is a common aid in the testing process. It is generally used for marking the source code segments that were executed and, more importantly, those that were not executed. Many code coverage tools exist, supporting a variety of languages and operating systems. Unfortunately, these tools provide little or no assistance when code coverage data is voluminous. Such quantities are typical of system tests and even for earlier testing phases. Drill-down capabilities that look at different granularities of the data, starting with directories and going through files to functions and lines of source code, are insufficient. Such capabilities make the assumption that the coverage issues themselves follow the code hierarchy. We argue that this is not the case for much of the uncovered code. Two notable examples are error handling code and platform-specific constructs. Both tend to be spread throughout the source in many files, even though the related coverage, or lack thereof, is highly dependent. To make the task more manageable, and therefore more likely to be performed by users, we developed a hole analysis algorithm and tool that is based on common substrings in the names of functions. We tested its effectiveness using two large IBM software systems. In both of them, we asked domain experts to judge the results of several hole-ranking heuristics. They found that 57\% - 87\% of the 30 top-ranked holes identified by the effective heuristics are relevant. Moreover, these holes are often unexpected. This is especially impressive because substring hole analysis relies only on the names of functions, whereas domain experts have a broad and deep understanding of the system. We grounded our results in a theoretical framework that states desirable mathematical properties of hole ranking heuristics. The empirical results show that heuristics with these properties tend to perform better, and do so more consistently, than heuristics lacking them.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1572272.1572278},
 doi = {http://doi.acm.org/10.1145/1572272.1572278},
 acmid = {1572278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage analysis, software testing},
} 

@inproceedings{Pecheur:2009:FAR:1572272.1572279,
 author = {Pecheur, Charles and Raimondi, Franco and Brat, Guillaume},
 title = {A formal analysis of requirements-based testing},
 abstract = {The aim of requirements-based testing is to generate test cases from a set of requirements for a given system or piece of software. In this paper we propose a formal semantics for the generation of test cases from requirements by revising and extending the results presented in previous works (e.g.: [21, 20, 13]). We give a syntactic characterisation of our method, defined inductively over the syntax of LTL formulae, and prove that this characterisation is sound and complete, given some restrictions on the formulae that can be used to encode requirements. We provide various examples to show the applicability of our approach.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1572272.1572279},
 doi = {http://doi.acm.org/10.1145/1572272.1572279},
 acmid = {1572279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage metrics, requirements-based testing},
} 

@inproceedings{Namin:2009:ISC:1572272.1572280,
 author = {Namin, Akbar Siami and Andrews, James H.},
 title = {The influence of size and coverage on test suite effectiveness},
 abstract = {We study the relationship between three properties of test suites: size, structural coverage, and fault-finding effectiveness. In particular, we study the question of whether achieving high coverage leads directly to greater effectiveness, or only indirectly through forcing a test suite to be larger. Our experiments indicate that coverage is sometimes correlated with effectiveness when size is controlled for, and that using both size and coverage yields a more accurate prediction of effectiveness than size alone. This in turn suggests that both size and coverage are important to test suite effectiveness. Our experiments also indicate that no linear relationship exists among the three variables of size, coverage and effectiveness, but that a nonlinear relationship does exist.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {57--68},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572280},
 doi = {http://doi.acm.org/10.1145/1572272.1572280},
 acmid = {1572280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage criteria, statistical analysis},
} 

@inproceedings{Schuler:2009:EMT:1572272.1572282,
 author = {Schuler, David and Dallmeier, Valentin and Zeller, Andreas},
 title = {Efficient mutation testing by checking invariant violations},
 abstract = {Mutation testing</i> measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a mutation is not detected by the test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the program's semantics unchanged-and thus cannot be detected by any test. Such equivalent mutants</i> have to be eliminated manually, which is tedious. We assess the impact of mutations by checking dynamic invariants. In an evaluation of our JAVALANCHE framework on seven industrial-size programs, we found that mutations that violate invariants are significantly more likely to be detectable by a test suite. As a consequence, mutations with impact on invariants should be focused upon when improving test suites. With less than 3\% of equivalent mutants, our approach provides an efficient, precise, and fully automatic measure of the adequacy of a test suite.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {69--80},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572282},
 doi = {http://doi.acm.org/10.1145/1572272.1572282},
 acmid = {1572282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic invariants, mutation testing},
} 

@inproceedings{Jiang:2009:AMF:1572272.1572283,
 author = {Jiang, Lingxiao and Su, Zhendong},
 title = {Automatic mining of functionally equivalent code fragments via random testing},
 abstract = {Similar code may exist in large software projects due to some common software engineering practices, such as copying and pasting code and n</i>-version programming. Although previous work has studied syntactic equivalence and small-scale, coarse-grained program-level and function-level semantic equivalence, it is not known whether significant fine-grained, code-level semantic duplications exist. Detecting such semantic equivalence is also desirable because it can enable many applications such as code understanding, maintenance, and optimization. In this paper, we introduce the first algorithm to automatically mine functionally equivalent code fragments of arbitrary size - down to an executable statement. Our notion of functional equivalence is based on input and output behavior. Inspired by Schwartz's randomized polynomial identity testing, we develop our core algorithm using automated random testing: (1) candidate code fragments are automatically extracted from the input program; and (2) random inputs are generated to partition the code fragments based on their output values on the generated inputs. We implemented the algorithm and conducted a large-scale empirical evaluation of it on the Linux kernel 2.6.24. Our results show that there exist many functionally equivalent</i> code fragments that are syntactically different</i> (i.e., they are unlikely due to copying and pasting code). The algorithm also scales to million-line programs; it was able to analyze the Linux kernel with several days of parallel processing.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {81--92},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572283},
 doi = {http://doi.acm.org/10.1145/1572272.1572283},
 acmid = {1572283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code clones, functional equivalence, random testing},
} 

@inproceedings{Polikarpova:2009:CSP:1572272.1572284,
 author = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
 title = {A comparative study of programmer-written and automatically inferred contracts},
 abstract = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination? Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect. Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts. We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60\% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {93--104},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572284},
 doi = {http://doi.acm.org/10.1145/1572272.1572284},
 acmid = {1572284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic contract inference, eiffel},
} 

@inproceedings{Kiezun:2009:HSS:1572272.1572286,
 author = {Kiezun, Adam and Ganesh, Vijay and Guo, Philip J. and Hooimeijer, Pieter and Ernst, Michael D.},
 title = {HAMPI: a solver for string constraints},
 abstract = {Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis techniques for string-manipulating programs. We designed and implemented H<sc>ampi</sc>, a solver for string constraints over fixed-size string variables. H<sc>ampi</sc> constraints express membership in regular languages and fixed-size context-free languages. H<sc>ampi</sc> constraints may contain context-free-language definitions, regular language definitions and operations, and the membership predicate. Given a set of constraints, H<sc>ampi</sc> outputs a string that satisfies all the constraints, or reports that the constraints are unsatisfiable. H<sc>ampi</sc> is expressive and efficient, and can be successfully applied to testing and analysis of real programs. Our experiments use H<sc>ampi</sc> in: static and dynamic analyses for finding SQL injection vulnerabilities in Web applications; automated bug finding in C programs using systematic testing; and compare H<sc>ampi</sc> with another string solver. H<sc>ampi's</sc> source code, documentation, and the experimental data are available at http://people.csail.mit.edu/akiezun/hampi.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {105--116},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572286},
 doi = {http://doi.acm.org/10.1145/1572272.1572286},
 acmid = {1572286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-free languages, regular languages, string constraints},
} 

@inproceedings{Saebjornsen:2009:DCC:1572272.1572287,
 author = {S\aebj{\o}rnsen, Andreas and Willcock, Jeremiah and Panas, Thomas and Quinlan, Daniel and Su, Zhendong},
 title = {Detecting code clones in binary executables},
 abstract = {Large software projects contain significant code duplication, mainly due to copying and pasting code. Many techniques have been developed to identify duplicated code to enable applications such as refactoring, detecting bugs, and protecting intellectual property. Because source code is often unavailable, especially for third-party software, finding duplicated code in binaries becomes particularly important. However, existing techniques operate primarily on source code, and no effective tool exists for binaries. In this paper, we describe the first practical clone detection algorithm for binary executables. Our algorithm extends an existing tree similarity framework based on clustering of characteristic vectors of labeled trees with novel techniques to normalize assembly instructions and to accurately and compactly model their structural information. We have implemented our technique and evaluated it on Windows XP system binaries totaling over 50 million assembly instructions. Results show that it is both scalable and precise: it analyzed Windows XP system binaries in a few hours and produced few false positives. We believe our technique is a practical, enabling technology for many applications dealing with binary code.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572287},
 doi = {http://doi.acm.org/10.1145/1572272.1572287},
 acmid = {1572287},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary analysis, clone detection, software tools},
} 

@inproceedings{Elkarablieh:2009:PPR:1572272.1572288,
 author = {Elkarablieh, Bassem and Godefroid, Patrice and Levin, Michael Y.},
 title = {Precise pointer reasoning for dynamic test generation},
 abstract = {Dynamic test generation consists of executing a program while gathering symbolic constraints on inputs from predicates encountered in branch statements, and of using a constraint solver to infer new program inputs from previous constraints in order to steer next executions towards new program paths. Variants of this technique have recently been adopted in several bug detection tools, including our whitebox fuzzer SAGE, which has found dozens of new expensive security-related bugs in many Windows applications and is now routinely used in various Microsoft groups. In this paper, we discuss how to perform precise symbolic pointer reasoning in the context of dynamic test generation. We present a new memory model for representing arbitrary symbolic pointer dereferences to memory regions accessible by a program during its execution, and show that this memory model is the most precise one can hope for in our context, under some realistic assumptions. We also describe how the symbolic constraints generated by our model can be solved using modern SMT solvers, which provide powerful constructs for reasoning about bit-vectors and arrays. This new memory model has been implemented in SAGE, and we present results of experiments with several large Windows applications showing that an increase in precision can often be obtained at a reasonable cost. Better precision in symbolic pointer reasoning means more relevant constraints and fewer imprecise ones, hence better test coverage, more bugs found and fewer redundant test cases.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572288},
 doi = {http://doi.acm.org/10.1145/1572272.1572288},
 acmid = {1572288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, pointer reasoning, program verification, software testing},
} 

@inproceedings{Cheng:2009:IBS:1572272.1572290,
 author = {Cheng, Hong and Lo, David and Zhou, Yang and Wang, Xiaoyin and Yan, Xifeng},
 title = {Identifying bug signatures using discriminative graph mining},
 abstract = {Bug localization has attracted a lot of attention recently. Most existing methods focus on pinpointing a single statement or function call which is very likely to contain bugs. Although such methods could be very accurate, it is usually very hard for developers to understand the context of the bug, given each bug location in isolation. In this study, we propose to model software executions with graphs at two levels of granularity: methods and basic blocks. An individual node represents a method or basic block and an edge represents a method call, method return or transition (at the method or basic block granularity). Given a set of graphs of correct and faulty executions, we propose to extract the most discriminative subgraphs which contrast the program flow of correct and faulty executions. The extracted subgraphs not only pinpoint the bug, but also provide an informative context for understanding and fixing the bug. Different from traditional graph mining which mines a very large set of frequent subgraphs, we formulate subgraph mining as an optimization problem and directly generate the most discriminative subgraph with a recently proposed graph mining algorithm LEAP. We further extend it to generate a ranked list of top-k</i> discriminative subgraphs representing distinct locations which may contain bugs. Experimental results and case studies show that our proposed method is both effective and efficient to mine discriminative subgraphs for bug localization and context identification.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {141--152},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572290},
 doi = {http://doi.acm.org/10.1145/1572272.1572290},
 acmid = {1572290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug signature, discriminative subgraph mining},
} 

@inproceedings{Sinha:2009:FLR:1572272.1572291,
 author = {Sinha, Saurabh and Shah, Hina and G\"{o}rg, Carsten and Jiang, Shujuan and Kim, Mijung and Harrold, Mary Jean},
 title = {Fault localization and repair for Java runtime exceptions},
 abstract = {This paper presents a new approach for locating and repairing faults that cause runtime exceptions in Java programs. The approach handles runtime exceptions that involve a flow of an incorrect value that finally leads to the exception. This important class of exceptions includes exceptions related to dereferences of null pointers, arithmetic faults (e.g., ArithmeticException), and type faults (e.g., ArrayStoreException). Given a statement at which such an exception occurred, the technique combines dynamic analysis (using stack-trace information) with static backward data-flow analysis (beginning at the point where the runtime exception occurred) to identify the source statement at which an incorrect assignment was made; this information is required to locate the fault. The approach also identifies the source statements that may cause this same exception on other executions, along with the reference statements that may raise an exception in other executions because of this incorrect assignment; this information is required to repair the fault. The paper also presents an application of our technique to null pointer exceptions. Finally, the paper describes an implementation of the null-pointer-exception analysis and a set of studies that demonstrate the advantages of our approach for locating and repairing faults in the program.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {153--164},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572291},
 doi = {http://doi.acm.org/10.1145/1572272.1572291},
 acmid = {1572291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault localization, null dereference, runtime exceptions, static analysis},
} 

@inproceedings{Xin:2009:MS:1572272.1572292,
 author = {Xin, Bin and Zhang, Xiangyu},
 title = {Memory slicing},
 abstract = {Traditional dynamic program slicing techniques are code-centric, meaning dependences are introduced between executed statement instances, which gives rise to various problems such as space requirement is decided by execution length; dependence graphs are highly redundant so that inspecting them is labor intensive. In this paper, we propose a data-centric dynamic slicing technique, in which dependences are introduced between memory locations. Doing so, the space complexity is bounded by memory footprint instead of execution length. Moreover, presenting dependences between memory locations is often more desirable for human inspection during debugging as redundant dependences are suppressed. Our evaluation shows that the proposed technique supersedes traditional dynamic slicing techniques in terms of effectiveness and efficiency.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572292},
 doi = {http://doi.acm.org/10.1145/1572272.1572292},
 acmid = {1572292},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-centric slicing, dynamic program slicing, fault localization, memory dependence graph},
} 

@inproceedings{Fouche:2009:ICA:1572272.1572294,
 author = {Fouch\'{e}, Sandro and Cohen, Myra B. and Porter, Adam},
 title = {Incremental covering array failure characterization in large configuration spaces},
 abstract = {The increasing complexity of configurable software systems has created a need for more intelligent sampling mechanisms to detect and characterize failure-inducing dependencies between configurations. Prior work - in idealized environments - has shown that test schedules based on a mathematical object, called a covering array, in combination with classification techniques, can meet this need. Applying this approach in practice, however, is tricky because testing time and resource availability are unpredictable, and because failure characteristics can change from release to release. With current approaches developers must set a key covering array parameter (its strength) based on estimated release times and failure characterizations. This will influence the outcome of their results. In this paper we propose a new approach that incrementally builds covering array schedules. This approach begins at a low strength, and then iteratively increases strength as resources allow. At each stage previously tested configurations are reused, thus avoiding duplication of work. With the incremental approach developers need never commit to a specific covering array strength. Instead, by using progressively stronger covering array schedules, failures due to few configuration dependencies can be found and classified as soon and as cheaply as possibly. Additionally, it eliminates the risks of committing to overly strong test schedules. We evaluate this new approach through a case study on three consecutive releases of MySQL, an open source database. Our results suggest that our approach is as good or better than previous approaches, costing less in most cases, and allowing greater flexibility in environments with unpredictable development constraints.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572294},
 doi = {http://doi.acm.org/10.1145/1572272.1572294},
 acmid = {1572294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed testing, testing},
} 

@inproceedings{Murphy:2009:AST:1572272.1572295,
 author = {Murphy, Christian and Shen, Kuang and Kaiser, Gail},
 title = {Automatic system testing of programs without test oracles},
 abstract = {Metamorphic testing has been shown to be a simple yet effective technique in addressing the quality assurance of applications that do not have test oracles, i.e.</i>, for which it is difficult or impossible to know what the correct output should be for arbitrary input. In metamorphic testing, existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can easily be computed based on the original output. That is, if input x</i> produces output f</i>(x</i>), then we create input x</i>' such that we can predict f</i>(x</i>') based on f</i>(x</i>); if the application does not produce the expected output, then a defect must exist, and either f</i>(x</i>), or f</i>(x</i>') (or both) is wrong. In practice, however, metamorphic testing can be a manually intensive technique for all but the simplest cases. The transformation of input data can be laborious for large data sets, or practically impossible for input that is not in human-readable format. Similarly, comparing the outputs can be error-prone for large result sets, especially when slight variations in the results are not actually indicative of errors (i.e.</i>, are false positives), for instance when there is non-determinism in the application and multiple outputs can be considered correct. In this paper, we present an approach called Automated Metamorphic System Testing</i>. This involves the automation of metamorphic testing at the system level by checking that the metamorphic properties of the entire application hold after its execution. The tester is able to easily set up and conduct metamorphic tests with little manual intervention, and testing can continue in the field with minimal impact on the user. Additionally, we present an approach called Heuristic Metamorphic Testing</i> which seeks to reduce false positives and address some cases of non-determinism. We also describe an implementation framework called Amsterdam</i>, and present the results of empirical studies in which we demonstrate the effectiveness of the technique on real-world programs without test oracles.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572295},
 doi = {http://doi.acm.org/10.1145/1572272.1572295},
 acmid = {1572295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {metamorphic testing, oracle problem, software testing},
} 

@inproceedings{Yoo:2009:CTC:1572272.1572296,
 author = {Yoo, Shin and Harman, Mark and Tonella, Paolo and Susi, Angelo},
 title = {Clustering test cases to achieve effective and scalable prioritisation incorporating expert knowledge},
 abstract = {Pair-wise comparison has been successfully utilised in order to prioritise test cases by exploiting the rich, valuable and unique knowledge of the tester. However, the prohibitively large cost of the pair-wise comparison method prevents it from being applied to large test suites. In this paper, we introduce a cluster-based test case prioritisation technique. By clustering test cases, based on their dynamic runtime behaviour, we can reduce the required number of pair-wise comparisons significantly. The approach is evaluated on seven test suites ranging in size from 154 to 1,061 test cases. We present an empirical study that shows that the resulting prioritisation is more effective than existing coverage-based prioritisation techniques in terms of rate of fault detection. Perhaps surprisingly, the paper also demonstrates that clustering (even without human input) can outperform unclustered coverage-based technologies, and discusses an automated process that can be used to determine whether the application of the proposed approach would yield improvement.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {201--212},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572296},
 doi = {http://doi.acm.org/10.1145/1572272.1572296},
 acmid = {1572296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ahp, clustering, test case prioritisation},
} 

@inproceedings{Zhang:2009:TTP:1572272.1572297,
 author = {Zhang, Lu and Hou, Shan-Shan and Guo, Chao and Xie, Tao and Mei, Hong},
 title = {Time-aware test-case prioritization using integer linear programming},
 abstract = {Techniques for test-case prioritization re-order test cases to increase their rate of fault detection. When there is a fixed time budget that does not allow the execution of all the test cases, time-aware techniques for test-case prioritization may achieve a better rate of fault detection than traditional techniques for test-case prioritization. In this paper, we propose a novel approach to time-aware test-case prioritization using integer linear programming. To evaluate our approach, we performed experiments on two subject programs involving four techniques for our approach, two techniques for an approach to time-aware test-case prioritization based on genetic algorithms, and four traditional techniques for test-case prioritization. The empirical results indicate that two of our techniques outperform all the other techniques for the two subjects under the scenarios of both general and version-specific prioritization. The empirical results also indicate that some traditional techniques with lower analysis time cost for test-case prioritization may still perform competitively when the time budget is not quite tight.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572297},
 doi = {http://doi.acm.org/10.1145/1572272.1572297},
 acmid = {1572297},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {integer linear programming, test-case prioritization},
} 

@inproceedings{Saxena:2009:LSE:1572272.1572299,
 author = {Saxena, Prateek and Poosankam, Pongsin and McCamant, Stephen and Song, Dawn},
 title = {Loop-extended symbolic execution on binary programs},
 abstract = {Mixed concrete and symbolic execution is an important technique for finding and understanding software bugs, including security-relevant ones. However, existing symbolic execution techniques are limited to examining one execution path at a time, in which symbolic variables reflect only direct data dependencies. We introduce loop-extended symbolic execution, a generalization that broadens the coverage of symbolic results in programs with loops. It introduces symbolic variables for the number of times each loop executes, and links these with features of a known input grammar such as variable-length or repeating fields. This allows the symbolic constraints to cover a class of paths that includes different numbers of loop iterations, expressing loop-dependent program values in terms of properties of the input. By performing more reasoning symbolically, instead of by undirected exploration, applications of loop-extended symbolic execution can achieve better results and/or require fewer program executions. To demonstrate our technique, we apply it to the problem of discovering and diagnosing buffer-overflow vulnerabilities in software given only in binary form. Our tool finds vulnerabilities in both a standard benchmark suite and 3 real-world applications, after generating only a handful of candidate inputs, and also diagnoses general vulnerability conditions.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {225--236},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572299},
 doi = {http://doi.acm.org/10.1145/1572272.1572299},
 acmid = {1572299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop summaries, vulnerability discovery and diagnosis},
} 

@inproceedings{Babenko:2009:AAI:1572272.1572300,
 author = {Babenko, Anton and Mariani, Leonardo and Pastore, Fabrizio},
 title = {AVA: automated interpretation of dynamically detected anomalies},
 abstract = {Dynamic analysis techniques have been extensively adopted to discover causes of observed failures. In particular, anomaly detection techniques can infer behavioral models from observed legal executions and compare failing executions with the inferred models to automatically identify the likely anomalous events that caused observed failures. Unfortunately the output of these techniques is limited to a set of independent suspicious anomalous events that does not capture the structure and the rationale of the differences between the correct and the failing executions. Thus, testers spend a relevant amount of time and effort to investigate executions and interpret these differences, reducing effectiveness of anomaly detection techniques. In this paper, we present Automata Violations Analyzer (AVA), a technique to automatically produce candidate interpretations of detected failures from anomalies identified by anomaly detection techniques. Interpretations capture the rationale of the differences between legal and failing executions with user understandable patterns that simplify identification of failure causes. The empirical validation with synthetic cases and third-party systems shows that AVA produces useful interpretations.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572300},
 doi = {http://doi.acm.org/10.1145/1572272.1572300},
 acmid = {1572300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, dynamic analysis},
} 

@inproceedings{Clause:2009:PAI:1572272.1572301,
 author = {Clause, James and Orso, Alessandro},
 title = {Penumbra: automatically identifying failure-relevant inputs using dynamic tainting},
 abstract = {Most existing automated debugging techniques focus on reducing the amount of code to be inspected and tend to ignore an important component of software failures: the inputs that cause the failure to manifest. In this paper, we present a new technique based on dynamic tainting for automatically identifying subsets of a program's inputs that are relevant to a failure. The technique (1) marks program inputs when they enter the application, (2) tracks them as they propagate during execution, and (3) identifies, for an observed failure, the subset of inputs that are potentially relevant for debugging that failure. To investigate feasibility and usefulness of our technique, we created a prototype tool, PENUMBRA, and used it to evaluate our technique on several failures in real programs. Our results are promising, as they show that PENUMBRA can point developers to inputs that are actually relevant for investigating a failure and can be more practical than existing alternative approaches.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572301},
 doi = {http://doi.acm.org/10.1145/1572272.1572301},
 acmid = {1572301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, dynamic information flow, dynamic tainting, failure-relevant inputs},
} 

@inproceedings{Martignoni:2009:TCE:1572272.1572303,
 author = {Martignoni, Lorenzo and Paleari, Roberto and Roglia, Giampaolo Fresi and Bruschi, Danilo},
 title = {Testing CPU emulators},
 abstract = {A CPU emulator is a software that simulates a hardware CPU. Emulators are widely used by computer scientists for various kind of activities (e.g., debugging, profiling, and malware analysis). Although no theoretical limitation prevents to develop an emulator that faithfully emulates a physical CPU, writing a fully featured emulator is a very challenging and error-prone task. Modern CISC architectures have a very rich instruction set, some instructions lack proper specifications, and others may have undefined effects in corner-cases. This paper presents a testing methodology specific for CPU emulators, based on fuzzing. The emulator is "stressed" with specially crafted test-cases, to verify whether the CPU is properly emulated or not. Improper behaviours of the emulator are detected by running the same test-case concurrently on the emulated and on the physical CPUs and by comparing the state of the two after the execution. Differences in the final state testify defects in the code of the emulator. We implemented this methodology in a prototype (codenamed <b>EmuFuzzer</b>), analysed four state-of-the-art IA-32 emulators (QEMU, Valgrind, Pin and BOCHS), and found several defects in each of them, some of which can prevent the proper execution of programs.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572303},
 doi = {http://doi.acm.org/10.1145/1572272.1572303},
 acmid = {1572303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, emulation, fuzzing, software testing},
} 

@inproceedings{Botaschanjan:2009:SWC:1572272.1572304,
 author = {Botaschanjan, Jewgenij and Hummel, Benjamin},
 title = {Specifying the worst case: orthogonal modeling of hardware errors},
 abstract = {During testing, the execution of valid cases is only one part of the task. Checking the behavior in boundary situations and in the presence of errors is an equally important subject. This is especially true in embedded systems where parts of a system's function are realized by sensors and actuators, which are subject to wear and defects. As testing with the real hardware is costly and hardware defects are hard to stimulate, such tests are often performed using behavior models of the system which allow to execute the controller software against simulated hardware and environment. However, these models seldom contain possible hardware errors, as this makes the models more complex and, thus, harder to create and maintain. This paper presents a modeling technique for the description of system errors without modifying the original model. Error specifications for individual system components are modeled separately and can be used to augment the system model.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {273--284},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572304},
 doi = {http://doi.acm.org/10.1145/1572272.1572304},
 acmid = {1572304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {af/stem case tool, error filters, error mode specification, error models, modelling hardware errors},
} 

@inproceedings{Halfond:2009:PII:1572272.1572305,
 author = {Halfond, William G.J. and Anand, Saswat and Orso, Alessandro},
 title = {Precise interface identification to improve testing and analysis of web applications},
 abstract = {As web applications become more widespread, sophisticated, and complex, automated quality assurance techniques for such applications have grown in importance. Accurate interface identification is fundamental for many of these techniques, as the components of a web application communicate extensively via implicitly-defined interfaces to generate customized and dynamic content. However, current techniques for identifying web application interfaces can be incomplete or imprecise, which hinders the effectiveness of quality assurance techniques. To address these limitations, we present a new approach for identifying web application interfaces that is based on a specialized form of symbolic execution. In our empirical evaluation, we show that the set of interfaces identified by our approach is more accurate than those identified by other approaches. We also show that this increased accuracy leads to improvements in several important quality assurance techniques for web applications: test-input generation, penetration testing, and invocation verification.},
 booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
 series = {ISSTA '09},
 year = {2009},
 isbn = {978-1-60558-338-9},
 location = {Chicago, IL, USA},
 pages = {285--296},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1572272.1572305},
 doi = {http://doi.acm.org/10.1145/1572272.1572305},
 acmid = {1572305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interface identification, web application testing},
} 

@inproceedings{Larus:2008:RVT:1390630.1390631,
 author = {Larus, Jim},
 title = {The real value of testing},
 abstract = {A decade ago, Tony Hoare noted that "The real value of tests is not that they detect bugs in the code but that they detect inadequacies in the methods, concentration, and skills of those who design and produce the code." As usual, Tony saw far ahead of the current reality. At that time, Microsoft Research was very focused on a specific aspect of software development (finding code defects). Over the intervening years, Microsoft Research's efforts in this area grew greatly and our research agenda broadened considerably. This talk will trace the evolution of Microsoft Research's efforts to improve software development and explore how testing fits into the more people-centric approach that we have reached.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390631},
 doi = {http://doi.acm.org/10.1145/1390630.1390631},
 acmid = {1390631},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {testing},
} 

@inproceedings{Beckman:2008:PT:1390630.1390634,
 author = {Beckman, Nels E. and Nori, Aditya V. and Rajamani, Sriram K. and Simmons, Robert J.},
 title = {Proofs from tests},
 abstract = {We present an algorithm DASH to check if a program P satisfies a safety property phi. The unique feature of the algorithm is that it uses only test generation operations, and it refines and maintains a sound program abstraction as a consequence of failed test generation operations. Thus, each iteration of the algorithm is inexpensive, and can be implemented without any global may-alias information. In particular, we introduce a new refinement operator WP_alpha that uses only the alias information obtained by executing a test to refine abstractions in a sound manner. We present a full exposition of the Dash algorithm, its theoretical properties, and its implementation.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390634},
 doi = {http://doi.acm.org/10.1145/1390630.1390634},
 acmid = {1390634},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstraction refinemen, directed testing, software model checking},
} 

@inproceedings{Pasareanu:2008:CUS:1390630.1390635,
 author = {P\v{a}s\v{a}reanu, Corina S. and Mehlitz, Peter C. and Bushnell, David H. and Gundy-Burlet, Karen and Lowry, Michael and Person, Suzette and Pape, Mark},
 title = {Combining unit-level symbolic execution and system-level concrete execution for testing nasa software},
 abstract = {We describe an approach to testing complex safety critical software that combines unit-level symbolic execution and system-level concrete execution for generating test cases that satisfy user-specified testing criteria. We have developed Symbolic Java PathFinder, a symbolic execution framework that implements a non-standard bytecode interpreter on top of the Java PathFinder model checking tool. The framework propagates the symbolic information via attributes associated with the program data. Furthermore, we use two techniques that leverage system-level concrete program executions to gather information about a unit's input to improve the precision of the unit-level test case generation. We applied our approach to testing a prototype NASA flight software component. Our analysis helped discover a serious bug that resulted in design changes to the software. Although we give our presentation in the context of a NASA project, we believe that our work is relevant for other critical systems that require thorough testing.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390635},
 doi = {http://doi.acm.org/10.1145/1390630.1390635},
 acmid = {1390635},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software model checking, symbolic execution, system testing, unit testing},
} 

@inproceedings{Xu:2008:TBO:1390630.1390636,
 author = {Xu, Ru-Gang and Godefroid, Patrice and Majumdar, Rupak},
 title = {Testing for buffer overflows with length abstraction},
 abstract = {We present <b>Splat</b>, a tool for automatically generating inputs that lead to memory safety violations in C programs. <b>Splat</b> performs directed random testing of the code, guided by symbolic execution. However, instead of representing the entire contents of an input buffer symbolically, Splat tracks only a prefix of the buffer symbolically, and a symbolic length</i> that may exceed the size of the symbolic prefix. The part of the buffer beyond the symbolic prefix is filled with concrete random inputs. The use of symbolic buffer lengths makes it possible to compactly summarize the behavior of standard buffer manipulation functions, such as string library functions, leading to a more scalable search for possible memory errors. While reasoning only about prefixes of buffer contents makes the search theoretically incomplete, we experimentally demonstrate that the symbolic length abstraction is both scalable and sufficient to uncover many real buffer overflows in C programs. In experiments on a set of benchmarks developed independently to evaluate buffer overflow checkers, <b>Splat</b> was able to detect buffer overflows quickly, sometimes several orders of magnitude faster than when symbolically representing entire buffers. Splat was also able to find two previously unknown buffer overflows in a heavily-tested storage system.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390636},
 doi = {http://doi.acm.org/10.1145/1390630.1390636},
 acmid = {1390636},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflows, directed testing, length abstractions, testing c programs, underapproximation},
} 

@inproceedings{Elkarablieh:2008:ESS:1390630.1390637,
 author = {Elkarablieh, Bassem and Marinov, Darko and Khurshid, Sarfraz},
 title = {Efficient solving of structural constraints},
 abstract = {Structural constraint solving is being increasingly used for software reliability tasks such as systematic testing or error recovery. For example, the Korat algorithm provides constraint-based test generation: given a Java predicate that describes desired input constraints and a bound on the input size, Korat systematically searches the bounded input space of the predicate to generate all inputs that satisfy the constraints. As another example, the STARC tool uses a constraint-based search to repair broken data structures. A key issue for these approaches is the efficiency of search. This paper presents a novel approach that significantly improves the efficiency of structural constraint solvers. Specifically, most existing approaches use backtracking through code re-execution to explore their search space. In contrast, our approach performs checkpoint-based backtracking by storing partial program states and performing abstract undo operations. The heart of our approach is a light-weight search that is performed purely through code instrumentation. The experimental results on Korat and STARC for generating and repairing a set of complex data structures show an order to two orders of magnitude speed-up over the traditionally used searches.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390637},
 doi = {http://doi.acm.org/10.1145/1390630.1390637},
 acmid = {1390637},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backtracking, model checking, systematic testing},
} 

@inproceedings{Do:2008:USA:1390630.1390639,
 author = {Do, Hyunsook and Rothermel, Gregg},
 title = {Using sensitivity analysis to create simplified economic models for regression testing},
 abstract = {Software engineering methodologies are subject to complex cost-benefit tradeoffs. Economic models can help practitioners and researchers assess methodologies relative to these tradeoffs. Effective economic models, however, can be established only through an iterative process of refinement involving analytical and empirical methods. Sensitivity analysis provides one such method. By identifying the factors that are most important to models, sensitivity analysis can help simplify those models; it can also identify factors that must be measured with care, leading to guidelines for better test strategy definition and application. In prior work we presented the first comprehensive economic model for the regression testing process, that captures both cost and benefit factors relevant to that process while supporting evaluation of these processes across entire system lifetimes. In this work we use sensitivity analysis to examine our model analytically and assess the factors that are most important to the model. Based on the results of that analysis, we propose two new models of increasing simplicity. We assess these models empirically on data obtained by using regression testing techniques on several non-trivial software systems. Our results show that one of the simplified models assesses the relationships between techniques in the same way as the full model.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390639},
 doi = {http://doi.acm.org/10.1145/1390630.1390639},
 acmid = {1390639},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {economic models, empirical studies, regression test selection, regression testing, sensitivity analysis, test case prioritization},
} 

@inproceedings{Yoon:2008:ESS:1390630.1390640,
 author = {Yoon, Il-Chul and Sussman, Alan and Memon, Atif and Porter, Adam},
 title = {Effective and scalable software compatibility testing},
 abstract = {Today's software systems are typically composed of multiple components, each with different versions. Software compatibility testing is a quality assurance task aimed at ensuring that multi-component based systems build and/or execute correctly across all their versions' combinations, or configurations</i>. Because there are complex and changing interdependencies between components and their versions, and because there are such a large number of configurations, it is generally infeasible to test all potential configurations. Consequently, in practice, compatibility testing examines only a handful of default or popular configurations to detect problems; as a result costly errors can and do escape to the field. This paper presents a new approach to compatibility testing, called Rachet. We formally model the entire configuration space for software systems and use the model to generate test plans to sample a portion of the space. In this paper, we test all direct dependencies</i> between components and execute the test plan efficiently in parallel. We present empirical results obtained by applying our approach to two large-scale scientific middleware systems. The results show that for these systems Rachet scaled well and discovered incompatibilities between components, and that testing only direct dependences did not compromise test quality.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390640},
 doi = {http://doi.acm.org/10.1145/1390630.1390640},
 acmid = {1390640},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {component-based software system, software compatibility testing},
} 

@inproceedings{Qu:2008:CRT:1390630.1390641,
 author = {Qu, Xiao and Cohen, Myra B. and Rothermel, Gregg},
 title = {Configuration-aware regression testing: an empirical study of sampling and prioritization},
 abstract = {Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Researchers have created techniques for testing configurable software, but to date, only a little research has addressed the problems of regression testing configurable systems as they evolve. Whereas problems such as selective retesting and test prioritization at the test case level have been extensively researched, these problems have rarely been considered at the configuration level. In this paper we address the problem of providing configuration-aware regression testing for evolving software systems. We use combinatorial interaction testing techniques to model and generate configuration samples for use in regression testing. We conduct an empirical study on a non-trivial evolving software system to measure the impact of configurations on testing effectiveness, and to compare the effectiveness of different configuration prioritization techniques on early fault detection during regression testing. Our results show that configurations can have a large impact on fault detection and that prioritization of configurations can be effective.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390641},
 doi = {http://doi.acm.org/10.1145/1390630.1390641},
 acmid = {1390641},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {combinatorial interaction testing, configurable software, prioritization, regression testing},
} 

@inproceedings{Pacheco:2008:FEN:1390630.1390643,
 author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ball, Thomas},
 title = {Finding errors in .net with feedback-directed random testing},
 abstract = {We present a case study in which a team of test engineers at Microsoft applied a feedback-directed random testing tool to a critical component of the .NET architecture. Due to its complexity and high reliability requirements, the component had already been tested by 40 test engineers over five years, using manual testing and many automated testing techniques. Nevertheless, the feedback-directed random testing tool found errors in the component that eluded previous testing, and did so two orders of magnitude faster than a typical test engineer (including time spent inspecting the results of the tool). The tool also led the test team to discover errors in other testing and analysis tools, and deficiencies in previous best-practice guidelines for manual testing. Finally, we identify challenges that random testing faces for continued effectiveness, including an observed decrease in the technique's error detection rate over time.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {87--96},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390630.1390643},
 doi = {http://doi.acm.org/10.1145/1390630.1390643},
 acmid = {1390643},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {random testing},
} 

@inproceedings{Dor:2008:CCI:1390630.1390644,
 author = {Dor, Nurit and Lev-Ami, Tal and Litvak, Shay and Sagiv, Mooly and Weiss, Dror},
 title = {Customization change impact analysis for erp professionals via program slicing},
 abstract = {We describe a new tool that automatically identifies impact of customization changes, i.e., how changes affect software behavior. As opposed to existing static analysis tools that aim at aiding programmers or improve performance, our tool is designed for end-users without prior knowledge in programming. We utilize state-of-the-art static analysis algorithms for the programs within an Enterprise Resource Planning system (ERP). Key challenges in analyzing real world ERP programs are their significant size and the interdependency between programs. In particular, we describe and compare three customization change impact analyses for real-world programs, and a balancing algorithm built upon the three independent analyses. This paper presents PanayaImpactAnalysis (PanayaIA), a web on-demand tool, providing ERP professionals a clear view of the impact of a customization change on the system. In addition we report empirical results of PanayaIA when used by end-users on an ERP system of tens of millions LOCs.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390644},
 doi = {http://doi.acm.org/10.1145/1390630.1390644},
 acmid = {1390644},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {customization change impact analysis},
} 

@inproceedings{Kondoh:2008:FBJ:1390630.1390645,
 author = {Kondoh, Goh and Onodera, Tamiya},
 title = {Finding bugs in java native interface programs},
 abstract = {In this paper, we describe static analysis techniques for finding bugs in programs using the Java Native Interface (JNI). The JNI is both tedious and error-prone because there are many JNI-specific mistakes that are not caught by a native compiler. This paper is focused on four kinds of common mistakes. First, explicit statements to handle a possible exception need to be inserted after a statement calling a Java method. However, such statements tend to be forgotten. We present a typestate analysis to detect this exception-handling mistake. Second, while the native code can allocate resources in a Java VM, those resources must be manually released, unlike Java. Mistakes in resource management cause leaks and other errors. To detect Java resource errors, we used the typestate analysis also used for detecting general memory errors. Third, if a reference to a Java resource lives across multiple native method invocations, it should be converted into a global reference. However, programmers sometimes forget this rule and, for example, store a local reference in a global variable for later uses. We provide a syntax checker that detects this bad coding practice. Fourth, no JNI function should be called in a critical region. If called there, the current thread might block and cause a deadlock. Misinterpreting the end of the critical region, programmers occasionally break this rule. We present a simple typestate analysis to detect an improper JNI function call in a critical region. We have implemented our analysis techniques in a bug-finding tool called BEAM, and executed it on opensource software including JNI code. In the experiment, our analysis techniques found 86 JNI-specific bugs without any overhead and increased the total number of bug reports by 76\%.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {109--118},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390630.1390645},
 doi = {http://doi.acm.org/10.1145/1390630.1390645},
 acmid = {1390645},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java native interface, static analysis, typestate analysis},
} 

@inproceedings{Liblit:2008:CDF:1390630.1390632,
 author = {Liblit, Ben},
 title = {Cooperative debugging with five hundred million test cases},
 abstract = {The resources available for testing and verifying software are always limited, and through sheer numbers an application's user community will uncover many flaws not caught during development. The Cooperative Bug Isolation Project (CBI) marshals large user communities into a massive distributed debugging army to help programmers find and fix problems that appear after deployment. Dynamic instrumentation based on sparse random sampling provides our raw data; statistical machine learning techniques mine this data for critical bug predictors; static program analysis places bug predictors back in context of the program under study. We discuss CBI's dynamic, statistical, and static views of postdeployment debugging and show how these three different approaches join together to help improve software quality in an imperfect world.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {119--120},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390632},
 doi = {http://doi.acm.org/10.1145/1390630.1390632},
 acmid = {1390632},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooperative bug isolation, statistical debugging},
} 

@inproceedings{Buse:2008:MSR:1390630.1390647,
 author = {Buse, Raymond P.L. and Weimer, Westley R.},
 title = {A metric for software readability},
 abstract = {In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80\% effective, and better than a human on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with two traditional measures of software quality, code changes and defect reports. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggests that comments, in of themselves, are less important than simple blank lines to local judgments of readability.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390630.1390647},
 doi = {http://doi.acm.org/10.1145/1390630.1390647},
 acmid = {1390647},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FindBugs, code metrics, machine learning, program understanding, software maintenance, software readability},
} 

@inproceedings{Lincke:2008:CSM:1390630.1390648,
 author = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
 title = {Comparing software metrics tools},
 abstract = {This paper shows that existing software metric tools interpret and implement the definitions of object-oriented software metrics differently. This delivers tool-dependent metrics results and has even implications on the results of analyses based on these metrics results. In short, the metrics-based assessment of a software system and measures taken to improve its design differ considerably from tool to tool. To support our case, we conducted an experiment with a number of commercial and free metrics tools. We calculated metrics values using the same set of standard metrics for three software systems of different sizes. Measurements show that, for the same software system and metrics, the metrics values are tool depended. We also defined a (simple) software quality model for "maintainability" based on the metrics selected. It defines a ranking of the classes that are most critical wrt. maintainability. Measurements show that even the ranking of classes in a software system is metrics tool dependent.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {131--142},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390648},
 doi = {http://doi.acm.org/10.1145/1390630.1390648},
 acmid = {1390648},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {comparing tools, software quality metrics},
} 

@inproceedings{Tian:2008:DRS:1390630.1390649,
 author = {Tian, Chen and Nagarajan, Vijay and Gupta, Rajiv and Tallam, Sriraman},
 title = {Dynamic recognition of synchronization operations for improved data race detection},
 abstract = {Debugging multithreaded programs, which involves detection and identification of the cause of data races, has proved to be a hard problem. Although there has been significant amount of research on this topic, prior works rely on one important assumption - the debuggers must be aware of all the synchronization operations that take place during a program run. This assumption is a significant limitation as multithreaded programs, including the popular SPLASH-2 benchmark, have barriers and flag synchronizations implemented in the user code. We show that the lack of knowledge of these synchronization operations leads to unnecessary reporting of numerous races. Our experiments with SPLASH-2 benchmark suite show that 12-131 distinct segments in source code, on an average, give rise to well over 4 million dynamic instances of falsely reported races for these programs. We propose a dynamic software technique that identifies the user defined synchronizations exercised during a program run. This information not only helps avoids reporting of unnecessary races, but also helps a record/replay system to speedup the replay. Our evaluation confirms that our synchronization detector is highly accurate with no false negatives and very few false positives. Thus, reporting of nearly all unnecessary races is avoided. Finally, we show that the knowledge of synchronization operations resulted in about 23\% reduction in replay time.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390649},
 doi = {http://doi.acm.org/10.1145/1390630.1390649},
 acmid = {1390649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data races, record and replay, synchronization and infeasible races},
} 

@inproceedings{Bodden:2008:RER:1390630.1390650,
 author = {Bodden, Eric and Havelund, Klaus},
 title = {Racer: effective race detection using aspectj},
 abstract = {Programming errors occur frequently in large software systems, and even more so if these systems are concurrent. In the past researchers have developed specialized programs to aid programmers detecting concurrent programming errors such as deadlocks, livelocks, starvation and data races. In this work we propose a language extension to the aspect-oriented programming language AspectJ, in the form of three new pointcuts, <b>lock()</b>, <b>unlock()</b> and <b>maybeShared()</b>. These pointcuts allow programmers to monitor program events where locks are granted or handed back, and where values are accessed that may be shared amongst multiple Java threads. We decide thread-locality using a static thread-local objects analysis developed by others. Using the three new primitive pointcuts, researchers can directly implement efficient monitoring algorithms to detect concurrent programming errors online. As an example, we expose a new algorithm which we call R<sc>acer</sc>, an adoption of the well-known E<sc>raser</sc> algorithm to the memory model of Java. We implemented the new pointcuts as an extension to the AspectBench Compiler, implemented the Racer algorithm using this language extension and then applied the algorithm to the NASA K9 Rover Executive. Our experiments proved our implementation very effective. In the Rover Executive R<sc>acer</sc> finds 70 data races. Only one of these races was previously known. We further applied the algorithm to two other multi-threaded programs written by Computer Science researchers, in which we found races as well.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390650},
 doi = {http://doi.acm.org/10.1145/1390630.1390650},
 acmid = {1390650},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aspect-oriented programming, race detection, runtime verification, semantic pointcuts, static analysis},
} 

@inproceedings{Jeffrey:2008:FLU:1390630.1390652,
 author = {Jeffrey, Dennis and Gupta, Neelam and Gupta, Rajiv},
 title = {Fault localization using value replacement},
 abstract = {We present a value profile based approach for ranking program statements according to their likelihood of being faulty. The key idea is to see which program statements exercised during a failing run use values that can be altered so that the execution instead produces correct output. Our approach is effective in locating statements that are either faulty or directly linked to a faulty statement. We present experimental results showing the effectiveness and efficiency of our approach. Our approach outperforms Tarantula</i> which, to our knowledge, is the most effective prior approach for statement ranking based fault localization using the benchmark programs we studied.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390652},
 doi = {http://doi.acm.org/10.1145/1390630.1390652},
 acmid = {1390652},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, fault localization, interesting value mapping pair, value replacement},
} 

@inproceedings{Edwards:2008:AAF:1390630.1390653,
 author = {Edwards, Alex and Tucker, Sean and Worms, S\'{e}bastien and Vaidya, Rahul and Demsky, Brian},
 title = {AFID: an automated fault identification tool},
 abstract = {We present the Automatic Fault IDentification Tool (AFID). AFID automatically constructs repositories of real software faults by monitoring the software development process. AFID records both a fault revealing test case and a faulty version of the source code for any crashing faults that the developer discovers and a fault correcting source code change for any crashing faults that the developer corrects. The test cases are a significant contribution, because they enable new research that explores the dynamic behaviors of the software faults. AFID uses a ptrace-based monitoring mechanism to monitor both the compilation and execution of the application. The ptrace-based technique makes it straightforward for AFID to support a wide range of programming languages and compilers. Our benchmark results indicate that the monitoring overhead will be acceptable for most developers. We performed a short case study to evaluate how effectively the AFID tool records software faults. In our case study, AFID recorded 12 software faults from the 8 participants.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390630.1390653},
 doi = {http://doi.acm.org/10.1145/1390630.1390653},
 acmid = {1390653},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault collection},
} 

@inproceedings{Baah:2008:PPD:1390630.1390654,
 author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
 title = {The probabilistic program dependence graph and its application to fault diagnosis},
 abstract = {This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), that facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG is an augmentation of the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are widely used in applications such as medical diagnosis. This paper presents algorithms for constructing PPDGs and applying the PPDG to fault diagnosis. This paper also presents preliminary evidence indicating that PPDGs can facilitate fault localization and fault comprehension.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390654},
 doi = {http://doi.acm.org/10.1145/1390630.1390654},
 acmid = {1390654},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault diagnosis, machine learning, probabilistic graphical models, program analysis},
} 

@inproceedings{Papi:2008:PPT:1390630.1390656,
 author = {Papi, Matthew M. and Ali, Mahmood and Correa,Jr., Telmo Luis and Perkins, Jeff H. and Ernst, Michael D.},
 title = {Practical pluggable types for java},
 abstract = {This paper introduces the Checker Framework, which supports adding pluggable type systems to the Java language in a backward-compatible way. A type system designer defines type qualifiers and their semantics, and a compiler plug-in enforces the semantics. Programmers can write the type qualifiers in their programs and use the plug-in to detect or prevent errors. The Checker Framework is useful both to programmers who wish to write error-free code, and to type system designers who wish to evaluate and deploy their type systems. The Checker Framework includes new Java syntax for expressing type qualifiers; declarative and procedural mechanisms for writing type-checking rules; and support for flow-sensitive local type qualifier inference and for polymorphism over types and qualifiers. The Checker Framework is well-integrated with the Java language and toolset. We have evaluated the Checker Framework by writing 5 checkers and running them on over 600K lines of existing code. The checkers found real errors, then confirmed the absence of further errors in the fixed code. The case studies also shed light on the type systems themselves.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {201--212},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390656},
 doi = {http://doi.acm.org/10.1145/1390630.1390656},
 acmid = {1390656},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {annotation, bug finding, case study, compiler, flow sensitivity, igj, immutable, intern, java, javac, javari, nonnull, pluggable type, polymorphism, readonly, type qualifier, type system, verification},
} 

@inproceedings{Loginov:2008:VDS:1390630.1390657,
 author = {Loginov, Alexey and Yahav, Eran and Chandra, Satish and Fink, Stephen and Rinetzky, Noam and Nanda, Mangala},
 title = {Verifying dereference safety via expanding-scope analysis},
 abstract = {This paper addresses the challenging problem of verifying the safety of pointer dereferences in real Java programs. We provide an automatic approach to this problem based on a sound interprocedural analysis. We present a staged expanding-scope algorithm for interprocedural abstract interpretation, which invokes sound analysis with partial programs of increasing scope. This algorithm achieves many benefits typical of whole-program interprocedural analysis, but scales to large programs by limiting analysis to small program fragments. To address cases where the static analysis of program fragments fails to prove safety, the analysis also suggests possible annotations which, if a user accepts, ensure the desired properties. Experimental evaluation on a number of Java programs shows that we are able to verify 90\% of all dereferences soundly and automatically, and further reduce the number of remaining dereferences using non-nullness annotations.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390657},
 doi = {http://doi.acm.org/10.1145/1390630.1390657},
 acmid = {1390657},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, specification inference, static analysis},
} 

@inproceedings{Xu:2008:MEC:1390630.1390658,
 author = {Xu, Guoqing and Rountev, Atanas},
 title = {Merging equivalent contexts for scalable heap-cloning-based context-sensitive points-to analysis},
 abstract = {A context-sensitive points-to analysis maintains separate points-to relationships for each possible (abstract) calling context of a method. Previous work has shown that a large number of equivalence classes exists in the representation of calling contexts. Such equivalent contexts provide opportunities for context-sensitive analyses based on binary decision diagrams (BDDs), in which BDDs automatically merge equivalent points-to relationships. However, the use of a BDD ``black box'' introduces additional overhead for analysis running time. Furthermore, with heap cloning (i.e., using context-sensitive object allocation sites), BDDs are not as effective because the number of equivalence classes increases significantly. A further step must be taken to look inside the BDD black box to investigate where the equivalence comes from, and what tradeoffs can be employed to enable practical large-scale heap cloning. This paper presents an analysis for Java that exploits equivalence classes in context representation. For a particular pointer variable or heap object, all abstract contexts within an equivalence class can be merged. This technique naturally results in a new non-BDD context-sensitive points-to analysis. Based on these equivalence classes,the analysis employs a last-k</i>-substring merging approach to define scalability and precision tradeoffs. We show that small values for k</i> can enable scalable heap cloning for large Java programs. The proposed analysis has been implemented and evaluated on a large set of Java programs. The experimental results show improvements over an existing 1-object-sensitive analysis with heap cloning, which is the most precise scalable analysis implemented in the state-of-the-art Paddle analysis framework. For computing a points-to solution for an entire program, our approach is an order of magnitude faster compared to this BDD-based analysis and to a related non-BDD refinement-based analysis.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {225--236},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390658},
 doi = {http://doi.acm.org/10.1145/1390630.1390658},
 acmid = {1390658},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context sensitivity, pointer analysis, points-to analysis},
} 

@inproceedings{Balzarotti:2008:YVR:1390630.1390660,
 author = {Balzarotti, Davide and Banks, Greg and Cova, Marco and Felmetsger, Viktoria and Kemmerer, Richard and Robertson, William and Valeur, Fredrik and Vigna, Giovanni},
 title = {Are your votes <i>really</i> counted?: testing the security of real-world electronic voting systems},
 abstract = {Electronic voting systems play a critical role in today's democratic societies, as they are responsible for recording and counting the citizens' votes. Unfortunately, there is an alarming number of reports describing the malfunctioning of these systems, suggesting that their quality is not up to the task. Recently, there has been a focus on the security testing of voting systems to determine if they can be compromised in order to control the results of an election. We have participated in two large-scale projects, sponsored by the Secretaries of State of California and Ohio, whose respective goals were to perform the security testing of the electronic voting systems used in those two states. The testing process identified major flaws in all the systems analyzed, and resulted in substantial changes in the voting procedures of both states. In this paper, we describe the testing methodology that we used in testing two real-world electronic voting systems, the findings of our analysis, and the lessons we learned.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390660},
 doi = {http://doi.acm.org/10.1145/1390630.1390660},
 acmid = {1390660},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dres, security testing, voting systems},
} 

@inproceedings{Wassermann:2008:DTI:1390630.1390661,
 author = {Wassermann, Gary and Yu, Dachuan and Chander, Ajay and Dhurjati, Dinakar and Inamura, Hiroshi and Su, Zhendong},
 title = {Dynamic test input generation for web applications},
 abstract = {Web applications routinely handle sensitive data, and many people rely on them to support various daily activities, so errors can have severe and broad-reaching consequences. Unlike most desktop applications, many web applications are written in scripting languages, such as PHP. The dynamic features commonly supported by these languages significantly inhibit static analysis and existing static analysis of these languages can fail to produce meaningful results on realworld web applications. Automated test input generation using the concolic testing framework has proven useful for finding bugs and improving test coverage on C and Java programs, which generally emphasize numeric values and pointer-based data structures. However, scripting languages, such as PHP, promote a style of programming for developing web applications that emphasizes string values, objects, and arrays. In this paper, we propose an automated input test generation algorithm that uses runtime values to analyze dynamic code, models the semantics of string operations, and handles operations whose argument and return values may not share a common type. As in the standard concolic testing framework, our algorithm gathers constraints during symbolic execution. Our algorithm resolves constraints over multiple types by considering each variable instance individually, so that it only needs to invert each operation. By recording constraints selectively, our implementation successfully finds bugs in real-world web applications which state-of-the-art static analysis tools fail to analyze.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {249--260},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390661},
 doi = {http://doi.acm.org/10.1145/1390630.1390661},
 acmid = {1390661},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, concolic testing, directed random testing, web applications},
} 

@inproceedings{Artzi:2008:FBD:1390630.1390662,
 author = {Artzi, Shay and Kiezun, Adam and Dolby, Julian and Tip, Frank and Dig, Danny and Paradkar, Amit and Ernst, Michael D.},
 title = {Finding bugs in dynamic web applications},
 abstract = {Web script crashes and malformed dynamically-generated Web pages are common errors, and they seriously impact usability of Web applications. Current tools for Web-page validation cannot handle the dynamically-generated pages that are ubiquitous on today's Internet. In this work, we apply a dynamic test generation technique, based on combined concrete and symbolic execution, to the domain of dynamic Web applications. The technique generates tests automatically, uses the tests to detect failures, and minimizes the conditions on the inputs exposing each failure, so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for PHP. Apollo generates test inputs for the Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 214 faults in 4 PHP Web applications.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390662},
 doi = {http://doi.acm.org/10.1145/1390630.1390662},
 acmid = {1390662},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, php, software testing, web applications},
} 

@inproceedings{Buse:2008:ADI:1390630.1390664,
 author = {Buse, Raymond P.L. and Weimer, Westley R.},
 title = {Automatic documentation inference for exceptions},
 abstract = {Exception handling is a powerful and widely-used programming language abstraction for constructing robust software systems. Unfortunately, it introduces an inter-procedural flow of control that can be difficult to reason about. Failure to do so correctly can lead to security vulnerabilities, breaches of API encapsulation, and any number of safety policy violations. We present a fully automated tool that statically infers and characterizes exception-causing conditions in Java programs. Our tool is based on an inter-procedural, context-sensitive analysis. The output of this tool is well-suited for use as human-readable documentation of exceptional conditions. We evaluate the output of our tool by comparing it to over 900 instances of existing exception documentation in almost two million lines of code. We find that the output of our tool is at least as good as existing documentation 85\% of the time and is better 25\% of the time.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {273--282},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390630.1390664},
 doi = {http://doi.acm.org/10.1145/1390630.1390664},
 acmid = {1390664},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {documentation inference, exception handling, software documentation},
} 

@inproceedings{Kannan:2008:USE:1390630.1390665,
 author = {Kannan, Yamini and Sen, Koushik},
 title = {Universal symbolic execution and its application to likely data structure invariant generation},
 abstract = {Local data structure invariants are asserted over a bounded fragment of a data structure around a distinguished node M of the data structure. An example of such an invariant for a sorted doubly linked list is "for all nodes M</i> of the list, if M</i> \&#8800; <b>null</b> and M.next</i> \&#8800; <b>null</b>, then M.next.prev</i> = M</i> and M.value \&#8804; M.next.value.</i>" It has been shown that such local invariants are both natural and sufficient for describing a large class of data structures. This paper explores a novel technique, called K<sc>rystal</sc>, to infer likely local data structure invariants using a variant of symbolic execution, called universal symbolic execution. Universal symbolic execution is like traditional symbolic execution except the fact that we create a fresh symbolic variable for every read of a lvalue that has no mapping in the symbolic state rather than creating a symbolic variable only for inputs. This helps universal symbolic execution to symbolically track data flow for all memory locations along an execution even if input values do not flow directly into those memory locations. We have implemented our algorithm and applied it to several data structure implementations in Java. Our experimental results show that we can infer many interesting local invariants for these data structures.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390665},
 doi = {http://doi.acm.org/10.1145/1390630.1390665},
 acmid = {1390665},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic analysis, execution traces, logical inference, program invariants, symbolic execution},
} 

@inproceedings{Sankaranarayanan:2008:DIL:1390630.1390666,
 author = {Sankaranarayanan, Sriram and Chaudhuri, Swarat and Ivan\v{c}i\'{c}, Franjo and Gupta, Aarti},
 title = {Dynamic inference of likely data preconditions over predicates by tree learning},
 abstract = {We present a technique to infer likely data preconditions forprocedures written in an imperative programming language. Given a procedure and a set of predicates over its inputs, our technique enumerates different truth assignments to the predicates, deriving test cases from each feasible truth assignment. The predicates themselves are derived automatically using simple heuristics. The enumeration of truth assignments is performed using a propositional SAT solver along with a theory satisfiability checker capable of generating unsatisfiable cores. For each assignment of truth values, a corresponding set of test cases are generated and executed. Based on the result of the execution, the truth assignment is classified as being safe or buggy. Finally, a decision tree classifier is used to generate a Boolean formula over the input predicates that explains the data obtained from the test cases. The resulting Boolean formula is, in effect, a likely data precondition for the procedure under consideration. We apply our techniques on a wide variety of functions from the standard C library. Our experiments show that the proposed technique is quite robust. For most cases, it successfully learns a precondition that captures a safe and permissive calling environment.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1390630.1390666},
 doi = {http://doi.acm.org/10.1145/1390630.1390666},
 acmid = {1390666},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decision trees, machine learning, sat, software specification, verification},
} 

@inproceedings{Devanbu:2008:DIW:1390630.1390668,
 author = {Devanbu, Premkumar and Murphy, Brendan and Nagappan, Nachiappan and Zimmermann, Thomas and Dallmeier, Valentin},
 title = {DEFECTS 2008: international workshop on defects in large software systems},
 abstract = {Bugs are everywhere in today's software and because of the huge economic damage they are actively studied. The goal of this one-day workshop is to connect the different research communities with each other and with industry.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {307--308},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390668},
 doi = {http://doi.acm.org/10.1145/1390630.1390668},
 acmid = {1390668},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bugs, defect localization, defect prediction, empirical studies, faults, software defects},
} 

@inproceedings{Ur:2008:WPD:1390630.1390669,
 author = {Ur, Shmuel and Stoller, Scott D. and Farchi, Eitan D.},
 title = {6th workshop on parallel and distributed systems: testing and debugging (PADTAD '08)},
 abstract = {PADTAD brings together researchers from academia and researchers and practitioners from industry to promote the development of techniques and tools that aid in testing, analysis, and debugging of multi-threaded/parallel/distributed software.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {309--310},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390669},
 doi = {http://doi.acm.org/10.1145/1390630.1390669},
 acmid = {1390669},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent, debugging, distributed, multi-threaded, parallel, testing},
} 

@inproceedings{Bultan:2008:WTA:1390630.1390670,
 author = {Bultan, Tevfik and Xie, Tao},
 title = {Workshop on testing, analysis and verification of web software (TAV-WEB 2008)},
 abstract = {TAV-WEB 2008 is the third in a series of workshops that focus on testing, analysis and verification of web software. The goal of these workshops has been to bring together researchers from academic, research, and industrial communities interested in the emerging area of dependable Web software development, to present and discuss their recent research results.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {311--312},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390670},
 doi = {http://doi.acm.org/10.1145/1390630.1390670},
 acmid = {1390670},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {web applications, web services},
} 

@inproceedings{Liblit:2008:WSI:1390630.1390671,
 author = {Liblit, Ben and Rountev, Atanas},
 title = {WODA 2008: the sixth international workshop on dynamic analysis},
 abstract = {Dynamic analysis techniques reason over program executions and deal with data produced at program execution time. At WODA 2008, we bring together researchers and practitioners working in all areas of dynamic analysis to discuss new issues, share results and ongoing work, and foster collaborations.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {313--314},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390671},
 doi = {http://doi.acm.org/10.1145/1390630.1390671},
 acmid = {1390671},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {development of dynamic analysis tools and frameworks, dynamic analysis, efficient instrumentation techniques, fault detection and debugging, optimization techniques, program evolution, remote analysis and measurement of software systems, runtime monitoring, software testing, statistical reasoning techniques, synergies between static and dynamic analysis techniques, visualization and classification of program behavior},
} 

@inproceedings{Marinov:2008:WSE:1390630.1390672,
 author = {Marinov, Darko and Schulte, Wolfram},
 title = {Workshop on state-space exploration for automated testing (SSEAT 2008)},
 abstract = {SSEAT 2008 is a workshop that focuses on the recent research approaches to automated testing using state-space exploration techniques. The goal of the workshop is to bring together researchers from both industry and academia to informally discuss the latest successes and remaining challenges in this domain. One important aspect of the workshop is to discuss techniques that were tried but did not work well in certain contexts. Another important aspect is to try to identify a set of programs that can be used for comparing various tools and techniques. There will be no proceedings, but a brief summary of the workshop will appear at the workshop web page.},
 booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
 series = {ISSTA '08},
 year = {2008},
 isbn = {978-1-60558-050-0},
 location = {Seattle, WA, USA},
 pages = {315--316},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1390630.1390672},
 doi = {http://doi.acm.org/10.1145/1390630.1390672},
 acmid = {1390672},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, constraint solving, genetic algorithms, model checking, random exploration, state-space exploration, symbolic execution},
} 

@inproceedings{Engler:2007:UEM:1273463.1273464,
 author = {Engler, Dawson and Dunbar, Daniel},
 title = {Under-constrained execution: making automatic code destruction easy and scalable},
 abstract = {},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1273463.1273464},
 doi = {http://doi.acm.org/10.1145/1273463.1273464},
 acmid = {1273464},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug finding, dynamic analysis, symbolic execution},
} 

@inproceedings{Arumuga Nainar:2007:SDU:1273463.1273467,
 author = {Arumuga Nainar, Piramanayagam and Chen, Ting and Rosin, Jake and Liblit, Ben},
 title = {Statistical debugging using compound boolean predicates},
 abstract = {Statistical debugging uses dynamic instrumentation and machine learning to identify predicates on program state that are strongly predictive of program failure. Prior approaches have only considered simple, atomic predicates such as the directions of branches or the return values of function calls. We enrich the predicate vocabulary by adding complex Boolean formulae derived from these simple predicates. We draw upon three-valued logic, static program structure, and statistical estimation techniques to efficiently sift through large numbers of candidate Boolean predicate formulae. We present qualitative and quantitative evidence that complex predicates are practical, precise, and informative. Furthermore, we demonstrate that our approach is robust in the face of incomplete data provided by the sparse random sampling that typifies postdeployment statistical debugging.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {5--15},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273467},
 doi = {http://doi.acm.org/10.1145/1273463.1273467},
 acmid = {1273467},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging effort metrics, dynamic feedback analysis, statistical bug isolation, three-valued logic},
} 

@inproceedings{Jones:2007:DP:1273463.1273468,
 author = {Jones, James A. and Bowring, James F. and Harrold, Mary Jean},
 title = {Debugging in Parallel},
 abstract = {The presence of multiple faults in a program can inhibit the ability of fault-localization techniques to locate the faults. This problem occurs for two reasons: when a program fails, the number of faults is, in general, unknown; and certain faults may mask or obfuscate other faults. This paper presents our approach to solving this problem that leverages the well-known advantages of parallel work flows to reduce the time-to-release of a program. Our approach consists of a technique that enables more effective debugging in the presence of multiple faults and a methodology that enables multiple developers to simultaneously debug multiple faults. The paper also presents an empirical study that demonstrates that our parallel-debugging</i> technique and methodology can yield a dramatic decrease in total debugging time compared to a one-fault-at-a-time, or conventionally sequential</i>, approach.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {16--26},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273468},
 doi = {http://doi.acm.org/10.1145/1273463.1273468},
 acmid = {1273468},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, empirical study, execution clustering, fault localization, program analysis},
} 

@inproceedings{Tzoref:2007:IHA:1273463.1273469,
 author = {Tzoref, Rachel and Ur, Shmuel and Yom-Tov, Elad},
 title = {Instrumenting where it hurts: an automatic concurrent debugging technique},
 abstract = {As concurrent and distributive applications are becoming more common and debugging such applications is very difficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instrumentation, timing "noise" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small programs. In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program locations that are in the vicinity of the bugs on a number of industrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with different instrumentations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using instrumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evaluation of the feature but the derivative of that value along the program execution path. As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging algorithms for concurrent applications, based on noise injection based concurrent testing using instrumentation. The infrastructure is described in this paper.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273463.1273469},
 doi = {http://doi.acm.org/10.1145/1273463.1273469},
 acmid = {1273469},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, debugging, feature selection},
} 

@inproceedings{Hughes:2007:IGM:1273463.1273471,
 author = {Hughes, Graham and Bultan, Tevfik},
 title = {Interface grammars for modular software model checking},
 abstract = {We propose an interface specification language based on grammars for modular software model checking. In our interface specification language, component interfaces are specified as context free grammars. An interface grammar for a component specifies the sequences of method invocations that are allowed by that component. Using interface grammars one can specify nested call sequences that cannot be specified using interface specification formalisms that rely on finite state machines. Moreover, our interface grammars allow specification of semantic predicates and actions, which are Java code segments that can be used to express additional interface constraints. We have built an interface compiler that takes the interface grammar for a component as input and generates a stub for that component. The resulting stub is a table-driven parser generated from the input interface grammar. Invocation of a method within the component becomes the lookahead symbol for the stub/parser. The stub/parser uses a parser stack, the lookahead, and a parse table to guide the parsing. The semantic predicates and semantic actions that appear in the right hand sides of the production rules are executed when they appear at the top of the stack. We conducted a case study by writing an interface grammar for the Enterprise JavaBeans (EJB) persistence interface. Using our interface compiler we automatically generated an EJB stub using the EJB interface grammar. We used the JPF model checker to check EJB clients using this automatically generated EJB stub. Our results show that EJB clients can be verified efficiently using our approach.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {39--49},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273471},
 doi = {http://doi.acm.org/10.1145/1273463.1273471},
 acmid = {1273471},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interface grammars, model checking, modular verification},
} 

@inproceedings{d'Amorim:2007:DEE:1273463.1273472,
 author = {d'Amorim, Marcelo and Lauterburg, Steven and Marinov, Darko},
 title = {Delta execution for efficient state-space exploration of object-oriented programs},
 abstract = {State-space exploration is the essence of model checking and an increasingly popular approach for automating test generation. A key issue in exploration of object-oriented programs is handling the program state, in particular the heap. Previous research has focused on standard program execution that operates on one state/heap. We present Delta Execution, a technique that simultaneously operates on several states/heaps. It exploits the fact that many execution paths in state-space exploration partially overlap and speeds up the exploration by sharing the common parts across the executions and separately executing only the "deltas" where the executions differ. We have implemented Delta Execution in JPF, a popular general-purpose model checker for Java programs, and in BOX, a specialized model checker that we have developed for efficient exploration of sequential Java programs. We have evaluated Delta Execution for (bounded) exhaustive exploration of ten basic subject programs without errors. The experimental results show that on average Delta Execution improves the exploration time 10.97x (over an order of magnitude) in JPF and 2.07x in BOX. We have also evaluated Delta Execution for one larger case study with errors, where the exploration time improved up to 1.43x.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {50--60},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273472},
 doi = {http://doi.acm.org/10.1145/1273463.1273472},
 acmid = {1273472},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delta execution, model checking},
} 

@inproceedings{Manolios:2007:ACS:1273463.1273473,
 author = {Manolios, Panagiotis and Vroon, Daron and Subramanian, Gayatri},
 title = {Automating component-based system assembly},
 abstract = {One of the major challenges in the development of large component-based software systems is the system assembly problem: from a sea of available components, which should be selected and how should they be connected, integrated, and assembled so that the overall system requirements are satisfied? We present a powerful framework for automatically solving the system assembly problem directly from system requirements. Our framework includes an expressive language for declaratively describing system-level requirements, including component interfaces and dependencies, resource requirements, safety properties, objective functions, and various types of constraints. We show how to automatically solve system assembly problems using verification technology that takes advantage of current advances in Boolean satisfiability methods. We have implemented our techniques in the CoBaSA tool (Component-Based System Assembly), and we have successfully applied it to several large-scale industrial examples.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273463.1273473},
 doi = {http://doi.acm.org/10.1145/1273463.1273473},
 acmid = {1273473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {component-based software development, integrated modular avionics, pseudo-boolean satisfiability, system assembly problem},
} 

@inproceedings{Harman:2007:TEZ:1273463.1273475,
 author = {Harman, Mark and McMinn, Phil},
 title = {A theoretical \& empirical znalysis of evolutionary testing and hill climbing for structural test data generation},
 abstract = {Evolutionary testing has been widely studied as a technique for automating the process of test case generation. However, to date, there has been no theoretical examination of when and why it works. Furthermore, the empirical evidence for the effectiveness of evolutionary testing consists largely of small scale laboratory studies. This paper presents a first theoretical analysis of the scenarios in which evolutionary algorithms are suitable for structural test case generation. The theory is backed up by an empirical study that considers real world programs, the search spaces of which are several orders of magnitude larger than those previously considered.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {73--83},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273475},
 doi = {http://doi.acm.org/10.1145/1273463.1273475},
 acmid = {1273475},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test data generation, evolutionary testing, genetic algorithms, hill climbing, royal road, schema theory},
} 

@inproceedings{Ciupa:2007:EAR:1273463.1273476,
 author = {Ciupa, Ilinca and Leitner, Andreas and Oriol, Manuel and Meyer, Bertrand},
 title = {Experimental assessment of random testing for object-oriented software},
 abstract = {Progress in testing requires that we evaluate the effectiveness of testing strategies on the basis of hard experimental evidence, not just intuition or a priori arguments. Random testing, the use of randomly generated test data, is an example of a strategy that the literature often deprecates because of such preconceptions. This view is worth revisiting since random testing</i> otherwise offers several attractive properties: simplicity of implementation, speed of execution, absence of human bias. We performed an intensive experimental analysis of the efficiency of random testing on an existing industrial-grade code base. The use of a large-scale cluster of computers, for a total of 1500 hours of CPU time, allowed a fine-grain analysis of the individual effect of the various parameters involved in the random testing strategy, such as the choice of seed for a random number generator. The results provide insights into the effectiveness of random testing and a number of lessons for testing researchers and practitioners.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {84--94},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273476},
 doi = {http://doi.acm.org/10.1145/1273463.1273476},
 acmid = {1273476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {experimental evaluation, random testing, software testing},
} 

@inproceedings{Trew:2007:CRI:1273463.1273465,
 author = {Trew, Tim},
 title = {Chasing rainbows: improving software testing in the real world},
 abstract = {When releasing a new consumer product, the anticipated profits can be slashed by being late to market or having poor quality. Companies are keen to improve the efficiency and effectiveness of testing, to reduce their lead time and to be confident of the quality. However, it might appear that progress in the deployment of new approaches has been agonizingly slow. In practice, with the rapid evolution in the technology and business of consumer electronics, a major challenge is to anticipate the testing needs of the future while addressing the detailed issues that hamper the adoption of new approaches. The presentation will give an overview of how the software in embedded consumer products, together with its development approach, has evolved over the last decade, with the shift of signal processing from analogue hardware to software and from monolithic development organizations to ones that integrate and test components developed by others. Integration testing, one of the least well-understood areas, becomes crucial and there will be illustrations of the insight that testability can give into how architectures must be constrained to ensure that this testing can be effective. The presentation will also give examples of the successful introduction of new test technology, current trials of model-checking techniques and the new testing challenges posed by the introduction of the increasing complexity of embedded systems with systems-on-chip and their network-on-chip interconnects.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {95--96},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1273463.1273465},
 doi = {http://doi.acm.org/10.1145/1273463.1273465},
 acmid = {1273465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tomb:2007:VIP:1273463.1273478,
 author = {Tomb, Aaron and Brat, Guillaume and Visser, Willem},
 title = {Variably interprocedural program analysis for runtime error detection},
 abstract = {This paper describes an analysis approach based on a of static and dynamic techniques to ?nd run-time errors in Java code. It uses symbolic execution to ?nd constraints under which an error (e.g.</i> a null pointer dereference, array out of bounds access, or assertion violation) may occur and then solves these constraints to ?nd test inputs that may expose the error. It only alerts the user to the possibility of a real error when it detects the expected exception during a program run. The analysis is customizable in two important ways. First, we can adjust how deeply to follow calls from each top-level method. Second, we can adjust the path termination tion for the symbolic execution engine to be either a bound on the path condition length or a bound on the number of times each instruction can be revisited. We evaluated the tool on a set of benchmarks from the literature as well as a number of real-world systems that range in size from a few thousand to 50,000 lines of code. The tool discovered all known errors in the benchmarks (as well as some not previously known) and reported on average 8 errors per 1000 lines of code for the industrial examples. In both cases the interprocedural call depth played little role in the error detection. That is, an intraprocedural analysis seems adequate for the class of errors we detect.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {97--107},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273478},
 doi = {http://doi.acm.org/10.1145/1273463.1273478},
 acmid = {1273478},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {can-test, defect detection, generation, symbolic execution},
} 

@inproceedings{Koster:2007:UPT:1273463.1273479,
 author = {Koster, Ken},
 title = {Using portfolio theory for better and more consistent quality},
 abstract = {The effectiveness of software quality techniques varies. Many uncertain or unpredictable factors influence effectiveness, including human factors, the types of defects in the program, and luck. Compared to using a single quality technique, a diversified portfolio of techniques will typically be more effective and less variable. This work postulates a simple model, adapted from financial Modern Portfolio Theory, for the variability and effectiveness of techniques, singly and in portfolios. Proofs and simulations analyze the model to evaluate factors influencing the success of diversification; the model is checked against data sets from previous work.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1273463.1273479},
 doi = {http://doi.acm.org/10.1145/1273463.1273479},
 acmid = {1273479},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {diversification, economic models, effectiveness, portfolio software quality, testing, variability},
} 

@inproceedings{Dufour:2007:BAP:1273463.1273480,
 author = {Dufour, Bruno and Ryder, Barbara G. and Sevitsky, Gary},
 title = {Blended analysis for performance understanding of framework-based applications},
 abstract = {This paper defines a new analysis paradigm, blended program analysis</i>, that enables practical, effective analysis of large framework-based Java applications for performance understanding. Blended analysis combines a dynamic</i> representation of the program calling structure, with a static</i> analysis applied to a region of that calling structure with observed performance problems. A blended escape analysis is presented which enables approximation of object effective lifetimes, to facilitate explanation of the usage of newly created objects in a program region. Performance bottlenecks stemming from overuse of temporary structures are common in framework-based applications. Metrics are introduced to expose how, in aggregate, these applications make use of new objects. Results of empirical experiments with the Trade</i> benchmark are presented. A case study demonstrates how results from a blended escape analysis can help locate, in a region which calls 223 distinct methods, the single call path responsible for a performance problem involving objects created at 9 distinct sites and as far as 6 call levels away.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {118--128},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273480},
 doi = {http://doi.acm.org/10.1145/1273463.1273480},
 acmid = {1273480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow analysis, escape analysis, framework-intensive applications, java, program understandingperfor-mance},
} 

@inproceedings{Cohen:2007:ITH:1273463.1273482,
 author = {Cohen, Myra B. and Dwyer, Matthew B. and Shi, Jiangfan},
 title = {Interaction testing of highly-configurable systems in the presence of constraints},
 abstract = {Combinatorial interaction testing (CIT) is a method to sample configurations of a software system systematically for testing. Many algorithms have been developed that create CIT samples, however few have considered the practical concerns that arise when adding constraints between combinations of options. In this paper, we survey constraint handling techniques in existing algorithms and discuss the challenges that they present. We examine two highly-configurable software systems to quantify the nature of constraints in real systems. We then present a general constraint representation and solving technique that can be integrated with existing CIT algorithms and compare two constraint-enhanced algorithm implementations with existing CIT tools to demonstrate feasibility.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {129--139},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273482},
 doi = {http://doi.acm.org/10.1145/1273463.1273482},
 acmid = {1273482},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, combinatorial interaction testing, constraints, covering arrays},
} 

@inproceedings{Yoo:2007:PEM:1273463.1273483,
 author = {Yoo, Shin and Harman, Mark},
 title = {Pareto efficient multi-objective test case selection},
 abstract = {Previous work has treated test case selection as a single objective optimisation problem. This paper introduces the concept of Pareto efficiency to test case selection. The Pareto efficient approach takes multiple objectives such as code coverage, past fault-detection history and execution cost, and constructs a group of non-dominating, equivalently optimal test case subsets. The paper describes the potential bene?ts of Pareto efficient multi-objective test case selection, illustrating with empirical studies of two and three objective formulations.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {140--150},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273483},
 doi = {http://doi.acm.org/10.1145/1273463.1273483},
 acmid = {1273483},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-objective evolutionary algorithm, test case selection},
} 

@inproceedings{Emmi:2007:DTI:1273463.1273484,
 author = {Emmi, Michael and Majumdar, Rupak and Sen, Koushik},
 title = {Dynamic test input generation for database applications},
 abstract = {We describe an algorithm for automatic test input generation for database applications. Given a program in an imperative language that interacts with a database through API calls, our algorithm generates both input data for the program as well as suitable database records to systematically explore all paths of the program, including those paths whose execution depend on data returned by database queries. Our algorithm is based on concolic execution, where the program is run with concrete inputs and simultaneously also with symbolic inputs for both program variables as well as the database state. The symbolic constraints generated along a path enable us to derive new input values and new database records that can cause execution to hit uncovered paths. Simultaneously, the concrete execution helps to retain precision in the symbolic computations by allowing dynamic values to be used in the symbolic executor. This allows our algorithm, for example, to identify concrete SQL queries made by the program, even if these queries are built dynamically. The contributions of this paper are the following. We develop an algorithm that can track symbolic constraints across language boundaries and use those constraints in conjunction with a novel constraint solver to generate both program inputs and database state. We propose a constraint solver that can solve symbolic constraints consisting of both linear arithmetic constraints over variables as well as string constraints (string equality, disequality, as well as membership in regular languages). Finally, we provide an evaluation of the algorithm on a Java implementation of MediaWiki, a popular wiki package that interacts with a database back-end.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273463.1273484},
 doi = {http://doi.acm.org/10.1145/1273463.1273484},
 acmid = {1273484},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test generation, concolic testing, database applications, directed random testing},
} 

@inproceedings{Chang:2007:FWN:1273463.1273486,
 author = {Chang, Ray-Yaung and Podgurski, Andy and Yang, Jiong},
 title = {Finding what's not there: a new approach to revealing neglected conditions in software},
 abstract = {Neglected conditions are an important but difficult-to-find class of software defects. This paper presents a novel approach to revealing neglected conditions that integrates static program analysis and advanced data mining techniques to discover implicit conditional rules in a code base and to discover rule violations that indicate neglected conditions. The approach requires the user to indicate minimal constraints on the context of the rules to be sought, rather than specific rule templates. To permit this generality, rules are modeled as graph minors</i> of program dependence graphs, and both frequent itemset mining and frequent subgraph mining algorithms are employed to identify candidate rules. We report the results of an empirical evaluation of the approach in which it was used to discover conditional rules and neglected conditions in ~25,000 lines of source code.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {163--173},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273486},
 doi = {http://doi.acm.org/10.1145/1273463.1273486},
 acmid = {1273486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic defect detection, frequent itemset mining, frequent subgraph mining, mining software repositories, program dependences},
} 

@inproceedings{Shoham:2007:SSM:1273463.1273487,
 author = {Shoham, Sharon and Yahav, Eran and Fink, Stephen and Pistoia, Marco},
 title = {Static specification mining using automata-based abstractions},
 abstract = {We present a novel approach to client-side mining of temporal API specifications based on static analysis. Specifically, we present an interprocedural analysis over a combined domain that abstracts both aliasing and event sequences for individual objects. The analysis uses a new family of automata-based abstractions to represent unbounded event sequences, designed to disambiguate distinct usage patterns and merge similar usage patterns. Additionally, our approach includes an algorithm that summarizes abstract traces based on automata clusters, and effectively rules out spurious behaviors. We show experimental results mining specifications from a number of Java clients and APIs. The results indicate that effective static analysis for client-side mining requires fairly precise treatment of aliasing and abstract event sequences. Based on the results, we conclude that static client-side specification mining shows promise as a complement or alternative to dynamic approaches.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {174--184},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273487},
 doi = {http://doi.acm.org/10.1145/1273463.1273487},
 acmid = {1273487},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, specification mining, static analysis, typestate},
} 

@inproceedings{Xin:2007:EOD:1273463.1273489,
 author = {Xin, Bin and Zhang, Xiangyu},
 title = {Efficient online detection of dynamic control dependence},
 abstract = {Capturing dynamic control dependence is critical for many dynamic program analysis such as dynamic slicing, dynamic information flow, and data lineage computation. Existing algorithms are mostly a simple runtime translation of the static definition, which fails to capture certain dynamic properties by its nature, leading to inefficiency. In this paper, we propose a novel online detection technique for dynamic control dependence. The technique is based upon a new definition, which is equivalent to the existing one in the intraprocedural case but it enables an efficient detection algorithm. The new algorithm naturally and efficiently handles interprocedural dynamic control dependence even in presence of irregular control flow. Our evaluation shows that the detection algorithm slows down program execution by a factor of 2.57, which is 2.54 times faster than the existing algorithm that was used in prior work.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {185--195},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273489},
 doi = {http://doi.acm.org/10.1145/1273463.1273489},
 acmid = {1273489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic control dependence, dynamic information flow, dynamic post-dominance, dynamic program slicing, irregular control flow},
} 

@inproceedings{Clause:2007:DGD:1273463.1273490,
 author = {Clause, James and Li, Wanchun and Orso, Alessandro},
 title = {Dytan: a generic dynamic taint analysis framework},
 abstract = {Dynamic taint analysis is gaining momentum. Techniques based on dynamic tainting have been successfully used in the context of application security, and now their use is also being explored in different areas, such as program understanding, software testing, and debugging. Unfortunately, most existing approaches for dynamic tainting are defined in an ad-hoc manner, which makes it difficult to extend them, experiment with them, and adapt them to new contexts. Moreover, most existing approaches are focused on data-flow based tainting only and do not consider tainting due to control flow, which limits their applicability outside the security domain. To address these limitations and foster experimentation with dynamic tainting techniques, we defined and developed a general framework for dynamic tainting that (1) is highly flexible and customizable, (2) allows for performing both data-flow and control-flow based tainting conservatively, and (3) does not rely on any customized run-time system. We also present DYTAN, an implementation of our framework that works on x86 executables, and a set of preliminary studies that show how DYTAN can be used to implement different tainting-based approaches with limited effort. In the studies, we also show that DYTAN can be used on real software, by using FIREFOX as one of our subjects, and illustrate how the specific characteristics of the tainting approach used can affect efficiency and accuracy of the taint analysis, which further justifies the use of our framework to experiment with different variants of an approach.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {196--206},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273490},
 doi = {http://doi.acm.org/10.1145/1273463.1273490},
 acmid = {1273490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic tainting, general framework, information flow},
} 

@inproceedings{Tallam:2007:ETL:1273463.1273491,
 author = {Tallam, Sriraman and Tian, Chen and Gupta, Rajiv and Zhang, Xiangyu},
 title = {Enabling tracing Of long-running multithreaded programs via dynamic execution reduction},
 abstract = {Debugging long running multithreaded programs is a very challenging problem when using tracing-based analyses. Since such programs are non-deterministic, reproducing the bug is non-trivial and generating and inspecting traces for long running programs can be prohibitively expensive. We propose a framework in which, to overcome the problem of bug reproducibility, a lightweight logging technique is used to log the events during the original execution. When a bug is encountered, it is reproduced using the generated log and during the replay, a fine-grained tracing technique is employed to collect control-flow/dependence traces that are then used to locate the root cause of the bug. In this paper, we address the key challenges resulting due to tracing, that is, the prohibitively high expense of collecting traces and the significant burden on the user who must examine the large amount of trace information to locate the bug in a long-running multithreaded program. These challenges are addressed through execution reduction</i> that realizes a combination of logging and tracing such that traces collected contain only the execution information from those regions of threads that are relevant to the fault. This approach is highly effective because we observe that for long running multithreaded programs, many threads that execute are irrelevant to the fault. Hence, these threads need not be replayed and traced when trying to reproduce the bug. We develop a novel lightweight scheme that identifies such threads by observing all the interthread data dependences and removes their execution footprint in the replay run. In addition, we identify regions of thread executions that need not be replayed or, if they must be replayed, we determine if they need not be traced. Following execution reduction, the replayed execution takes lesser time to run and it produces a much smaller trace than the original execution. Thus, the cost of collecting traces and the effort of examining the traces to locate the fault are greatly reduced.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1273463.1273491},
 doi = {http://doi.acm.org/10.1145/1273463.1273491},
 acmid = {1273491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, control flow, debugging, dependence tracing, event logging, replay},
} 

@inproceedings{Ostrand:2007:AAI:1273463.1273493,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
 title = {Automating algorithms for the identification of fault-prone files},
 abstract = {This research investigates ways of predicting which files would be most likely to contain large numbers of faults in the next release of a large industrial software system. Previous work involved making predictions using several different models ranging from a simple, fully-automatable model (the LOC model) to several different variants of a negative binomial regression model that were customized for the particular software system under study. Not surprisingly, the custom models invariably predicted faults more accurately than the simple model. However, development of customized models requires substantial time and analytic effort, as well as statistical expertise. We now introduce new, more sophisticated models that yield more accurate predictions than the earlier LOC model, but which nonetheless can be fully automated. We also extend our earlier research by presenting another large-scale empirical study of the value of these prediction models, using a new industrial software system over a nine year period.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {219--227},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1273463.1273493},
 doi = {http://doi.acm.org/10.1145/1273463.1273493},
 acmid = {1273493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
} 

@inproceedings{Wang:2007:HDS:1273463.1273494,
 author = {Wang, Tao and Roychoudhury, Abhik},
 title = {Hierarchical dynamic slicing},
 abstract = {Dynamic slicing is a widely used technique for program analysis, debugging, and comprehension. However, the reported slice is often too large to be inspected by the programmer. In this work, we address this deficiency by hierarchically applying dynamic slicing at various levels of granularity. The basic observation is to divide a program execution trace into "phases", with data/control dependencies inside each phase being suppressed. Only the inter-phase dependencies are presented to the programmer. The programmer then zooms into one of these phases which is further divided into sub-phases and analyzed. We also discuss how our ideas can be used to augment debugging methods other then slicing (such as "fault localization", a recently proposed trace comparison method for software debugging).},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {228--238},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273494},
 doi = {http://doi.acm.org/10.1145/1273463.1273494},
 acmid = {1273494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dynamic slicing, phase detection},
} 

@inproceedings{Ren:2007:HRJ:1273463.1273495,
 author = {Ren, Xiaoxia and Ryder, Barbara G.},
 title = {Heuristic ranking of java program edits for fault localization},
 abstract = {In modern software development, regression tests are used to confirm the fundamental functionalities of an edited program and to assure the code quality. Difficulties occur when testing reveals unexpected behaviors, which indicate potential defects introduced by the edit. However, the changes that caused the failure(s) are not always easy to find. We propose a heuristic that ranks method changes that might have affected a failed test, indicating the likelihood that they may have contributed to a test failure. Our heuristic is based on the calling structure of the failed test (e.g., the number of ancestors and descendents of a method in the test's call graph, whether the caller or callee was changed, etc.). We evaluated the effectiveness of the heuristic in 14 pairs of edited versions in the Eclipse jdt core</i> plug-in, using the test suite from its compiler tests</i> plug-in. Our results indicate that when a failure is caused by a single method change, our heuristic ranked the failure-inducing change as number 1 or number 2 of all the method changes in 67\% of the delegate tests (i.e., representatives of all failing tests). Even when the failure is caused by some combination of the changes, rather than a single change, our heuristic still helps.},
 booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {239--249},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273495},
 doi = {http://doi.acm.org/10.1145/1273463.1273495},
 acmid = {1273495},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {change impact analysis, debugging, fault localization, heuristic ranking, java, testing},
} 

@inproceedings{Walcott:2006:TTS:1146238.1146240,
 author = {Walcott, Kristen R. and Soffa, Mary Lou and Kapfhammer, Gregory M. and Roos, Robert S.},
 title = {TimeAware test suite prioritization},
 abstract = {Regression test prioritization is often performed in a time constrained execution environment in which testing only occurs for a fixed time period. For example, many organizations rely upon nightly building and regression testing of their applications every time source code changes are committed to a version control repository. This paper presents a regression test prioritization technique that uses a genetic algorithm to reorder test suites in light of testing time constraints. Experiment results indicate that our prioritization approach frequently yields higher average percentage of faults detected (APFD) values, for two case study applications, when basic block level coverage is used instead of method level coverage. The experiments also reveal fundamental trade offs in the performance of time-aware prioritization. This paper shows that our prioritization technique is appropriate for many regression testing environments and explains how the baseline approach can be extended to operate in additional time constrained testing circumstances.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146240},
 doi = {http://doi.acm.org/10.1145/1146238.1146240},
 acmid = {1146240},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage testing, genetic algorithms, test prioritization},
} 

@inproceedings{McMinn:2006:SPP:1146238.1146241,
 author = {McMinn, Phil and Harman, Mark and Binkley, David and Tonella, Paolo},
 title = {The species per path approach to SearchBased test data generation},
 abstract = {This paper introduces the Species per Path approach to search-based software test data generation. The approach transforms the program under test into a version in which multiple paths to the search target are factored out. Test data are then sought for each individual path by dedicated 'species' operating in parallel. The factoring out of paths results in several individual search landscapes, with feasible paths giving rise to landscapes that are potentially more conducive to test data discovery than the original overall landscape.The paper presents the results of two empirical studies that validate and verify the approach. The validation study supports the claim that the approach is widely applicable and practical. The verification study shows that it is possible to generate test data for targets with the approach that are troublesome for the standard evolutionary method.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146241},
 doi = {http://doi.acm.org/10.1145/1146238.1146241},
 acmid = {1146241},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test data generation, evolutionary testing, search-based software engineering, testability transformation},
} 

@inproceedings{Whalen:2006:CMR:1146238.1146242,
 author = {Whalen, Michael W. and Rajan, Ajitha and Heimdahl, Mats P.E. and Miller, Steven P.},
 title = {Coverage metrics for requirements-based testing},
 abstract = {In black-box</i> testing, one is interested in creating a suite of tests from requirements that adequately exercise the behavior of a software system without regard to the internal structure of the implementation. In current practice, the adequacy of black box test suites is inferred by examining coverage on an executable artifact, either source code or a software model.In this paper, we define structural coverage metrics directly on high-level formal software requirements. These metrics provide objective, implementation-independent</i> measures of how well a black-box test suite exercises a set of requirements. We focus on structural coverage criteria on requirements formalized as LTL properties and discuss how they can be adapted to measure finite test cases. These criteria can also be used to automatically generate a requirements-based test suite. Unlike model or code-derived test cases, these tests are immediately traceable to high-level requirements. To assess the practicality of our approach, we apply it on a realistic example from the avionics domain.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146242},
 doi = {http://doi.acm.org/10.1145/1146238.1146242},
 acmid = {1146242},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Visser:2006:TIG:1146238.1146243,
 author = {Visser, Willem and P\v{a}s\v{a}reanu, Corina S. and Pel\'{a}nek, Radek},
 title = {Test input generation for java containers using state matching},
 abstract = {The popularity of object-oriented programming has led to the wide use of container libraries. It is important for the reliability of these containers that they are tested adequately. We describe techniques for automated test input generation of Java container classes. Test inputs are sequences of method calls from the container interface. The techniques rely on state matching to avoid generation of redundant tests. Exhaustive techniques</i> use model checking with explicit or symbolic execution to explore all</i> the possible test sequences up to predefined input sizes. Lossy techniques</i> rely on abstraction mappings to compute and store abstract versions of the concrete states; they explore underapproximations</i> of all the possible test sequences.We have implemented the techniques on top of the Java PathFinder model checker and we evaluate them using four Java container classes. We compare state matching based techniques and random selection for generating test inputs, in terms of testing coverage. We consider basic block coverage and a form of predicate coverage - that measures whether all combinations of a predetermined set of predicates are covered at each basic block. The exhaustive techniques can easily obtain basic block coverage, but cannot obtain good predicate coverage before running out of memory. On the other hand, abstract matching turns out to be a powerful approach for generating test inputs to obtain high predicate coverage. Random selection performed well except on the examples that contained complex input spaces, where the lossy abstraction techniques performed better.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146243},
 doi = {http://doi.acm.org/10.1145/1146238.1146243},
 acmid = {1146243},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ruthruff:2006:EPA:1146238.1146245,
 author = {Ruthruff, Joseph R. and Elbaum, Sebastian and Rothermel, Gregg},
 title = {Experimental program analysis: a new program analysis paradigm},
 abstract = {Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that a new form of program analysis technique can be created by incorporating characteristics of experimentation into analyses. This paper reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We show how the use of an experimental program analysis paradigm can help researchers identify limitations of analysis techniques, improve existing experimental program analysis techniques, and create new experimental program analysis techniques.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146245},
 doi = {http://doi.acm.org/10.1145/1146238.1146245},
 acmid = {1146245},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {experimental program analysis, experimentation, program analysis},
} 

@inproceedings{Bell:2006:LBR:1146238.1146246,
 author = {Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
 title = {Looking for bugs in all the right places},
 abstract = {We continue investigating the use of a negative binomial regression model to predict which files in a large industrial software system are most likely to contain many faults in the next release. A new empirical study is described whose subject is an automated voice response system. Not only is this system's functionality substantially different from that of the earlier systems we studied (an inventory system and a service provisioning system), it also uses a significantly different software development process. Instead of having regularly scheduled releases as both of the earlier systems did, this system has what are referred to as "continuous releases." We explore the use of three versions of the negative binomial regression model, as well as a simple lines-of-code based model, to make predictions for this system and discuss the differences observed from the earlier studies. Despite the different development process, the best version of the prediction model was able to identify, over the lifetime of the project, 20\% of the system's files that contained, on average, nearly three quarters of the faults that were detected in the system's next releases.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146246},
 doi = {http://doi.acm.org/10.1145/1146238.1146246},
 acmid = {1146246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
} 

@inproceedings{Wagner:2006:MSA:1146238.1146247,
 author = {Wagner, Stefan},
 title = {A model and sensitivity analysis of the quality economics of defect-detection techniques},
 abstract = {One of the main cost factors in software development is the detection and removal of defects. However, the relationships and influencing factors of the costs and revenues of defect-detection techniques are still not well understood. This paper proposes an analytical, stochastic model of the economics of defect detection and removal to improve this understanding. The model is able to incorporate dynamic as well as static techniques in contrast to most other models of that kind. We especially analyse the model with state-ofthe-art sensitivity analysis methods to (1) identify the most relevant factors for model simplification and (2) prioritise the factors to guide further research and measurements.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146247},
 doi = {http://doi.acm.org/10.1145/1146238.1146247},
 acmid = {1146247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cost/benefit, defect-detection techniques, quality costs, sensitivity analysis, software quality economics},
} 

@inproceedings{Hamlet:2006:STU:1146238.1146249,
 author = {Hamlet, Dick},
 title = {Subdomain testing of units and systems with state},
 abstract = {This paper extends basic software-testing theory to software components and adds explicit state to the theory. The resulting theory is simple enough to abstractly model the construction of systems from their parts ('units'). It provides an unconventional insight into the relationship between testing units and testing systems. Experiments exploring the theory support the following conclusions: <ul><li>Units should be independent, more like what are called "components" than subroutines or object-oriented classes.</li> <li>Units' persistent state should be local.</li> <li>Units should be extensively tested.</li></ul>.A new kind of system testing is proposed: Unit-test results are combined to approximate the system behavior. Testing the approximation is cheaper and easier than testing the actual system and more likely to expose system problems.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146249},
 doi = {http://doi.acm.org/10.1145/1146238.1146249},
 acmid = {1146249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {persistent state, testing theory, unit/system testing},
} 

@inproceedings{Cobleigh:2006:BUH:1146238.1146250,
 author = {Cobleigh, Jamieson M. and Avrunin, George S. and Clarke, Lori A.},
 title = {Breaking up is hard to do: an investigation of decomposition for assume-guarantee reasoning},
 abstract = {Finite-state verification techniques are often hampered by the stateexplosion problem. One proposed approach for addressing this problem is assume-guarantee reasoning. Using recent advances in assume-guarantee reasoning that automatically generate assumptions, we undertook a study to determine if assume-guarantee reasoning provides an advantage over monolithic verification. In this study, we considered all</i> two-way decompositions for a set of systems and properties, using two different verifiers, FLAVERS and LTSA. By increasing the number of repeated tasks, we evaluated the decompositions as the systems were scaled. In only a few cases could assume-guarantee reasoning verify properties on larger systems than monolithic verification and, in these cases, assumeguarantee reasoning could only verify these properties on systems a few sizes larger than monolithic verification. This discouraging result, although preliminary, raises doubts about the usefulness of assume-guarantee reasoning.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146250},
 doi = {http://doi.acm.org/10.1145/1146238.1146250},
 acmid = {1146250},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assume-guarantee reasoning},
} 

@inproceedings{Dennis:2006:MVC:1146238.1146251,
 author = {Dennis, Greg and Chang, Felix Sheng-Ho and Jackson, Daniel},
 title = {Modular verification of code with SAT},
 abstract = {An approach is described for checking the methods of a class against a full specification. It shares with traditional model checking the idea of exhausting the entire space of executions within some finite bounds, and with traditional verification the idea of modular analysis, in which a method is analyzed, in isolation, for all possible calling contexts.The analysis involves an automatic two-phase reduction: first, to an intermediate form in relational logic (using a new encoding described here), and second, to a boolean formula (using existing techniques), which is then handed to an off the shelf SAT solver.A variety of implementations of the Java Collections Framework's List interface were checked against existing JML specifications. The analysis revealed bugs in the implementations, as well as errors in the specifications themselves.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146251},
 doi = {http://doi.acm.org/10.1145/1146238.1146251},
 acmid = {1146251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, alloy, first-order logic, formal methods, formal verification, software model checking},
} 

@inproceedings{Centonze:2006:RAC:1146238.1146253,
 author = {Centonze, Paolina and Naumovich, Gleb and Fink, Stephen J. and Pistoia, Marco},
 title = {Role-Based access control consistency validation},
 abstract = {Modern enterprise systems support Role-Based Access Control (RBAC). Although RBAC allows restricting access to privileged operations</i>, a deployer may actually intend to restrict access to privileged data</i>. This paper presents a theoretical foundation for correlating an operation-based RBAC policy with a data-based RBAC policy. Relying on a location consistency</i> property, this paper shows how to infer whether an operation-based RBAC policy is equivalent to any databased RBAC policy. We have built a static analysis tool for Java Platform, Enterprise Edition (Java EE) called Static Analysis for Validation of Enterprise Security (SAVES). Relying on interprocedural pointer analysis and dataflow analysis, SAVES analyzes Java EE bytecode to determine if the associated RBAC policy is location consistent, and reports potential security flaws where location consistency does not hold. The experimental results obtained by using SAVES on a number of production-level Java EE codes have identified several security flaws with no false positive reports.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146253},
 doi = {http://doi.acm.org/10.1145/1146238.1146253},
 acmid = {1146253},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {J2EE, Java, Java EE, RBAC, role-based access control, security, static analysis},
} 

@inproceedings{Fink:2006:ETV:1146238.1146254,
 author = {Fink, Stephen and Yahav, Eran and Dor, Nurit and Ramalingam, G. and Geay, Emmanuel},
 title = {Effective typestate verification in the presence of aliasing},
 abstract = {This paper addresses the challenge of sound typestate verification, with acceptable precision, for real-world Java programs. We present a novel framework for verification of typestate properties, including several new techniques to precisely treat aliases without undue performance costs. In particular, we present a flowsensitive, context-sensitive, integrated verifier that utilizes a parametric abstract domain combining typestate and aliasing information.To scale to real programs without compromising precision, we present a staged verification system in which faster verifiers run as early stages which reduce the workload for later, more precise, stages.We have evaluated our framework on a number of real Java programs, checking correct API usage for various Java standard libraries. The results show that our approach scales to hundreds of thousands of lines of code, and verifies correctness for 93\% of the potential points of failure.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146254},
 doi = {http://doi.acm.org/10.1145/1146238.1146254},
 acmid = {1146254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, program verification, typestate},
} 

@inproceedings{Yorsh:2006:TAT:1146238.1146255,
 author = {Yorsh, Greta and Ball, Thomas and Sagiv, Mooly},
 title = {Testing, abstraction, theorem proving: better together!},
 abstract = {We present a method for static program analysis that leverages tests and concrete program executions. State abstractions generalize the set of program states obtained from concrete executions. A theorem prover then checks that the generalized set of concrete states covers all potential executions and satisfies additional safety properties. Our method finds the same potential errors as the mostprecise abstract interpreter for a given abstraction and is potentially more efficient. Additionally, it provides a new way to tune the performance of the analysis by alternating between concrete execution and theorem proving. We have implemented our technique in a prototype for checking properties of C# programs.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146255},
 doi = {http://doi.acm.org/10.1145/1146238.1146255},
 acmid = {1146255},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, abstraction, adequacy criteria, coverage, fabricated states, program analysis, software fault injection, state-based coverage, testing, theorem prover},
} 

@inproceedings{Siegel:2006:UMC:1146238.1146256,
 author = {Siegel, Stephen F. and Mironova, Anastasia and Avrunin, George S. and Clarke, Lori A.},
 title = {Using model checking with symbolic execution to verify parallel numerical programs},
 abstract = {We present a method to verify the correctness of parallel programs that perform complex numerical computations, including computations involving floating-point arithmetic. The method requires that a sequential version of the program be provided, to serve as the specification for the parallel one. The key idea is to use model checking, together with symbolic execution, to establish the equivalence of the two programs.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146256},
 doi = {http://doi.acm.org/10.1145/1146238.1146256},
 acmid = {1146256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPI, concurrency, finite state verification, floating-point, high performance computing, message passing interface, model checking, numerical program, parallel programming, spin, symbolic execution},
} 

@inproceedings{Boshernitsan:2006:DAL:1146238.1146258,
 author = {Boshernitsan, Marat and Doong, Roongko and Savoia, Alberto},
 title = {From daikon to agitator: lessons and challenges in building a commercial tool for developer testing},
 abstract = {Developer testing is of one of the most effective strategies for improving the quality of software, reducing its cost, and accelerating its development. Despite its widely recognized benefits, developer testing is practiced by only a minority of developers. The slow adoption of developer testing is primarily due to the lack of tools that automate some of the more tedious and time-consuming aspects of this practice. Motivated by the need for a solution, and helped and inspired by the research in software test automation, we created a developer testing tool based on software agitation. Software agitation</i> is a testing technique that combines the results of research in test-input generation and dynamic invariant detection. We implemented software agitation in a commercial testing tool called Agitator. This paper gives a high-level overview of software agitation and its implementation in Agitator, focusing on the lessons and challenges of leveraging and applying the results of research to the implementation of a commercial product.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146258},
 doi = {http://doi.acm.org/10.1145/1146238.1146258},
 acmid = {1146258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing tools, developer testing, dynamic invariant detection, software agitation, technology transfer, test-input generation, unit testing},
} 

@inproceedings{Kiviluoma:2006:RMA:1146238.1146259,
 author = {Kiviluoma, Kimmo and Koskinen, Johannes and Mikkonen, Tommi},
 title = {Run-time monitoring of architecturally significant behaviors using behavioral profiles and aspects},
 abstract = {Although static structures are often advocated as the main ingredient of a software architecture, dynamic program behavior forms an essential part of it. Verifying the behavior is a crucial yet often troublesome part of testing. Hence, it is of great concern to find means to facilitate the testing of dynamic behaviors. This paper studies one approach to such behavioral monitoring. The details of the approach are the following. We have used the concept of behavioral profiles to specify the desired program behavior using UML. Provided with a behavioral profile created with a CASE tool, we are able to automatically generate AspectJ aspects that are woven into Java program code, thus adding a monitoring concern to the system. This results in the opportunity to monitor architecturally significant behaviors defined with architectural profiles at code level. Towards the end of the paper, we study the applicability of the approach in industrial use.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1146238.1146259},
 doi = {http://doi.acm.org/10.1145/1146238.1146259},
 acmid = {1146259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aspectJ, behavioral profile, monitoring, software architecture},
} 

@inproceedings{Zhang:2006:CAP:1146238.1146260,
 author = {Zhang, Xiaolan and Koved, Larry and Pistoia, Marco and Weber, Sam and Jaeger, Trent and Marceau, Guillaume and Zeng, Liangzhao},
 title = {The case for analysis preserving language transformation},
 abstract = {Static analysis has gained much attention over the past few years in applications such as bug finding and program verification. As software becomes more complex and componentized, it is common for software systems and applications to be implemented in multiple languages. There is thus a strong need for developing analysis tools for multi-language software. We introduce a technique called Analysis Preserving Language Transformation (aplt) that enables the analysis of multi-language software, and also allows analysis tools for one language to be applied to programs written in another. aplt preserves data and control flow information needed to perform static analyses, but allows the translation to deviate from the original program's semantics in ways that are not pertinent to the particular analysis. We discuss major technical difficulties in building such a translator, using a C-to-Java translator as an example. We demonstrate the feasibility and effectiveness of aplt using two usage cases: analysis of the Java runtime native methods and reuse of Java analysis tools for C. Our preliminary results show that a control- and data-flow equivalent model for native methods can eliminate unsoundness and produce reliable results, and that aplt enables seamless reuse of analysis tools for checking high-level program properties.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146260},
 doi = {http://doi.acm.org/10.1145/1146238.1146260},
 acmid = {1146260},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, Java, language transformation, language translation, security, static analysis, verification},
} 

@inproceedings{Tkachuk:2006:AAE:1146238.1146262,
 author = {Tkachuk, Oksana and Rajan, Sreeranga P.},
 title = {Application of automated environment generation to commercial software},
 abstract = {Model checking can be an effective technique for detecting concurrency-related errors in software systems. However, due to scalability issues, to handle industrial-strength software, model checking needs to be combined with powerful reduction techniques. In this work, we applied modular model checking to detect errors in Interstage Business Process Management (I-BPM) software, a Java client-server application spanning more than 500,000 lines of code. To model check a separate module, one needs to represent its context of execution, i.e., its environment</i>. Environment generation is a significant challenge, since the environment is to be general enough to uncover the module's errors, yet restrictive enough to allow tractable model checking.In this paper, we present an experimental study that demonstrates the effectiveness of environment generation as a reduction technique in general and the effectiveness of automated environment generation in particular. Since model checking of the original application was intractable, we compared performance of automatically generated environments to environments written previously by hand. Automatic environments were obtained using Bandera Environment Generator (BEG). We present results of modular verification for both manual and automatic environments. The results show that automatically generated environments produce systems with a smaller state space, yet, for faulty modules, uncover the errors and, for error-free modules, produce coverage similar to manual environments.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {203--214},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146262},
 doi = {http://doi.acm.org/10.1145/1146238.1146262},
 acmid = {1146262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {environment generation, modular model checking},
} 

@inproceedings{Sevcikova:2006:ATS:1146238.1146263,
 author = {\v{S}ev\v{c}\'{\i}kov\'{a}, Hana and Borning, Alan and Socha, David and Bleek, Wolf-Gideon},
 title = {Automated testing of stochastic systems: a statistically grounded approach},
 abstract = {Automated tests can play a key role in ensuring system quality in software development. However, significant problems arise in automating tests of stochastic algorithms. Normally, developers write tests that simply check whether the actual result is equal to the expected result (perhaps within some tolerance). But for stochastic algorithms, restricting ourselves in this way severely limits the kinds of tests we can write: either to trivial tests, or to fragile and hard-tounderstand tests that rely on a particular seed for a random number generator. A richer and more powerful set of tests is possible if we accommodate tests of statistical properties of the results of running an algorithm many times. The work described in this paper has been done in the context of a real-world application, a large-scale simulation of urban development designed to inform major decisions about land use and transportation. We describe our earlier experience with using automated testing for this system, in which we took a conventional approach, and the resulting difficulties. We then present a statistically based approach for testing stochastic algorithms based on hypothesis testing. Three different ways of constructing such tests are given, which cover the most commonly used distributions. We evaluate these tests in terms of frequency of failing when they should and when they should not, and conclude with guidelines and practical suggestions for implementing such unit tests for other stochastic applications.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {215--224},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1146238.1146263},
 doi = {http://doi.acm.org/10.1145/1146238.1146263},
 acmid = {1146263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hypothesis testing, software engineering, software testing, stochastic algorithms, unit tests},
} 

@inproceedings{Lencevicius:2006:PAM:1146238.1146264,
 author = {Lencevicius, Raimondas and Metz, Edu},
 title = {Performance assertions for mobile devices},
 abstract = {Assertions have long been used to validate the functionality of software systems. Researchers and practitioners have extended them for validation of non-functional requirements, such as performance. This paper presents the implementation and application of the performance assertions in mobile device software. When applying performance assertions for such systems, we have discovered and resolved a number of issues in assertion specification, matching, and evaluation that were unresolved in previous research. The paper describes a simple, but effective framework geared towards mobile devices that allows specification and validation of real world performance requirements.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {225--232},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1146238.1146264},
 doi = {http://doi.acm.org/10.1145/1146238.1146264},
 acmid = {1146264},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertions, mobile devices, performance},
} 

@inproceedings{Demsky:2006:IED:1146238.1146266,
 author = {Demsky, Brian and Ernst, Michael D. and Guo, Philip J. and McCamant, Stephen and Perkins, Jeff H. and Rinard, Martin},
 title = {Inference and enforcement of data structure consistency specifications},
 abstract = {Corrupt data structures are an important cause of unacceptable program execution. Data structure repair (which eliminates inconsistencies by updating corrupt data structures to conform to consistency constraints) promises to enable many programs to continue to execute acceptably in the face of otherwise fatal data structure corruption errors. A key issue is obtaining an accurate and comprehensive data structure consistency specification. We present a new technique for obtaining data structure consistency specifications for data structure repair. Instead of requiring the developer to manually generate such specifications, our approach automatically generates candidate data structure consistency properties using the Daikon invariant detection tool. The developer then reviews these properties, potentially rejecting or generalizing overly specific properties to obtain a specification suitable for automatic enforcement via data structure repair. We have implemented this approach and applied it to three sizable benchmark programs: CTAS (an air-traffic control system), BIND (a widely-used Internet name server) and Freeciv (an interactive game). Our results indicate that (1) automatic constraint generation produces constraints that enable programs to execute successfully through data structure consistency errors, (2) compared to manual specification, automatic generation can produce more comprehensive sets of constraints that cover a larger range of data structure consistency properties, and (3) reviewing the properties is relatively straightforward and requires substantially less programmer effort than manual generation, primarily because it reduces the need to examine the program text to understand its operation and extract the relevant consistency constraints. Moreover, when evaluated by a hostile third party "Red Team" contracted to evaluate the effectiveness of the technique, our data structure inference and enforcement tools successfully prevented several otherwise fatal attacks.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {233--244},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1146238.1146266},
 doi = {http://doi.acm.org/10.1145/1146238.1146266},
 acmid = {1146266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data structure repair, dynamic invariant detection},
} 

@inproceedings{Csallner:2006:DHA:1146238.1146267,
 author = {Csallner, Christoph and Smaragdakis, Yannis},
 title = {DSD-Crasher: a hybrid analysis tool for bug finding},
 abstract = {DSD-Crasher is a bug finding tool that follows a three-step approach to program analysis: <ol><li><b>D.</b> Capture the program's intended execution behavior with dynamic invariant detection. The derived invariants exclude many unwanted values from the program's input domain.</li> <li><b>S.</b> Statically analyze the program within the restricted input domain to explore many paths.</li> <li><b>D.</b> Automatically generate test cases that focus on verifying the results of the static analysis. Thereby confirmed results are never false positives, as opposed to the high false positive rate inherent in conservative static analysis.</li></ol>.This three-step approach yields benefits compared to past two-step combinations in the literature. In our evaluation with third-party applications, we demonstrate higher precision over tools that lack a dynamic step and higher efficiency over tools that lack a static step.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {245--254},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1146238.1146267},
 doi = {http://doi.acm.org/10.1145/1146238.1146267},
 acmid = {1146267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic testing, bug finding, dynamic analysis, dynamic invariant detection, extended static checking, false positives, static analysis, test case generation, usability},
} 

@inproceedings{Guo:2006:DIA:1146238.1146268,
 author = {Guo, Philip J. and Perkins, Jeff H. and McCamant, Stephen and Ernst, Michael D.},
 title = {Dynamic inference of abstract types},
 abstract = {An abstract type groups variables that are used for related purposes in a program. We describe a dynamic unification-based analysis for inferring abstract types. Initially, each run-time value gets a unique abstract type. A run-time interaction among values indicates that they have the same abstract type, so their abstract types are unified. Also at run time, abstract types for variables are accumulated from abstract types for values. The notion of interaction may be customized, permitting the analysis to compute finer or coarser abstract types; these different notions of abstract type are useful for different tasks. We have implemented the analysis for compiled x86 binaries and for Java bytecodes. Our experiments indicate that the inferred abstract types are useful for program comprehension, improve both the results and the run time of a follow-on program analysis, and are more precise than the output of a comparable static analysis, without suffering from overfitting.},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 series = {ISSTA '06},
 year = {2006},
 isbn = {1-59593-263-1},
 location = {Portland, Maine, USA},
 pages = {255--265},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1146238.1146268},
 doi = {http://doi.acm.org/10.1145/1146238.1146268},
 acmid = {1146268},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C, C++, Java, abstract types, dynamic analysis, interaction, mixed-level analysis, type inference, units, values and variables},
} 

@inproceedings{Rountev:2004:SDA:1007512.1007514,
 author = {Rountev, Atanas and Kagan, Scott and Gibas, Michael},
 title = {Static and dynamic analysis of call chains in java},
 abstract = {This work presents a parameterized framework for static and dynamic analysis of call chains in Java components. Such analyses have a wide range of uses in tools for software understanding and testing. We also describe a test coverage tool built with these analyses and the use of the tool on a real-world test suite. Our experiments evaluate the exact precision of several instances of the framework and provide a novel approach for estimating the limits of class analysis technology for computing precise call chains.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007514},
 doi = {http://doi.acm.org/10.1145/1007512.1007514},
 acmid = {1007514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {call chains, call graph, dynamic analysis, static analysis},
} 

@article{Rountev:2004:SDA:1013886.1007514,
 author = {Rountev, Atanas and Kagan, Scott and Gibas, Michael},
 title = {Static and dynamic analysis of call chains in java},
 abstract = {This work presents a parameterized framework for static and dynamic analysis of call chains in Java components. Such analyses have a wide range of uses in tools for software understanding and testing. We also describe a test coverage tool built with these analyses and the use of the tool on a real-world test suite. Our experiments evaluate the exact precision of several instances of the framework and provide a novel approach for estimating the limits of class analysis technology for computing precise call chains.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007514},
 doi = {http://doi.acm.org/10.1145/1013886.1007514},
 acmid = {1007514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {call chains, call graph, dynamic analysis, static analysis},
} 

@inproceedings{Dor:2004:SVV:1007512.1007515,
 author = {Dor, Nurit and Adams, Stephen and Das, Manuvir and Yang, Zhe},
 title = {Software validation via scalable path-sensitive value flow analysis},
 abstract = {In this paper, we present a new algorithm for tracking the flow of values through a program. Our algorithm represents a substantial improvement over the state of the art. Previously described value flow analyses that are control-flow sensitive do not scale well, nor do they eliminate value flow information from infeasible execution paths (i.e.</i>, they are path-insensitive). Our algorithm scales to large programs, and it is path-sensitive.The efficiency of our algorithm arises from three insights: The value flow problem can be "bit-vectorized" by tracking the flow of one value at a time; dataflow facts from different execution paths with the same value flow information can be merged; and information about complex aliasing that affects value flow can be plugged in from a different analysis.We have incorporated our analysis in ESP, a software validation tool. We have used ESP to validate the Windows operating system kernel (a million lines of code) against an important security property. This experience suggests that our algorithm scales to large programs, and is accurate enough to trace the flow of values in real code.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {12--22},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007515},
 doi = {http://doi.acm.org/10.1145/1007512.1007515},
 acmid = {1007515},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, path-sensitive analysis, value flow},
} 

@article{Dor:2004:SVV:1013886.1007515,
 author = {Dor, Nurit and Adams, Stephen and Das, Manuvir and Yang, Zhe},
 title = {Software validation via scalable path-sensitive value flow analysis},
 abstract = {In this paper, we present a new algorithm for tracking the flow of values through a program. Our algorithm represents a substantial improvement over the state of the art. Previously described value flow analyses that are control-flow sensitive do not scale well, nor do they eliminate value flow information from infeasible execution paths (i.e.</i>, they are path-insensitive). Our algorithm scales to large programs, and it is path-sensitive.The efficiency of our algorithm arises from three insights: The value flow problem can be "bit-vectorized" by tracking the flow of one value at a time; dataflow facts from different execution paths with the same value flow information can be merged; and information about complex aliasing that affects value flow can be plugged in from a different analysis.We have incorporated our analysis in ESP, a software validation tool. We have used ESP to validate the Windows operating system kernel (a million lines of code) against an important security property. This experience suggests that our algorithm scales to large programs, and is accurate enough to trace the flow of values in real code.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {12--22},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007515},
 doi = {http://doi.acm.org/10.1145/1013886.1007515},
 acmid = {1007515},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, path-sensitive analysis, value flow},
} 

@inproceedings{Fu:2004:TJW:1007512.1007516,
 author = {Fu, Chen and Ryder, Barbara G. and Milanova, Ana and Wonnacott, David},
 title = {Testing of java web services for robustness},
 abstract = {This paper presents a new compile-time analysis that enables a testing methodology for white-box coverage testing of error recovery code (i.e., exception handlers) in Java web services using compiler-directed fault injection. The analysis allows compiler-generated instrumentation to guide the fault injection and to record the recovery code exercised. (An injected fault is experienced as a Java exception.) The analysis (i) identifies the exception-flow 'def-uses'</i> to be tested in this manner, (ii) determines the kind of fault to be requested at a program point, and (iii) finds appropriate locations for code instrumentation. The analysis incorporates refinements that establish sufficient context sensitivity to ensure relatively precise def-use links and to eliminate some spurious def-uses due to demonstrably infeasible control flow. A runtime test harness calculates test coverage of these links using an exception def-catch</i> metric. Experiments with the methodology demonstrate the utility of the increased precision in obtaining good test coverage on a set of moderately-sized Java web services benchmarks.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1007512.1007516},
 doi = {http://doi.acm.org/10.1145/1007512.1007516},
 acmid = {1007516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {def-use testing, exceptions, java, test coverage metrics},
} 

@article{Fu:2004:TJW:1013886.1007516,
 author = {Fu, Chen and Ryder, Barbara G. and Milanova, Ana and Wonnacott, David},
 title = {Testing of java web services for robustness},
 abstract = {This paper presents a new compile-time analysis that enables a testing methodology for white-box coverage testing of error recovery code (i.e., exception handlers) in Java web services using compiler-directed fault injection. The analysis allows compiler-generated instrumentation to guide the fault injection and to record the recovery code exercised. (An injected fault is experienced as a Java exception.) The analysis (i) identifies the exception-flow 'def-uses'</i> to be tested in this manner, (ii) determines the kind of fault to be requested at a program point, and (iii) finds appropriate locations for code instrumentation. The analysis incorporates refinements that establish sufficient context sensitivity to ensure relatively precise def-use links and to eliminate some spurious def-uses due to demonstrably infeasible control flow. A runtime test harness calculates test coverage of these links using an exception def-catch</i> metric. Experiments with the methodology demonstrate the utility of the increased precision in obtaining good test coverage on a set of moderately-sized Java web services benchmarks.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1013886.1007516},
 doi = {http://doi.acm.org/10.1145/1013886.1007516},
 acmid = {1007516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {def-use testing, exceptions, java, test coverage metrics},
} 

@inproceedings{Christodorescu:2004:TMD:1007512.1007518,
 author = {Christodorescu, Mihai and Jha, Somesh},
 title = {Testing malware detectors},
 abstract = {In today's interconnected world, malware, such as worms and viruses, can cause havoc. A malware detector (commonly known as virus scanner) attempts to identify malware. In spite of the importance of malware detectors, there is a dearth of testing techniques for evaluating them. We present a technique based on program obfuscation for generating tests for malware detectors. Our technique is geared towards evaluating the resilience of malware detectors to various obfuscation transformations commonly used by hackers to disguise malware. We also demonstrate that a hacker can leverage a malware detector's weakness in handling obfuscation transformations and can extract the signature used by a detector for a specific malware. We evaluate three widely-used commercial virus scanners using our techniques and discover that the resilience of these scanners to various obfuscations is very poor.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {34--44},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007518},
 doi = {http://doi.acm.org/10.1145/1007512.1007518},
 acmid = {1007518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive testing, anti-virus, malware, obfuscation},
} 

@article{Christodorescu:2004:TMD:1013886.1007518,
 author = {Christodorescu, Mihai and Jha, Somesh},
 title = {Testing malware detectors},
 abstract = {In today's interconnected world, malware, such as worms and viruses, can cause havoc. A malware detector (commonly known as virus scanner) attempts to identify malware. In spite of the importance of malware detectors, there is a dearth of testing techniques for evaluating them. We present a technique based on program obfuscation for generating tests for malware detectors. Our technique is geared towards evaluating the resilience of malware detectors to various obfuscation transformations commonly used by hackers to disguise malware. We also demonstrate that a hacker can leverage a malware detector's weakness in handling obfuscation transformations and can extract the signature used by a detector for a specific malware. We evaluate three widely-used commercial virus scanners using our techniques and discover that the resilience of these scanners to various obfuscations is very poor.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {34--44},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007518},
 doi = {http://doi.acm.org/10.1145/1013886.1007518},
 acmid = {1007518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive testing, anti-virus, malware, obfuscation},
} 

@inproceedings{Yilmaz:2004:CAE:1007512.1007519,
 author = {Yilmaz, Cemal and Cohen, Myra B. and Porter, Adam},
 title = {Covering arrays for efficient fault characterization in complex configuration spaces},
 abstract = {Testing systems with large configurations spaces that change often is a challenging problem. The cost and complexity of QA explodes because often there isn't just one system, but a multitude of related systems. Bugs may appear in certain configurations, but not in others.The Skoll system and process has been developed to test these types of systems through distributed, continuous quality assurance, leveraging user resources around-the-world, around-the-clock. It has been shown to be effective in automatically characterizing configurations in which failures manifest. The derived information helps developers quickly narrow down the cause of failures which then improves turn around time for fixes. However, this method does not scale well. It requires one to exhaustively test each configuration in the configuration space.In this paper we examine an alternative approach. The idea is to systematically sample the configuration space, test only the selected configurations, and conduct fault characterization on the resulting data. The sampling approach we use is based on calculating a mathematical object called a covering array. We empirically assess the effect of using covering array derived test schedules on the resulting fault characterizations and provide guidelines to practitioners for their use.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {45--54},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007519},
 doi = {http://doi.acm.org/10.1145/1007512.1007519},
 acmid = {1007519},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {covering arrays, distributed continuous quality assurance, fault characterization, software testing},
} 

@article{Yilmaz:2004:CAE:1013886.1007519,
 author = {Yilmaz, Cemal and Cohen, Myra B. and Porter, Adam},
 title = {Covering arrays for efficient fault characterization in complex configuration spaces},
 abstract = {Testing systems with large configurations spaces that change often is a challenging problem. The cost and complexity of QA explodes because often there isn't just one system, but a multitude of related systems. Bugs may appear in certain configurations, but not in others.The Skoll system and process has been developed to test these types of systems through distributed, continuous quality assurance, leveraging user resources around-the-world, around-the-clock. It has been shown to be effective in automatically characterizing configurations in which failures manifest. The derived information helps developers quickly narrow down the cause of failures which then improves turn around time for fixes. However, this method does not scale well. It requires one to exhaustively test each configuration in the configuration space.In this paper we examine an alternative approach. The idea is to systematically sample the configuration space, test only the selected configurations, and conduct fault characterization on the resulting data. The sampling approach we use is based on calculating a mathematical object called a covering array. We empirically assess the effect of using covering array derived test schedules on the resulting fault characterizations and provide guidelines to practitioners for their use.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {45--54},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007519},
 doi = {http://doi.acm.org/10.1145/1013886.1007519},
 acmid = {1007519},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {covering arrays, distributed continuous quality assurance, fault characterization, software testing},
} 

@inproceedings{Nachmanson:2004:OST:1007512.1007520,
 author = {Nachmanson, Lev and Veanes, Margus and Schulte, Wolfram and Tillmann, Nikolai and Grieskamp, Wolfgang},
 title = {Optimal strategies for testing nondeterministic systems},
 abstract = {This paper deals with testing of nondeterministic software systems. We assume that a model of the nondeterministic system is given by a directed graph with two kind of vertices: states and choice points. Choice points represent the nondeterministic behaviour of the implementation under test (IUT). Edges represent transitions. They have costs and probabilities. Test case generation in this setting amounts to generation of a game strategy. The two players are the testing tool (TT) and the IUT. The game explores the graph. The TT leads the IUT by selecting an edge at the state vertices. At the choice points the control goes to the IUT. A game strategy decides which edge should be taken by the TT in each state. This paper presents three novel algorithms 1) to determine an optimal strategy for the bounded reachability game, where optimality means maximizing the probability to reach any of the given final states from a given start state while at the same time minimizing the costs of traversal; 2) to determine a winning strategy for the bounded reachability game, which guarantees that given final vertices are reached, regardless how the IUT reacts; 3) to determine a fast converging edge covering strategy, which guarantees that the probability to cover all edges quickly converges to 1 if TT follows the strategy.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {55--64},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007520},
 doi = {http://doi.acm.org/10.1145/1007512.1007520},
 acmid = {1007520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract state machines, optimal game strategies, probabilistic nondeterministic finite state machines},
} 

@article{Nachmanson:2004:OST:1013886.1007520,
 author = {Nachmanson, Lev and Veanes, Margus and Schulte, Wolfram and Tillmann, Nikolai and Grieskamp, Wolfgang},
 title = {Optimal strategies for testing nondeterministic systems},
 abstract = {This paper deals with testing of nondeterministic software systems. We assume that a model of the nondeterministic system is given by a directed graph with two kind of vertices: states and choice points. Choice points represent the nondeterministic behaviour of the implementation under test (IUT). Edges represent transitions. They have costs and probabilities. Test case generation in this setting amounts to generation of a game strategy. The two players are the testing tool (TT) and the IUT. The game explores the graph. The TT leads the IUT by selecting an edge at the state vertices. At the choice points the control goes to the IUT. A game strategy decides which edge should be taken by the TT in each state. This paper presents three novel algorithms 1) to determine an optimal strategy for the bounded reachability game, where optimality means maximizing the probability to reach any of the given final states from a given start state while at the same time minimizing the costs of traversal; 2) to determine a winning strategy for the bounded reachability game, which guarantees that given final vertices are reached, regardless how the IUT reacts; 3) to determine a fast converging edge covering strategy, which guarantees that the probability to cover all edges quickly converges to 1 if TT follows the strategy.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {55--64},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007520},
 doi = {http://doi.acm.org/10.1145/1013886.1007520},
 acmid = {1007520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract state machines, optimal game strategies, probabilistic nondeterministic finite state machines},
} 

@article{Elbaum:2004:ESP:1013886.1007522,
 author = {Elbaum, Sebastian and Hardojo, Madeline},
 title = {An empirical study of profiling strategies for released software and their impact on testing activities},
 abstract = {An understanding of how software is employed in the field can yield many opportunities for quality improvements. Profiling released software can provide such an understanding. However, profiling released software is diffcult due to the potentially large number of deployed sites that must be profiled, the extreme transparency expectations, and the remote data collection and deployment management process. Researchers have recently proposed various approaches to tap into the opportunities and overcome those challenges. Initial studies have illustrated the application of these approaches and have shown their feasibility. Still, the promising proposed approaches, and the tradeoffs between overhead, accuracy, and potential benefits for the testing activity have been barely quantified. This paper aims to over-come those limitations. Our analysis of 1200 user sessions on a 155 KLOC system substantiates the ability of field data to support test suite improvements, quantifies different approaches previously introduced in isolation, and assesses the efficiency of profiling techniques for released software and the effectiveness of their associated testing efforts.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {65--75},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007522},
 doi = {http://doi.acm.org/10.1145/1013886.1007522},
 acmid = {1007522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical studies, instrumentation, profiling, software deployment, testing},
} 

@inproceedings{Elbaum:2004:ESP:1007512.1007522,
 author = {Elbaum, Sebastian and Hardojo, Madeline},
 title = {An empirical study of profiling strategies for released software and their impact on testing activities},
 abstract = {An understanding of how software is employed in the field can yield many opportunities for quality improvements. Profiling released software can provide such an understanding. However, profiling released software is diffcult due to the potentially large number of deployed sites that must be profiled, the extreme transparency expectations, and the remote data collection and deployment management process. Researchers have recently proposed various approaches to tap into the opportunities and overcome those challenges. Initial studies have illustrated the application of these approaches and have shown their feasibility. Still, the promising proposed approaches, and the tradeoffs between overhead, accuracy, and potential benefits for the testing activity have been barely quantified. This paper aims to over-come those limitations. Our analysis of 1200 user sessions on a 155 KLOC system substantiates the ability of field data to support test suite improvements, quantifies different approaches previously introduced in isolation, and assesses the efficiency of profiling techniques for released software and the effectiveness of their associated testing efforts.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {65--75},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007522},
 doi = {http://doi.acm.org/10.1145/1007512.1007522},
 acmid = {1007522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical studies, instrumentation, profiling, software deployment, testing},
} 

@article{Saff:2004:EEC:1013886.1007523,
 author = {Saff, David and Ernst, Michael D.},
 title = {An experimental evaluation of continuous testing during development},
 abstract = {Continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as source code is edited. It is intended to reduce the time and energy required to keep code well-tested and prevent regression errors from persisting uncaught for long periods of time. This paper reports on a controlled human experiment to evaluate whether students using continuous testing are more successful in completing programming assignments. We also summarize users' subjective impressions and discuss why the results may generalize.The experiment indicates that the tool has a statistically significant effect on success in completing a programming task, but no such effect on time worked. Participants using continuous testing were three times more likely to complete the task before the deadline than those without. Participants using continuous compilation were twice as likely to complete the task, providing empirical support to a common feature in modern development environments. Most participants found continuous testing to be useful and believed that it helped them write better code faster, and 90\% would recommend the tool to others. The participants did not find the tool distracting, and intuitively developed ways of incorporating the feedback into their workflow.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {76--85},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007523},
 doi = {http://doi.acm.org/10.1145/1013886.1007523},
 acmid = {1007523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous compilation, continuous testing, test-first development, unit testing},
} 

@inproceedings{Saff:2004:EEC:1007512.1007523,
 author = {Saff, David and Ernst, Michael D.},
 title = {An experimental evaluation of continuous testing during development},
 abstract = {Continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as source code is edited. It is intended to reduce the time and energy required to keep code well-tested and prevent regression errors from persisting uncaught for long periods of time. This paper reports on a controlled human experiment to evaluate whether students using continuous testing are more successful in completing programming assignments. We also summarize users' subjective impressions and discuss why the results may generalize.The experiment indicates that the tool has a statistically significant effect on success in completing a programming task, but no such effect on time worked. Participants using continuous testing were three times more likely to complete the task before the deadline than those without. Participants using continuous compilation were twice as likely to complete the task, providing empirical support to a common feature in modern development environments. Most participants found continuous testing to be useful and believed that it helped them write better code faster, and 90\% would recommend the tool to others. The participants did not find the tool distracting, and intuitively developed ways of incorporating the feedback into their workflow.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {76--85},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007523},
 doi = {http://doi.acm.org/10.1145/1007512.1007523},
 acmid = {1007523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous compilation, continuous testing, test-first development, unit testing},
} 

@article{Ostrand:2004:BUG:1013886.1007524,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
 title = {Where the bugs are},
 abstract = {The ability to predict which files in a large software system are most likely to contain the largest numbers of faults in the next release can be a very valuable asset. To accomplish this, a negative binomial regression model using information from previous releases has been developed and used to predict the numbers of faults for a large industrial inventory system. The files of each release were sorted in descending order based on the predicted number of faults and then the first 20\% of the files were selected. This was done for each of fifteen consecutive releases, representing more than four years of field usage. The predictions were extremely accurate, correctly selecting files that contained between 71\% and 92\% of the faults, with the overall average being 83\%. In addition, the same model was used on data for the same system's releases, but with all fault data prior to integration testing removed. The prediction was again very accurate, ranging from 71\% to 93\%, with the average being 84\%. Predictions were made for a second system, and again the first 20\% of files accounted for 83\% of the identified faults. Finally, a highly simplified predictor was considered which correctly predicted 73\% and 74\% of the faults for the two systems.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007524},
 doi = {http://doi.acm.org/10.1145/1013886.1007524},
 acmid = {1007524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
} 

@inproceedings{Ostrand:2004:BUG:1007512.1007524,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
 title = {Where the bugs are},
 abstract = {The ability to predict which files in a large software system are most likely to contain the largest numbers of faults in the next release can be a very valuable asset. To accomplish this, a negative binomial regression model using information from previous releases has been developed and used to predict the numbers of faults for a large industrial inventory system. The files of each release were sorted in descending order based on the predicted number of faults and then the first 20\% of the files were selected. This was done for each of fifteen consecutive releases, representing more than four years of field usage. The predictions were extremely accurate, correctly selecting files that contained between 71\% and 92\% of the faults, with the overall average being 83\%. In addition, the same model was used on data for the same system's releases, but with all fault data prior to integration testing removed. The prediction was again very accurate, ranging from 71\% to 93\%, with the average being 84\%. Predictions were made for a second system, and again the first 20\% of files accounted for 83\% of the identified faults. Finally, a highly simplified predictor was considered which correctly predicted 73\% and 74\% of the faults for the two systems.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007524},
 doi = {http://doi.acm.org/10.1145/1007512.1007524},
 acmid = {1007524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
} 

@article{Visser:2004:TIG:1013886.1007526,
 author = {Visser, Willem and P\v{a}s\v{a}reanu, Corina S. and Khurshid, Sarfraz},
 title = {Test input generation with java PathFinder},
 abstract = {We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java <b>TreeMap</b> library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {97--107},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007526},
 doi = {http://doi.acm.org/10.1145/1013886.1007526},
 acmid = {1007526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage, model checking, red-black trees, symbolic execution, testing object-oriented programs},
} 

@inproceedings{Visser:2004:TIG:1007512.1007526,
 author = {Visser, Willem and P\v{a}s\v{a}reanu, Corina S. and Khurshid, Sarfraz},
 title = {Test input generation with java PathFinder},
 abstract = {We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java <b>TreeMap</b> library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {97--107},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007526},
 doi = {http://doi.acm.org/10.1145/1007512.1007526},
 acmid = {1007526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage, model checking, red-black trees, symbolic execution, testing object-oriented programs},
} 

@article{Baresel:2004:ETP:1013886.1007527,
 author = {Baresel, Andr\'{e} and Binkley, David and Harman, Mark and Korel, Bogdan},
 title = {Evolutionary testing in the presence of loop-assigned flags: a testability transformation approach},
 abstract = {Evolutionary testing is an effective technique for automatically generating good quality test data. However, for structural testing, the technique degenerates to random testing in the presence of flag variables, which also present problems for other automated test data generation techniques. Previous work on the flag problem does not address flags assigned in loops.This paper introduces a testability transformation that transforms programs with loop--assigned flags so that existing genetic approaches can be successfully applied. It then presents empirical data demonstrating the effectiveness of the transformation. Untransformed, the genetic algorithm flounders and is unable to find a solution. Two transformations are considered. The first allows the search to find a solution. The second reduces the time taken by an order of magnitude and, more importantly, reduces the slope of the cost increase; thus, greatly increasing the complexity of the problem to which the genetic algorithm can be applied. The paper also presents a second empirical study showing that loop--assigned flags are prevalent in real world code. They account for just under 11\% of all flags.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {108--118},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007527},
 doi = {http://doi.acm.org/10.1145/1013886.1007527},
 acmid = {1007527},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical evaluation, evolutionary testing, flags, testability transformation},
} 

@inproceedings{Baresel:2004:ETP:1007512.1007527,
 author = {Baresel, Andr\'{e} and Binkley, David and Harman, Mark and Korel, Bogdan},
 title = {Evolutionary testing in the presence of loop-assigned flags: a testability transformation approach},
 abstract = {Evolutionary testing is an effective technique for automatically generating good quality test data. However, for structural testing, the technique degenerates to random testing in the presence of flag variables, which also present problems for other automated test data generation techniques. Previous work on the flag problem does not address flags assigned in loops.This paper introduces a testability transformation that transforms programs with loop--assigned flags so that existing genetic approaches can be successfully applied. It then presents empirical data demonstrating the effectiveness of the transformation. Untransformed, the genetic algorithm flounders and is unable to find a solution. Two transformations are considered. The first allows the search to find a solution. The second reduces the time taken by an order of magnitude and, more importantly, reduces the slope of the cost increase; thus, greatly increasing the complexity of the problem to which the genetic algorithm can be applied. The paper also presents a second empirical study showing that loop--assigned flags are prevalent in real world code. They account for just under 11\% of all flags.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {108--118},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007527},
 doi = {http://doi.acm.org/10.1145/1007512.1007527},
 acmid = {1007527},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical evaluation, evolutionary testing, flags, testability transformation},
} 

@article{Tonella:2004:ETC:1013886.1007528,
 author = {Tonella, Paolo},
 title = {Evolutionary testing of classes},
 abstract = {Object oriented programming promotes reuse of classes in multiple contexts. Thus, a class is designed and implemented with several usage scenarios in mind, some of which possibly open and generic. Correspondingly, the unit testing of classes cannot make too strict assumptions on the actual method invocation sequences, since these vary from application to application.In this paper, a genetic algorithm is exploited to automatically produce test cases for the unit testing of classes in a generic usage scenario. Test cases are described by chromosomes, which include information on which objects to create, which methods to invoke and which values to use as inputs. The proposed algorithm mutates them with the aim of maximizing a given coverage measure. The implementation of the algorithm and its application to classes from the Java standard library are described.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {119--128},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007528},
 doi = {http://doi.acm.org/10.1145/1013886.1007528},
 acmid = {1007528},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test case generation, genetic algorithms, object-oriented testing},
} 

@inproceedings{Tonella:2004:ETC:1007512.1007528,
 author = {Tonella, Paolo},
 title = {Evolutionary testing of classes},
 abstract = {Object oriented programming promotes reuse of classes in multiple contexts. Thus, a class is designed and implemented with several usage scenarios in mind, some of which possibly open and generic. Correspondingly, the unit testing of classes cannot make too strict assumptions on the actual method invocation sequences, since these vary from application to application.In this paper, a genetic algorithm is exploited to automatically produce test cases for the unit testing of classes in a generic usage scenario. Test cases are described by chromosomes, which include information on which objects to create, which methods to invoke and which values to use as inputs. The proposed algorithm mutates them with the aim of maximizing a given coverage measure. The implementation of the algorithm and its application to classes from the Java standard library are described.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {119--128},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007528},
 doi = {http://doi.acm.org/10.1145/1007512.1007528},
 acmid = {1007528},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test case generation, genetic algorithms, object-oriented testing},
} 

@article{Hartman:2004:ATM:1013886.1007529,
 author = {Hartman, A. and Nagin, K.},
 title = {The AGEDIS tools for model based testing},
 abstract = {We describe the tools and interfaces created by the AGEDIS project, a European Commission sponsored project for the creation of a methodology and tools for automated model driven test generation and execution for distributed systems. The project includes an integrated environment for modeling, test generation, test execution, and other test related activities. The tools support a model based testing methodology that features a large degree of automation and also includes a feedback loop integrating coverage and defect analysis tools with the test generator and execution framework. Prototypes of the tools have been tried in industrial settings providing important feedback for the creation of the next generation of tools in this area.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {129--132},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1013886.1007529},
 doi = {http://doi.acm.org/10.1145/1013886.1007529},
 acmid = {1007529},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {UML modeling, automated test generation, coverage analysis, defect analysis, test execution framework, validation},
} 

@inproceedings{Hartman:2004:ATM:1007512.1007529,
 author = {Hartman, A. and Nagin, K.},
 title = {The AGEDIS tools for model based testing},
 abstract = {We describe the tools and interfaces created by the AGEDIS project, a European Commission sponsored project for the creation of a methodology and tools for automated model driven test generation and execution for distributed systems. The project includes an integrated environment for modeling, test generation, test execution, and other test related activities. The tools support a model based testing methodology that features a large degree of automation and also includes a feedback loop integrating coverage and defect analysis tools with the test generator and execution framework. Prototypes of the tools have been tried in industrial settings providing important feedback for the creation of the next generation of tools in this area.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {129--132},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1007512.1007529},
 doi = {http://doi.acm.org/10.1145/1007512.1007529},
 acmid = {1007529},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {UML modeling, automated test generation, coverage analysis, defect analysis, test execution framework, validation},
} 

@inproceedings{Sullivan:2004:SAB:1007512.1007531,
 author = {Sullivan, Kevin and Yang, Jinlin and Coppit, David and Khurshid, Sarfraz and Jackson, Daniel},
 title = {Software assurance by bounded exhaustive testing},
 abstract = {The contribution of this paper is an experiment that shows the potential value of a combination of selective reverse engineering to formal specifications and bounded exhaustive testing to improve the assurance levels of complex software. A key problem is to scale up test input generation so that meaningful results can be obtained. We present an approach, using Alloy and TestEra for test input generation, which we evaluate by experimental application to the Galileo dynamic fault tree analysis tool.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007531},
 doi = {http://doi.acm.org/10.1145/1007512.1007531},
 acmid = {1007531},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TestEra, automated test case generation, bounded exhaustive testing, formal methods, reverse engineering, specification-based testing},
} 

@article{Sullivan:2004:SAB:1013886.1007531,
 author = {Sullivan, Kevin and Yang, Jinlin and Coppit, David and Khurshid, Sarfraz and Jackson, Daniel},
 title = {Software assurance by bounded exhaustive testing},
 abstract = {The contribution of this paper is an experiment that shows the potential value of a combination of selective reverse engineering to formal specifications and bounded exhaustive testing to improve the assurance levels of complex software. A key problem is to scale up test input generation so that meaningful results can be obtained. We present an approach, using Alloy and TestEra for test input generation, which we evaluate by experimental application to the Galileo dynamic fault tree analysis tool.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007531},
 doi = {http://doi.acm.org/10.1145/1013886.1007531},
 acmid = {1007531},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TestEra, automated test case generation, bounded exhaustive testing, formal methods, reverse engineering, specification-based testing},
} 

@article{Meinke:2004:ABT:1013886.1007532,
 author = {Meinke, Karl},
 title = {Automated black-box testing of functional correctness using function approximation},
 abstract = {We consider black-box testing of functional correctness as a special case of a satisfiability or constraint solving problem. We introduce a general method for solving this problem based on function approximation. We then describe some practical results obtained for an automated testing algorithm using approximation by piecewise polynomial functions.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007532},
 doi = {http://doi.acm.org/10.1145/1013886.1007532},
 acmid = {1007532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximation, black-box test, constraint solving, formal specification, functional test, satisfiability problem, test coverage},
} 

@inproceedings{Meinke:2004:ABT:1007512.1007532,
 author = {Meinke, Karl},
 title = {Automated black-box testing of functional correctness using function approximation},
 abstract = {We consider black-box testing of functional correctness as a special case of a satisfiability or constraint solving problem. We introduce a general method for solving this problem based on function approximation. We then describe some practical results obtained for an automated testing algorithm using approximation by piecewise polynomial functions.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007532},
 doi = {http://doi.acm.org/10.1145/1007512.1007532},
 acmid = {1007532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximation, black-box test, constraint solving, formal specification, functional test, satisfiability problem, test coverage},
} 

@inproceedings{Morasca:2004:ACT:1007512.1007533,
 author = {Morasca, Sandro and Serra-Capizzano, Stefano},
 title = {On the analytical comparison of testing techniques},
 abstract = {We introduce necessary and sufficient conditions for comparing the expected values of the number of failures caused by applications of software testing techniques. Our conditions are based only on the knowledge of a total or even a hierarchical order among the failure rates of the subdomains of a program's input domain. We also prove conditions for comparing the probability of causing at least one failure in three important special cases.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {154--164},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007533},
 doi = {http://doi.acm.org/10.1145/1007512.1007533},
 acmid = {1007533},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {majorization, random testing, software testing, subdomain-based testing},
} 

@article{Morasca:2004:ACT:1013886.1007533,
 author = {Morasca, Sandro and Serra-Capizzano, Stefano},
 title = {On the analytical comparison of testing techniques},
 abstract = {We introduce necessary and sufficient conditions for comparing the expected values of the number of failures caused by applications of software testing techniques. Our conditions are based only on the knowledge of a total or even a hierarchical order among the failure rates of the subdomains of a program's input domain. We also prove conditions for comparing the probability of causing at least one failure in three important special cases.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {154--164},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007533},
 doi = {http://doi.acm.org/10.1145/1013886.1007533},
 acmid = {1007533},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {majorization, random testing, software testing, subdomain-based testing},
} 

@inproceedings{Dennis:2004:ACA:1007512.1007535,
 author = {Dennis, Greg and Seater, Robert and Rayside, Derek and Jackson, Daniel},
 title = {Automating commutativity analysis at the design level},
 abstract = {Two operations commute if executing them serially in either order results in the same change of state. In a system in which commands may be issued simultaneously by different users, lack of commutativity can result in unpredictable behaviour, even if the commands are serialized, because one user's command may be preempted by another's, and thus executed in an unanticipated state. This paper describes an automated approach to analyzing commutativity. The operations are expressed as constraints in a declarative modelling language such as Alloy, and a constraint solver is used to find violating scenarios. A case study application to the beam scheduling component of a proton therapy machine (originally specified in OCL) revealed several violations of commutativity in which requests from medical technicians in treatment rooms could conflict with the actions of a beam operator in a master control room. Some of the issues involved in automating the analysis for OCL itself are also discussed.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {165--174},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007512.1007535},
 doi = {http://doi.acm.org/10.1145/1007512.1007535},
 acmid = {1007535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OCL, alloy, case study, commutativity, concurrency, critical systems, formal specification, lightweight formal methods, model checking, proton therapy, radiation therapy, testing},
} 

@article{Dennis:2004:ACA:1013886.1007535,
 author = {Dennis, Greg and Seater, Robert and Rayside, Derek and Jackson, Daniel},
 title = {Automating commutativity analysis at the design level},
 abstract = {Two operations commute if executing them serially in either order results in the same change of state. In a system in which commands may be issued simultaneously by different users, lack of commutativity can result in unpredictable behaviour, even if the commands are serialized, because one user's command may be preempted by another's, and thus executed in an unanticipated state. This paper describes an automated approach to analyzing commutativity. The operations are expressed as constraints in a declarative modelling language such as Alloy, and a constraint solver is used to find violating scenarios. A case study application to the beam scheduling component of a proton therapy machine (originally specified in OCL) revealed several violations of commutativity in which requests from medical technicians in treatment rooms could conflict with the actions of a beam operator in a master control room. Some of the issues involved in automating the analysis for OCL itself are also discussed.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {165--174},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013886.1007535},
 doi = {http://doi.acm.org/10.1145/1013886.1007535},
 acmid = {1007535},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OCL, alloy, case study, commutativity, concurrency, critical systems, formal specification, lightweight formal methods, model checking, proton therapy, radiation therapy, testing},
} 

@article{Munoz:2004:MVA:1013886.1007536,
 author = {Mu\~{n}oz, C\'{e}sar A. and Dowek, Gilles and Carre\~{n}o, V\'{\i}ctor},
 title = {Modeling and verification of an air traffic concept of operations},
 abstract = {A high level model of the concept of operations of NASA's Small Aircraft Transportation System for Higher Volume Operations (SATS-HVO) is presented. The model is a non-deterministic, asynchronous transition system. It provides a robust notion of safety that relies on the logic of the concept rather than on physical constraints such as aircraft performances. Several safety properties were established on this model. The modeling and verification effort resulted in the identification of 9 issues, including one major flaw, in the original concept. Ten recommendations were made to the SATS-HVO concept development working group. All the recommendations were accepted and incorporated into the current concept of operations. The model was written in PVS. The verification is performed using an explicit state exploration algorithm written and proven correct in PVS.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {175--182},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013886.1007536},
 doi = {http://doi.acm.org/10.1145/1013886.1007536},
 acmid = {1007536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {air traffic management systems, model checking, theorem proving},
} 

@inproceedings{Munoz:2004:MVA:1007512.1007536,
 author = {Mu\~{n}oz, C\'{e}sar A. and Dowek, Gilles and Carre\~{n}o, V\'{\i}ctor},
 title = {Modeling and verification of an air traffic concept of operations},
 abstract = {A high level model of the concept of operations of NASA's Small Aircraft Transportation System for Higher Volume Operations (SATS-HVO) is presented. The model is a non-deterministic, asynchronous transition system. It provides a robust notion of safety that relies on the logic of the concept rather than on physical constraints such as aircraft performances. Several safety properties were established on this model. The modeling and verification effort resulted in the identification of 9 issues, including one major flaw, in the original concept. Ten recommendations were made to the SATS-HVO concept development working group. All the recommendations were accepted and incorporated into the current concept of operations. The model was written in PVS. The verification is performed using an explicit state exploration algorithm written and proven correct in PVS.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {175--182},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007512.1007536},
 doi = {http://doi.acm.org/10.1145/1007512.1007536},
 acmid = {1007536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {air traffic management systems, model checking, theorem proving},
} 

@article{Ezick:2004:OCB:1013886.1007537,
 author = {Ezick, James},
 title = {An optimizing compiler for batches of temporal logic formulas},
 abstract = {Model checking based on validating temporal logic formulas has proven practical and effective for numerous software engineering applications. As systems based on this approach have become more mainstream, a need has arisen to deal effectively with large batches of formulas over a common model. Presently, most systems validate formulas one at a time, with little or no interaction between validation of separate formulas. This is the case despite the fact that, for a wide range of applications, a certain level of redundancy between domain-related formulas can be anticipated.This paper presents an optimizing compiler for batches of temporal logic formulas. A component of the Carnauba model checking system, this compiler addresses the need to handle batches of temporal logic formulas by leveraging the framework common to optimizing programming language compilers. Just as traditional optimizing compilers attempt to exploit redundancy and other solvable properties in a program to reduce the demand on a runtime system, this compiler exploits similar properties in groups of formulas to reduce the demand on a model checking engine. Optimizations are performed via a set of distinct, interchangeable optimization passes operating on a common intermediate representation. The intermediate representation captures the full modal mu-calculus, and the optimization techniques are applicable to any temporal logic subsumed by that logic. The compiler offers a unified framework for expressing some well understood single-formula optimizations as well as numerous inter-formula optimizations that capitalize on redundancy, logical implication, and, optionally, model-specific knowledge. It is capable of working either in place of, or as a preprocessor for, other optimization algorithms. The result is a system that, when applied to a potentially heterogeneous collection of formulas over a common problem domain, is able to measurably reduce the time and space requirements of the subsequent model checking engine.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1013886.1007537},
 doi = {http://doi.acm.org/10.1145/1013886.1007537},
 acmid = {1007537},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {model checking, optimizing compiler, temporal logic},
} 

@inproceedings{Ezick:2004:OCB:1007512.1007537,
 author = {Ezick, James},
 title = {An optimizing compiler for batches of temporal logic formulas},
 abstract = {Model checking based on validating temporal logic formulas has proven practical and effective for numerous software engineering applications. As systems based on this approach have become more mainstream, a need has arisen to deal effectively with large batches of formulas over a common model. Presently, most systems validate formulas one at a time, with little or no interaction between validation of separate formulas. This is the case despite the fact that, for a wide range of applications, a certain level of redundancy between domain-related formulas can be anticipated.This paper presents an optimizing compiler for batches of temporal logic formulas. A component of the Carnauba model checking system, this compiler addresses the need to handle batches of temporal logic formulas by leveraging the framework common to optimizing programming language compilers. Just as traditional optimizing compilers attempt to exploit redundancy and other solvable properties in a program to reduce the demand on a runtime system, this compiler exploits similar properties in groups of formulas to reduce the demand on a model checking engine. Optimizations are performed via a set of distinct, interchangeable optimization passes operating on a common intermediate representation. The intermediate representation captures the full modal mu-calculus, and the optimization techniques are applicable to any temporal logic subsumed by that logic. The compiler offers a unified framework for expressing some well understood single-formula optimizations as well as numerous inter-formula optimizations that capitalize on redundancy, logical implication, and, optionally, model-specific knowledge. It is capable of working either in place of, or as a preprocessor for, other optimization algorithms. The result is a system that, when applied to a potentially heterogeneous collection of formulas over a common problem domain, is able to measurably reduce the time and space requirements of the subsequent model checking engine.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1007512.1007537},
 doi = {http://doi.acm.org/10.1145/1007512.1007537},
 acmid = {1007537},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {model checking, optimizing compiler, temporal logic},
} 

@article{Bowring:2004:ALA:1013886.1007539,
 author = {Bowring, James F. and Rehg, James M. and Harrold, Mary Jean},
 title = {Active learning for automatic classification of software behavior},
 abstract = {A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning</i> approach. In contrast, we explore an active-learning</i> paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {195--205},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007539},
 doi = {http://doi.acm.org/10.1145/1013886.1007539},
 acmid = {1007539},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov models, machine learning, software behavior, software testing},
} 

@inproceedings{Bowring:2004:ALA:1007512.1007539,
 author = {Bowring, James F. and Rehg, James M. and Harrold, Mary Jean},
 title = {Active learning for automatic classification of software behavior},
 abstract = {A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning</i> approach. In contrast, we explore an active-learning</i> paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {195--205},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007539},
 doi = {http://doi.acm.org/10.1145/1007512.1007539},
 acmid = {1007539},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov models, machine learning, software behavior, software testing},
} 

@inproceedings{Lin:2004:IAM:1007512.1007540,
 author = {Lin, Lee and Ernst, Michael D.},
 title = {Improving the adaptability of multi-mode systems via program steering},
 abstract = {A multi-mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes. Even when developers rigorously test a multi-mode system before deployment, they cannot foresee and test for every possible usage scenario. As a result, unexpected situations in which the program fails or underperforms (for example, by choosing a non-optimal mode) may arise. This research aims to mitigate such problems by creating a new mode selector that examines the current situation, then chooses a mode that has been successful in the past, in situations like the current one. The technique, called program steering, creates a new mode selector via machine learning from good behavior in testing or in successful operation. Such a strategy, which generalizes the knowledge that a programmer has built into the system, may select an appropriate mode even when the original controller cannot. We have performed experiments on robot control programs written in a month-long programming competition. Augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {206--216},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007540},
 doi = {http://doi.acm.org/10.1145/1007512.1007540},
 acmid = {1007540},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptability, mode selection, multi-mode systems, program steering},
} 

@article{Lin:2004:IAM:1013886.1007540,
 author = {Lin, Lee and Ernst, Michael D.},
 title = {Improving the adaptability of multi-mode systems via program steering},
 abstract = {A multi-mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes. Even when developers rigorously test a multi-mode system before deployment, they cannot foresee and test for every possible usage scenario. As a result, unexpected situations in which the program fails or underperforms (for example, by choosing a non-optimal mode) may arise. This research aims to mitigate such problems by creating a new mode selector that examines the current situation, then chooses a mode that has been successful in the past, in situations like the current one. The technique, called program steering, creates a new mode selector via machine learning from good behavior in testing or in successful operation. Such a strategy, which generalizes the knowledge that a programmer has built into the system, may select an appropriate mode even when the original controller cannot. We have performed experiments on robot control programs written in a month-long programming competition. Augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {206--216},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007540},
 doi = {http://doi.acm.org/10.1145/1013886.1007540},
 acmid = {1007540},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptability, mode selection, multi-mode systems, program steering},
} 

@inproceedings{Ramesh:2004:STS:1007512.1007541,
 author = {Ramesh, S. and Kulkarni, A. and Kamat, V.},
 title = {Slicing tools for synchronous reactive programs},
 abstract = {In this paper, we present two slicing tools: VHDL_Slice and Est_slice that compute static executable slices of VHDL and Esterel programs respectively. The slicers have been tested on a number of small and medium sized examples.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {217--220},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1007512.1007541},
 doi = {http://doi.acm.org/10.1145/1007512.1007541},
 acmid = {1007541},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal verification, slicing, synchronous programs},
} 

@article{Ramesh:2004:STS:1013886.1007541,
 author = {Ramesh, S. and Kulkarni, A. and Kamat, V.},
 title = {Slicing tools for synchronous reactive programs},
 abstract = {In this paper, we present two slicing tools: VHDL_Slice and Est_slice that compute static executable slices of VHDL and Esterel programs respectively. The slicers have been tested on a number of small and medium sized examples.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {217--220},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1013886.1007541},
 doi = {http://doi.acm.org/10.1145/1013886.1007541},
 acmid = {1007541},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal verification, slicing, synchronous programs},
} 

@inproceedings{Flanagan:2004:EPA:1007512.1007543,
 author = {Flanagan, Cormac and Freund, Stephen N. and Qadeer, Shaz},
 title = {Exploiting purity for atomicity},
 abstract = {The notion that certain procedures are atomic</i> is a fundamental correctness property of many multithreaded software systems. A procedure is atomic if for every execution there is an equivalent serial execution in which the actions performed by any thread while executing the atomic procedure are not interleaved with actions of other threads. Several existing tools verify atomicity by using commutativity of actions to show that every execution reduces</i> to a corresponding serial execution. However, experiments with these tools have highlighted a number of interesting procedures that, while intuitively atomic, are not reducible.In this paper, we exploit the notion of pure</i> code blocks to verify the atomicity of such irreducible procedures. If a pure block terminates normally, then its evaluation does not change the program state, and hence these evaluation steps can be removed from the program trace before reduction. We develop a static analysis for atomicity based on this insight, and we illustrate this analysis on a number of interesting examples that could not be verified using earlier tools based purely on reduction. The techniques developed in this paper may also be applicable in other approaches for verifying atomicity, such as model checking and dynamic analysis.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {221--231},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007543},
 doi = {http://doi.acm.org/10.1145/1007512.1007543},
 acmid = {1007543},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, concurrent programs, purity, reduction},
} 

@article{Flanagan:2004:EPA:1013886.1007543,
 author = {Flanagan, Cormac and Freund, Stephen N. and Qadeer, Shaz},
 title = {Exploiting purity for atomicity},
 abstract = {The notion that certain procedures are atomic</i> is a fundamental correctness property of many multithreaded software systems. A procedure is atomic if for every execution there is an equivalent serial execution in which the actions performed by any thread while executing the atomic procedure are not interleaved with actions of other threads. Several existing tools verify atomicity by using commutativity of actions to show that every execution reduces</i> to a corresponding serial execution. However, experiments with these tools have highlighted a number of interesting procedures that, while intuitively atomic, are not reducible.In this paper, we exploit the notion of pure</i> code blocks to verify the atomicity of such irreducible procedures. If a pure block terminates normally, then its evaluation does not change the program state, and hence these evaluation steps can be removed from the program trace before reduction. We develop a static analysis for atomicity based on this insight, and we illustrate this analysis on a number of interesting examples that could not be verified using earlier tools based purely on reduction. The techniques developed in this paper may also be applicable in other approaches for verifying atomicity, such as model checking and dynamic analysis.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {221--231},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007543},
 doi = {http://doi.acm.org/10.1145/1013886.1007543},
 acmid = {1007543},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, concurrent programs, purity, reduction},
} 

@inproceedings{Edwards:2004:FCS:1007512.1007544,
 author = {Edwards, Jonathan and Jackson, Daniel and Torlak, Emina and Yeung, Vincent},
 title = {Faster constraint solving with subtypes},
 abstract = {Constraints in predicate or relational logic can be translated into boolean logic and solved with a SAT solver. For faster solving, it is common to exploit the typing of predicates or relations, in order to reduce the number of boolean variables needed to encode the constraint. Here we show how to extend this idea to constraints expressed in a language with subtyping. Our technique, called atomization</i>, refactors the type hierarchy into a flat collection of disjoint atomic types. The constraints are then decomposed into equivalent constraints involving smaller relations or predicates over these new types, which can then be solved in the normal fashion. Experiments with an implementation of this technique within the Alloy Analyzer show improved performance on practical software checking problems.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {232--242},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007544},
 doi = {http://doi.acm.org/10.1145/1007512.1007544},
 acmid = {1007544},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, analysis, constraints, relational logic, subtypes, verification},
} 

@article{Edwards:2004:FCS:1013886.1007544,
 author = {Edwards, Jonathan and Jackson, Daniel and Torlak, Emina and Yeung, Vincent},
 title = {Faster constraint solving with subtypes},
 abstract = {Constraints in predicate or relational logic can be translated into boolean logic and solved with a SAT solver. For faster solving, it is common to exploit the typing of predicates or relations, in order to reduce the number of boolean variables needed to encode the constraint. Here we show how to extend this idea to constraints expressed in a language with subtyping. Our technique, called atomization</i>, refactors the type hierarchy into a flat collection of disjoint atomic types. The constraints are then decomposed into equivalent constraints involving smaller relations or predicates over these new types, which can then be solved in the normal fashion. Experiments with an implementation of this technique within the Alloy Analyzer show improved performance on practical software checking problems.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {232--242},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007544},
 doi = {http://doi.acm.org/10.1145/1013886.1007544},
 acmid = {1007544},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, analysis, constraints, relational logic, subtypes, verification},
} 

@article{Reimer:2004:SSA:1013886.1007545,
 author = {Reimer, Darrell and Schonberg, Edith and Srinivas, Kavitha and Srinivasan, Harini and Alpern, Bowen and Johnson, Robert D. and Kershenbaum, Aaron and Koved, Larry},
 title = {SABER: smart analysis based error reduction},
 abstract = {In this paper, we present an approach to automatically detect high impact coding errors in large Java applications which use frameworks. These high impact errors cause serious performance degradation and outages in real world production environments, are very time-consuming to detect, and potentially cost businesses thousands of dollars. Based on 3 years experience working with IBM customer production systems, we have identified over 400 high impact coding patterns, from which we have been able to distill a small set of pattern detection algorithms. These algorithms use deep static analysis, thus moving problem detection earlier in the development cycle from production to development. Additionally, we have developed an automatic false positive filtering mechanism based on domain specific knowledge to achieve a level of usability acceptable to IBM field engineers. Our approach also provides necessary contextual information around the sources of the problems to help in problem remediation. We outline how our approach to problem determination can be extended to multiple programming models and domains. We have implemented this problem determination approach in the SABER tool and have used it successfully to detect many serious code defects in several large commercial applications. This paper shows results from four such applications that had over 60 coding defects.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {243--251},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013886.1007545},
 doi = {http://doi.acm.org/10.1145/1013886.1007545},
 acmid = {1007545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect understanding, frameworks, program analysis},
} 

@inproceedings{Reimer:2004:SSA:1007512.1007545,
 author = {Reimer, Darrell and Schonberg, Edith and Srinivas, Kavitha and Srinivasan, Harini and Alpern, Bowen and Johnson, Robert D. and Kershenbaum, Aaron and Koved, Larry},
 title = {SABER: smart analysis based error reduction},
 abstract = {In this paper, we present an approach to automatically detect high impact coding errors in large Java applications which use frameworks. These high impact errors cause serious performance degradation and outages in real world production environments, are very time-consuming to detect, and potentially cost businesses thousands of dollars. Based on 3 years experience working with IBM customer production systems, we have identified over 400 high impact coding patterns, from which we have been able to distill a small set of pattern detection algorithms. These algorithms use deep static analysis, thus moving problem detection earlier in the development cycle from production to development. Additionally, we have developed an automatic false positive filtering mechanism based on domain specific knowledge to achieve a level of usability acceptable to IBM field engineers. Our approach also provides necessary contextual information around the sources of the problems to help in problem remediation. We outline how our approach to problem determination can be extended to multiple programming models and domains. We have implemented this problem determination approach in the SABER tool and have used it successfully to detect many serious code defects in several large commercial applications. This paper shows results from four such applications that had over 60 coding defects.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {243--251},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007512.1007545},
 doi = {http://doi.acm.org/10.1145/1007512.1007545},
 acmid = {1007545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect understanding, frameworks, program analysis},
} 

@article{Fu:2004:MCX:1013886.1007547,
 author = {Fu, Xiang and Bultan, Tevfik and Su, Jianwen},
 title = {Model checking XML manipulating software},
 abstract = {The use of XML as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages. On the other hand, the rich tree-structured data representation, and the expressive XML query languages (such as XPath) make formal specification and verification of software systems that manipulate XML data a challenge. In this paper, we present our initial efforts in automated verification of XML data manipulation operations using the SPIN model checker. We present algorithms for translating (bounded) XML data and XPath expressions to Promela, the input language of SPIN. The techniques presented in this paper constitute the basis of our Web Service Analysis Tool (WSAT) which verifies LTL properties of composite web services.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007547},
 doi = {http://doi.acm.org/10.1145/1013886.1007547},
 acmid = {1007547},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MSL, SPIN, XML, XML schema, XPath, model checking, promela, web service},
} 

@inproceedings{Fu:2004:MCX:1007512.1007547,
 author = {Fu, Xiang and Bultan, Tevfik and Su, Jianwen},
 title = {Model checking XML manipulating software},
 abstract = {The use of XML as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages. On the other hand, the rich tree-structured data representation, and the expressive XML query languages (such as XPath) make formal specification and verification of software systems that manipulate XML data a challenge. In this paper, we present our initial efforts in automated verification of XML data manipulation operations using the SPIN model checker. We present algorithms for translating (bounded) XML data and XPath expressions to Promela, the input language of SPIN. The techniques presented in this paper constitute the basis of our Web Service Analysis Tool (WSAT) which verifies LTL properties of composite web services.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007547},
 doi = {http://doi.acm.org/10.1145/1007512.1007547},
 acmid = {1007547},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MSL, SPIN, XML, XML schema, XPath, model checking, promela, web service},
} 

@article{Rossi:2004:FAM:1013886.1007548,
 author = {Rossi, Matteo and Mandrioli, Dino},
 title = {A formal approach for modeling and verification of RTCORBA-based applications},
 abstract = {We introduce a formal model for describing Real-Time CORBA-based applications, and a set of guidelines to formally check that the design of such an application is consistent with its specification. The model and the guidelines are then applied to the verification of a simple test application.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {263--273},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007548},
 doi = {http://doi.acm.org/10.1145/1013886.1007548},
 acmid = {1007548},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CORBA, distributed applications, formal verification, middleware, real-time},
} 

@inproceedings{Rossi:2004:FAM:1007512.1007548,
 author = {Rossi, Matteo and Mandrioli, Dino},
 title = {A formal approach for modeling and verification of RTCORBA-based applications},
 abstract = {We introduce a formal model for describing Real-Time CORBA-based applications, and a set of guidelines to formally check that the design of such an application is consistent with its specification. The model and the guidelines are then applied to the verification of a simple test application.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {263--273},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007548},
 doi = {http://doi.acm.org/10.1145/1007512.1007548},
 acmid = {1007548},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CORBA, distributed applications, formal verification, middleware, real-time},
} 

@inproceedings{Lerner:2004:VPM:1007512.1007549,
 author = {Lerner, Barbara Staudt},
 title = {Verifying process models built using parameterized state machines},
 abstract = {Software process and work flow languages are increasingly used to define loosely-coupled systems of systems. These languages focus on coordination issues such as data flow and control flow among the subsystems and exception handling activities. The resulting systems are often highly concurrent with activities distributed over many computers. Adequately testing these systems is not feasible due to their size, concurrency, and distributed implementation. Furthermore, the concurrent nature of their activities makes it likely that errors related to the order in which activities are interleaved will go undetected during testing. As a result, verification using static analysis seems necessary to increase confidence in the correctness of these systems. In this paper, we describe our experiences applying LTSA to the analysis of software processes written in Little-JIL. A key aspect to the approach taken in this analysis is that the model that is analyzed consists of a reusable portion that defines language semantics and a process-specific portion that uses parameterization and composition of pieces of the reusable portion to capture the semantics of a Little-JIL process. While the reusable portion was constructed by hand, the parameterization and composition required to model a process is automated. Furthermore, the reusable portion of the model encodes the state machines used in the implementation of the Little-JIL interpreter. As a result, analysis is based not just on the intended semantics of the Little-JIL constructs but on their actual execution semantics. This paper describes how Little-JIL processes are translated into models and reports on analysis results, which have uncovered seven errors in the Little-JIL interpreter that were previously unknown as well as an error in a software process that had previously been analyzed with a different approach without finding the error.},
 booktitle = {Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '04},
 year = {2004},
 isbn = {1-58113-820-2},
 location = {Boston, Massachusetts, USA},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007512.1007549},
 doi = {http://doi.acm.org/10.1145/1007512.1007549},
 acmid = {1007549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LTSA, Little-JIL, SMC, finite state machine, software process, work flow},
} 

@article{Lerner:2004:VPM:1013886.1007549,
 author = {Lerner, Barbara Staudt},
 title = {Verifying process models built using parameterized state machines},
 abstract = {Software process and work flow languages are increasingly used to define loosely-coupled systems of systems. These languages focus on coordination issues such as data flow and control flow among the subsystems and exception handling activities. The resulting systems are often highly concurrent with activities distributed over many computers. Adequately testing these systems is not feasible due to their size, concurrency, and distributed implementation. Furthermore, the concurrent nature of their activities makes it likely that errors related to the order in which activities are interleaved will go undetected during testing. As a result, verification using static analysis seems necessary to increase confidence in the correctness of these systems. In this paper, we describe our experiences applying LTSA to the analysis of software processes written in Little-JIL. A key aspect to the approach taken in this analysis is that the model that is analyzed consists of a reusable portion that defines language semantics and a process-specific portion that uses parameterization and composition of pieces of the reusable portion to capture the semantics of a Little-JIL process. While the reusable portion was constructed by hand, the parameterization and composition required to model a process is automated. Furthermore, the reusable portion of the model encodes the state machines used in the implementation of the Little-JIL interpreter. As a result, analysis is based not just on the intended semantics of the Little-JIL constructs but on their actual execution semantics. This paper describes how Little-JIL processes are translated into models and reports on analysis results, which have uncovered seven errors in the Little-JIL interpreter that were previously unknown as well as an error in a software process that had previously been analyzed with a different approach without finding the error.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {29},
 issue = {4},
 month = {July},
 year = {2004},
 issn = {0163-5948},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013886.1007549},
 doi = {http://doi.acm.org/10.1145/1013886.1007549},
 acmid = {1007549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LTSA, Little-JIL, SMC, finite state machine, software process, work flow},
} 

@inproceedings{Milanova:2002:POS:566172.566174,
 author = {Milanova, Ana and Rountev, Atanas and Ryder, Barbara G.},
 title = {Parameterized object sensitivity for points-to and side-effect analyses for Java},
 abstract = {The goal of points-to analysis</i> for Java is to determine the set of objects pointed to by a reference variable or a reference objet field. Improving the precision of practical points-to analysis is important because points-to information has a wide variety of client applications in optimizing compilers and software engineering tools. In this paper we present object sensitivity,</i> a new form of context sensitivity for flow-insensitive points-to analysis for Java. The key idea of our approach is to analyze a method separately for each of the objects on which this method is invoked. To ensure flexibility and practicality, we propose a parameterization framework that allows analysis designers to control the tradeoffs between cost and precision in the object-sensitive analysis.Side-effect analysis</i> determines the memory locations that may be modified by the execution of a program statement. This information is needed for various compiler optimizations and software engineering tools. We present a new form of side-effect analysis for Java which is based on object-sensitive points-to analysis.We have implemented one instantiation of our parameterized object-sensitive points-to analysis. We compare this instantiation with a context-insensitive points-to analysis for Java which is based on Andersen's analysis for C [4]. On a set of 23 Java programs, our experiments show that the two analyses have comparable cost. In some cases the object-sensitive analysis is actually faster than the context-insensitive analysis. Our results also show that object sensitivity significantly improves the precision of side-effect analysis, call graph construction, and virtual call resolution. These experiments demonstrate that object-sensitive analyses can achieve significantly better precision than context-insensitive ones, while at the same time remaining efficient and practical.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566174},
 doi = {http://doi.acm.org/10.1145/566172.566174},
 acmid = {566174},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Milanova:2002:POS:566171.566174,
 author = {Milanova, Ana and Rountev, Atanas and Ryder, Barbara G.},
 title = {Parameterized object sensitivity for points-to and side-effect analyses for Java},
 abstract = {The goal of points-to analysis</i> for Java is to determine the set of objects pointed to by a reference variable or a reference objet field. Improving the precision of practical points-to analysis is important because points-to information has a wide variety of client applications in optimizing compilers and software engineering tools. In this paper we present object sensitivity,</i> a new form of context sensitivity for flow-insensitive points-to analysis for Java. The key idea of our approach is to analyze a method separately for each of the objects on which this method is invoked. To ensure flexibility and practicality, we propose a parameterization framework that allows analysis designers to control the tradeoffs between cost and precision in the object-sensitive analysis.Side-effect analysis</i> determines the memory locations that may be modified by the execution of a program statement. This information is needed for various compiler optimizations and software engineering tools. We present a new form of side-effect analysis for Java which is based on object-sensitive points-to analysis.We have implemented one instantiation of our parameterized object-sensitive points-to analysis. We compare this instantiation with a context-insensitive points-to analysis for Java which is based on Andersen's analysis for C [4]. On a set of 23 Java programs, our experiments show that the two analyses have comparable cost. In some cases the object-sensitive analysis is actually faster than the context-insensitive analysis. Our results also show that object sensitivity significantly improves the precision of side-effect analysis, call graph construction, and virtual call resolution. These experiments demonstrate that object-sensitive analyses can achieve significantly better precision than context-insensitive ones, while at the same time remaining efficient and practical.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566174},
 doi = {http://doi.acm.org/10.1145/566171.566174},
 acmid = {566174},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Groce:2002:MCJ:566171.566175,
 author = {Groce, Alex and Visser, Willem},
 title = {Model checking Java programs using structural heuristics},
 abstract = {We describe work in troducing heuristic search into the Java PathFinder model checker, which targets Java bytecode. Rather than focusing on heuristics aimed at a particular kind of error (such as deadlocks) we describe heuristics based on a modification of traditional branch coverage metrics and other structure</i> measures, such as thread inter-dependency. We present experimental results showing the utility of these heuristics, and argue for the usefulness of structural heuristics</i> as a class.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566171.566175},
 doi = {http://doi.acm.org/10.1145/566171.566175},
 acmid = {566175},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage metrics, heuristics, model checking, testing},
} 

@inproceedings{Groce:2002:MCJ:566172.566175,
 author = {Groce, Alex and Visser, Willem},
 title = {Model checking Java programs using structural heuristics},
 abstract = {We describe work in troducing heuristic search into the Java PathFinder model checker, which targets Java bytecode. Rather than focusing on heuristics aimed at a particular kind of error (such as deadlocks) we describe heuristics based on a modification of traditional branch coverage metrics and other structure</i> measures, such as thread inter-dependency. We present experimental results showing the utility of these heuristics, and argue for the usefulness of structural heuristics</i> as a class.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566172.566175},
 doi = {http://doi.acm.org/10.1145/566172.566175},
 acmid = {566175},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coverage metrics, heuristics, model checking, testing},
} 

@article{Liang:2002:EPS:566171.566176,
 author = {Liang, Donglin and Pennings, Maikel and Harrold, Mary Jean},
 title = {Evaluating the precision of static reference analysis using profiling},
 abstract = {Program analyses and optimizations of Java programs require reference information that determines the instances that may be accessed through dereferences. Reference information can be computed using reference analysis. This paper presents a set of studies that evaluate the precision of two existing approaches for identifying instances and one approach for computing reference information in a reference analysis. The studies use dynamic reference information collected during run-time as a lower bound approximation to the precise reference information. The studies measure the precision of an existing approach by comparing the information computed using the approach with the lower bound approximation. The paper also presents case studies that attempt to identify the cases under which an existing approach is not effective. The presented studies provide information that may guide the usage of existing reference-analysis techniques and the development of new reference analysis techniques.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566176},
 doi = {http://doi.acm.org/10.1145/566171.566176},
 acmid = {566176},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Liang:2002:EPS:566172.566176,
 author = {Liang, Donglin and Pennings, Maikel and Harrold, Mary Jean},
 title = {Evaluating the precision of static reference analysis using profiling},
 abstract = {Program analyses and optimizations of Java programs require reference information that determines the instances that may be accessed through dereferences. Reference information can be computed using reference analysis. This paper presents a set of studies that evaluate the precision of two existing approaches for identifying instances and one approach for computing reference information in a reference analysis. The studies use dynamic reference information collected during run-time as a lower bound approximation to the precise reference information. The studies measure the precision of an existing approach by comparing the information computed using the approach with the lower bound approximation. The paper also presents case studies that attempt to identify the cases under which an existing approach is not effective. The presented studies provide information that may guide the usage of existing reference-analysis techniques and the development of new reference analysis techniques.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566176},
 doi = {http://doi.acm.org/10.1145/566172.566176},
 acmid = {566176},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Naumovich:2002:CAC:566172.566178,
 author = {Naumovich, Gleb},
 title = {A conservative algorithm for computing the flow of permissions in Java programs},
 abstract = {Open distributed systems are becoming increasingly popular. Such systems include components that may be obtained from a number of different sources. For example, Java allows run-time loading of software components residing on remote machines. One unfortunate side-effect of this openness is the possibility that "hostile" software components may compromise the security of both the program and the system on which it runs. Java offers a built-in security mechanism, using which programmers can give permissions to distributed components and check these permissions at run-time. This security model is flexible, but using it is not straightforward, which may lead to insufficiently tight permission checking and therefore breaches of security.In this paper, we propose a data flow algorithm for automated analysis of the flow of permissions in Java programs. Our algorithm produces, for a given instruction in the program, a set of permissions that are checked on all possible executions up to this instruction. This information can be used in program understanding tools or directly for checking properties that assert what permissions must always be checked before access to certain functionality is allowed. The worst-case complexity of our algorithm is low-order polynomial in the number of program statements and permission types, while comparable previous approaches have exponential costs.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566178},
 doi = {http://doi.acm.org/10.1145/566172.566178},
 acmid = {566178},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, java, security, static analysis, verification},
} 

@article{Naumovich:2002:CAC:566171.566178,
 author = {Naumovich, Gleb},
 title = {A conservative algorithm for computing the flow of permissions in Java programs},
 abstract = {Open distributed systems are becoming increasingly popular. Such systems include components that may be obtained from a number of different sources. For example, Java allows run-time loading of software components residing on remote machines. One unfortunate side-effect of this openness is the possibility that "hostile" software components may compromise the security of both the program and the system on which it runs. Java offers a built-in security mechanism, using which programmers can give permissions to distributed components and check these permissions at run-time. This security model is flexible, but using it is not straightforward, which may lead to insufficiently tight permission checking and therefore breaches of security.In this paper, we propose a data flow algorithm for automated analysis of the flow of permissions in Java programs. Our algorithm produces, for a given instruction in the program, a set of permissions that are checked on all possible executions up to this instruction. This information can be used in program understanding tools or directly for checking properties that assert what permissions must always be checked before access to certain functionality is allowed. The worst-case complexity of our algorithm is low-order polynomial in the number of program statements and permission types, while comparable previous approaches have exponential costs.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566178},
 doi = {http://doi.acm.org/10.1145/566171.566178},
 acmid = {566178},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, java, security, static analysis, verification},
} 

@inproceedings{Stoller:2002:DPO:566172.566179,
 author = {Stoller, Scott D.},
 title = {Domain partitioning for open reactive systems},
 abstract = {Testing or model-checking an open reactive system often requires generating a model of the environment. We describe a static analysis for Java that computes a partition of a system's inputs: inputs in the same equivalence class lead to identical behavior. The partition provides a basis for generation of code for a most general environment of the system, i.e., one that exercises all possible behaviors of the system. The partition also helps the generated environment avoid exercising the same behavior multipletimes. Many distributed systems with security requirements can be regarded as open reactive systems whose environment is an adversary-controlled network. We illustrate our approach by applying it to a fault-tolerant and intrusion-tolerant distributed voting system and model-checking the system together with the generated environment.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566179},
 doi = {http://doi.acm.org/10.1145/566172.566179},
 acmid = {566179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stoller:2002:DPO:566171.566179,
 author = {Stoller, Scott D.},
 title = {Domain partitioning for open reactive systems},
 abstract = {Testing or model-checking an open reactive system often requires generating a model of the environment. We describe a static analysis for Java that computes a partition of a system's inputs: inputs in the same equivalence class lead to identical behavior. The partition provides a basis for generation of code for a most general environment of the system, i.e., one that exercises all possible behaviors of the system. The partition also helps the generated environment avoid exercising the same behavior multipletimes. Many distributed systems with security requirements can be regarded as open reactive systems whose environment is an adversary-controlled network. We illustrate our approach by applying it to a fault-tolerant and intrusion-tolerant distributed voting system and model-checking the system together with the generated environment.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566179},
 doi = {http://doi.acm.org/10.1145/566171.566179},
 acmid = {566179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ostrand:2002:DFL:566172.566181,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
 title = {The distribution of faults in a large industrial software system},
 abstract = {A case study is presented using thirteen releases of a large industrial inventory tracking system. Several types of questions are addressed in this study. The first involved examining how faults are distributed over the different files. This included making a distinction between the release during which they were discovered, the lifecycle stage at which they were first detected, and the severity of the fault. The second category of questions we considered involved studying how the size of modules affected their fault density. This included looking at questions like whether or not files with high fault densities at early stages of the lifecycle also had high fault densities during later stages. A third type of question we considered was whether files that contained large numbers of faults during early stages of development, also had large numbers of faults during later stages, and whether faultiness persisted from release to release. Finally, we examined whether newly written files were more fault-prone than ones that were written for earlier releases of the product. The ultimate goal of this study is to help identify characteristics of files that can be used as predictors of fault-proneness, thereby helping organizations determine how best to use their testing resources.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {55--64},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566172.566181},
 doi = {http://doi.acm.org/10.1145/566172.566181},
 acmid = {566181},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, pareto, software faults, software testing},
} 

@article{Ostrand:2002:DFL:566171.566181,
 author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
 title = {The distribution of faults in a large industrial software system},
 abstract = {A case study is presented using thirteen releases of a large industrial inventory tracking system. Several types of questions are addressed in this study. The first involved examining how faults are distributed over the different files. This included making a distinction between the release during which they were discovered, the lifecycle stage at which they were first detected, and the severity of the fault. The second category of questions we considered involved studying how the size of modules affected their fault density. This included looking at questions like whether or not files with high fault densities at early stages of the lifecycle also had high fault densities during later stages. A third type of question we considered was whether files that contained large numbers of faults during early stages of development, also had large numbers of faults during later stages, and whether faultiness persisted from release to release. Finally, we examined whether newly written files were more fault-prone than ones that were written for earlier releases of the product. The ultimate goal of this study is to help identify characteristics of files that can be used as predictors of fault-proneness, thereby helping organizations determine how best to use their testing resources.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {55--64},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566171.566181},
 doi = {http://doi.acm.org/10.1145/566171.566181},
 acmid = {566181},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical study, fault-prone, pareto, software faults, software testing},
} 

@inproceedings{Orso:2002:GSC:566172.566182,
 author = {Orso, Alessandro and Liang, Donglin and Harrold, Mary Jean and Lipton, Richard},
 title = {Gamma system: continuous evolution of software after deployment},
 abstract = {In this paper, we present the G<sc>AMMA</sc> system, which facilitates remote monitoring of deployed software using a new approach that exploits the opportunities presented by a software product being used by many users connected through a network. G<sc>AMMA</sc> splits monitoring tasks across different instances of the software, so that partial information can be collected from different users by means of light-weight instrumentation, and integrated to gather the overall monitoring information. This system enables software producers (1) to perform continuous, minimally intrusive analyses of their software's behavior, and (2) to use the information thus gathered to improve and evolve their software.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {65--69},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566182},
 doi = {http://doi.acm.org/10.1145/566172.566182},
 acmid = {566182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Orso:2002:GSC:566171.566182,
 author = {Orso, Alessandro and Liang, Donglin and Harrold, Mary Jean and Lipton, Richard},
 title = {Gamma system: continuous evolution of software after deployment},
 abstract = {In this paper, we present the G<sc>AMMA</sc> system, which facilitates remote monitoring of deployed software using a new approach that exploits the opportunities presented by a software product being used by many users connected through a network. G<sc>AMMA</sc> splits monitoring tasks across different instances of the software, so that partial information can be collected from different users by means of light-weight instrumentation, and integrated to gather the overall monitoring information. This system enables software producers (1) to perform continuous, minimally intrusive analyses of their software's behavior, and (2) to use the information thus gathered to improve and evolve their software.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {65--69},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566182},
 doi = {http://doi.acm.org/10.1145/566171.566182},
 acmid = {566182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briand:2002:IUA:566172.566183,
 author = {Briand, L. C. and Labiche, Y. and Sun, H.},
 title = {Investigating the use of analysis contracts to support fault isolation in object oriented code},
 abstract = {A number of activities involved in testing software are known to be difficult and time consuming. Among them is the isolation of faults once failures have been detected. In this paper, we investigate how the instrumentation of contracts could address this issue. Contracts are known to be a useful technique to specify the precondition and postcondition of operations and class invariants, thus making the definition of object-oriented analysis or design elements more precise. Our aim in this paper is to reuse and instrument contracts to ease testing. A thorough case study is run where we define contracts, instrument them using a commercial tool, and assess the benefits and limitations of doing so to support the isolation of faults. We then draw practical conclusions regarding the applicability of the approach and its limitations.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566183},
 doi = {http://doi.acm.org/10.1145/566172.566183},
 acmid = {566183},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {contracts, object-oriented analysis, object-oriented testing, testability},
} 

@article{Briand:2002:IUA:566171.566183,
 author = {Briand, L. C. and Labiche, Y. and Sun, H.},
 title = {Investigating the use of analysis contracts to support fault isolation in object oriented code},
 abstract = {A number of activities involved in testing software are known to be difficult and time consuming. Among them is the isolation of faults once failures have been detected. In this paper, we investigate how the instrumentation of contracts could address this issue. Contracts are known to be a useful technique to specify the precondition and postcondition of operations and class invariants, thus making the definition of object-oriented analysis or design elements more precise. Our aim in this paper is to reuse and instrument contracts to ease testing. A thorough case study is run where we define contracts, instrument them using a commercial tool, and assess the benefits and limitations of doing so to support the isolation of faults. We then draw practical conclusions regarding the applicability of the approach and its limitations.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566183},
 doi = {http://doi.acm.org/10.1145/566171.566183},
 acmid = {566183},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {contracts, object-oriented analysis, object-oriented testing, testability},
} 

@inproceedings{Hiller:2002:PEE:566172.566184,
 author = {Hiller, Martin and Jhumka, Arshad and Suri, Neeraj},
 title = {PROPANE: an environment for examining the propagation of errors in software},
 abstract = {In order to produce reliable software, it is important to have knowledge on how faults and errors may affect the software. In particular, designing efficient error detection mechanisms requires not only knowledge on which types of errors to detect but also the effect these errors may have on the software as well as how they propagate through the software. This paper presents the Propagation Analysis Environment (PROPANE) which is a tool for profiling and conducting fault injection experiments on software running on desktop computers. PROPANE supports the injection of both software faults (by mutation of source code) and data errors (by manipulating variable and memory contents). PROPANE supports various error types out-of-the-box and has support for user-defined error types. For logging, probes are provided for charting the values of variables and memory areas as well as for registering events during execution of the system under test. PROPANE has a flexible design making it useful for development of a wide range of software systems, e.g., embedded software, generic software components, or user-level desktop applications. We show examples of results obtained using PROPANE and how these can guide software developers to where software error detection and recovery could increase the reliability of the software system.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {81--85},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566184},
 doi = {http://doi.acm.org/10.1145/566172.566184},
 acmid = {566184},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error propagation analysis, fault injection, software development tools, software reliability},
} 

@article{Hiller:2002:PEE:566171.566184,
 author = {Hiller, Martin and Jhumka, Arshad and Suri, Neeraj},
 title = {PROPANE: an environment for examining the propagation of errors in software},
 abstract = {In order to produce reliable software, it is important to have knowledge on how faults and errors may affect the software. In particular, designing efficient error detection mechanisms requires not only knowledge on which types of errors to detect but also the effect these errors may have on the software as well as how they propagate through the software. This paper presents the Propagation Analysis Environment (PROPANE) which is a tool for profiling and conducting fault injection experiments on software running on desktop computers. PROPANE supports the injection of both software faults (by mutation of source code) and data errors (by manipulating variable and memory contents). PROPANE supports various error types out-of-the-box and has support for user-defined error types. For logging, probes are provided for charting the values of variables and memory areas as well as for registering events during execution of the system under test. PROPANE has a flexible design making it useful for development of a wide range of software systems, e.g., embedded software, generic software components, or user-level desktop applications. We show examples of results obtained using PROPANE and how these can guide software developers to where software error detection and recovery could increase the reliability of the software system.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {81--85},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566184},
 doi = {http://doi.acm.org/10.1145/566171.566184},
 acmid = {566184},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error propagation analysis, fault injection, software development tools, software reliability},
} 

@inproceedings{Tikir:2002:EIC:566172.566186,
 author = {Tikir, Mustafa M. and Hollingsworth, Jeffrey K.},
 title = {Efficient instrumentation for code coverage testing},
 abstract = {Evaluation of Code Coverage is the problem of identifying the parts of a program that did not execute in one or more runs of a program. The traditional approach for code coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of code coverage. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38-90\% compared with purecov,</i> a commercial code coverage tool. Our tool is fully automated and available for download from the Internet.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566186},
 doi = {http://doi.acm.org/10.1145/566172.566186},
 acmid = {566186},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code coverage, dominator tree, dynamic code deletion, dynamic code patching, on-demand instrumentation, testing},
} 

@article{Tikir:2002:EIC:566171.566186,
 author = {Tikir, Mustafa M. and Hollingsworth, Jeffrey K.},
 title = {Efficient instrumentation for code coverage testing},
 abstract = {Evaluation of Code Coverage is the problem of identifying the parts of a program that did not execute in one or more runs of a program. The traditional approach for code coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of code coverage. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38-90\% compared with purecov,</i> a commercial code coverage tool. Our tool is fully automated and available for download from the Internet.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566186},
 doi = {http://doi.acm.org/10.1145/566171.566186},
 acmid = {566186},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code coverage, dominator tree, dynamic code deletion, dynamic code patching, on-demand instrumentation, testing},
} 

@inproceedings{Srivastava:2002:EPT:566172.566187,
 author = {Srivastava, Amitabh and Thiagarajan, Jay},
 title = {Effectively prioritizing tests in development environment},
 abstract = {Software testing helps ensure not only that the software under development has been implemented correctly, but also that further development does not break it. If developers introduce new defects into the software, these should be detected as early and inexpensively as possible in the development cycle. To help optimize which tests are run at what points in the design cycle, we have built Echelon,</i> a test prioritization system, which prioritizes the application's given set of tests, based on what changes have been made to the program.Echelon builds on the previous work on test prioritization and proposes a practical binary code based approach that scales well to large systems. Echelon utilizes a binary matching system that can accurately compute the differences at a basic block granularity between two versions of the program in binary form. Echelon utilizes a fast, simple and intuitive heuristic that works well in practice to compute what tests will cover the affected basic blocks in the program. Echelon orders the given tests to maximally cover the affected program so that defects are likely to be found quickly and inexpensively. Although the primary focus in Echelon is on program changes, other criteria can be added in computing the priorities.Echelon is part of a test effectiveness infrastructure that runs under the Windows environment. It is currently being integrated into the Microsoft software development process. Echelon has been tested on large Microsoft product binaries. The results show that Echelon is effective in ordering tests based on changes between two program versions.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566172.566187},
 doi = {http://doi.acm.org/10.1145/566172.566187},
 acmid = {566187},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {regression testing, software testing, test minimization, test prioritization, test selection},
} 

@article{Srivastava:2002:EPT:566171.566187,
 author = {Srivastava, Amitabh and Thiagarajan, Jay},
 title = {Effectively prioritizing tests in development environment},
 abstract = {Software testing helps ensure not only that the software under development has been implemented correctly, but also that further development does not break it. If developers introduce new defects into the software, these should be detected as early and inexpensively as possible in the development cycle. To help optimize which tests are run at what points in the design cycle, we have built Echelon,</i> a test prioritization system, which prioritizes the application's given set of tests, based on what changes have been made to the program.Echelon builds on the previous work on test prioritization and proposes a practical binary code based approach that scales well to large systems. Echelon utilizes a binary matching system that can accurately compute the differences at a basic block granularity between two versions of the program in binary form. Echelon utilizes a fast, simple and intuitive heuristic that works well in practice to compute what tests will cover the affected basic blocks in the program. Echelon orders the given tests to maximally cover the affected program so that defects are likely to be found quickly and inexpensively. Although the primary focus in Echelon is on program changes, other criteria can be added in computing the priorities.Echelon is part of a test effectiveness infrastructure that runs under the Windows environment. It is currently being integrated into the Microsoft software development process. Echelon has been tested on large Microsoft product binaries. The results show that Echelon is effective in ordering tests based on changes between two program versions.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566171.566187},
 doi = {http://doi.acm.org/10.1145/566171.566187},
 acmid = {566187},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {regression testing, software testing, test minimization, test prioritization, test selection},
} 

@inproceedings{Vaysburg:2002:DAR:566172.566188,
 author = {Vaysburg, Boris and Tahat, Luay H. and Korel, Bogdan},
 title = {Dependence analysis in reduction of requirement based test suites},
 abstract = {Requirement-based automated test case generation is a model-based technique for generating test suites related to individual requirements. The technique supports test generation from EFSM (Extended Finite State Machine) system models. Several requirement-based selective test generation techniques were proposed. These techniques may significantly reduce a number of test cases with respect to a requirement under test as opposed to a complete system testing. However, the number of test cases may still be very large especially for large systems. In this paper, we present an approach of reduction of requirement based test suites using EFSM dependence analysis. Different types of dependencies are identified between elements of the EFSM system model. These dependencies capture potential interactions between elements of the model and are used to determine parts of the model that affect a requirement under test. This information is used to reduce the test suite by identifying repetitive tests, i.e.,</i> tests that exhibit the same pattern of interactions with respect to the requirement under test. Our initial experience shows that this approach may significantly reduce the size of selective test suites.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {107--111},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566188},
 doi = {http://doi.acm.org/10.1145/566172.566188},
 acmid = {566188},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EFSM dependence analysis, EFSM system model, interaction pattern, model-based testing, system testing, test suite reduction},
} 

@article{Vaysburg:2002:DAR:566171.566188,
 author = {Vaysburg, Boris and Tahat, Luay H. and Korel, Bogdan},
 title = {Dependence analysis in reduction of requirement based test suites},
 abstract = {Requirement-based automated test case generation is a model-based technique for generating test suites related to individual requirements. The technique supports test generation from EFSM (Extended Finite State Machine) system models. Several requirement-based selective test generation techniques were proposed. These techniques may significantly reduce a number of test cases with respect to a requirement under test as opposed to a complete system testing. However, the number of test cases may still be very large especially for large systems. In this paper, we present an approach of reduction of requirement based test suites using EFSM dependence analysis. Different types of dependencies are identified between elements of the EFSM system model. These dependencies capture potential interactions between elements of the model and are used to determine parts of the model that affect a requirement under test. This information is used to reduce the test suite by identifying repetitive tests, i.e.,</i> tests that exhibit the same pattern of interactions with respect to the requirement under test. Our initial experience shows that this approach may significantly reduce the size of selective test suites.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {107--111},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566188},
 doi = {http://doi.acm.org/10.1145/566171.566188},
 acmid = {566188},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EFSM dependence analysis, EFSM system model, interaction pattern, model-based testing, system testing, test suite reduction},
} 

@article{Grieskamp:2002:GFS:566171.566190,
 author = {Grieskamp, Wolfgang and Gurevich, Yuri and Schulte, Wolfram and Veanes, Margus},
 title = {Generating finite state machines from abstract state machines},
 abstract = {We give an algorithm that derives a finite state machine (FSM) from a given abstract state machine (ASM) specification. This allows us to integrate ASM specs with the existing tools for test case generation from FSMs. ASM specs are executable but have typically too many, often infinitely many states. We group ASM states into finitely many hyperstates which are the nodes of the FSM. The links of the FSM are induced by the ASM state transitions.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {112--122},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566190},
 doi = {http://doi.acm.org/10.1145/566171.566190},
 acmid = {566190},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASM, FSM, abstract state machine, executable specification, finite state machine, test case generation},
} 

@inproceedings{Grieskamp:2002:GFS:566172.566190,
 author = {Grieskamp, Wolfgang and Gurevich, Yuri and Schulte, Wolfram and Veanes, Margus},
 title = {Generating finite state machines from abstract state machines},
 abstract = {We give an algorithm that derives a finite state machine (FSM) from a given abstract state machine (ASM) specification. This allows us to integrate ASM specs with the existing tools for test case generation from FSMs. ASM specs are executable but have typically too many, often infinitely many states. We group ASM states into finitely many hyperstates which are the nodes of the FSM. The links of the FSM are induced by the ASM state transitions.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {112--122},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566190},
 doi = {http://doi.acm.org/10.1145/566172.566190},
 acmid = {566190},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASM, FSM, abstract state machine, executable specification, finite state machine, test case generation},
} 

@article{Boyapati:2002:KAT:566171.566191,
 author = {Boyapati, Chandrasekhar and Khurshid, Sarfraz and Marinov, Darko},
 title = {Korat: automated testing based on Java predicates},
 abstract = {This paper presents Korat, a novel framework for automated testing of Java programs. Given a formal specification for a method, Korat uses the method precondition to automatically generate all (nonisomorphic) test cases up to a given small size. Korat then executes the method on each test case, and uses the method postcondition as a test oracle to check the correctness of each output.To generate test cases for a method, Korat constructs a Java predicate (i.e., a method that returns a boolean) from the method's pre-condition. The heart of Korat is a technique for automatic test case generation: given a predicate and a bound on the size of its inputs, Korat generates all (nonisomorphic) inputs for which the predicate returns true. Korat exhaustively explores the bounded input space of the predicate but does so efficiently by monitoring the predicate's executions and pruning large portions of the search space.This paper illustrates the use of Korat for testing several data structures, including some from the Java Collections Framework. The experimental results show that it is feasible to generate test cases from Java predicates, even when the search space for inputs is very large. This paper also compares Korat with a testing framework based on declarative specifications. Contrary to our initial expectation, the experiments show that Korat generates test cases much faster than the declarative framework.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {123--133},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566191},
 doi = {http://doi.acm.org/10.1145/566171.566191},
 acmid = {566191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boyapati:2002:KAT:566172.566191,
 author = {Boyapati, Chandrasekhar and Khurshid, Sarfraz and Marinov, Darko},
 title = {Korat: automated testing based on Java predicates},
 abstract = {This paper presents Korat, a novel framework for automated testing of Java programs. Given a formal specification for a method, Korat uses the method precondition to automatically generate all (nonisomorphic) test cases up to a given small size. Korat then executes the method on each test case, and uses the method postcondition as a test oracle to check the correctness of each output.To generate test cases for a method, Korat constructs a Java predicate (i.e., a method that returns a boolean) from the method's pre-condition. The heart of Korat is a technique for automatic test case generation: given a predicate and a bound on the size of its inputs, Korat generates all (nonisomorphic) inputs for which the predicate returns true. Korat exhaustively explores the bounded input space of the predicate but does so efficiently by monitoring the predicate's executions and pruning large portions of the search space.This paper illustrates the use of Korat for testing several data structures, including some from the Java Collections Framework. The experimental results show that it is feasible to generate test cases from Java predicates, even when the search space for inputs is very large. This paper also compares Korat with a testing framework based on declarative specifications. Contrary to our initial expectation, the experiments show that Korat generates test cases much faster than the declarative framework.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {123--133},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566191},
 doi = {http://doi.acm.org/10.1145/566172.566191},
 acmid = {566191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Friedman:2002:PSM:566172.566192,
 author = {Friedman, G. and Hartman, A. and Nagin, K. and Shiran, T.},
 title = {Projected state machine coverage for software testing},
 abstract = {Our research deals with test generation for software based on finite state machine (FSM) models of the program specification. We describe a set of coverage criteria and testing constraints for use in the automatic generation of test suites. We also describe the algorithms used to generate test suites based on these coverage criteria, and the implementation of these algorithms as an extension of the Mur\&phiv; model checker[4]. The coverage criteria are simple but powerful in that they generate test suites of high quality and moderate volume, without requiring the user to have a sophisticated grasp of the test generation technology. The testing constraints are used to combat the endemic problem of state explosion, typically encountered in FSM techniques. We illustrate our techniques on several well-known problems from the literature and describe two industrial trials, to demonstrate the validity of our claims.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566172.566192},
 doi = {http://doi.acm.org/10.1145/566172.566192},
 acmid = {566192},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test generation, finite state machine modeling, state machine projection., validation},
} 

@article{Friedman:2002:PSM:566171.566192,
 author = {Friedman, G. and Hartman, A. and Nagin, K. and Shiran, T.},
 title = {Projected state machine coverage for software testing},
 abstract = {Our research deals with test generation for software based on finite state machine (FSM) models of the program specification. We describe a set of coverage criteria and testing constraints for use in the automatic generation of test suites. We also describe the algorithms used to generate test suites based on these coverage criteria, and the implementation of these algorithms as an extension of the Mur\&phiv; model checker[4]. The coverage criteria are simple but powerful in that they generate test suites of high quality and moderate volume, without requiring the user to have a sophisticated grasp of the test generation technology. The testing constraints are used to combat the endemic problem of state explosion, typically encountered in FSM techniques. We illustrate our techniques on several well-known problems from the literature and describe two industrial trials, to demonstrate the validity of our claims.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566171.566192},
 doi = {http://doi.acm.org/10.1145/566171.566192},
 acmid = {566192},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test generation, finite state machine modeling, state machine projection., validation},
} 

@inproceedings{Latella:2002:TCR:566172.566194,
 author = {Latella, Diego and Massink, Mieke},
 title = {On testing and conformance relations for UML statechart diagrams behaviours},
 abstract = {In this paper we study the formal relationship between testing preorder/equivalences for a behavioural subset of UML Statechart Diagrams and a conformance relation for implementations with respect to specifications given using such diagrams. We study the impact of stuttering</i> on the above mentioned relationship. In the context of UMLSDs, stuttering</i> occurs when no transition of the UMLSD is enabled by the current event in the current (global) state of the underlying state-machine. We consider both the case in which the semantics underlying the testing relations does not</i> model stuttering explicitly - we call it the non-stuttering</i> semantics - and the case in which it does it</i> - i.e. the stuttering</i> semantics. We show that in the first case the conformance relation is stronger than the reverse of the MUST preorder and, consequently, stronger than the MAY preorder. Much more interesting results can be proven in the second case, possibly under proper conditions on the sets of events under consideration. In fact the conformance relation is shown to coincide with the MAY preorder, and thus be implied by the reverse MUST preorder. Finally, we show important substitutivity properties which hold in the case of stuttering semantics.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {144--153},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566172.566194},
 doi = {http://doi.acm.org/10.1145/566172.566194},
 acmid = {566194},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {UML statechart diagrams, conformance testing, formal semantics, formal testing theory, testing theory},
} 

@article{Latella:2002:TCR:566171.566194,
 author = {Latella, Diego and Massink, Mieke},
 title = {On testing and conformance relations for UML statechart diagrams behaviours},
 abstract = {In this paper we study the formal relationship between testing preorder/equivalences for a behavioural subset of UML Statechart Diagrams and a conformance relation for implementations with respect to specifications given using such diagrams. We study the impact of stuttering</i> on the above mentioned relationship. In the context of UMLSDs, stuttering</i> occurs when no transition of the UMLSD is enabled by the current event in the current (global) state of the underlying state-machine. We consider both the case in which the semantics underlying the testing relations does not</i> model stuttering explicitly - we call it the non-stuttering</i> semantics - and the case in which it does it</i> - i.e. the stuttering</i> semantics. We show that in the first case the conformance relation is stronger than the reverse of the MUST preorder and, consequently, stronger than the MAY preorder. Much more interesting results can be proven in the second case, possibly under proper conditions on the sets of events under consideration. In fact the conformance relation is shown to coincide with the MAY preorder, and thus be implied by the reverse MUST preorder. Finally, we show important substitutivity properties which hold in the case of stuttering semantics.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {144--153},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/566171.566194},
 doi = {http://doi.acm.org/10.1145/566171.566194},
 acmid = {566194},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {UML statechart diagrams, conformance testing, formal semantics, formal testing theory, testing theory},
} 

@article{Moors:2002:EAT:566171.566195,
 author = {Moors, Tim and Veeraraghavan, Malathi and Tao, Zhifeng and Zheng, Xuan and Badri, Ramesh},
 title = {Experiences in automating the testing of SS7 signalling transfer points},
 abstract = {Signalling System 7 (SS7) is widely used for telephone signalling. Service providers need to frequently test their Signalling Transfer Points (STPs), which switch SS7 messages, for both protocol conformance and interoperability. This paper describes a system that automatically analyzes the data collected during STP tests. It consists of files that describe how the STPs are expected to behave during the test, and Perl code that translates this Expected Behavior into a program that can search the data collected during the test for the expected events, and report on whether the system passed the test. The system readily processed over 30,000 events for each test run, and identified abnormal behavior that could interfere with interoperability and protocol conformance.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {154--158},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566195},
 doi = {http://doi.acm.org/10.1145/566171.566195},
 acmid = {566195},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SS7, STP, automation, signaling system 7, signalling system 7},
} 

@inproceedings{Moors:2002:EAT:566172.566195,
 author = {Moors, Tim and Veeraraghavan, Malathi and Tao, Zhifeng and Zheng, Xuan and Badri, Ramesh},
 title = {Experiences in automating the testing of SS7 signalling transfer points},
 abstract = {Signalling System 7 (SS7) is widely used for telephone signalling. Service providers need to frequently test their Signalling Transfer Points (STPs), which switch SS7 messages, for both protocol conformance and interoperability. This paper describes a system that automatically analyzes the data collected during STP tests. It consists of files that describe how the STPs are expected to behave during the test, and Perl code that translates this Expected Behavior into a program that can search the data collected during the test for the expected events, and report on whether the system passed the test. The system readily processed over 30,000 events for each test run, and identified abnormal behavior that could interfere with interoperability and protocol conformance.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {154--158},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566195},
 doi = {http://doi.acm.org/10.1145/566172.566195},
 acmid = {566195},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SS7, STP, automation, signaling system 7, signalling system 7},
} 

@inproceedings{Cardell-Oliver:2002:CTE:566172.566196,
 author = {Cardell-Oliver, Rachel},
 title = {Conformance test experiments for distributed real-time systems},
 abstract = {This paper introduces a new technique for testing that a distributed real-time system satisfies a formal timed automata specification. It outlines how to write test specifications in the language of Uppaal timed automata, how to translate those specifications into program code for executing the tests, and describes the results of test experiments on a distributed real-time system with limited hardware and software resources.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {159--163},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566196},
 doi = {http://doi.acm.org/10.1145/566172.566196},
 acmid = {566196},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Lego RCX, Uppaal, conformance testing, design for testability, distributed real-time systems, timed automata},
} 

@article{Cardell-Oliver:2002:CTE:566171.566196,
 author = {Cardell-Oliver, Rachel},
 title = {Conformance test experiments for distributed real-time systems},
 abstract = {This paper introduces a new technique for testing that a distributed real-time system satisfies a formal timed automata specification. It outlines how to write test specifications in the language of Uppaal timed automata, how to translate those specifications into program code for executing the tests, and describes the results of test experiments on a distributed real-time system with limited hardware and software resources.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {159--163},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566196},
 doi = {http://doi.acm.org/10.1145/566171.566196},
 acmid = {566196},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Lego RCX, Uppaal, conformance testing, design for testability, distributed real-time systems, timed automata},
} 

@article{Cheng:2002:RDM:566171.566198,
 author = {Cheng, Yung-Pin},
 title = {Refactoring design models for inductive verification},
 abstract = {Systems composed of many identical processes can sometimes be verified inductively using a network invariant, but systems whose component processes vary in some systematic way are not amenable to direct application of that method. We describe how variations in behavior can be "factored out" into additional processes, thus enabling induction over the number of processes. The process is semi-automatic: The designer must choose from among a set of idiomatic transformations, but each transformation is applied and checked automatically.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {164--168},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566198},
 doi = {http://doi.acm.org/10.1145/566171.566198},
 acmid = {566198},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compositional analysis, concurrency, network invariants, parameterized system, refactoring},
} 

@inproceedings{Cheng:2002:RDM:566172.566198,
 author = {Cheng, Yung-Pin},
 title = {Refactoring design models for inductive verification},
 abstract = {Systems composed of many identical processes can sometimes be verified inductively using a network invariant, but systems whose component processes vary in some systematic way are not amenable to direct application of that method. We describe how variations in behavior can be "factored out" into additional processes, thus enabling induction over the number of processes. The process is semi-automatic: The designer must choose from among a set of idiomatic transformations, but each transformation is applied and checked automatically.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {164--168},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566198},
 doi = {http://doi.acm.org/10.1145/566172.566198},
 acmid = {566198},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compositional analysis, concurrency, network invariants, parameterized system, refactoring},
} 

@article{Yavuz-Kahveci:2002:SVS:566171.566199,
 author = {Yavuz-Kahveci, Tuba and Bultan, Tevfik},
 title = {Specification, verification, and synthesis of concurrency control components},
 abstract = {Run-time errors in concurrent programs are generally due to the wrong usage of synchronization primitives such as monitors. Conventional validation techniques such as testing become ineffective for concurrent programs since the state space increases exponentially with the number of concurrent processes. In this paper, we propose an approach in which 1) the concurrency control component of a concurrent program is formally specified, 2) it is verified automatically using model checking, and 3) the code for concurrency control component is automatically generated. We use monitors as the synchronization primitive to control access to a shared resource by multipleconcurrent processes. Since our approach decouples the concurrency control component from the rest of the implementation it is scalable. We demonstrate the usefulness of our approach by applying it to a case study on Airport Ground Traffic Control.We use the Action Language to specify the concurrency control component of a system. Action Language is a specification language for reactive software systems. It is supported by an infinite-state model checker that can verify systems with boolean, enumerated and udbounded integer variables. Our code generation tool automatically translates the verified Action Language specification into a Java monitor. Our translation algorithm employs symbolic manipulation techniques and the specific notification pattern to generate an optimized monitor class by eliminating the context switch overhead introduced as a result of unnecessary thread notification. Using counting abstraction, we show that we can automatically verify the monitor specifications for arbitrary number of threads.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {169--179},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566199},
 doi = {http://doi.acm.org/10.1145/566171.566199},
 acmid = {566199},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent programming, infinite-state model checking, monitors, specification languages},
} 

@inproceedings{Yavuz-Kahveci:2002:SVS:566172.566199,
 author = {Yavuz-Kahveci, Tuba and Bultan, Tevfik},
 title = {Specification, verification, and synthesis of concurrency control components},
 abstract = {Run-time errors in concurrent programs are generally due to the wrong usage of synchronization primitives such as monitors. Conventional validation techniques such as testing become ineffective for concurrent programs since the state space increases exponentially with the number of concurrent processes. In this paper, we propose an approach in which 1) the concurrency control component of a concurrent program is formally specified, 2) it is verified automatically using model checking, and 3) the code for concurrency control component is automatically generated. We use monitors as the synchronization primitive to control access to a shared resource by multipleconcurrent processes. Since our approach decouples the concurrency control component from the rest of the implementation it is scalable. We demonstrate the usefulness of our approach by applying it to a case study on Airport Ground Traffic Control.We use the Action Language to specify the concurrency control component of a system. Action Language is a specification language for reactive software systems. It is supported by an infinite-state model checker that can verify systems with boolean, enumerated and udbounded integer variables. Our code generation tool automatically translates the verified Action Language specification into a Java monitor. Our translation algorithm employs symbolic manipulation techniques and the specific notification pattern to generate an optimized monitor class by eliminating the context switch overhead introduced as a result of unnecessary thread notification. Using counting abstraction, we show that we can automatically verify the monitor specifications for arbitrary number of threads.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {169--179},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566199},
 doi = {http://doi.acm.org/10.1145/566172.566199},
 acmid = {566199},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent programming, infinite-state model checking, monitors, specification languages},
} 

@article{Bishop:2002:RRB:566171.566201,
 author = {Bishop, Peter G},
 title = {Rescaling reliability bounds for a new operational profile},
 abstract = {One of the main problems with reliability testing and prediction is that the result is specific to a particular operational profile. This paper extends an earlier reliability theory for computing a worst case reliability bound. The extended theory derives a re-scaled reliability bound based on the change in execution rates of the code segments in the program. In some cases it is possible to derive a maximum failure rate bound that applies to any</i> change in the profile. It also predicts that (in principle) a "fair" test profile can be derived where the reliability bounds are relatively insensitive to the operational profile. In addition the theory allows unit and module test coverage measures to be incorporated into an operational reliability bound prediction. The implications of the theory are discussed, and the theory is evaluated by applying it to two example programs with known faults.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566201},
 doi = {http://doi.acm.org/10.1145/566171.566201},
 acmid = {566201},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operational profile, operational reliability testing, worst case bounds.},
} 

@inproceedings{Bishop:2002:RRB:566172.566201,
 author = {Bishop, Peter G},
 title = {Rescaling reliability bounds for a new operational profile},
 abstract = {One of the main problems with reliability testing and prediction is that the result is specific to a particular operational profile. This paper extends an earlier reliability theory for computing a worst case reliability bound. The extended theory derives a re-scaled reliability bound based on the change in execution rates of the code segments in the program. In some cases it is possible to derive a maximum failure rate bound that applies to any</i> change in the profile. It also predicts that (in principle) a "fair" test profile can be derived where the reliability bounds are relatively insensitive to the operational profile. In addition the theory allows unit and module test coverage measures to be incorporated into an operational reliability bound prediction. The implications of the theory are discussed, and the theory is evaluated by applying it to two example programs with known faults.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566201},
 doi = {http://doi.acm.org/10.1145/566172.566201},
 acmid = {566201},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operational profile, operational reliability testing, worst case bounds.},
} 

@inproceedings{Chen:2002:SIM:566172.566202,
 author = {Chen, T. Y. and Tse, T. H. and Zhou, Zhiquan},
 title = {Semi-proving: an integrated method based on global symbolic evaluation and metamorphic testing},
 abstract = {We present a semi-proving method for verifying necessary conditions for program correctness. Our approach is based on the integration of global symbolic evaluation and metamorphic testing. It is relatively easier than conventional program proving, and helps to alleviate the problem that software testing cannot show the absence of faults.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {191--195},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566202},
 doi = {http://doi.acm.org/10.1145/566172.566202},
 acmid = {566202},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {global symbolic evaluation, metamorphic testing, program proving, program testing, semi-proving, symbolic execution},
} 

@article{Chen:2002:SIM:566171.566202,
 author = {Chen, T. Y. and Tse, T. H. and Zhou, Zhiquan},
 title = {Semi-proving: an integrated method based on global symbolic evaluation and metamorphic testing},
 abstract = {We present a semi-proving method for verifying necessary conditions for program correctness. Our approach is based on the integration of global symbolic evaluation and metamorphic testing. It is relatively easier than conventional program proving, and helps to alleviate the problem that software testing cannot show the absence of faults.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {191--195},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566202},
 doi = {http://doi.acm.org/10.1145/566171.566202},
 acmid = {566202},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {global symbolic evaluation, metamorphic testing, program proving, program testing, semi-proving, symbolic execution},
} 

@article{Hamlet:2002:CSS:566171.566203,
 author = {Hamlet, Dick},
 title = {Continuity in software systems},
 abstract = {Most engineering artifacts behave in a continuous fashion, and this property is generally believed to underlie their dependability. In contrast, software systems do not have continuous behavior, which is taken to be an underlying cause of their undependability. The theory of software reliability has been questioned because technically the sampling on which it is based applies only to continuous functions.This paper examines the role of continuity in engineering, particularly in testing and certifying artifacts, then considers the analogous software situations and the ways in which software is intrinsically unlike other engineered objects. Several definitions of software 'continuity' are proposed and related to ideas in software testing. It is shown how 'continuity' can be established in practice, and the consequences for testing and analysis of knowing that a program is 'continuous.Underlying any use of software 'continuity' is the continuity of its specification in the usual mathematical sense. However, many software applications are intrinsically discontinuous and one reason why software is so valuable is its natural ability to handle these applications, where it makes no sense to seek software 'continuity' or to blame poor dependability on its absence.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {196--200},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566171.566203},
 doi = {http://doi.acm.org/10.1145/566171.566203},
 acmid = {566203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hamlet:2002:CSS:566172.566203,
 author = {Hamlet, Dick},
 title = {Continuity in software systems},
 abstract = {Most engineering artifacts behave in a continuous fashion, and this property is generally believed to underlie their dependability. In contrast, software systems do not have continuous behavior, which is taken to be an underlying cause of their undependability. The theory of software reliability has been questioned because technically the sampling on which it is based applies only to continuous functions.This paper examines the role of continuity in engineering, particularly in testing and certifying artifacts, then considers the analogous software situations and the ways in which software is intrinsically unlike other engineered objects. Several definitions of software 'continuity' are proposed and related to ideas in software testing. It is shown how 'continuity' can be established in practice, and the consequences for testing and analysis of knowing that a program is 'continuous.Underlying any use of software 'continuity' is the continuity of its specification in the usual mathematical sense. However, many software applications are intrinsically discontinuous and one reason why software is so valuable is its natural ability to handle these applications, where it makes no sense to seek software 'continuity' or to blame poor dependability on its absence.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {196--200},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/566172.566203},
 doi = {http://doi.acm.org/10.1145/566172.566203},
 acmid = {566203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bertolino:2002:IPI:566171.566205,
 author = {Bertolino, Antonia},
 title = {ISSTA 2002 panel: is ISSTA research relevant to industrial users?},
 abstract = {ISSTA is at its twelfth edition. Also this year, researchers from academy and industry have contributed with many interesting studies and experience reports in software analysis and testing. We --- the ISSTA partakers- have (or at least believe to have) clear ideas about which are the problems to be solved, which are the real challenges, and probably each of us has already settled an agenda of the next steps to take for solving them looking ahead to the next ISSTA edition.Are we doing right? Do we know which are the real issues in the field? Is our research addressing relevant points, or just aesthetic questions? Do, and how much, industrial users ---the ISSTA addressees- value our papers and our achievements?This panel will address such questions by grouping a set of managers from different industries around a table and asking their opinions. As the above questions are very general, in the intent to tackle the theme in a concrete way, we will trigger the discussion by looking at the research results in terms of their potential impact in their respective enterprises. We would like to know which are currently the costs of testing and analysis activities in their industries, which are the items summing up such costs, and how we ISSTA researchers could help improving them.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {201--202},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566171.566205},
 doi = {http://doi.acm.org/10.1145/566171.566205},
 acmid = {566205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bertolino:2002:IPI:566172.566205,
 author = {Bertolino, Antonia},
 title = {ISSTA 2002 panel: is ISSTA research relevant to industrial users?},
 abstract = {ISSTA is at its twelfth edition. Also this year, researchers from academy and industry have contributed with many interesting studies and experience reports in software analysis and testing. We --- the ISSTA partakers- have (or at least believe to have) clear ideas about which are the problems to be solved, which are the real challenges, and probably each of us has already settled an agenda of the next steps to take for solving them looking ahead to the next ISSTA edition.Are we doing right? Do we know which are the real issues in the field? Is our research addressing relevant points, or just aesthetic questions? Do, and how much, industrial users ---the ISSTA addressees- value our papers and our achievements?This panel will address such questions by grouping a set of managers from different industries around a table and asking their opinions. As the above questions are very general, in the intent to tackle the theme in a concrete way, we will trigger the discussion by looking at the research results in terms of their potential impact in their respective enterprises. We would like to know which are currently the costs of testing and analysis activities in their industries, which are the items summing up such costs, and how we ISSTA researchers could help improving them.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {201--202},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566172.566205},
 doi = {http://doi.acm.org/10.1145/566172.566205},
 acmid = {566205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Encontre:2002:IRR:566171.566206,
 author = {Encontre, Vincent},
 title = {Is ISSTA research relevant to industrial users? panel - ISSTA 2002: empowering the developer to be a tester too!},
 abstract = {In this paper - scoped for the panel discussion at ISSTA 2002 - we are discussing some techniques to ease the adoption of testing techniques by the developers, by extending the debugging activity. We also briefly discuss a longer term vision where the same paradigm applies but at model level, when coding will be achieved using visual notations such as UML.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {203--204},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566171.566206},
 doi = {http://doi.acm.org/10.1145/566171.566206},
 acmid = {566206},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JUnit, UML, debugging, runtime analysis, testing},
} 

@inproceedings{Encontre:2002:IRR:566172.566206,
 author = {Encontre, Vincent},
 title = {Is ISSTA research relevant to industrial users? panel - ISSTA 2002: empowering the developer to be a tester too!},
 abstract = {In this paper - scoped for the panel discussion at ISSTA 2002 - we are discussing some techniques to ease the adoption of testing techniques by the developers, by extending the debugging activity. We also briefly discuss a longer term vision where the same paradigm applies but at model level, when coding will be achieved using visual notations such as UML.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {203--204},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566172.566206},
 doi = {http://doi.acm.org/10.1145/566172.566206},
 acmid = {566206},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JUnit, UML, debugging, runtime analysis, testing},
} 

@inproceedings{Hartman:2002:IRR:566172.566207,
 author = {Hartman, A.},
 title = {Is ISSTA research relevant to industry?},
 abstract = {},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {205--206},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566172.566207},
 doi = {http://doi.acm.org/10.1145/566172.566207},
 acmid = {566207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hartman:2002:IRR:566171.566207,
 author = {Hartman, A.},
 title = {Is ISSTA research relevant to industry?},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {205--206},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566171.566207},
 doi = {http://doi.acm.org/10.1145/566171.566207},
 acmid = {566207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peciola:2002:ELI:566171.566208,
 author = {Peciola, Emilia},
 title = {Ericsson lab Italy: is ISSTA research relevant to industrial users?},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {207--207},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/566171.566208},
 doi = {http://doi.acm.org/10.1145/566171.566208},
 acmid = {566208},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peciola:2002:ELI:566172.566208,
 author = {Peciola, Emilia},
 title = {Ericsson lab Italy: is ISSTA research relevant to industrial users?},
 abstract = {},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {207--207},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/566172.566208},
 doi = {http://doi.acm.org/10.1145/566172.566208},
 acmid = {566208},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sreenivas:2002:PDI:566172.566209,
 author = {Sreenivas, Ashok},
 title = {Panel discussion: is ISSTA testing research relevant to industrial users?},
 abstract = {We discuss the direct relevance of on-going testing research to the 'users' of the research, namely the industrial practitioners. The current state-of-the-practice in software testing is quite ad-hoc</i> and provides little or no assertions</i> about the quality of the delivered software product. We propose the view that research that is aligned with formal approaches to software development is the best bet to achieve this goal.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566172.566209},
 doi = {http://doi.acm.org/10.1145/566172.566209},
 acmid = {566209},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sreenivas:2002:PDI:566171.566209,
 author = {Sreenivas, Ashok},
 title = {Panel discussion: is ISSTA testing research relevant to industrial users?},
 abstract = {We discuss the direct relevance of on-going testing research to the 'users' of the research, namely the industrial practitioners. The current state-of-the-practice in software testing is quite ad-hoc</i> and provides little or no assertions</i> about the quality of the delivered software product. We propose the view that research that is aligned with formal approaches to software development is the best bet to achieve this goal.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/566171.566209},
 doi = {http://doi.acm.org/10.1145/566171.566209},
 acmid = {566209},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Choi:2002:IFT:566171.566211,
 author = {Choi, Jong-Deok and Zeller, Andreas},
 title = {Isolating failure-inducing thread schedules},
 abstract = {Consider a multi-threaded application that occasionally fails due to non-determinism. Using the DEJAVU capture/replay tool, it is possible to record the thread schedule and replay the application in a deterministic way. By systematically narrowing down the difference between a thread schedule that makes the program pass and another schedule that makes the program fail, the Delta Debugging approach can pinpoint the error location automatically---namely, the location(s) where a thread switch causes the program to fail. In a case study, Delta Debugging isolated the failure-inducing schedule difference from 3.8 billion differences in only 50 tests.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566211},
 doi = {http://doi.acm.org/10.1145/566171.566211},
 acmid = {566211},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Choi:2002:IFT:566172.566211,
 author = {Choi, Jong-Deok and Zeller, Andreas},
 title = {Isolating failure-inducing thread schedules},
 abstract = {Consider a multi-threaded application that occasionally fails due to non-determinism. Using the DEJAVU capture/replay tool, it is possible to record the thread schedule and replay the application in a deterministic way. By systematically narrowing down the difference between a thread schedule that makes the program pass and another schedule that makes the program fail, the Delta Debugging approach can pinpoint the error location automatically---namely, the location(s) where a thread switch causes the program to fail. In a case study, Delta Debugging isolated the failure-inducing schedule difference from 3.8 billion differences in only 50 tests.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566211},
 doi = {http://doi.acm.org/10.1145/566172.566211},
 acmid = {566211},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Whaley:2002:AEO:566172.566212,
 author = {Whaley, John and Martin, Michael C. and Lam, Monica S.},
 title = {Automatic extraction of object-oriented component interfaces},
 abstract = {Component-based software design is a popular and effective approach to designing large systems. While components typically have well-defined interfaces, sequencing information---which calls must come in which order---is often not formally specified.This paper proposes using multiple finite statemachine (FSM) submodels to model the interface of a class. A submodel includes a subset of methods that, for example, implement a Java interface, or access some particular field. Each state-modifying method is represented as a state in the FSM, and transitions of the FSMs represent allow able pairs of consecutive methods. In addition, state-preserving methods are constrained to execute only under certain states.We have designed and implemented a system that includes static analyses to deduce illegal call sequences in a program, dynamic instrumentation techniques to extract models from execution runs, and a dynamic model checker that ensures that the code conforms to the model. Extracted models can serve as documentation; they can serve as constraints to be enforced by a static checker; they can be studied directly by developers to determine if the program is exhibiting unexpected behavior; or they can be used to determine the completeness of a test suite.Our system has been run on several large code bases, including the joeq virtual machine, the basic Java libraries, and the Java 2 Enterprise Edition library code. Our experience suggests that this approach yields useful information.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {218--228},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566212},
 doi = {http://doi.acm.org/10.1145/566172.566212},
 acmid = {566212},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Whaley:2002:AEO:566171.566212,
 author = {Whaley, John and Martin, Michael C. and Lam, Monica S.},
 title = {Automatic extraction of object-oriented component interfaces},
 abstract = {Component-based software design is a popular and effective approach to designing large systems. While components typically have well-defined interfaces, sequencing information---which calls must come in which order---is often not formally specified.This paper proposes using multiple finite statemachine (FSM) submodels to model the interface of a class. A submodel includes a subset of methods that, for example, implement a Java interface, or access some particular field. Each state-modifying method is represented as a state in the FSM, and transitions of the FSMs represent allow able pairs of consecutive methods. In addition, state-preserving methods are constrained to execute only under certain states.We have designed and implemented a system that includes static analyses to deduce illegal call sequences in a program, dynamic instrumentation techniques to extract models from execution runs, and a dynamic model checker that ensures that the code conforms to the model. Extracted models can serve as documentation; they can serve as constraints to be enforced by a static checker; they can be studied directly by developers to determine if the program is exhibiting unexpected behavior; or they can be used to determine the completeness of a test suite.Our system has been run on several large code bases, including the joeq virtual machine, the basic Java libraries, and the Java 2 Enterprise Edition library code. Our experience suggests that this approach yields useful information.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {218--228},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566212},
 doi = {http://doi.acm.org/10.1145/566171.566212},
 acmid = {566212},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nimmer:2002:AGP:566171.566213,
 author = {Nimmer, Jeremy W. and Ernst, Michael D.},
 title = {Automatic generation of program specifications},
 abstract = {Producing specifications by dynamic (runtime) analysis of program executions is potentially unsound, because the analyzed executions may not fully characterize all possible executions of the program. In practice, how accurate are the results of a dynamic analysis? This paper describes the results of an investigation into this question, determining how much specifications generalized from program runs must be changed in order to be verified by a static checker. Surprisingly, small test suites captured nearly all program behavior required by a specific type of static checking; the static checker guaranteed that the implementations satisfy the generated specifications, and ensured the absence of runtime exceptions. Measured against this verification task, the generated specifications scored over 90\% on precision, a measure of soundness, and on recall, a measure of completeness.This is a positive result for testing, because it suggests that dynamic analyses can capture all semantic information of interest for certain applications. The experimental results demonstrate that a specific technique, dynamic invariant detection, is effective at generating consistent, sufficient specifications for use by a static checker. Finally, the research shows that combining static and dynamic analyses over program specifications has benefits for users of each technique, guaranteeing soundness of the dynamic analysis and lessening the annotation burden for users of the static analysis.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {27},
 issue = {4},
 month = {July},
 year = {2002},
 issn = {0163-5948},
 pages = {229--239},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566171.566213},
 doi = {http://doi.acm.org/10.1145/566171.566213},
 acmid = {566213},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nimmer:2002:AGP:566172.566213,
 author = {Nimmer, Jeremy W. and Ernst, Michael D.},
 title = {Automatic generation of program specifications},
 abstract = {Producing specifications by dynamic (runtime) analysis of program executions is potentially unsound, because the analyzed executions may not fully characterize all possible executions of the program. In practice, how accurate are the results of a dynamic analysis? This paper describes the results of an investigation into this question, determining how much specifications generalized from program runs must be changed in order to be verified by a static checker. Surprisingly, small test suites captured nearly all program behavior required by a specific type of static checking; the static checker guaranteed that the implementations satisfy the generated specifications, and ensured the absence of runtime exceptions. Measured against this verification task, the generated specifications scored over 90\% on precision, a measure of soundness, and on recall, a measure of completeness.This is a positive result for testing, because it suggests that dynamic analyses can capture all semantic information of interest for certain applications. The experimental results demonstrate that a specific technique, dynamic invariant detection, is effective at generating consistent, sufficient specifications for use by a static checker. Finally, the research shows that combining static and dynamic analyses over program specifications has benefits for users of each technique, guaranteeing soundness of the dynamic analysis and lessening the annotation burden for users of the static analysis.},
 booktitle = {Proceedings of the 2002 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '02},
 year = {2002},
 isbn = {1-58113-562-9},
 location = {Roma, Italy},
 pages = {229--239},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/566172.566213},
 doi = {http://doi.acm.org/10.1145/566172.566213},
 acmid = {566213},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pincus:2000:ANB:347636.347826,
 author = {Pincus, Jon},
 title = {Analysis is necessary, but far from sufficient (abstract only): Experiences building and deploying successful tools for developers and testers},
 abstract = {Why are there so few successful "real-world" programming and testing tools based on academic research? This talk focuses on program analysis tools, and proposes a surprisingly simple explanation with interesting ramifications.For a tool aimed at developers or testers to be successful, people must use it - and must use it to help accomplish their existing tasks, rather than as an end in itself. If the tool does not help them get their job done, or the effort to learn and/or use the tool is too great, users will not perceive enough value; the tool will not get significant usage, even if it is free.This talk focuses on the often-overlooked consequences of this seemingly basic statement in two major areas: program analysis, and the work beyond core analysis that must be done to make a successful tool. Examples will be drawn from tools that have been successfully used in industry (sold commercially, and developed for internal use).
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/347636.347826},
 doi = {http://doi.acm.org/10.1145/347636.347826},
 acmid = {347826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pincus:2000:ANB:347324.347826,
 author = {Pincus, Jon},
 title = {Analysis is necessary, but far from sufficient (abstract only): Experiences building and deploying successful tools for developers and testers},
 abstract = {Why are there so few successful "real-world" programming and testing tools based on academic research? This talk focuses on program analysis tools, and proposes a surprisingly simple explanation with interesting ramifications.For a tool aimed at developers or testers to be successful, people must use it - and must use it to help accomplish their existing tasks, rather than as an end in itself. If the tool does not help them get their job done, or the effort to learn and/or use the tool is too great, users will not perceive enough value; the tool will not get significant usage, even if it is free.This talk focuses on the often-overlooked consequences of this seemingly basic statement in two major areas: program analysis, and the work beyond core analysis that must be done to make a successful tool. Examples will be drawn from tools that have been successfully used in industry (sold commercially, and developed for internal use).
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/347324.347826},
 doi = {http://doi.acm.org/10.1145/347324.347826},
 acmid = {347826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bhargavan:2000:VFA:347324.347833,
 author = {Bhargavan, Karthikeyan and Gunter, Carl A. and Kim, Moonjoo and Lee, Insup and Obradovic, Davor and Sokolsky, Oleg and Viswanathan, Mahesh},
 title = {Verisim: Formal analysis of network simulations},
 abstract = {Why are there so few successful "real-world" programming and testing tools based on academic research? This talk focuses on program analysis tools, and proposes a surprisingly simple explanation with interesting ramifications.For a tool aimed at developers or testers to be successful, people must use it - and must use it to help accomplish their existing tasks, rather than as an end in itself. If the tool does not help them get their job done, or the effort to learn and/or use the tool is too great, users will not perceive enough value; the tool will not get significant usage, even if it is free.This talk focuses on the often-overlooked consequences of this seemingly basic statement in two major areas: program analysis, and the work beyond core analysis that must be done to make a successful tool. Examples will be drawn from tools that have been successfully used in industry (sold commercially, and developed for internal use).},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/347324.347833},
 doi = {http://doi.acm.org/10.1145/347324.347833},
 acmid = {347833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhargavan:2000:VFA:347636.347833,
 author = {Bhargavan, Karthikeyan and Gunter, Carl A. and Kim, Moonjoo and Lee, Insup and Obradovic, Davor and Sokolsky, Oleg and Viswanathan, Mahesh},
 title = {Verisim: Formal analysis of network simulations},
 abstract = {Why are there so few successful "real-world" programming and testing tools based on academic research? This talk focuses on program analysis tools, and proposes a surprisingly simple explanation with interesting ramifications.For a tool aimed at developers or testers to be successful, people must use it - and must use it to help accomplish their existing tasks, rather than as an end in itself. If the tool does not help them get their job done, or the effort to learn and/or use the tool is too great, users will not perceive enough value; the tool will not get significant usage, even if it is free.This talk focuses on the often-overlooked consequences of this seemingly basic statement in two major areas: program analysis, and the work beyond core analysis that must be done to make a successful tool. Examples will be drawn from tools that have been successfully used in industry (sold commercially, and developed for internal use).},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/347636.347833},
 doi = {http://doi.acm.org/10.1145/347636.347833},
 acmid = {347833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jackson:2000:FBC:347636.383378,
 author = {Jackson, Daniel and Vaziri, Mandana},
 title = {Finding bugs with a constraint solver},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/347636.383378},
 doi = {http://doi.acm.org/10.1145/347636.383378},
 acmid = {383378},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alloy language, constraint solvers, detecting bugs, model checking, relational formulas, static analysis, testing},
} 

@inproceedings{Jackson:2000:FBC:347324.383378,
 author = {Jackson, Daniel and Vaziri, Mandana},
 title = {Finding bugs with a constraint solver},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/347324.383378},
 doi = {http://doi.acm.org/10.1145/347324.383378},
 acmid = {383378},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alloy language, constraint solvers, detecting bugs, model checking, relational formulas, static analysis, testing},
} 

@inproceedings{Lev-Ami:2000:PSA:347324.348031,
 author = {Lev-Ami, Tal and Reps, Thomas and Sagiv, Mooly and Wilhelm, Reinhard},
 title = {Putting static analysis to work for verification: A case study},
 abstract = {A method for finding bugs in code is presented. For given small numbers j and k, the code of a procedure is translated into a rela-tional formula whose models represent all execution traces that involve at most j heap cells and k loop iterations.  This formula is conjoined with the negation of the procedure's specification. The models of the resulting formula, obtained using a constraint solver, are counterexamples: executions of the code that violate the specification.The method can analyze millions of executions in seconds, and thus rapidly expose quite subtle flaws. It can accommodate calls to procedures for which specifications but no code is avail-able. A range of standard properties (such as absence of null pointer dereferences) can also be easily checked, using prede-fined specifications.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {26--38},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/347324.348031},
 doi = {http://doi.acm.org/10.1145/347324.348031},
 acmid = {348031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alloy language, constraint solvers, detecting bugs, model checking, relational formulas, static analysis, testing},
} 

@article{Lev-Ami:2000:PSA:347636.348031,
 author = {Lev-Ami, Tal and Reps, Thomas and Sagiv, Mooly and Wilhelm, Reinhard},
 title = {Putting static analysis to work for verification: A case study},
 abstract = {A method for finding bugs in code is presented. For given small numbers j and k, the code of a procedure is translated into a rela-tional formula whose models represent all execution traces that involve at most j heap cells and k loop iterations.  This formula is conjoined with the negation of the procedure's specification. The models of the resulting formula, obtained using a constraint solver, are counterexamples: executions of the code that violate the specification.The method can analyze millions of executions in seconds, and thus rapidly expose quite subtle flaws. It can accommodate calls to procedures for which specifications but no code is avail-able. A range of standard properties (such as absence of null pointer dereferences) can also be easily checked, using prede-fined specifications.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {26--38},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/347636.348031},
 doi = {http://doi.acm.org/10.1145/347636.348031},
 acmid = {348031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alloy language, constraint solvers, detecting bugs, model checking, relational formulas, static analysis, testing},
} 

@inproceedings{Buy:2000:ATC:347324.348870,
 author = {Buy, Ugo and Orso, Alessandro and Pezze, Mauro},
 title = {Automated Testing of Classes},
 abstract = {Programs developed with object technologies have unique features  that often make traditional testing methods inadequate.  Consider,  for instance, the dependence between the state of an object and the  behavior of that object: The outcome of a method executed by an  object often depends on the state of the object when the method is  invoked.  It is therefore crucial that techniques for testing of  classes exercise class methods when the method's receiver is in  different states.  The state of an object at any given time depends  on the sequence of messages received by the object up to that time.  Thus, methods for testing object-oriented software should identify  sequences of method invocations that are likely to uncover potential  defects in the code under test.  However, testing methods for  traditional software do not provide this kind of information.In this paper, we use data flow analysis, symbolic execution, and  automated deduction to produce sequences of method invocations  exercising a class under test.  Since the static analysis techniques  that we use are applied to different subproblems, the method proposed in  this paper can automatically generate information relevant to testing  even when symbolic execution and automated deduction cannot be completed  successfully.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347324.348870},
 doi = {http://doi.acm.org/10.1145/347324.348870},
 acmid = {348870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class testing, data flow analysis, symbolic execution, testing and analysis, testing object-oriented software},
} 

@article{Buy:2000:ATC:347636.348870,
 author = {Buy, Ugo and Orso, Alessandro and Pezze, Mauro},
 title = {Automated Testing of Classes},
 abstract = {Programs developed with object technologies have unique features  that often make traditional testing methods inadequate.  Consider,  for instance, the dependence between the state of an object and the  behavior of that object: The outcome of a method executed by an  object often depends on the state of the object when the method is  invoked.  It is therefore crucial that techniques for testing of  classes exercise class methods when the method's receiver is in  different states.  The state of an object at any given time depends  on the sequence of messages received by the object up to that time.  Thus, methods for testing object-oriented software should identify  sequences of method invocations that are likely to uncover potential  defects in the code under test.  However, testing methods for  traditional software do not provide this kind of information.In this paper, we use data flow analysis, symbolic execution, and  automated deduction to produce sequences of method invocations  exercising a class under test.  Since the static analysis techniques  that we use are applied to different subproblems, the method proposed in  this paper can automatically generate information relevant to testing  even when symbolic execution and automated deduction cannot be completed  successfully.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347636.348870},
 doi = {http://doi.acm.org/10.1145/347636.348870},
 acmid = {348870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class testing, data flow analysis, symbolic execution, testing and analysis, testing object-oriented software},
} 

@inproceedings{Souter:2000:OST:347324.348871,
 author = {Souter, Amie L. and Pollock, Lori L.},
 title = {OMEN: A strategy for testing object-oriented software},
 abstract = {This paper presents a strategy for structural testing of object-oriented software systems with possibly unknown clients and unknown information about invoked methods.  By exploiting the combined points-to and escape analysis developed for compiler optimization, our testing paradigm does not require a whole program representation to be in memory simultaneously for testing analysis. Potential effects from outside the component under test are easily identified and reported to the tester.  As client and server methods become known, the graph representation of object relationships is  easily extended, allowing the computation of test tuples to be  performed in a demand-driven manner, without requiring unnecessary  computation of test tuples based on predictions of potential  clients.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {49--59},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348871},
 doi = {http://doi.acm.org/10.1145/347324.348871},
 acmid = {348871},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Souter:2000:OST:347636.348871,
 author = {Souter, Amie L. and Pollock, Lori L.},
 title = {OMEN: A strategy for testing object-oriented software},
 abstract = {This paper presents a strategy for structural testing of object-oriented software systems with possibly unknown clients and unknown information about invoked methods.  By exploiting the combined points-to and escape analysis developed for compiler optimization, our testing paradigm does not require a whole program representation to be in memory simultaneously for testing analysis. Potential effects from outside the component under test are easily identified and reported to the tester.  As client and server methods become known, the graph representation of object relationships is  easily extended, allowing the computation of test tuples to be  performed in a demand-driven manner, without requiring unnecessary  computation of test tuples based on predictions of potential  clients.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {49--59},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348871},
 doi = {http://doi.acm.org/10.1145/347636.348871},
 acmid = {348871},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hartmann:2000:UIT:347636.348872,
 author = {Hartmann, Jean and Imoberdorf, Claudio and Meisinger, Michael},
 title = {UML-Based integration testing},
 abstract = {Increasing numbers of software developers are using the Unified Modeling Language (UML) and associated visual modeling tools as a basis for the design and implementation of their distributed, component-based applications. At the same time, it is necessary to test these components, especially during unit and integration testing.At Siemens Corporate Research, we have addressed the issue of testing components by integrating test generation and test execution technology with commercial UML modeling tools such as Rational Rose; the goal being a design-based testing environment. In order to generate test cases automatically, developers first define the dynamic behavior of their components via UML Statecharts, specify the interactions amongst them and finally annotate them with test requirements. Test cases are then derived from these annotated Statecharts using our test generation engine and executed with the help of our test execution tool. The latter tool was developed specifically for interfacing to components based on COM/DCOM and CORBA middleware.In this paper, we present our approach to modeling components and their interactions, describe how test cases are derived from these component models and then executed to verify their conformant behavior. We outline the implementation strategy of our TnT environment and use it to evaluate our approach by means of a simple example.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {60--70},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348872},
 doi = {http://doi.acm.org/10.1145/347636.348872},
 acmid = {348872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {COM/DCOM, CORBA, UML statecharts, distributed components, functional testing, test execution, test generation},
} 

@inproceedings{Hartmann:2000:UIT:347324.348872,
 author = {Hartmann, Jean and Imoberdorf, Claudio and Meisinger, Michael},
 title = {UML-Based integration testing},
 abstract = {Increasing numbers of software developers are using the Unified Modeling Language (UML) and associated visual modeling tools as a basis for the design and implementation of their distributed, component-based applications. At the same time, it is necessary to test these components, especially during unit and integration testing.At Siemens Corporate Research, we have addressed the issue of testing components by integrating test generation and test execution technology with commercial UML modeling tools such as Rational Rose; the goal being a design-based testing environment. In order to generate test cases automatically, developers first define the dynamic behavior of their components via UML Statecharts, specify the interactions amongst them and finally annotate them with test requirements. Test cases are then derived from these annotated Statecharts using our test generation engine and executed with the help of our test execution tool. The latter tool was developed specifically for interfacing to components based on COM/DCOM and CORBA middleware.In this paper, we present our approach to modeling components and their interactions, describe how test cases are derived from these component models and then executed to verify their conformant behavior. We outline the implementation strategy of our TnT environment and use it to evaluate our approach by means of a simple example.},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {60--70},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348872},
 doi = {http://doi.acm.org/10.1145/347324.348872},
 acmid = {348872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {COM/DCOM, CORBA, UML statecharts, distributed components, functional testing, test execution, test generation},
} 

@article{Hamlet:2000:STP:347636.348873,
 author = {Hamlet, Dick},
 title = {On subdomains: Testing, profiles, and components},
 abstract = {Subdomains of a program's input space are a concept around which ideas about testing can be organized. This paper considers the questions, ``What are the best subdomains for:testing to detecting failuresdefining operational profilesmeasuring component reliability?},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {71--76},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/347636.348873},
 doi = {http://doi.acm.org/10.1145/347636.348873},
 acmid = {348873},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hamlet:2000:STP:347324.348873,
 author = {Hamlet, Dick},
 title = {On subdomains: Testing, profiles, and components},
 abstract = {Subdomains of a program's input space are a concept around which ideas about testing can be organized. This paper considers the questions, ``What are the best subdomains for:testing to detecting failuresdefining operational profilesmeasuring component reliability?},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {71--76},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/347324.348873},
 doi = {http://doi.acm.org/10.1145/347324.348873},
 acmid = {348873},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peters:2000:RMR:347636.348874,
 author = {Peters, Dennis K. and Parnas, David L.},
 title = {Requirements-based monitors for real-time systems},
 abstract = {Before designing safety- or mission-critical real-time systems, a specification of the required behaviour of the system should be produced and reviewed by domain experts. After the system has been implemented, it should be thoroughly tested to ensure that it behaves correctly. This is best done using a monitor, a system that observes the behaviour of a target system and reports if that behaviour is consistent with the requirements. Such a monitor can be used both as an oracle during testing and as a supervisor during operation.  Monitors should be based on the documented requirements of the system.If the target system is required to monitor or control real-valued quantities, then the requirements, which are expressed in terms of the monitored and controlled quantities, will allow a range of behaviours to account for errors and imprecision in observation and control of these quantities. Even if the controlled variables are discrete valued, the requirements must specify the timing tolerance. Because of the limitations of the devices used by the monitor to observe the environmental quantities, there is unavoidable potential for false reports, both negative and positive.This paper discusses design of monitors for real-time systems, and examines the conditions under which a monitor will produce false reports. We describe the conclusions that can be drawn when using a monitor to observe system behaviour.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {77--85},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/347636.348874},
 doi = {http://doi.acm.org/10.1145/347636.348874},
 acmid = {348874},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, real-time system, supervisor, test oracle},
} 

@inproceedings{Peters:2000:RMR:347324.348874,
 author = {Peters, Dennis K. and Parnas, David L.},
 title = {Requirements-based monitors for real-time systems},
 abstract = {Before designing safety- or mission-critical real-time systems, a specification of the required behaviour of the system should be produced and reviewed by domain experts. After the system has been implemented, it should be thoroughly tested to ensure that it behaves correctly. This is best done using a monitor, a system that observes the behaviour of a target system and reports if that behaviour is consistent with the requirements. Such a monitor can be used both as an oracle during testing and as a supervisor during operation.  Monitors should be based on the documented requirements of the system.If the target system is required to monitor or control real-valued quantities, then the requirements, which are expressed in terms of the monitored and controlled quantities, will allow a range of behaviours to account for errors and imprecision in observation and control of these quantities. Even if the controlled variables are discrete valued, the requirements must specify the timing tolerance. Because of the limitations of the devices used by the monitor to observe the environmental quantities, there is unavoidable potential for false reports, both negative and positive.This paper discusses design of monitors for real-time systems, and examines the conditions under which a monitor will produce false reports. We describe the conclusions that can be drawn when using a monitor to observe system behaviour.},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {77--85},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/347324.348874},
 doi = {http://doi.acm.org/10.1145/347324.348874},
 acmid = {348874},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, real-time system, supervisor, test oracle},
} 

@inproceedings{Kolano:2000:CSA:347324.348875,
 author = {Kolano, Paul Z. and Demmerer, Richard A.},
 title = {Classification schemes to aid in the analysis of real-time systems},
 abstract = {This paper presents three sets of classification schemes for processes, properties, and transitions that can be used to assist in the analysis of real-time systems.  These classification schemes are discussed in the context of ASTRAL, which is a formal specification language for real-time systems.  Eight testbed systems were specified in ASTRAL, and their proofs were performed to determine proof patterns that occur most often.  The specifications were then examined in an attempt to derive specific characteristics that could be used to statically identify each pattern within a specification.  Once the classifications were obtained, they were then used to provide systematic guidance for analyzing real-time systems by directing the prover to the proof techniques most applicable to each proof pattern.  This paper presents the set of classification schemes that were developed and discusses how they can be used to assist the proof process.},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347324.348875},
 doi = {http://doi.acm.org/10.1145/347324.348875},
 acmid = {348875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASTRAL, analysis guidance, formal methods, formal specification and verification, real-time systems, system classification, timing requirements},
} 

@article{Kolano:2000:CSA:347636.348875,
 author = {Kolano, Paul Z. and Demmerer, Richard A.},
 title = {Classification schemes to aid in the analysis of real-time systems},
 abstract = {This paper presents three sets of classification schemes for processes, properties, and transitions that can be used to assist in the analysis of real-time systems.  These classification schemes are discussed in the context of ASTRAL, which is a formal specification language for real-time systems.  Eight testbed systems were specified in ASTRAL, and their proofs were performed to determine proof patterns that occur most often.  The specifications were then examined in an attempt to derive specific characteristics that could be used to statically identify each pattern within a specification.  Once the classifications were obtained, they were then used to provide systematic guidance for analyzing real-time systems by directing the prover to the proof techniques most applicable to each proof pattern.  This paper presents the set of classification schemes that were developed and discusses how they can be used to assist the proof process.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347636.348875},
 doi = {http://doi.acm.org/10.1145/347636.348875},
 acmid = {348875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASTRAL, analysis guidance, formal methods, formal specification and verification, real-time systems, system classification, timing requirements},
} 

@article{Cobleigh:2000:VPP:347636.348876,
 author = {Cobleigh, Jamieson M. and Clark, Lori A. and Osterweil, Leon J.},
 title = {Verifying properties of process definitions},
 abstract = {It seems important that the complex processes that synergize humans and computers to solve widening classes of societal problems be subjected to rigorous analysis.  One approach is to use a process definition language to specify these processes and to then use analysis techniques to evaluate these definitions for important correctness properties.  Because humans demand flexibility in their participation in complex processes, process definition languages must incorporate complicated control structures, such as various concurrency, choice, reactive control, and exception mechanisms.  The underlying complexity of these control abstractions, however, often confounds the users' intuitions as well as complicates any analysis.Thus, the control abstraction complexity in process definition languages presents analysis challenges beyond those posed by traditional programming languages.  This paper explores some of the difficulties of analyzing process definitions. We explore issues arising when applying the FLAVERS finite state verification system to processes written in the Little-JIL process definition language and illustrate these issues using a realistic auction example. Although we employ a particular process definition language and analysis technique, our results seem more generally applicable.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {96--101},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/347636.348876},
 doi = {http://doi.acm.org/10.1145/347636.348876},
 acmid = {348876},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cobleigh:2000:VPP:347324.348876,
 author = {Cobleigh, Jamieson M. and Clark, Lori A. and Osterweil, Leon J.},
 title = {Verifying properties of process definitions},
 abstract = {It seems important that the complex processes that synergize humans and computers to solve widening classes of societal problems be subjected to rigorous analysis.  One approach is to use a process definition language to specify these processes and to then use analysis techniques to evaluate these definitions for important correctness properties.  Because humans demand flexibility in their participation in complex processes, process definition languages must incorporate complicated control structures, such as various concurrency, choice, reactive control, and exception mechanisms.  The underlying complexity of these control abstractions, however, often confounds the users' intuitions as well as complicates any analysis.Thus, the control abstraction complexity in process definition languages presents analysis challenges beyond those posed by traditional programming languages.  This paper explores some of the difficulties of analyzing process definitions. We explore issues arising when applying the FLAVERS finite state verification system to processes written in the Little-JIL process definition language and illustrate these issues using a realistic auction example. Although we employ a particular process definition language and analysis technique, our results seem more generally applicable.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {96--101},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/347324.348876},
 doi = {http://doi.acm.org/10.1145/347324.348876},
 acmid = {348876},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Elbaum:2000:PTC:347636.348910,
 author = {Elbaum, Sebastian and Malishevsky, Alexey G. and Rothermel, Gregg},
 title = {Prioritizing test cases for regression testing},
 abstract = {Test case prioritization techniques schedule test cases in an order that increases their effectiveness in meeting some performance goal. One performance goal, rate of fault detection, is a measure of how quickly faults are detected within the testing process; an improved rate of fault detection can provide faster feedback on the system under test, and let software engineers begin locating and correcting faults earlier than might otherwise be possible. In previous work, we reported the results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: (1) can prioritization techniques be effective when aimed at specific modified versions; (2) what tradeoffs exist between fine granularity and coarse granularity prioritization techniques; (3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? This paper reports the results of new experiments addressing these questions.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348910},
 doi = {http://doi.acm.org/10.1145/347636.348910},
 acmid = {348910},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Elbaum:2000:PTC:347324.348910,
 author = {Elbaum, Sebastian and Malishevsky, Alexey G. and Rothermel, Gregg},
 title = {Prioritizing test cases for regression testing},
 abstract = {Test case prioritization techniques schedule test cases in an order that increases their effectiveness in meeting some performance goal. One performance goal, rate of fault detection, is a measure of how quickly faults are detected within the testing process; an improved rate of fault detection can provide faster feedback on the system under test, and let software engineers begin locating and correcting faults earlier than might otherwise be possible. In previous work, we reported the results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: (1) can prioritization techniques be effective when aimed at specific modified versions; (2) what tradeoffs exist between fine granularity and coarse granularity prioritization techniques; (3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? This paper reports the results of new experiments addressing these questions.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348910},
 doi = {http://doi.acm.org/10.1145/347324.348910},
 acmid = {348910},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hind:2000:PAI:347324.348916,
 author = {Hind, Michael and Pioli, Anthony},
 title = {Which pointer analysis should I use?},
 abstract = {During the past two decades many different pointer analysis algorithms have been published.  Although some descriptions include measurements of the effectiveness of the algorithm, qualitative comparisons among algorithms are difficult because of varying infrastructure, benchmarks, and performance metrics.  Without such comparisons it is not only difficult for an implementor to determine which pointer analysis is appropriate for their application, but also for a researcher to know which algorithms should be used as a basis for future advances.This paper describes an  empirical comparison of the effectiveness of five pointer analysis algorithms on C programs.  The algorithms vary in their use of control flow information (flow-sensitivity) and alias data structure, resulting in worst-case complexity from linear to polynomial.  The effectiveness of the analyses is quantified in terms of compile-time precision and efficiency.  In addition to measuring the direct effects of pointer analysis, precision is also reported by determining how the information computed by the five pointer analyses affects typical client analyses of pointer information: Mod/Ref analysis, live variable analysis and dead assignment identification, reaching definitions analysis, dependence analysis, and conditional constant propagation and unreachable code identification.  Efficiency is reported by measuring analysis time and memory consumption of the pointer analyses and their clients.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {113--123},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348916},
 doi = {http://doi.acm.org/10.1145/347324.348916},
 acmid = {348916},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, interprocedural pointer analysis},
} 

@article{Hind:2000:PAI:347636.348916,
 author = {Hind, Michael and Pioli, Anthony},
 title = {Which pointer analysis should I use?},
 abstract = {During the past two decades many different pointer analysis algorithms have been published.  Although some descriptions include measurements of the effectiveness of the algorithm, qualitative comparisons among algorithms are difficult because of varying infrastructure, benchmarks, and performance metrics.  Without such comparisons it is not only difficult for an implementor to determine which pointer analysis is appropriate for their application, but also for a researcher to know which algorithms should be used as a basis for future advances.This paper describes an  empirical comparison of the effectiveness of five pointer analysis algorithms on C programs.  The algorithms vary in their use of control flow information (flow-sensitivity) and alias data structure, resulting in worst-case complexity from linear to polynomial.  The effectiveness of the analyses is quantified in terms of compile-time precision and efficiency.  In addition to measuring the direct effects of pointer analysis, precision is also reported by determining how the information computed by the five pointer analyses affects typical client analyses of pointer information: Mod/Ref analysis, live variable analysis and dead assignment identification, reaching definitions analysis, dependence analysis, and conditional constant propagation and unreachable code identification.  Efficiency is reported by measuring analysis time and memory consumption of the pointer analyses and their clients.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {113--123},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348916},
 doi = {http://doi.acm.org/10.1145/347636.348916},
 acmid = {348916},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, interprocedural pointer analysis},
} 

@article{Frankl:2000:CDR:347636.348926,
 author = {Frankl, Phyllis G. and Deng, Yuetang},
 title = {Comparison of delivered reliability of branch, data flow and operational testing: A case study},
 abstract = {Many analytical and empirical studies of software testing effectiveness have used the probability that a test set exposes at least one fault as the measure of effectiveness. That measure is useful for evaluating testing techniques when the goal of testing is to gain confidence that the program is free from faults. However, if the goal of testing is to improve the reliability of the program (by discovering and removing those faults that are most likely to cause failures when the software is in the field) then the measure of test effectiveness must distinguish between those faults that are likely to cause failures and those that are unlikely to do so. Delivered reliability was previously introduced as a means of comparing testing techniques in that setting. This paper empirically compares reliability delivered by three testing techniques, branch testing, the all-uses data flow testing criterion, and operational testing. The subject program is a moderate-sized C-program (about 10,000 LOC) produced by professional programmers and containing naturally occurring faults.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {124--134},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348926},
 doi = {http://doi.acm.org/10.1145/347636.348926},
 acmid = {348926},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software reliability, software testing},
} 

@inproceedings{Frankl:2000:CDR:347324.348926,
 author = {Frankl, Phyllis G. and Deng, Yuetang},
 title = {Comparison of delivered reliability of branch, data flow and operational testing: A case study},
 abstract = {Many analytical and empirical studies of software testing effectiveness have used the probability that a test set exposes at least one fault as the measure of effectiveness. That measure is useful for evaluating testing techniques when the goal of testing is to gain confidence that the program is free from faults. However, if the goal of testing is to improve the reliability of the program (by discovering and removing those faults that are most likely to cause failures when the software is in the field) then the measure of test effectiveness must distinguish between those faults that are likely to cause failures and those that are unlikely to do so. Delivered reliability was previously introduced as a means of comparing testing techniques in that setting. This paper empirically compares reliability delivered by three testing techniques, branch testing, the all-uses data flow testing criterion, and operational testing. The subject program is a moderate-sized C-program (about 10,000 LOC) produced by professional programmers and containing naturally occurring faults.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {124--134},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348926},
 doi = {http://doi.acm.org/10.1145/347324.348926},
 acmid = {348926},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software reliability, software testing},
} 

@article{Hildebrandt:2000:SFI:347636.348938,
 author = {Hildebrandt, Ralf and Zeller, Andreas},
 title = {Simplifying failure-inducing input},
 abstract = {Given some test case, a program fails.  Which part of the test case  is responsible for the particular failure?  We show how our delta debugging algorithm generalizes and simplifies some  failing input to a minimal test case that produces the  failure.In a case study, the Mozilla web browser crashed after 95 user  actions.  Our prototype implementation automatically simplified the  input to 3 relevant user actions.  Likewise, it simplified 896~lines  of HTML to the single line that caused the failure.  The case study required 139 automated test runs, or 35 minutes on a 500 MHz PC.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348938},
 doi = {http://doi.acm.org/10.1145/347636.348938},
 acmid = {348938},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, combinatorial testing},
} 

@inproceedings{Hildebrandt:2000:SFI:347324.348938,
 author = {Hildebrandt, Ralf and Zeller, Andreas},
 title = {Simplifying failure-inducing input},
 abstract = {Given some test case, a program fails.  Which part of the test case  is responsible for the particular failure?  We show how our delta debugging algorithm generalizes and simplifies some  failing input to a minimal test case that produces the  failure.In a case study, the Mozilla web browser crashed after 95 user  actions.  Our prototype implementation automatically simplified the  input to 3 relevant user actions.  Likewise, it simplified 896~lines  of HTML to the single line that caused the failure.  The case study required 139 automated test runs, or 35 minutes on a 500 MHz PC.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348938},
 doi = {http://doi.acm.org/10.1145/347324.348938},
 acmid = {348938},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated debugging, combinatorial testing},
} 

@article{Clarke:2000:FSV:347636.348946,
 author = {Clarke, Lori A.},
 title = {Finite state verification (abstract only): An emerging technology for validating software systems},
 abstract = {Ever since formal verification was first proposed in the late sixties, the idea of being able to definitively determine if a program meets its specifications has been an appealing, but elusive, goal. Although verification systems based on theorem proving have improved considerably over the years, they are still inherently undecidable and require significant guidance from mathematically astute users. The human effort required for formal verification is so significant that it is usually only applied to the most critical software components.Alternative approaches to theorem proving based verification have also been under development for some time. These approaches usually restrict the problem domain in some way, such as focusing on hardware descriptions, communication protocols, or a limited specification language. These restrictions allow the problem to be solved by using reasoning algorithms that are guaranteed to terminate and by representing the problem with a finite state model, and thus these approaches have been called finite state verification. Systems based on these approaches are starting to be effectively applied to interesting software systems and there is increasing optimism that such approaches will become widely applicable.In this presentation, I will overview some of the different approaches to finite state verification. In particular I will describe symbolic model checking, integer necessary constraints, and incremental data flow analysis approaches. The strengths and weaknesses of these approaches will be described. In addition, I will outline the major challenges that must be addressed before finite state verification will become a common tool for the typical well-trained software engineer.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {146--146},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/347636.348946},
 doi = {http://doi.acm.org/10.1145/347636.348946},
 acmid = {348946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Clarke:2000:FSV:347324.348946,
 author = {Clarke, Lori A.},
 title = {Finite state verification (abstract only): An emerging technology for validating software systems},
 abstract = {Ever since formal verification was first proposed in the late sixties, the idea of being able to definitively determine if a program meets its specifications has been an appealing, but elusive, goal. Although verification systems based on theorem proving have improved considerably over the years, they are still inherently undecidable and require significant guidance from mathematically astute users. The human effort required for formal verification is so significant that it is usually only applied to the most critical software components.Alternative approaches to theorem proving based verification have also been under development for some time. These approaches usually restrict the problem domain in some way, such as focusing on hardware descriptions, communication protocols, or a limited specification language. These restrictions allow the problem to be solved by using reasoning algorithms that are guaranteed to terminate and by representing the problem with a finite state model, and thus these approaches have been called finite state verification. Systems based on these approaches are starting to be effectively applied to interesting software systems and there is increasing optimism that such approaches will become widely applicable.In this presentation, I will overview some of the different approaches to finite state verification. In particular I will describe symbolic model checking, integer necessary constraints, and incremental data flow analysis approaches. The strengths and weaknesses of these approaches will be described. In addition, I will outline the major challenges that must be addressed before finite state verification will become a common tool for the typical well-trained software engineer.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {146--146},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/347324.348946},
 doi = {http://doi.acm.org/10.1145/347324.348946},
 acmid = {348946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chays:2000:FTD:347636.348954,
 author = {Chays, David and Dan, Saikat and Frankl, Phyllis G. and Vokolos, Filippos I. and Weber, Elaine J.},
 title = {A framework for testing database applications},
 abstract = {Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. This paper discusses issues arising in testing database systems and presents an approach to testing database applications. In testing such applications, the state of the database before and after the user's operation plays an important role, along with the user's input and the system output. A tool for populating the database with meaningful data that satisfy database constraints has been prototyped. Its design and its role in a larger database application testing tool set are discussed.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.348954},
 doi = {http://doi.acm.org/10.1145/347636.348954},
 acmid = {348954},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {database, software testing, test data},
} 

@inproceedings{Chays:2000:FTD:347324.348954,
 author = {Chays, David and Dan, Saikat and Frankl, Phyllis G. and Vokolos, Filippos I. and Weber, Elaine J.},
 title = {A framework for testing database applications},
 abstract = {Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. This paper discusses issues arising in testing database systems and presents an approach to testing database applications. In testing such applications, the state of the database before and after the user's operation plays an important role, along with the user's input and the system output. A tool for populating the database with meaningful data that satisfy database constraints has been prototyped. Its design and its role in a larger database application testing tool set are discussed.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.348954},
 doi = {http://doi.acm.org/10.1145/347324.348954},
 acmid = {348954},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {database, software testing, test data},
} 

@article{Steven:2000:JCT:347636.348993,
 author = {Steven, John and Chandra, Pravir and Fleck, Bob and Podgurski, Andy},
 title = {jRapture: A Capture/Replay tool for observation-based testing},
 abstract = {We describe the design of jRapture: a tool for capturing and replaying Java program executions in the field. jRapture works with Java binaries (byte code) and any compliant implementation of the Java virtual machine. It employs a lightweight, transparent capture process that permits unobtrusive capture of a Java programs executions. jRapture captures interactions between a Java program and the system, including GUI, file, and console inputs, among other types, and on replay it presents each thread with exactly the same input sequence it saw during capture. In addition, jRapture has a profiling interface that permits a Java program to be instrumented for profiling \&#243; after its executions have been captured. Using an XML-based profiling specification language a tester can specify various forms of profiling to be carried out during replay.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {158--167},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347636.348993},
 doi = {http://doi.acm.org/10.1145/347636.348993},
 acmid = {348993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, capture/replay, execution profiling, observation-based testing, software testing},
} 

@inproceedings{Steven:2000:JCT:347324.348993,
 author = {Steven, John and Chandra, Pravir and Fleck, Bob and Podgurski, Andy},
 title = {jRapture: A Capture/Replay tool for observation-based testing},
 abstract = {We describe the design of jRapture: a tool for capturing and replaying Java program executions in the field. jRapture works with Java binaries (byte code) and any compliant implementation of the Java virtual machine. It employs a lightweight, transparent capture process that permits unobtrusive capture of a Java programs executions. jRapture captures interactions between a Java program and the system, including GUI, file, and console inputs, among other types, and on replay it presents each thread with exactly the same input sequence it saw during capture. In addition, jRapture has a profiling interface that permits a Java program to be instrumented for profiling \&#243; after its executions have been captured. Using an XML-based profiling specification language a tester can specify various forms of profiling to be carried out during replay.},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {158--167},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347324.348993},
 doi = {http://doi.acm.org/10.1145/347324.348993},
 acmid = {348993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, capture/replay, execution profiling, observation-based testing, software testing},
} 

@article{Woodward:2000:TFS:347636.349016,
 author = {Woodward, Martin R. and Al-Khanjari, Zuhoor A.},
 title = {Testability, fault size and the domain-to-range ratio: An eternal triangle},
 abstract = {A number of different concepts have been proposed that, loosely speaking, revolve around the notion of software testability. Indeed, the concept of testability itself has been interpreted in a variety of ways by the software community. One interpretation is concerned with the extent of the modifications a program component requires, in terms of its input and output variables, so that the entire behaviour of the component is observable and controllable. Another interpretation is the ease with which faults, if present in a program, can be revealed by the testing process and the propagation, infection and execution (PIE) model has been proposed as a method of estimating this. It has been suggested that this particular interpretation of testability might be linked with the metric domain-to-range ratio (DRR), i.e. the ratio of the cardinality of the set of all inputs (the domain) to the cardinality of the set of all outputs (the range). This paper reports work in progress exploring some of the connections between the concepts mentioned. In particular, a simple mathematical link is established between domain-to-range ratio and the observability and controllability aspects of testability. In addition, the PIE model is re-considered and a relationship with fault size is observed. This leads to the suggestion that it might be more straightforward to estimate PIE testability by an adaptation of traditional mutation analysis. The latter suggestion exemplifies the main goals of the work described here, namely to seek greater understanding of testability in general and, ultimately, to find easier ways of determining it.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {168--172},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/347636.349016},
 doi = {http://doi.acm.org/10.1145/347636.349016},
 acmid = {349016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {controllability, domain-to-range ratio, fault size, observability, testability},
} 

@inproceedings{Woodward:2000:TFS:347324.349016,
 author = {Woodward, Martin R. and Al-Khanjari, Zuhoor A.},
 title = {Testability, fault size and the domain-to-range ratio: An eternal triangle},
 abstract = {A number of different concepts have been proposed that, loosely speaking, revolve around the notion of software testability. Indeed, the concept of testability itself has been interpreted in a variety of ways by the software community. One interpretation is concerned with the extent of the modifications a program component requires, in terms of its input and output variables, so that the entire behaviour of the component is observable and controllable. Another interpretation is the ease with which faults, if present in a program, can be revealed by the testing process and the propagation, infection and execution (PIE) model has been proposed as a method of estimating this. It has been suggested that this particular interpretation of testability might be linked with the metric domain-to-range ratio (DRR), i.e. the ratio of the cardinality of the set of all inputs (the domain) to the cardinality of the set of all outputs (the range). This paper reports work in progress exploring some of the connections between the concepts mentioned. In particular, a simple mathematical link is established between domain-to-range ratio and the observability and controllability aspects of testability. In addition, the PIE model is re-considered and a relationship with fault size is observed. This leads to the suggestion that it might be more straightforward to estimate PIE testability by an adaptation of traditional mutation analysis. The latter suggestion exemplifies the main goals of the work described here, namely to seek greater understanding of testability in general and, ultimately, to find easier ways of determining it.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {168--172},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/347324.349016},
 doi = {http://doi.acm.org/10.1145/347324.349016},
 acmid = {349016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {controllability, domain-to-range ratio, fault size, observability, testability},
} 

@article{Schroeder:2000:BTR:347636.349042,
 author = {Schroeder, Patrick J. and Korel, Bogdan},
 title = {Black-box test reduction using input-output analysis},
 abstract = {Test reduction is an important issue in black-box testing.  The number of possible black-box tests for any non-trivial software application is extremely large.  For the class of programs with multiple inputs and outputs, the number of possible tests grows very rapidly as combinations of input test data are considered.  In this paper, we introduce an approach to test reduction that uses automated input-output analysis to identify relationships between program inputs and outputs.  Our initial experience with the approach has shown that it can significantly reduce the number of black-box tests.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {173--177},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/347636.349042},
 doi = {http://doi.acm.org/10.1145/347636.349042},
 acmid = {349042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black-box testing, combinatorial testing, input-output analysis, test reduction},
} 

@inproceedings{Schroeder:2000:BTR:347324.349042,
 author = {Schroeder, Patrick J. and Korel, Bogdan},
 title = {Black-box test reduction using input-output analysis},
 abstract = {Test reduction is an important issue in black-box testing.  The number of possible black-box tests for any non-trivial software application is extremely large.  For the class of programs with multiple inputs and outputs, the number of possible tests grows very rapidly as combinations of input test data are considered.  In this paper, we introduce an approach to test reduction that uses automated input-output analysis to identify relationships between program inputs and outputs.  Our initial experience with the approach has shown that it can significantly reduce the number of black-box tests.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {173--177},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/347324.349042},
 doi = {http://doi.acm.org/10.1145/347324.349042},
 acmid = {349042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black-box testing, combinatorial testing, input-output analysis, test reduction},
} 

@inproceedings{Wittenberg:2000:PTC:347324.349099,
 author = {Wittenberg, Craig H.},
 title = {Progress in testing component-based software (abstract only)},
 abstract = {Software components enable practical reuse of software parts and amortization of investments over multiple applications. Each part or component is well defined and independently deployable. Composition is the key technique by which systems of software components are constructed. The composition step can be done before or after the delivery of the system. It is this late composition (or at least the possibility of it) which yields the greatest challenges from a testing standpoint. That is, a component-based application may be composed out of parts that were never tested together. Thus the most useful and reliable parts are those which have been tested independently in as many ways as possible.The Component Applications Group in Microsoft Research is developing tools,   techniques, and a large component library to enable the development of sophisticated office, home and web-based applications. For the past three and a half years we have been working on two main efforts. First, we have created a prototype of a highly factored (i.e., customizable, flexible, etc.) architecture for the construction of the UI of applications. Our work can be applied to traditional window-ed applications as well as to the look and feel of Web applications. During this effort we have developed a variety of design techniques, two different composition mechanisms, a visual tool for compositions, and have built several application prototypes out of the same set of components.Most of our time has been spent on tools and techniques for building reliable components.   Certain pieces of our infrastructure formed the domain in which we tried out our ideas. The first component we tested was one of our composition mechanisms. That was followed by the testing of a dynamic, binary, aspect composition mechanism and of a particularly generic implementation of collection classes. Our well-factored, versioned build system will also be described. All of the results of our work are compatible with COM.The talk will focus on our key lessons in composition, specification, processes, and tools with a particular emphasis on our test harness and our results in testing. A discussion of the last few bugs found in each of several projects should prove intersting. Some comparisons will be made with other projects inside and outside Microsoft. Since we can only claim progress, not perfection, there are still many areas for further research. As an example, we are looking at ways we can use language annotations to simplifying whole classes of problems (e.g., tests for reentrancy). One of the points here is that we can improve our ability to create reliable components by improving the languages used to implement them (like Java has popularized the use of a garbage collector). Another example is that we hope to improve the automation of the sequencing of test cases.Finally, as a tribute to the power of standing on other's shoulders, many of the roots of our ideas will be traced to techniques published elsewhere. You might say we only composed together many already good ideas. Our group includes people who developed COM itself (myself and Tony Williams), many people from within Microsoft who have delivered successful component-based products (e.g., in Visual Studio), and world-renowned component-ologist (:-) Clemens Szyperski who wrote Component Software: Beyond Object-Oriented Programming.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {178--},
 url = {http://doi.acm.org/10.1145/347324.349099},
 doi = {http://doi.acm.org/10.1145/347324.349099},
 acmid = {349099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wittenberg:2000:PTC:347636.349099,
 author = {Wittenberg, Craig H.},
 title = {Progress in testing component-based software (abstract only)},
 abstract = {Software components enable practical reuse of software parts and amortization of investments over multiple applications. Each part or component is well defined and independently deployable. Composition is the key technique by which systems of software components are constructed. The composition step can be done before or after the delivery of the system. It is this late composition (or at least the possibility of it) which yields the greatest challenges from a testing standpoint. That is, a component-based application may be composed out of parts that were never tested together. Thus the most useful and reliable parts are those which have been tested independently in as many ways as possible.The Component Applications Group in Microsoft Research is developing tools,   techniques, and a large component library to enable the development of sophisticated office, home and web-based applications. For the past three and a half years we have been working on two main efforts. First, we have created a prototype of a highly factored (i.e., customizable, flexible, etc.) architecture for the construction of the UI of applications. Our work can be applied to traditional window-ed applications as well as to the look and feel of Web applications. During this effort we have developed a variety of design techniques, two different composition mechanisms, a visual tool for compositions, and have built several application prototypes out of the same set of components.Most of our time has been spent on tools and techniques for building reliable components.   Certain pieces of our infrastructure formed the domain in which we tried out our ideas. The first component we tested was one of our composition mechanisms. That was followed by the testing of a dynamic, binary, aspect composition mechanism and of a particularly generic implementation of collection classes. Our well-factored, versioned build system will also be described. All of the results of our work are compatible with COM.The talk will focus on our key lessons in composition, specification, processes, and tools with a particular emphasis on our test harness and our results in testing. A discussion of the last few bugs found in each of several projects should prove intersting. Some comparisons will be made with other projects inside and outside Microsoft. Since we can only claim progress, not perfection, there are still many areas for further research. As an example, we are looking at ways we can use language annotations to simplifying whole classes of problems (e.g., tests for reentrancy). One of the points here is that we can improve our ability to create reliable components by improving the languages used to implement them (like Java has popularized the use of a garbage collector). Another example is that we hope to improve the automation of the sequencing of test cases.Finally, as a tribute to the power of standing on other's shoulders, many of the roots of our ideas will be traced to techniques published elsewhere. You might say we only composed together many already good ideas. Our group includes people who developed COM itself (myself and Tony Williams), many people from within Microsoft who have delivered successful component-based products (e.g., in Visual Studio), and world-renowned component-ologist (:-) Clemens Szyperski who wrote Component Software: Beyond Object-Oriented Programming.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {178--},
 url = {http://doi.acm.org/10.1145/347636.349099},
 doi = {http://doi.acm.org/10.1145/347636.349099},
 acmid = {349099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dill:2000:MCJ:347636.349113,
 author = {Dill, David},
 title = {Model checking Java programs (abstract only)},
 abstract = {Automatic state exploration tools (model checkers) have had some success when applied to protocols and hardware designs, but there are fewer success stories about software. This is unfortunate, since the software problem is worsening even faster than the hardware and protocol problems. Model checking of concurrent programs is especially interesting, because they are notoriously difficult to test, analyze, and debug by other methods.This talk will be a description of our initial efforts to check Java programs using a model checker. The model checker supports dynamic allocation, thread creation, and recursive procedures (features that are not necessary for hardware verification), and has some special optimizations and checks tailored to multi-threaded Java program. I will also discuss some of the challenges for future efforts in this area.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {179--},
 url = {http://doi.acm.org/10.1145/347636.349113},
 doi = {http://doi.acm.org/10.1145/347636.349113},
 acmid = {349113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dill:2000:MCJ:347324.349113,
 author = {Dill, David},
 title = {Model checking Java programs (abstract only)},
 abstract = {Automatic state exploration tools (model checkers) have had some success when applied to protocols and hardware designs, but there are fewer success stories about software. This is unfortunate, since the software problem is worsening even faster than the hardware and protocol problems. Model checking of concurrent programs is especially interesting, because they are notoriously difficult to test, analyze, and debug by other methods.This talk will be a description of our initial efforts to check Java programs using a model checker. The model checker supports dynamic allocation, thread creation, and recursive procedures (features that are not necessary for hardware verification), and has some special optimizations and checks tailored to multi-threaded Java program. I will also discuss some of the challenges for future efforts in this area.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {179--},
 url = {http://doi.acm.org/10.1145/347324.349113},
 doi = {http://doi.acm.org/10.1145/347324.349113},
 acmid = {349113},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nanda:2000:SCP:347636.349121,
 author = {Nanda, Mangala Gowri and Ramesh, S.},
 title = {Slicing concurrent programs},
 abstract = {
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.349121},
 doi = {http://doi.acm.org/10.1145/347636.349121},
 acmid = {349121},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Static program slicing, concurrency, data dependence},
} 

@inproceedings{Nanda:2000:SCP:347324.349121,
 author = {Nanda, Mangala Gowri and Ramesh, S.},
 title = {Slicing concurrent programs},
 abstract = {
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.349121},
 doi = {http://doi.acm.org/10.1145/347324.349121},
 acmid = {349121},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Static program slicing, concurrency, data dependence},
} 

@inproceedings{Siegel:2000:IPI:347324.349130,
 author = {Siegel, Stephen F. and Avrunin, George S.},
 title = {Improving the precision of INCA by preventing spurious cycles},
 abstract = {The Inequality Necessary Condition Analyzer (INCA) is a finite-state  verification tool that has been able to check properties of some  very large concurrent systems. INCA checks a property of a  concurrent system by generating a system of inequalities that must  have integer solutions if the property can be violated. There may,  however, be integer solutions to the inequalities that do not  correspond to an execution violating the property. INCA thus accepts  the possibility of an inconclusive result in exchange for greater  tractability. We describe here a method for eliminating one of the two main sources of these inconclusive results.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {191--200},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347324.349130},
 doi = {http://doi.acm.org/10.1145/347324.349130},
 acmid = {349130},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {INCA, cycles, finite-state verification, integer programming},
} 

@article{Siegel:2000:IPI:347636.349130,
 author = {Siegel, Stephen F. and Avrunin, George S.},
 title = {Improving the precision of INCA by preventing spurious cycles},
 abstract = {The Inequality Necessary Condition Analyzer (INCA) is a finite-state  verification tool that has been able to check properties of some  very large concurrent systems. INCA checks a property of a  concurrent system by generating a system of inequalities that must  have integer solutions if the property can be violated. There may,  however, be integer solutions to the inequalities that do not  correspond to an execution violating the property. INCA thus accepts  the possibility of an inconclusive result in exchange for greater  tractability. We describe here a method for eliminating one of the two main sources of these inconclusive results.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {191--200},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/347636.349130},
 doi = {http://doi.acm.org/10.1145/347636.349130},
 acmid = {349130},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {INCA, cycles, finite-state verification, integer programming},
} 

@inproceedings{Schulz:2000:TDO:347324.349141,
 author = {Schulz, Daniel and Mueller, Frank},
 title = {A thread-aware debugger with an open interface},
 abstract = {While threads have become an accepted and standardized model for expressing concurrency and exploiting parallelism for the shared-memory model, debugging threads is still poorly supported. This paper identifies challenges in debugging threads and offers solutions to them.  The contributions of this paper are threefold. First, an open interface for debugging as an extension to thread implementations is proposed. Second, extensions for thread-aware debugging are identified and implemented within the Gnu Debugger to provide additional features beyond the scope of existing debuggers. Third, an active debugging framework is proposed that includes a language-independent protocol to communicate between debugger and application via relational queries ensuring that the enhancements of the debugger are independent of actual thread implementations. Partial or complete implementations of the interface for debugging can be added to thread implementations to work in unison with the enhanced debugger without any modifications to the debugger itself. Sample implementations of the interface for debugging have shown its adequacy for user-level threads, kernel threads and mixed thread implementations while providing extended debugging functionality at improved efficiency and portability at the same time.
},
 booktitle = {Proceedings of the 2000 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '00},
 year = {2000},
 isbn = {1-58113-266-2},
 location = {Portland, Oregon, United States},
 pages = {201--211},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347324.349141},
 doi = {http://doi.acm.org/10.1145/347324.349141},
 acmid = {349141},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active debugging, concurrency, debugging, open interface, threads},
} 

@article{Schulz:2000:TDO:347636.349141,
 author = {Schulz, Daniel and Mueller, Frank},
 title = {A thread-aware debugger with an open interface},
 abstract = {While threads have become an accepted and standardized model for expressing concurrency and exploiting parallelism for the shared-memory model, debugging threads is still poorly supported. This paper identifies challenges in debugging threads and offers solutions to them.  The contributions of this paper are threefold. First, an open interface for debugging as an extension to thread implementations is proposed. Second, extensions for thread-aware debugging are identified and implemented within the Gnu Debugger to provide additional features beyond the scope of existing debuggers. Third, an active debugging framework is proposed that includes a language-independent protocol to communicate between debugger and application via relational queries ensuring that the enhancements of the debugger are independent of actual thread implementations. Partial or complete implementations of the interface for debugging can be added to thread implementations to work in unison with the enhanced debugger without any modifications to the debugger itself. Sample implementations of the interface for debugging have shown its adequacy for user-level threads, kernel threads and mixed thread implementations while providing extended debugging functionality at improved efficiency and portability at the same time.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {25},
 issue = {5},
 month = {August},
 year = {2000},
 issn = {0163-5948},
 pages = {201--211},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/347636.349141},
 doi = {http://doi.acm.org/10.1145/347636.349141},
 acmid = {349141},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active debugging, concurrency, debugging, open interface, threads},
} 

@inproceedings{Corbett:1998:CCM:271771.271778,
 author = {Corbett, James C.},
 title = {Constructing compact models of concurrent Java programs},
 abstract = {Finite-state verification technology (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. In this paper, we explore the extraction of compact concurrency models from Java code. In particular, we show how static pointer analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271778},
 doi = {http://doi.acm.org/10.1145/271771.271778},
 acmid = {271778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {finite-state verification, model extraction, static analysis},
} 

@article{Corbett:1998:CCM:271775.271778,
 author = {Corbett, James C.},
 title = {Constructing compact models of concurrent Java programs},
 abstract = {Finite-state verification technology (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. In this paper, we explore the extraction of compact concurrency models from Java code. In particular, we show how static pointer analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271778},
 doi = {http://doi.acm.org/10.1145/271775.271778},
 acmid = {271778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {finite-state verification, model extraction, static analysis},
} 

@article{Harrold:1998:CIC:271775.271780,
 author = {Harrold, Mary Jean and Rothermel, Gregg and Sinha, Saurabh},
 title = {Computation of interprocedural control dependence},
 abstract = {Program dependence information is useful for a variety of software testing and maintenance tasks. Properly defined, control and data dependencies can be used to identify semantic dependencies. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependencies: dependencies that exist because of interactions among procedures. Many techniques for computing data and control dependencies exist; however, in our search of the literature we find only one attempt to define and compute interprocedural control dependencies. Unfortunately, that approach can omit important control dependencies, and incorrectly identifies control dependencies for a large class of programs. This paper presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence, an efficient algorithm for calculating interprocedural control dependencies, and empirical results obtained by our implementation of the algorithm.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271780},
 doi = {http://doi.acm.org/10.1145/271775.271780},
 acmid = {271780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control and semantic dependence, interprocedual analysis},
} 

@inproceedings{Harrold:1998:CIC:271771.271780,
 author = {Harrold, Mary Jean and Rothermel, Gregg and Sinha, Saurabh},
 title = {Computation of interprocedural control dependence},
 abstract = {Program dependence information is useful for a variety of software testing and maintenance tasks. Properly defined, control and data dependencies can be used to identify semantic dependencies. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependencies: dependencies that exist because of interactions among procedures. Many techniques for computing data and control dependencies exist; however, in our search of the literature we find only one attempt to define and compute interprocedural control dependencies. Unfortunately, that approach can omit important control dependencies, and incorrectly identifies control dependencies for a large class of programs. This paper presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence, an efficient algorithm for calculating interprocedural control dependencies, and empirical results obtained by our implementation of the algorithm.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271780},
 doi = {http://doi.acm.org/10.1145/271771.271780},
 acmid = {271780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control and semantic dependence, interprocedual analysis},
} 

@article{Stocks:1998:CFC:271775.271782,
 author = {Stocks, Philip A. and Ryder, Barbara G. and Landi, William A. and Zhang, Sean},
 title = {Comparing flow and context sensitivity on the modification-side-effects problem},
 abstract = {Precision and scalability are two desirable, yet often conflicting, features of data-flow analyses. This paper reports on a case study of the modification---ide-effects problem for C in the presence of pointers from the perspective of contrasting the flow and context sensitivity of the solution procedure with respect to precision and scalability. The results show that the cost of precision of flow- and context-sensitive analysis is not always prohibitive, and that the precision of flow- and context-insensitive analysis is substantially better than worst-case estimates and can be sufficient for certain applications. Program characteristics that affect the performance of data-flow analysis are identified.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {21--31},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271775.271782},
 doi = {http://doi.acm.org/10.1145/271775.271782},
 acmid = {271782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context sensitivity, empirical study, flow sensitivity, interprocedual data-flow analysis, modification side effects, pointer aliasing},
} 

@inproceedings{Stocks:1998:CFC:271771.271782,
 author = {Stocks, Philip A. and Ryder, Barbara G. and Landi, William A. and Zhang, Sean},
 title = {Comparing flow and context sensitivity on the modification-side-effects problem},
 abstract = {Precision and scalability are two desirable, yet often conflicting, features of data-flow analyses. This paper reports on a case study of the modification---ide-effects problem for C in the presence of pointers from the perspective of contrasting the flow and context sensitivity of the solution procedure with respect to precision and scalability. The results show that the cost of precision of flow- and context-sensitive analysis is not always prohibitive, and that the precision of flow- and context-insensitive analysis is substantially better than worst-case estimates and can be sufficient for certain applications. Program characteristics that affect the performance of data-flow analysis are identified.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {21--31},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271771.271782},
 doi = {http://doi.acm.org/10.1145/271771.271782},
 acmid = {271782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context sensitivity, empirical study, flow sensitivity, interprocedual data-flow analysis, modification side effects, pointer aliasing},
} 

@article{Mitchell:1998:EER:271775.271784,
 author = {Mitchell, Brian and Zeil, Steven J.},
 title = {An experiment in estimating reliability growth under both representative and directed testing},
 abstract = {The Order Statistic model of reliability growth offers improved\&amp;bull; experimental design, and\&amp;bull; flexibility in testing methodologycompared to conventional reliability growth models. It permits prediction of operational reliability without requiring that testing be conducted according to the operation profile of the program input space.This paper presents the first experimental use of the Order Statistic model under a test plan that combines both representative and directed tests. Results suggest that this is an effective way to obtain quantified measures of test quality without abandoning the advantages of directed test methods.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {32--41},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271784},
 doi = {http://doi.acm.org/10.1145/271775.271784},
 acmid = {271784},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitchell:1998:EER:271771.271784,
 author = {Mitchell, Brian and Zeil, Steven J.},
 title = {An experiment in estimating reliability growth under both representative and directed testing},
 abstract = {The Order Statistic model of reliability growth offers improved\&amp;bull; experimental design, and\&amp;bull; flexibility in testing methodologycompared to conventional reliability growth models. It permits prediction of operational reliability without requiring that testing be conducted according to the operation profile of the program input space.This paper presents the first experimental use of the Order Statistic model under a test plan that combines both representative and directed tests. Results suggest that this is an effective way to obtain quantified measures of test quality without abandoning the advantages of directed test methods.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {32--41},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271784},
 doi = {http://doi.acm.org/10.1145/271771.271784},
 acmid = {271784},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ntafos:1998:RPT:271771.271785,
 author = {Ntafos, Simeon},
 title = {On random and partition testing},
 abstract = {There have been many comparisons of random and partition testing. Proportional partition testing has been suggested as the optimum way to perform partition testing. In this paper we show that this might not be so and discuss some of the problems with previous studies. We look at the expected cost of failures as a way to evaluate the effectiveness of testing strategies and use it to compare random testing, uniform partition testing and proportional partition testing. Also, we introduce partition testing strategies that try to take the cost of failures into account and present some results on their effectiveness.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {42--48},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/271771.271785},
 doi = {http://doi.acm.org/10.1145/271771.271785},
 acmid = {271785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {partition testing, program testing, random testing},
} 

@article{Ntafos:1998:RPT:271775.271785,
 author = {Ntafos, Simeon},
 title = {On random and partition testing},
 abstract = {There have been many comparisons of random and partition testing. Proportional partition testing has been suggested as the optimum way to perform partition testing. In this paper we show that this might not be so and discuss some of the problems with previous studies. We look at the expected cost of failures as a way to evaluate the effectiveness of testing strategies and use it to compare random testing, uniform partition testing and proportional partition testing. Also, we introduce partition testing strategies that try to take the cost of failures into account and present some results on their effectiveness.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {42--48},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/271775.271785},
 doi = {http://doi.acm.org/10.1145/271775.271785},
 acmid = {271785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {partition testing, program testing, random testing},
} 

@article{Hamlet:1998:MIP:271775.271787,
 author = {Hamlet, Richard and Kemmerer, Richard and Miller, Edward F. and Richardson, Debra J.},
 title = {The most influential papers from the ISSTA research community (panel)},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {49--},
 url = {http://doi.acm.org/10.1145/271775.271787},
 doi = {http://doi.acm.org/10.1145/271775.271787},
 acmid = {271787},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hamlet:1998:MIP:271771.271787,
 author = {Hamlet, Richard and Kemmerer, Richard and Miller, Edward F. and Richardson, Debra J.},
 title = {The most influential papers from the ISSTA research community (panel)},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {49--},
 url = {http://doi.acm.org/10.1145/271771.271787},
 doi = {http://doi.acm.org/10.1145/271771.271787},
 acmid = {271787},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hamlet:1998:WLT:271771.271788,
 author = {Hamlet, Dick},
 title = {What can we learn by testing a program?},
 abstract = {It is conventional to start research papers on software engineering, particularly on testing and software quality, with a statement of how important software has become in the world, and the potential dangers of using it when those who construct it really don't know what they are doing. The author then goes on to show that his or her theory/method/insights/tools will make the world safe (or safer, if the author is modest) by providing the understanding that is lacking. ISSTA authors (I among them) have started many of their papers in just this way, but speaking for myself, these statements are window dressing --- they disguise my real concern. Long before software was any part of the workaday world, before there was any "software problem" or even much software, I was interested in program analysis because programs are arguably the most intriguing mathematical objects people create. I have happily pursued the study of programs for over 30 years.I wrote my first program (in ALGOL 58) in 1962 --- it failed to properly calculate a table of values of the error integral, being somewhat off in the third significant figure. (I remember the failure, and the debugging, poring over a decimal memory dump. But I can't recall the fault.) It took me until the mid-1960s to recognize that programs per se</i> were much more interesting than their applications: I stumbled on Maurice Halstead's book [4] on self-compiling NELIAC compilers. Here was magical stuff: the master program defining a language, and itself written in that language! In my application to the University of Washington, I explained that I wanted to study computers "for themselves, not as they are used." Paul Klee, the topologist who was saddled with the task of replying to mathematics grad students that year, wrote back to assure me that "we've got a computer around here somewhere, and by the time you arrive I'm sure I can locate it." It was an IBM 7090, complete with IBSYS and FORTRAN, and who could have asked for software more in need of analysis?There were standards of respectability for a PhD dissertation even in those days, so I took up recursive function theory and the program-equivalence problem. Its application to testing is that we would like to know if the program under test differs from its functional specification --- that is, can it fail? Any understanding of programs through testing (a mechanical process) must come up against the program-equivalence problem: we cannot hope to gain full understanding, because to do so would be to solve the unsolvable problem. My dissertation was an exploration of the additional information (beyond the purely functional) needed to make the program-equivalence problem solvable [5]. What brought me to the first ISSTA in 1978 was Bill Howden's 1976 paper "Reliability of the path analysis strategy" [6].},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {50--52},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/271771.271788},
 doi = {http://doi.acm.org/10.1145/271771.271788},
 acmid = {271788},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hamlet:1998:WLT:271775.271788,
 author = {Hamlet, Dick},
 title = {What can we learn by testing a program?},
 abstract = {It is conventional to start research papers on software engineering, particularly on testing and software quality, with a statement of how important software has become in the world, and the potential dangers of using it when those who construct it really don't know what they are doing. The author then goes on to show that his or her theory/method/insights/tools will make the world safe (or safer, if the author is modest) by providing the understanding that is lacking. ISSTA authors (I among them) have started many of their papers in just this way, but speaking for myself, these statements are window dressing --- they disguise my real concern. Long before software was any part of the workaday world, before there was any "software problem" or even much software, I was interested in program analysis because programs are arguably the most intriguing mathematical objects people create. I have happily pursued the study of programs for over 30 years.I wrote my first program (in ALGOL 58) in 1962 --- it failed to properly calculate a table of values of the error integral, being somewhat off in the third significant figure. (I remember the failure, and the debugging, poring over a decimal memory dump. But I can't recall the fault.) It took me until the mid-1960s to recognize that programs per se</i> were much more interesting than their applications: I stumbled on Maurice Halstead's book [4] on self-compiling NELIAC compilers. Here was magical stuff: the master program defining a language, and itself written in that language! In my application to the University of Washington, I explained that I wanted to study computers "for themselves, not as they are used." Paul Klee, the topologist who was saddled with the task of replying to mathematics grad students that year, wrote back to assure me that "we've got a computer around here somewhere, and by the time you arrive I'm sure I can locate it." It was an IBM 7090, complete with IBSYS and FORTRAN, and who could have asked for software more in need of analysis?There were standards of respectability for a PhD dissertation even in those days, so I took up recursive function theory and the program-equivalence problem. Its application to testing is that we would like to know if the program under test differs from its functional specification --- that is, can it fail? Any understanding of programs through testing (a mechanical process) must come up against the program-equivalence problem: we cannot hope to gain full understanding, because to do so would be to solve the unsolvable problem. My dissertation was an exploration of the additional information (beyond the purely functional) needed to make the program-equivalence problem solvable [5]. What brought me to the first ISSTA in 1978 was Bill Howden's 1976 paper "Reliability of the path analysis strategy" [6].},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {50--52},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/271775.271788},
 doi = {http://doi.acm.org/10.1145/271775.271788},
 acmid = {271788},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gotlieb:1998:ATD:271771.271790,
 author = {Gotlieb, Arnaud and Botella, Bernard and Rueher, Michel},
 title = {Automatic test data generation using constraint solving techniques},
 abstract = {Automatic test data generation leads to identify input values on which a selected point in a procedure is executed. This paper introduces a new method for this problem based on constraint solving techniques. First, we statically transform a procedure into a constraint system by using well-known "Static Single Assignment" form and control-dependencies. Second, we solve this system to check whether at least one feasible control flow path going through the selected point exists and to generate test data that correspond to one of these paths.The key point of our approach is to take advantage of current advances in constraint techniques when solving the generated constraint system. Global constraints are used in a preliminary step to detect some of the non feasible paths. Partial consistency techniques are employed to reduce the domains of possible values of the test data. A prototype implementation has been developped on a restricted subset of the C language. Advantages of our approach are illustrated on a non-trivial example.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271790},
 doi = {http://doi.acm.org/10.1145/271771.271790},
 acmid = {271790},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test data generation, constraint solving techniques, global constraints, structural testing},
} 

@article{Gotlieb:1998:ATD:271775.271790,
 author = {Gotlieb, Arnaud and Botella, Bernard and Rueher, Michel},
 title = {Automatic test data generation using constraint solving techniques},
 abstract = {Automatic test data generation leads to identify input values on which a selected point in a procedure is executed. This paper introduces a new method for this problem based on constraint solving techniques. First, we statically transform a procedure into a constraint system by using well-known "Static Single Assignment" form and control-dependencies. Second, we solve this system to check whether at least one feasible control flow path going through the selected point exists and to generate test data that correspond to one of these paths.The key point of our approach is to take advantage of current advances in constraint techniques when solving the generated constraint system. Global constraints are used in a preliminary step to detect some of the non feasible paths. Partial consistency techniques are employed to reduce the domains of possible values of the test data. A prototype implementation has been developped on a restricted subset of the C language. Advantages of our approach are illustrated on a non-trivial example.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271790},
 doi = {http://doi.acm.org/10.1145/271775.271790},
 acmid = {271790},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test data generation, constraint solving techniques, global constraints, structural testing},
} 

@inproceedings{Hajnal:1998:ATD:271771.271791,
 author = {Hajnal, \'{A}kos and Forg\'{a}cs, Istv\'{a}n},
 title = {An applicable test data generation algorithm for domain errors},
 abstract = {An integrated testing criterion is proposed that extends traditional criteria to be effective to reveal domain errors. The method requires many fewer test cases and is applicable for any kind of predicates. An automated test data generation algorithm is developed to satisfy the criterion. This is the first integrated algorithm that unites path selection and test data generation. The method is based on function minimization and is extended to find required test cases corresponding to ON-OFF points very quickly. In this way the algorithm is dynamic and thus can be used in practice.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271791},
 doi = {http://doi.acm.org/10.1145/271771.271791},
 acmid = {271791},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test data generation, border testing, domain testing, software testing, testing criterion},
} 

@article{Hajnal:1998:ATD:271775.271791,
 author = {Hajnal, \'{A}kos and Forg\'{a}cs, Istv\'{a}n},
 title = {An applicable test data generation algorithm for domain errors},
 abstract = {An integrated testing criterion is proposed that extends traditional criteria to be effective to reveal domain errors. The method requires many fewer test cases and is applicable for any kind of predicates. An automated test data generation algorithm is developed to satisfy the criterion. This is the first integrated algorithm that unites path selection and test data generation. The method is based on function minimization and is extended to find required test cases corresponding to ON-OFF points very quickly. In this way the algorithm is dynamic and thus can be used in practice.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271791},
 doi = {http://doi.acm.org/10.1145/271775.271791},
 acmid = {271791},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated test data generation, border testing, domain testing, software testing, testing criterion},
} 

@article{Tracey:1998:APF:271775.271792,
 author = {Tracey, Nigel and Clark, John and Mander, Keith},
 title = {Automated program flaw finding using simulated annealing},
 abstract = {One of the major costs in a software project is the construction of test-data. This paper outlines a generalised test-case data generation framework based on optimisation techniques. The framework can incorporate a number of testing criteria, for both functional and non-functional properties. Application of the optimisation framework to testing specification failures and exception conditions is illustrated. The results of a number of small case studies are presented and show the efficiency and effectiveness of this dynamic optimisation-base approach to generating test-data.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271775.271792},
 doi = {http://doi.acm.org/10.1145/271775.271792},
 acmid = {271792},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test-case generation, exception conditions, formal specifications, optimisation techniques, simulated annealing, software testing},
} 

@inproceedings{Tracey:1998:APF:271771.271792,
 author = {Tracey, Nigel and Clark, John and Mander, Keith},
 title = {Automated program flaw finding using simulated annealing},
 abstract = {One of the major costs in a software project is the construction of test-data. This paper outlines a generalised test-case data generation framework based on optimisation techniques. The framework can incorporate a number of testing criteria, for both functional and non-functional properties. Application of the optimisation framework to testing specification failures and exception conditions is illustrated. The results of a number of small case studies are presented and show the efficiency and effectiveness of this dynamic optimisation-base approach to generating test-data.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271771.271792},
 doi = {http://doi.acm.org/10.1145/271771.271792},
 acmid = {271792},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic test-case generation, exception conditions, formal specifications, optimisation techniques, simulated annealing, software testing},
} 

@article{Ostrand:1998:VTD:271775.271793,
 author = {Ostrand, Thomas and Anodide, Aaron and Foster, Herbert and Goradia, Tarak},
 title = {A visual test development environment for GUI systems},
 abstract = {We have implemented an experimental test development environment (TDE) intended to raise the effectiveness of tests produced for GUI systems, and raise the productivity of the GUI system tester.The environment links a test designer, a test design library, and a test generation engine with a standard commercial capture/replay tool. These components provide a human tester the capabilities to capture sequences of interactions with the system under test (SUT), to visually manipulate and modify the sequences, and to create test designs that represent multiple individual test sequences. Test development is done using a high-level model of the SUT's GUI, and graphical representations of test designs. TDE performs certain test maintenance tasks automatically, permitting previously written test scripts to run on a revised version of the SUT.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271775.271793},
 doi = {http://doi.acm.org/10.1145/271775.271793},
 acmid = {271793},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GUI-based system, capture/reply, test coverage, test designer, test generation, test maintenance, test scenario, testing, visual editor},
} 

@inproceedings{Ostrand:1998:VTD:271771.271793,
 author = {Ostrand, Thomas and Anodide, Aaron and Foster, Herbert and Goradia, Tarak},
 title = {A visual test development environment for GUI systems},
 abstract = {We have implemented an experimental test development environment (TDE) intended to raise the effectiveness of tests produced for GUI systems, and raise the productivity of the GUI system tester.The environment links a test designer, a test design library, and a test generation engine with a standard commercial capture/replay tool. These components provide a human tester the capabilities to capture sequences of interactions with the system under test (SUT), to visually manipulate and modify the sequences, and to create test designs that represent multiple individual test sequences. Test development is done using a high-level model of the SUT's GUI, and graphical representations of test designs. TDE performs certain test maintenance tasks automatically, permitting previously written test scripts to run on a revised version of the SUT.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271771.271793},
 doi = {http://doi.acm.org/10.1145/271771.271793},
 acmid = {271793},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GUI-based system, capture/reply, test coverage, test designer, test generation, test maintenance, test scenario, testing, visual editor},
} 

@article{Molloy:1998:AIT:271775.271796,
 author = {Molloy, Mark and Andrews, Kristy and Herren, James and Cutler, David and Del Vigna, Paul},
 title = {Automatic interoperability test generation for source-to-source translators},
 abstract = {This paper describes a strategy for automatically generating tests which ensure interface compatibility between software components expressed in two different languages. This strategy is useful for testing code produced by a source-to-source computer language translator and for testing the compatibility of compilers for the different languages.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {93--101},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271775.271796},
 doi = {http://doi.acm.org/10.1145/271775.271796},
 acmid = {271796},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, language conversion, legacy data, legacy software, source-to-source translation, test generation},
} 

@inproceedings{Molloy:1998:AIT:271771.271796,
 author = {Molloy, Mark and Andrews, Kristy and Herren, James and Cutler, David and Del Vigna, Paul},
 title = {Automatic interoperability test generation for source-to-source translators},
 abstract = {This paper describes a strategy for automatically generating tests which ensure interface compatibility between software components expressed in two different languages. This strategy is useful for testing code produced by a source-to-source computer language translator and for testing the compatibility of compilers for the different languages.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {93--101},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271771.271796},
 doi = {http://doi.acm.org/10.1145/271771.271796},
 acmid = {271796},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, language conversion, legacy data, legacy software, source-to-source translation, test generation},
} 

@article{Chan:1998:IES:271775.271798,
 author = {Chan, William and Anderson, Richard J. and Bea Paul and Notkin, David},
 title = {Improving efficiency of symbolic model checking for state-based system requirements},
 abstract = {We present various techniques for improving the time and space efficiency of symbolic model checking for system requirements specified as synchronous finite state machines. We used these techniques in our analysis of the system requirements specification of TCAS II, a complex aircraft collision avoidance system. They together reduce the time and space complexities by orders of magnitude, making feasible some analysis that was previously intractable. The TCAS II requirements were written in RSML, a dialect of state-charts.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271775.271798},
 doi = {http://doi.acm.org/10.1145/271775.271798},
 acmid = {271798},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RSML, TCAS II, abstraction, binary decision diagrams, formal verification, partitioned transition relation, reachability analysis, statecharts, symbolic model checking, system requirements specification},
} 

@inproceedings{Chan:1998:IES:271771.271798,
 author = {Chan, William and Anderson, Richard J. and Bea Paul and Notkin, David},
 title = {Improving efficiency of symbolic model checking for state-based system requirements},
 abstract = {We present various techniques for improving the time and space efficiency of symbolic model checking for system requirements specified as synchronous finite state machines. We used these techniques in our analysis of the system requirements specification of TCAS II, a complex aircraft collision avoidance system. They together reduce the time and space complexities by orders of magnitude, making feasible some analysis that was previously intractable. The TCAS II requirements were written in RSML, a dialect of state-charts.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271771.271798},
 doi = {http://doi.acm.org/10.1145/271771.271798},
 acmid = {271798},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RSML, TCAS II, abstraction, binary decision diagrams, formal verification, partitioned transition relation, reachability analysis, statecharts, symbolic model checking, system requirements specification},
} 

@inproceedings{Bultan:1998:VSI:271771.271799,
 author = {Bultan, Tevfik and Gerber, Richard and League, Christopher},
 title = {Verifying systems with integer constraints and Boolean predicates: a composite approach},
 abstract = {Symbolic mode, checking has proved highly successful for large finite-state systems, in which states can be compactly encoded using binary decision diagrams (BDDs) or their variants. The inherent limitation of this approach is that it cannot be applied to systems with an infinite number of states --- even those with a single unbounded integer.Alternatively, we recently proposed a model checker for integer-based systems that uses Presburger constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of Boolean and (unordered) enumerated types --- which possess no natural mapping to the Euclidean coordinate space.In this paper we describe a model checker which combines the strengths of both approaches. We use a composite model, in which a formula's valuations are encoded in a mixed BDD-Presburger form, depending on the variables used. We demonstrate our technique's effectiveness on a nontrivial requirements specification, which includes a mixture of Booleans, integers and enumerated types.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {113--123},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271771.271799},
 doi = {http://doi.acm.org/10.1145/271771.271799},
 acmid = {271799},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bultan:1998:VSI:271775.271799,
 author = {Bultan, Tevfik and Gerber, Richard and League, Christopher},
 title = {Verifying systems with integer constraints and Boolean predicates: a composite approach},
 abstract = {Symbolic mode, checking has proved highly successful for large finite-state systems, in which states can be compactly encoded using binary decision diagrams (BDDs) or their variants. The inherent limitation of this approach is that it cannot be applied to systems with an infinite number of states --- even those with a single unbounded integer.Alternatively, we recently proposed a model checker for integer-based systems that uses Presburger constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of Boolean and (unordered) enumerated types --- which possess no natural mapping to the Euclidean coordinate space.In this paper we describe a model checker which combines the strengths of both approaches. We use a composite model, in which a formula's valuations are encoded in a mixed BDD-Presburger form, depending on the variables used. We demonstrate our technique's effectiveness on a nontrivial requirements specification, which includes a mixture of Booleans, integers and enumerated types.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {113--123},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/271775.271799},
 doi = {http://doi.acm.org/10.1145/271775.271799},
 acmid = {271799},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Godefroid:1998:MCW:271775.271800,
 author = {Godefroid, Patrice and Hanmer, Robert S. and Jagadeesan, Lalita Jategaonkar},
 title = {Model checking without a model: an analysis of the heart-beat monitor of a telephone switch using VeriSoft},
 abstract = {VeriSoft is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary code written in full-fledged programming languages such as C or C++. The state space of a concurrent system is a directed graph that represents the combined behavior of all concurrent components in the system. By exploring its state space, VeriSoft can automatically detect coordination problems between the processes of a concurrent system.We report in this paper our analysis with VeriSoft of the "Heart-Beat Monitor" (HBM), a telephone switching application developed at Lucent Technologies. The HBM of a telephone switch determines the status of different elements connected to the switch by measuring propagation delays of messages transmitted via these elements. This information plays an important role in the routing of data in the switch, and can significantly impact switch performance.We discuss the steps of our analysis of the HBM using VeriSoft. Because no modeling of the HBM code is necessary with this tool, the total elapsed time before being able to run the first tests was on the order of a few hours, instead of several days or weeks that would have been needed for the (error-prone) modeling phase required with traditional model checkers or theorem provers.We then present the results of our analysis. Since VeriSoft automatically generates, executes and evaluates thousands of tests per minute and has complete control over nondeterminism, our analysis revealed HBM behavior that is virtually impossible to detect or test in a traditional lab-testing environment. Specifically, we discovered flaws in the existing documentation on this application and unexpected behaviors in the software itself. These results are being used as the basis for the redesign of the HBM software in the next commercial release of the switching software.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271800},
 doi = {http://doi.acm.org/10.1145/271775.271800},
 acmid = {271800},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Godefroid:1998:MCW:271771.271800,
 author = {Godefroid, Patrice and Hanmer, Robert S. and Jagadeesan, Lalita Jategaonkar},
 title = {Model checking without a model: an analysis of the heart-beat monitor of a telephone switch using VeriSoft},
 abstract = {VeriSoft is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary code written in full-fledged programming languages such as C or C++. The state space of a concurrent system is a directed graph that represents the combined behavior of all concurrent components in the system. By exploring its state space, VeriSoft can automatically detect coordination problems between the processes of a concurrent system.We report in this paper our analysis with VeriSoft of the "Heart-Beat Monitor" (HBM), a telephone switching application developed at Lucent Technologies. The HBM of a telephone switch determines the status of different elements connected to the switch by measuring propagation delays of messages transmitted via these elements. This information plays an important role in the routing of data in the switch, and can significantly impact switch performance.We discuss the steps of our analysis of the HBM using VeriSoft. Because no modeling of the HBM code is necessary with this tool, the total elapsed time before being able to run the first tests was on the order of a few hours, instead of several days or weeks that would have been needed for the (error-prone) modeling phase required with traditional model checkers or theorem provers.We then present the results of our analysis. Since VeriSoft automatically generates, executes and evaluates thousands of tests per minute and has complete control over nondeterminism, our analysis revealed HBM behavior that is virtually impossible to detect or test in a traditional lab-testing environment. Specifically, we discovered flaws in the existing documentation on this application and unexpected behaviors in the software itself. These results are being used as the basis for the redesign of the HBM software in the next commercial release of the switching software.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271800},
 doi = {http://doi.acm.org/10.1145/271771.271800},
 acmid = {271800},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ball:1998:LCF:271775.271802,
 author = {Ball, Thomas},
 title = {On the limit of control flow analysis for regression test selection},
 abstract = {Automated analyses for regression test selection (RTS) attempt to determine if a modified program, when run on a test t,</i> will have the same behavior as an old version of the program run on t,</i> but without running the new program on t.</i> RTS analyses must confront a price/performance tradeoff: a more precise analysis might be able to eliminate more tests, but could take much longer to run.We focus on the application of control flow analysis and control flow coverage, relatively inexpensive analyses, to the RTS problem, considering how the precision of RTS algorithms can be affected by the type of coverage information collected. We define a strong optimality condition (edge-optimality) for RTS algorithms based on edge coverage that precisely captures when such an algorithm will report that re-testing is needed, when, in actuality, it is not. We reformulate Rothermel and Harrold's RTS algorithm and present three new algorithms that improve on it, culminating in an edge-optimal algorithm. Finally, we consider how path coverage can be used to improve the precision of RTS algorithms.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271775.271802},
 doi = {http://doi.acm.org/10.1145/271775.271802},
 acmid = {271802},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control flow analysis, coverage, profiling, regression testing},
} 

@inproceedings{Ball:1998:LCF:271771.271802,
 author = {Ball, Thomas},
 title = {On the limit of control flow analysis for regression test selection},
 abstract = {Automated analyses for regression test selection (RTS) attempt to determine if a modified program, when run on a test t,</i> will have the same behavior as an old version of the program run on t,</i> but without running the new program on t.</i> RTS analyses must confront a price/performance tradeoff: a more precise analysis might be able to eliminate more tests, but could take much longer to run.We focus on the application of control flow analysis and control flow coverage, relatively inexpensive analyses, to the RTS problem, considering how the precision of RTS algorithms can be affected by the type of coverage information collected. We define a strong optimality condition (edge-optimality) for RTS algorithms based on edge coverage that precisely captures when such an algorithm will report that re-testing is needed, when, in actuality, it is not. We reformulate Rothermel and Harrold's RTS algorithm and present three new algorithms that improve on it, culminating in an edge-optimal algorithm. Finally, we consider how path coverage can be used to improve the precision of RTS algorithms.},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/271771.271802},
 doi = {http://doi.acm.org/10.1145/271771.271802},
 acmid = {271802},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control flow analysis, coverage, profiling, regression testing},
} 

@inproceedings{Korel:1998:ART:271771.271803,
 author = {Korel, Bogdan and Al-Yami, Ali M.},
 title = {Automated regression test generation},
 abstract = {Regression testing involves testing the modified program in order to establish the confidence in the modifications. Existing regression testing methods generate test cases to satisfy selected testing criteria in the hope that this process may reveal faults in the modified program. In this paper we present a novel approach of automated regression test generation in which all generated test cases uncover an error(s). This approach is used to test the common functionality of the original program and its modified version, i.e., it is used for programs whose functionality is unchanged after modifications. The goal in this approach is to identify test cases for which the original program and the modified program produce different outputs. If such a test is found, then this test uncovers an error. The problem of finding such a test case may be reduced to the problem of finding program input on which a selected statement is executed. As a result, existing methods of automated test data generation for white-box testing may be used to generate these tests. Our experiments have shown that our approach may improve the chances of finding software errors as compared to the existing methods of regression testing. The advantage of our approach is that it is fully automated and that all generated test cases reveal an error(s).},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {143--152},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271803},
 doi = {http://doi.acm.org/10.1145/271771.271803},
 acmid = {271803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Korel:1998:ART:271775.271803,
 author = {Korel, Bogdan and Al-Yami, Ali M.},
 title = {Automated regression test generation},
 abstract = {Regression testing involves testing the modified program in order to establish the confidence in the modifications. Existing regression testing methods generate test cases to satisfy selected testing criteria in the hope that this process may reveal faults in the modified program. In this paper we present a novel approach of automated regression test generation in which all generated test cases uncover an error(s). This approach is used to test the common functionality of the original program and its modified version, i.e., it is used for programs whose functionality is unchanged after modifications. The goal in this approach is to identify test cases for which the original program and the modified program produce different outputs. If such a test is found, then this test uncovers an error. The problem of finding such a test case may be reduced to the problem of finding program input on which a selected statement is executed. As a result, existing methods of automated test data generation for white-box testing may be used to generate these tests. Our experiments have shown that our approach may improve the chances of finding software errors as compared to the existing methods of regression testing. The advantage of our approach is that it is fully automated and that all generated test cases reveal an error(s).},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {143--152},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271803},
 doi = {http://doi.acm.org/10.1145/271775.271803},
 acmid = {271803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:1998:ACP:271771.271804,
 author = {Yang, Cheer-Sun D. and Souter, Amie L. and Pollock, Lori L.},
 title = {All-du-path coverage for parallel programs},
 abstract = {One significant challenge in bringing the power of parallel machines to application programmers is providing them with a suite of software tools similar to the tools that sequential programmers currently utilize. In particular, automatic or semi-automatic testing tools for parallel programs are lacking. This paper describes our work in automatic generation of all-du-paths for testing parallel programs. Our goal is to demonstrate that, with some extension, sequential test data adequacy criteria are still applicable to parallel program testing. The concepts and algorithms in this paper have been incorporated as the foundation of our DELaware PArallel Software Testing Aid, <b>della pasta.</b>},
 booktitle = {Proceedings of the 1998 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '98},
 year = {1998},
 isbn = {0-89791-971-8},
 location = {Clearwater Beach, Florida, United States},
 pages = {153--162},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271771.271804},
 doi = {http://doi.acm.org/10.1145/271771.271804},
 acmid = {271804},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {all-du-path coverage, parallel programming, testing tool},
} 

@article{Yang:1998:ACP:271775.271804,
 author = {Yang, Cheer-Sun D. and Souter, Amie L. and Pollock, Lori L.},
 title = {All-du-path coverage for parallel programs},
 abstract = {One significant challenge in bringing the power of parallel machines to application programmers is providing them with a suite of software tools similar to the tools that sequential programmers currently utilize. In particular, automatic or semi-automatic testing tools for parallel programs are lacking. This paper describes our work in automatic generation of all-du-paths for testing parallel programs. Our goal is to demonstrate that, with some extension, sequential test data adequacy criteria are still applicable to parallel program testing. The concepts and algorithms in this paper have been incorporated as the foundation of our DELaware PArallel Software Testing Aid, <b>della pasta.</b>},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {23},
 issue = {2},
 month = {March},
 year = {1998},
 issn = {0163-5948},
 pages = {153--162},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/271775.271804},
 doi = {http://doi.acm.org/10.1145/271775.271804},
 acmid = {271804},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {all-du-path coverage, parallel programming, testing tool},
} 

@article{Tracz:1996:TAS:226295.226296,
 author = {Tracz, Will},
 title = {Test and analysis of software architectures},
 abstract = {Some DoD programs now require prospective contractors to demonstrate the superiority of their software architectures for new weapons systems. This acquisition policy provides new software engineering challenges that focus heavily on the test and analysis of software architectures in order to determine the "best" architecture in terms of its implementability, affordability, extendability, scalability, adaptability, and maintainability --- not overlooking whether or not it will meet the functional requirements of the system.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {1--3},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/226295.226296},
 doi = {http://doi.acm.org/10.1145/226295.226296},
 acmid = {226296},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tracz:1996:TAS:229000.226296,
 author = {Tracz, Will},
 title = {Test and analysis of software architectures},
 abstract = {Some DoD programs now require prospective contractors to demonstrate the superiority of their software architectures for new weapons systems. This acquisition policy provides new software engineering challenges that focus heavily on the test and analysis of software architectures in order to determine the "best" architecture in terms of its implementability, affordability, extendability, scalability, adaptability, and maintainability --- not overlooking whether or not it will meet the functional requirements of the system.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {1--3},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/229000.226296},
 doi = {http://doi.acm.org/10.1145/229000.226296},
 acmid = {226296},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kang:1996:ESS:226295.226297,
 author = {Kang, Inhye and Lee, Insup},
 title = {An efficient state space generation for analysis of real-time systems},
 abstract = {State explosion is a well-known problem that impedes analysis and testing based on state-space exploration. This problem is particularly serious in real-time systems because unbounded time values cause the state space to be infinite. In this paper, we present an algorithm that produces a compact representation of reachable state space of a real-time system. The algorithm yields a small state space, but still retains enough timing information for analysis. To avoid the state explosion which can be caused by simply adding time values to states, our algorithm first uses history equivalence and transition bisimulation to collapse states into equivalent classes. In this approach, equivalent states have identical observable events although transitions into the states may happen at different times. The algorithm then augments the resultant state space with timing relations that describe time distances between transition executions. For example, the relation @(tr</i><inf>1</inf>) + 3 \&amp;le; @(tr</i><inf>2</inf>) \&amp;le; @(tr</i><inf>1</inf>) + 5 means that transition tr</i><inf>2</inf> is taken 3 to 5 time units before transition tr</i><inf>2</inf> is taken. This is used to analyze timing properties such as minimum and maximum time distances between events. To show the effectiveness of our algorithm, we have implemented the algorithm and are currently comparing it to other existing techniques which generate state space for real-time systems.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {4--13},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/226295.226297},
 doi = {http://doi.acm.org/10.1145/226295.226297},
 acmid = {226297},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kang:1996:ESS:229000.226297,
 author = {Kang, Inhye and Lee, Insup},
 title = {An efficient state space generation for analysis of real-time systems},
 abstract = {State explosion is a well-known problem that impedes analysis and testing based on state-space exploration. This problem is particularly serious in real-time systems because unbounded time values cause the state space to be infinite. In this paper, we present an algorithm that produces a compact representation of reachable state space of a real-time system. The algorithm yields a small state space, but still retains enough timing information for analysis. To avoid the state explosion which can be caused by simply adding time values to states, our algorithm first uses history equivalence and transition bisimulation to collapse states into equivalent classes. In this approach, equivalent states have identical observable events although transitions into the states may happen at different times. The algorithm then augments the resultant state space with timing relations that describe time distances between transition executions. For example, the relation @(tr</i><inf>1</inf>) + 3 \&amp;le; @(tr</i><inf>2</inf>) \&amp;le; @(tr</i><inf>1</inf>) + 5 means that transition tr</i><inf>2</inf> is taken 3 to 5 time units before transition tr</i><inf>2</inf> is taken. This is used to analyze timing properties such as minimum and maximum time distances between events. To show the effectiveness of our algorithm, we have implemented the algorithm and are currently comparing it to other existing techniques which generate state space for real-time systems.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {4--13},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/229000.226297},
 doi = {http://doi.acm.org/10.1145/229000.226297},
 acmid = {226297},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Koppol:1996:IAS:229000.226298,
 author = {Koppol, Pramod V. and Tai, Kuo-Chung},
 title = {An incremental approach to structural testing of concurrent software},
 abstract = {Structural testing of a concurrent program P involves the selection of paths of P according to a structure-based criterion. A common approach is to derive the reachability graph (RG) of P, select a set of paths of P, derive one or more inputs for each selected path, and force deterministic executions of P according to the selected paths and their inputs. The use of RG(P) for test path selection has the state explosion problem, since the number of states of RG(P) is an exponential function of the number of processes in P.In this paper, we present a new incremental approach to structural testing of P. Based on the hierarchy of processes in P, our incremental testing approach is to integrate processes in P in a bottom-to-top manner. When a set S of processes in P at the same level are integrated, we construct a reduced RG for S such that the reduced RG contains all synchronizations involving the processes in S and some of the synchronizations involving processes at lower levels in order to connect synchronizations involving processes in S. Based on the reduced RG for S, we can select test paths to focus on the detection of interface faults involving processes in S. After the selection of paths, RG(S) is further reduced in order to retain only some of the synchronizations involving processes in S that are needed in order to connect synchronizations between S and other processes in P. Our incremental approach alleviates the state explosion problem and offers other advantages.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {14--23},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/229000.226298},
 doi = {http://doi.acm.org/10.1145/229000.226298},
 acmid = {226298},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Koppol:1996:IAS:226295.226298,
 author = {Koppol, Pramod V. and Tai, Kuo-Chung},
 title = {An incremental approach to structural testing of concurrent software},
 abstract = {Structural testing of a concurrent program P involves the selection of paths of P according to a structure-based criterion. A common approach is to derive the reachability graph (RG) of P, select a set of paths of P, derive one or more inputs for each selected path, and force deterministic executions of P according to the selected paths and their inputs. The use of RG(P) for test path selection has the state explosion problem, since the number of states of RG(P) is an exponential function of the number of processes in P.In this paper, we present a new incremental approach to structural testing of P. Based on the hierarchy of processes in P, our incremental testing approach is to integrate processes in P in a bottom-to-top manner. When a set S of processes in P at the same level are integrated, we construct a reduced RG for S such that the reduced RG contains all synchronizations involving the processes in S and some of the synchronizations involving processes at lower levels in order to connect synchronizations involving processes in S. Based on the reduced RG for S, we can select test paths to focus on the detection of interface faults involving processes in S. After the selection of paths, RG(S) is further reduced in order to retain only some of the synchronizations involving processes in S that are needed in order to connect synchronizations between S and other processes in P. Our incremental approach alleviates the state explosion problem and offers other advantages.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {14--23},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/226295.226298},
 doi = {http://doi.acm.org/10.1145/226295.226298},
 acmid = {226298},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chamillard:1996:IAP:229000.226299,
 author = {Chamillard, A. T. and Clarke, Lori A.},
 title = {Improving the accuracy of Petri net-based analysis of concurrent programs},
 abstract = {Spurious results are an inherent problem of most static analysis methods. These methods, in an effort to produce conservative results, overestimate the executable behavior of a program. Infeasible paths and imprecise alias resolution are the two causes of such inaccuracies. In this paper we present an approach for improving the accuracy of Petri net-based analysis of concurrent programs by including additional program state information in the Petri net. We present empirical results that demonstrate the improvements in accuracy and, in some cases, the reduction in the search space that result from applying this approach to concurrent Ada programs.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {24--38},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/229000.226299},
 doi = {http://doi.acm.org/10.1145/229000.226299},
 acmid = {226299},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chamillard:1996:IAP:226295.226299,
 author = {Chamillard, A. T. and Clarke, Lori A.},
 title = {Improving the accuracy of Petri net-based analysis of concurrent programs},
 abstract = {Spurious results are an inherent problem of most static analysis methods. These methods, in an effort to produce conservative results, overestimate the executable behavior of a program. Infeasible paths and imprecise alias resolution are the two causes of such inaccuracies. In this paper we present an approach for improving the accuracy of Petri net-based analysis of concurrent programs by including additional program state information in the Petri net. We present empirical results that demonstrate the improvements in accuracy and, in some cases, the reduction in the search space that result from applying this approach to concurrent Ada programs.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {24--38},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/226295.226299},
 doi = {http://doi.acm.org/10.1145/226295.226299},
 acmid = {226299},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Morasca:1996:GFT:229000.226300,
 author = {Morasca, Sandro and Morzenti, Angelo and SanPietro, Pieluigi},
 title = {Generating functional test cases in-the-large for time-critical systems from logic-based specifications},
 abstract = {We address the problem of generating functional test cases for complex, highly structured time-critical systems starting from a modularized logic-based specification written in the TRIOR<sup>+</sup> language, an object-oriented extension of the temporal logic TRIO.First, we present methods for producing test cases for a TRIO<sup>+</sup> specification module, referring both to the internal, hidden, portion of the module and to its interface. Then, we discuss criteria to be used in the construction of test cases from a TRIO<sup>+</sup> specification based on its composing modules and the connections among their interfaces. We formally define the notions related to test case derivation from TRIO<sup>+</sup> modules and we introduce an executable language for describing a variety of strategies for constructing test cases for structured TRIO<sup>+</sup> specifications starting from (parts of) the test cases of the composing modules. This language can be the basis for the implementation of an interactive tool for the semiautomatic construction of functional test cases from complex time-critical systems starting from their TRIO<sup>+</sup> specification.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/229000.226300},
 doi = {http://doi.acm.org/10.1145/229000.226300},
 acmid = {226300},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Morasca:1996:GFT:226295.226300,
 author = {Morasca, Sandro and Morzenti, Angelo and SanPietro, Pieluigi},
 title = {Generating functional test cases in-the-large for time-critical systems from logic-based specifications},
 abstract = {We address the problem of generating functional test cases for complex, highly structured time-critical systems starting from a modularized logic-based specification written in the TRIOR<sup>+</sup> language, an object-oriented extension of the temporal logic TRIO.First, we present methods for producing test cases for a TRIO<sup>+</sup> specification module, referring both to the internal, hidden, portion of the module and to its interface. Then, we discuss criteria to be used in the construction of test cases from a TRIO<sup>+</sup> specification based on its composing modules and the connections among their interfaces. We formally define the notions related to test case derivation from TRIO<sup>+</sup> modules and we introduce an executable language for describing a variety of strategies for constructing test cases for structured TRIO<sup>+</sup> specifications starting from (parts of) the test cases of the composing modules. This language can be the basis for the implementation of an interactive tool for the semiautomatic construction of functional test cases from complex time-critical systems starting from their TRIO<sup>+</sup> specification.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/226295.226300},
 doi = {http://doi.acm.org/10.1145/226295.226300},
 acmid = {226300},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hughes:1996:DSA:226295.226301,
 author = {Hughes, Merlin and Stotts, David},
 title = {Daistish: systematic algebraic testing for OO programs in the presence of side-effects},
 abstract = {Daistish is a tool that performs systematic algebraic testing similar to Gannon's DAISTS tool [2]. However, Daistish creates effective test drivers for programs in languages that use side effects to implement ADTs; this includes C++ and most other object-oriented languages. The functional approach of DAISTS does not apply directly in these cases. The approach in our work is most similar to the ASTOOT system of Doong and Frankl [1]; Daistish differs from ASTOOT by using Guttag-style algebraic specs (functional notation), by allowing aliasing of type names to tailor the application of parameters in test cases, and by retaining the abilities of DAISTS to compose new test points from existing ones. Daistish is a Perl script, and is compact and practical to apply. We describe the implementation and our experiments in both Eiffel and C++. Our work has concentrated on solving the semantics-specific issues of correctly duplicating objects for comparison; we have not worked on methods for selecting specific test cases.Daistish consists of a perl script and supporting documentation. The current distribution can be obtained via WWW at URLhttp://www.cs.unc.edu/~stotts/Daistish/},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {53--61},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/226295.226301},
 doi = {http://doi.acm.org/10.1145/226295.226301},
 acmid = {226301},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hughes:1996:DSA:229000.226301,
 author = {Hughes, Merlin and Stotts, David},
 title = {Daistish: systematic algebraic testing for OO programs in the presence of side-effects},
 abstract = {Daistish is a tool that performs systematic algebraic testing similar to Gannon's DAISTS tool [2]. However, Daistish creates effective test drivers for programs in languages that use side effects to implement ADTs; this includes C++ and most other object-oriented languages. The functional approach of DAISTS does not apply directly in these cases. The approach in our work is most similar to the ASTOOT system of Doong and Frankl [1]; Daistish differs from ASTOOT by using Guttag-style algebraic specs (functional notation), by allowing aliasing of type names to tailor the application of parameters in test cases, and by retaining the abilities of DAISTS to compose new test points from existing ones. Daistish is a Perl script, and is compact and practical to apply. We describe the implementation and our experiments in both Eiffel and C++. Our work has concentrated on solving the semantics-specific issues of correctly duplicating objects for comparison; we have not worked on methods for selecting specific test cases.Daistish consists of a perl script and supporting documentation. The current distribution can be obtained via WWW at URLhttp://www.cs.unc.edu/~stotts/Daistish/},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {53--61},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/229000.226301},
 doi = {http://doi.acm.org/10.1145/229000.226301},
 acmid = {226301},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chang:1996:SST:229000.226302,
 author = {Chang, Juei and Richardson, Debra J. and Sankar, Sriram},
 title = {Structural specification-based testing with ADL},
 abstract = {This paper describes a specification-based black-box technique for testing program units. The main contribution is the method that we have developed to derive test conditions,</i> which are descriptions of test cases, from the formal specification of each program unit. The derived test conditions are used to guide test selection and to measure comprehensiveness of existing test suites. Our technique complements traditional code-based techniques such as statement coverage and branch coverage. It allows the tester to quickly develop a black-box test suite.In particular, this paper presents techniques for deriving test conditions from specifications written in the Assertion Definition Language (ADL) [SH94], a predicate logic-based language that is used to describe the relationships between inputs and outputs of a program unit. Our technique is fully automatable, and we are currently implementing a tool based on the techniques presented in this paper.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {62--70},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/229000.226302},
 doi = {http://doi.acm.org/10.1145/229000.226302},
 acmid = {226302},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chang:1996:SST:226295.226302,
 author = {Chang, Juei and Richardson, Debra J. and Sankar, Sriram},
 title = {Structural specification-based testing with ADL},
 abstract = {This paper describes a specification-based black-box technique for testing program units. The main contribution is the method that we have developed to derive test conditions,</i> which are descriptions of test cases, from the formal specification of each program unit. The derived test conditions are used to guide test selection and to measure comprehensiveness of existing test suites. Our technique complements traditional code-based techniques such as statement coverage and branch coverage. It allows the tester to quickly develop a black-box test suite.In particular, this paper presents techniques for deriving test conditions from specifications written in the Assertion Definition Language (ADL) [SH94], a predicate logic-based language that is used to describe the relationships between inputs and outputs of a program unit. Our technique is fully automatable, and we are currently implementing a tool based on the techniques presented in this paper.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {62--70},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/226295.226302},
 doi = {http://doi.acm.org/10.1145/226295.226302},
 acmid = {226302},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Crowley:1996:IFS:229000.226303,
 author = {Crowley, J. L. and Leathrum, J. F. and Liburdy, K. A.},
 title = {Issues in the full scale use of formal methods for automated testing},
 abstract = {Experience from a full scale effort to apply formal methods to automated testing in the open systems software arena is described. The formal method applied in this work is based upon the Clemson Automated Testing System (CATS) which includes a formal specification language, a set of guidelines describing how to use the method effectively, and tool support capable of translating formal specifications into executable tests. This method is currently being used to develop a full scale test suite for IEEE's Ada Language Binding to POSIX. Following an overview of CATS, an experience report consisting of results, lessons learned and future directions is presented.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {71--78},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226303},
 doi = {http://doi.acm.org/10.1145/229000.226303},
 acmid = {226303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Crowley:1996:IFS:226295.226303,
 author = {Crowley, J. L. and Leathrum, J. F. and Liburdy, K. A.},
 title = {Issues in the full scale use of formal methods for automated testing},
 abstract = {Experience from a full scale effort to apply formal methods to automated testing in the open systems software arena is described. The formal method applied in this work is based upon the Clemson Automated Testing System (CATS) which includes a formal specification language, a set of guidelines describing how to use the method effectively, and tool support capable of translating formal specifications into executable tests. This method is currently being used to develop a full scale test suite for IEEE's Ada Language Binding to POSIX. Following an overview of CATS, an experience report consisting of results, lessons learned and future directions is presented.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {71--78},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226303},
 doi = {http://doi.acm.org/10.1145/226295.226303},
 acmid = {226303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heimdahl:1996:ELA:226295.226304,
 author = {Heimdahl, Mats P. E.},
 title = {Experiences and lessons from the analysis of TCAS II},
 abstract = {This report highlights some of the experiences gathered while analyzing the requirements specification for a commercial avionics system called TCAS II (Traffic alert and Collision Avoidance System II) for consistency and completeness. Completeness in this context is defined as a complete set of requirements, that is, there is a behavior specified for every possible input and input sequence.Under the leadership of Dr. Nancy G. Leveson, the Irvine Safety Research Group has developed a state-based requirements specification language RSML (Requirements State Machine Language) using TCAS II as a testbed [6]. The TCAS requirements specification project was very successful; RSML was well liked by all participants in the project, and the formal specification has been adopted as the official TCAS II requirements. The requirements document has been delivered to the FAA and has undergone an extensive independent validation and verification effort (IV\&amp;amp;V).In a previous investigation, we defined procedures for analyzing state-based requirements specifications for completeness and consistency [5]. To demonstrate that our approach is feasible and is applicable to realistic systems, we have implemented a draft analysis tool and we have applied the analysis to the TCAS II requirements. The initial results from the analysis effort were encouraging [4, 5] and scaled well to a large requirements specification. The most complex parts of the TCAS requirements specification have recently been analyzed. Even though the effort was largely successful, some limitations with the approach have surfaced. Most importantly, the accuracy of the analysis algorithms needs improvement. When analyzing the most complex parts of the TCAS requirements, the number of spurious error reports can occasionally be overwhelming. Furthermore, we discovered that once the analysis has identified problems, it has been unexpectedly difficult to correct some of them.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {79--83},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/226295.226304},
 doi = {http://doi.acm.org/10.1145/226295.226304},
 acmid = {226304},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heimdahl:1996:ELA:229000.226304,
 author = {Heimdahl, Mats P. E.},
 title = {Experiences and lessons from the analysis of TCAS II},
 abstract = {This report highlights some of the experiences gathered while analyzing the requirements specification for a commercial avionics system called TCAS II (Traffic alert and Collision Avoidance System II) for consistency and completeness. Completeness in this context is defined as a complete set of requirements, that is, there is a behavior specified for every possible input and input sequence.Under the leadership of Dr. Nancy G. Leveson, the Irvine Safety Research Group has developed a state-based requirements specification language RSML (Requirements State Machine Language) using TCAS II as a testbed [6]. The TCAS requirements specification project was very successful; RSML was well liked by all participants in the project, and the formal specification has been adopted as the official TCAS II requirements. The requirements document has been delivered to the FAA and has undergone an extensive independent validation and verification effort (IV\&amp;amp;V).In a previous investigation, we defined procedures for analyzing state-based requirements specifications for completeness and consistency [5]. To demonstrate that our approach is feasible and is applicable to realistic systems, we have implemented a draft analysis tool and we have applied the analysis to the TCAS II requirements. The initial results from the analysis effort were encouraging [4, 5] and scaled well to a large requirements specification. The most complex parts of the TCAS requirements specification have recently been analyzed. Even though the effort was largely successful, some limitations with the approach have surfaced. Most importantly, the accuracy of the analysis algorithms needs improvement. When analyzing the most complex parts of the TCAS requirements, the number of spurious error reports can occasionally be overwhelming. Furthermore, we discovered that once the analysis has identified problems, it has been unexpectedly difficult to correct some of them.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {79--83},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/229000.226304},
 doi = {http://doi.acm.org/10.1145/229000.226304},
 acmid = {226304},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hamlet:1996:PDT:229000.226305,
 author = {Hamlet, Dick},
 title = {Predicting dependability by testing},
 abstract = {In assessing the quality of software, we would like to make engineering judgements similar to those based on statistical quality control. Ideally, we want to support statements like: "The confidence that this program's result at X</i> is correct is p,</i>" where X</i> is a particular vector of inputs, and confidence p</i> is obtained from measurements of the software (perhaps involving X</i>). For the theory to be useful, it must be feasible to predict values of p</i> near 1 for many programs, for most values of X.</i>Blum's theory of self-checking/correcting programs has exactly the right character, but it applies to only a few unusual problems. Conventional software reliability theory is widely applicable, but it yields only confidence in a failure intensity, and the measurements required to support a correctness-like failure intensity (say 10<sup>-9</sup>/demand) are infeasible. Voas's sensitivity theory remedies these problems of reliability theory, but his model is too simple to be very plausible. In this paper we combine these ideas: reliability, sensitivity, and self-checking, to obtain new results on "dependability," plausible predictions of software quality.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {84--91},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226305},
 doi = {http://doi.acm.org/10.1145/229000.226305},
 acmid = {226305},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hamlet:1996:PDT:226295.226305,
 author = {Hamlet, Dick},
 title = {Predicting dependability by testing},
 abstract = {In assessing the quality of software, we would like to make engineering judgements similar to those based on statistical quality control. Ideally, we want to support statements like: "The confidence that this program's result at X</i> is correct is p,</i>" where X</i> is a particular vector of inputs, and confidence p</i> is obtained from measurements of the software (perhaps involving X</i>). For the theory to be useful, it must be feasible to predict values of p</i> near 1 for many programs, for most values of X.</i>Blum's theory of self-checking/correcting programs has exactly the right character, but it applies to only a few unusual problems. Conventional software reliability theory is widely applicable, but it yields only confidence in a failure intensity, and the measurements required to support a correctness-like failure intensity (say 10<sup>-9</sup>/demand) are infeasible. Voas's sensitivity theory remedies these problems of reliability theory, but his model is too simple to be very plausible. In this paper we combine these ideas: reliability, sensitivity, and self-checking, to obtain new results on "dependability," plausible predictions of software quality.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {84--91},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226305},
 doi = {http://doi.acm.org/10.1145/226295.226305},
 acmid = {226305},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Morell:1996:UPA:226295.226306,
 author = {Morell, Larry and Murrill, Branson},
 title = {Using perturbation analysis to measure variation in the information content of test sets},
 abstract = {We define the information content of test set T with respect to a program P to be the degree to which the behavior of P on T approximates the overall behavior of P. Informally, the higher the information content of a test set, the greater the likelihood an error in the data state of a program will be manifested under testing.Perturbation analysis injects errors into the data state of an executing program and traces the impact of those errors on the intervening states and the program's output. The injection is performed by perturbation functions that randomly change the program's data state. Using perturbation analysis we demonstrate that different test sets may satisfy the same testing criterion but have significantly different information content.We believe that "consistency of information content" is a crucial measure of the quality of a testing strategy. We show how perturbation analysis may be used to assess individual testing strategies and to compare different testing strategies.The "coupling effect" of mutation testing implies that there is little variation among mutation-adequate test sets for a program. This implication is investigated for two simple programs by analyzing the variation among several mutation-adequate test sets.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {92--97},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/226295.226306},
 doi = {http://doi.acm.org/10.1145/226295.226306},
 acmid = {226306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Morell:1996:UPA:229000.226306,
 author = {Morell, Larry and Murrill, Branson},
 title = {Using perturbation analysis to measure variation in the information content of test sets},
 abstract = {We define the information content of test set T with respect to a program P to be the degree to which the behavior of P on T approximates the overall behavior of P. Informally, the higher the information content of a test set, the greater the likelihood an error in the data state of a program will be manifested under testing.Perturbation analysis injects errors into the data state of an executing program and traces the impact of those errors on the intervening states and the program's output. The injection is performed by perturbation functions that randomly change the program's data state. Using perturbation analysis we demonstrate that different test sets may satisfy the same testing criterion but have significantly different information content.We believe that "consistency of information content" is a crucial measure of the quality of a testing strategy. We show how perturbation analysis may be used to assess individual testing strategies and to compare different testing strategies.The "coupling effect" of mutation testing implies that there is little variation among mutation-adequate test sets for a program. This implication is investigated for two simple programs by analyzing the variation among several mutation-adequate test sets.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {92--97},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/229000.226306},
 doi = {http://doi.acm.org/10.1145/229000.226306},
 acmid = {226306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Howden:1996:LSE:226295.226307,
 author = {Howden, W. E. and Shi, G. M.},
 title = {Linear and structural event sequence analysis},
 abstract = {An approach to systematic informal program analysis is discussed in which comments that describe hypotheses and assertions about the behavior of programs are analyzed. Event sequence comments analysis methods analyze the consistency of comments that describe events. Two event sequence analysis methods are discussed, one of which uses a linear event sequence model, and which has been applied to the analysis of large data processing systems. The other uses a new approach involving rule-based structural models, and has been applied to the analysis of Ada programs.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {98--106},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/226295.226307},
 doi = {http://doi.acm.org/10.1145/226295.226307},
 acmid = {226307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis, comments, event-sequence, event-structures, validation},
} 

@inproceedings{Howden:1996:LSE:229000.226307,
 author = {Howden, W. E. and Shi, G. M.},
 title = {Linear and structural event sequence analysis},
 abstract = {An approach to systematic informal program analysis is discussed in which comments that describe hypotheses and assertions about the behavior of programs are analyzed. Event sequence comments analysis methods analyze the consistency of comments that describe events. Two event sequence analysis methods are discussed, one of which uses a linear event sequence model, and which has been applied to the analysis of large data processing systems. The other uses a new approach involving rule-based structural models, and has been applied to the analysis of Ada programs.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {98--106},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/229000.226307},
 doi = {http://doi.acm.org/10.1145/229000.226307},
 acmid = {226307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis, comments, event-sequence, event-structures, validation},
} 

@article{Harrold:1996:SCA:226295.309037,
 author = {Harrold, Mary Jean and Rothermel, Gregg},
 title = {Separate computation of alias information for reuse},
 abstract = {Interprocedural dataflow information is useful for many software testing and analysis techniques, including dataflow testing, regression testing, program slicing and impact analysis. For programs with aliases, these testing and analysis techniques can yield invalid results, unless the dataflow information accounts for aliasing effects. Recent research provides algorithms for performing interprocedural dataflow analysis in the presence of aliases; however, these algorithms are expensive, and achieve precise results only on complete programs. This paper presents an algorithm for performing alias analysis on incomplete programs, that lets individual software components such as library routines, subroutines, or subsystems be independently analyzed. The paper also presents an algorithm for reusing the results of this separate analysis when linking the individual software components with calling modules. The primary advantage of our algorithms is that they let us analyze frequently used software components, such as library routines or classes, independently, and reuse the results of that analysis when analyzing calling programs, without incurring the expense of completely reanalyzing each calling program.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/226295.309037},
 doi = {http://doi.acm.org/10.1145/226295.309037},
 acmid = {309037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harrold:1996:SCA:229000.309037,
 author = {Harrold, Mary Jean and Rothermel, Gregg},
 title = {Separate computation of alias information for reuse},
 abstract = {Interprocedural dataflow information is useful for many software testing and analysis techniques, including dataflow testing, regression testing, program slicing and impact analysis. For programs with aliases, these testing and analysis techniques can yield invalid results, unless the dataflow information accounts for aliasing effects. Recent research provides algorithms for performing interprocedural dataflow analysis in the presence of aliases; however, these algorithms are expensive, and achieve precise results only on complete programs. This paper presents an algorithm for performing alias analysis on incomplete programs, that lets individual software components such as library routines, subroutines, or subsystems be independently analyzed. The paper also presents an algorithm for reusing the results of this separate analysis when linking the individual software components with calling modules. The primary advantage of our algorithms is that they let us analyze frequently used software components, such as library routines or classes, independently, and reuse the results of that analysis when analyzing calling programs, without incurring the expense of completely reanalyzing each calling program.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/229000.309037},
 doi = {http://doi.acm.org/10.1145/229000.309037},
 acmid = {309037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{DeMillo:1996:CSS:226295.226310,
 author = {DeMillo, Richard A. and Pan, Hsin and Spafford, Eugene H.},
 title = {Critical slicing for software fault localization},
 abstract = {Developing effective debugging strategies to guarantee the reliability of software is important. By analyzing the debugging process used by experienced programmers, we have found that four distinct tasks are consistently performed: (1) determining statements involved in program failures, (2) selecting suspicious statements that might contain faults, (3) making hypotheses about suspicious faults (variables and locations), and (4) restoring program state to a specific statement for verification. This research focuses support for the second task, reducing the search domain for faults, which we refer to as fault localization.</i>We explored a new approach to enhancing the process of fault localization based on dynamic program slicing and mutation-based testing. In this new approach, we have developed the technique of Critical Slicing to enable debuggers to highlight suspicious statements and thus to confine the search domain to a small region. The Critical Slicing technique is partly based on "statement deletion" mutant operator of the mutation-based testing methodology. We have explored properties of Critical Slicing, such as the relationship among Critical Slicing, Dynamic Program Slicing, and Executable Static Program Slicing; the cost to construct critical slices; and the effectiveness of Critical Slicing. Results of experiments support our conjecture as to the effectiveness and feasibility of using Critical Slicing for fault localization.This paper explains our technique and summarizes some of our findings. From these, we conclude that a debugger equipped with our proposed fault localization method can reduce human interaction time significantly and aid in the debugging of complex software.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {121--134},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/226295.226310},
 doi = {http://doi.acm.org/10.1145/226295.226310},
 acmid = {226310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical slicing, debugging, dynamic program slicing, failures, fault localization, faults, mutation analysis, static program slicing, testing},
} 

@inproceedings{DeMillo:1996:CSS:229000.226310,
 author = {DeMillo, Richard A. and Pan, Hsin and Spafford, Eugene H.},
 title = {Critical slicing for software fault localization},
 abstract = {Developing effective debugging strategies to guarantee the reliability of software is important. By analyzing the debugging process used by experienced programmers, we have found that four distinct tasks are consistently performed: (1) determining statements involved in program failures, (2) selecting suspicious statements that might contain faults, (3) making hypotheses about suspicious faults (variables and locations), and (4) restoring program state to a specific statement for verification. This research focuses support for the second task, reducing the search domain for faults, which we refer to as fault localization.</i>We explored a new approach to enhancing the process of fault localization based on dynamic program slicing and mutation-based testing. In this new approach, we have developed the technique of Critical Slicing to enable debuggers to highlight suspicious statements and thus to confine the search domain to a small region. The Critical Slicing technique is partly based on "statement deletion" mutant operator of the mutation-based testing methodology. We have explored properties of Critical Slicing, such as the relationship among Critical Slicing, Dynamic Program Slicing, and Executable Static Program Slicing; the cost to construct critical slices; and the effectiveness of Critical Slicing. Results of experiments support our conjecture as to the effectiveness and feasibility of using Critical Slicing for fault localization.This paper explains our technique and summarizes some of our findings. From these, we conclude that a debugger equipped with our proposed fault localization method can reduce human interaction time significantly and aid in the debugging of complex software.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {121--134},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/229000.226310},
 doi = {http://doi.acm.org/10.1145/229000.226310},
 acmid = {226310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical slicing, debugging, dynamic program slicing, failures, fault localization, faults, mutation analysis, static program slicing, testing},
} 

@article{Marx:1996:PAD:226295.226311,
 author = {Marx, Delia I. S. and Frankl, Phyllis G.},
 title = {The path-wise approach to data flow testing with pointer variables},
 abstract = {This paper describes a new approach to performing data flow testing on programs that use pointer variables and a tool based on this approach. Our technique is based on the observation that, under certain reasonable assumptions, we can determine which dereferenced pointers are aliased whenever control reaches a given program point via a particular path.</i> Furthermore, we can group together paths which behave similarly and represent them by regular expressions. The resulting test requirements demand that the test data execute representatives of particular sets of paths between variable definitions and uses.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {135--146},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/226295.226311},
 doi = {http://doi.acm.org/10.1145/226295.226311},
 acmid = {226311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marx:1996:PAD:229000.226311,
 author = {Marx, Delia I. S. and Frankl, Phyllis G.},
 title = {The path-wise approach to data flow testing with pointer variables},
 abstract = {This paper describes a new approach to performing data flow testing on programs that use pointer variables and a tool based on this approach. Our technique is based on the observation that, under certain reasonable assumptions, we can determine which dereferenced pointers are aliased whenever control reaches a given program point via a particular path.</i> Furthermore, we can group together paths which behave similarly and represent them by regular expressions. The resulting test requirements demand that the test data execute representatives of particular sets of paths between variable definitions and uses.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {135--146},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/229000.226311},
 doi = {http://doi.acm.org/10.1145/229000.226311},
 acmid = {226311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marre:1996:UDU:229000.226312,
 author = {Marr\'{e}, Martina and Bertolino, Antonia},
 title = {Unconstrained duals and their use in achieving all-uses coverage},
 abstract = {Testing takes a considerable amount of the time and resources spent on producing software. It would therefore be useful to have ways 1) to reduce the cost of testing and 2) to estimate this cost. In particular, the number of tests</i> to be executed is an important and useful attribute of the entity "testing effort". All-uses coverage</i> is a data flow testing strategy widely researched in recent years. In this paper we present spanning sets of duas</i> for the all-uses coverage criterion. A spanning set of duas is a minimum set of duas (definition-use associations) such that a set of test paths covering them covers every dua in the program. We give a method to find a spanning set of duas using the relation of subsumption between duas. Intuitively, there exists a natural ordering between the duas in a program: some duas are covered more easily than others, since coverage of the former is automatically guaranteed whenever the latter are covered. Those duas that are the most difficult to be covered according to this ordering are called unconstrained.</i> A spanning set of duas is composed of unconstrained duas. Our results are useful for reducing the cost of testing, since the generation of test paths can be targeted to cover the smaller spanning set of duas, rather than all those in a program. On the other hand, assuming that a different path is taken to cover each dua in a spanning set, the cardinality of spanning sets can be used to estimate the cost of testing. Other interesting uses of spanning sets of duas are also discussed.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/229000.226312},
 doi = {http://doi.acm.org/10.1145/229000.226312},
 acmid = {226312},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marre:1996:UDU:226295.226312,
 author = {Marr\'{e}, Martina and Bertolino, Antonia},
 title = {Unconstrained duals and their use in achieving all-uses coverage},
 abstract = {Testing takes a considerable amount of the time and resources spent on producing software. It would therefore be useful to have ways 1) to reduce the cost of testing and 2) to estimate this cost. In particular, the number of tests</i> to be executed is an important and useful attribute of the entity "testing effort". All-uses coverage</i> is a data flow testing strategy widely researched in recent years. In this paper we present spanning sets of duas</i> for the all-uses coverage criterion. A spanning set of duas is a minimum set of duas (definition-use associations) such that a set of test paths covering them covers every dua in the program. We give a method to find a spanning set of duas using the relation of subsumption between duas. Intuitively, there exists a natural ordering between the duas in a program: some duas are covered more easily than others, since coverage of the former is automatically guaranteed whenever the latter are covered. Those duas that are the most difficult to be covered according to this ordering are called unconstrained.</i> A spanning set of duas is composed of unconstrained duas. Our results are useful for reducing the cost of testing, since the generation of test paths can be targeted to cover the smaller spanning set of duas, rather than all those in a program. On the other hand, assuming that a different path is taken to cover each dua in a spanning set, the cardinality of spanning sets can be used to estimate the cost of testing. Other interesting uses of spanning sets of duas are also discussed.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/226295.226312},
 doi = {http://doi.acm.org/10.1145/226295.226312},
 acmid = {226312},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Daran:1996:SEA:229000.226313,
 author = {Daran, Murial and Th\'{e}venod-Fosse, Pascale},
 title = {Software error analysis: a real case study involving real faults and mutations},
 abstract = {The paper reports on a first experimental comparison of software errors generated by real faults and by 1st-order mutations. The experiments were conducted on a program developed by a student from the industrial specification of a critical software from the civil nuclear field. Emphasis was put on the analysis of errors produced upon activation of 12 real faults by focusing on the mechanisms of error creation, masking, and propagation up to failure occurrence, and on the comparison of these errors with those created by 24 mutations. The results involve a total of 3730 errors recorded from program execution traces: 1458 errors were produced by the real faults, and the 2272 others by the mutations. They are in favor of a suitable consistency between errors generated by mutations and by real faults: 85\% of the 2272 errors due to the mutations were also produced by the real faults. Moreover, it was observed that although the studied mutations were simple faults, they can create erroneous behaviors as complex as those identified for the real faults. This lends support to the representativeness of errors due to mutations.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {158--171},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/229000.226313},
 doi = {http://doi.acm.org/10.1145/229000.226313},
 acmid = {226313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Daran:1996:SEA:226295.226313,
 author = {Daran, Murial and Th\'{e}venod-Fosse, Pascale},
 title = {Software error analysis: a real case study involving real faults and mutations},
 abstract = {The paper reports on a first experimental comparison of software errors generated by real faults and by 1st-order mutations. The experiments were conducted on a program developed by a student from the industrial specification of a critical software from the civil nuclear field. Emphasis was put on the analysis of errors produced upon activation of 12 real faults by focusing on the mechanisms of error creation, masking, and propagation up to failure occurrence, and on the comparison of these errors with those created by 24 mutations. The results involve a total of 3730 errors recorded from program execution traces: 1458 errors were produced by the real faults, and the 2272 others by the mutations. They are in favor of a suitable consistency between errors generated by mutations and by real faults: 85\% of the 2272 errors due to the mutations were also produced by the real faults. Moreover, it was observed that although the studied mutations were simple faults, they can create erroneous behaviors as complex as those identified for the real faults. This lends support to the representativeness of errors due to mutations.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {158--171},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/226295.226313},
 doi = {http://doi.acm.org/10.1145/226295.226313},
 acmid = {226313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pezze:1996:GMS:229000.226314,
 author = {Pezz\`{e}, Mauro and Young, Michal},
 title = {Generation of multi-formalism state-space analysis tools},
 abstract = {As software evolves from early architectural sketches to final code, a variety of representations are appropriate. Moreover, at most points in development, different portions of a software system are at different stages in development, and consequently in different representations. State-space analysis techniques (reachability analysis, model checking, simulation, etc.) have been developed for several representations of concurrent systems, but each tool or technique has typically been targeted to a single design or program notation.We describe an approach to constructing space analysis tools using a core set of basic representations and components. Such a tool generation approach differs from translation to a common formalism. We need not map every supported design formalism to a single internal form that completely captures the original semantics; rather, a shared "inframodel" represents only the essential information for interpretation by tool components that can be customized to reflect the semantics of each formalism. This results in more natural and compact internal representations, and more efficient analysis, than a purely translational approach.We illustrate the approach by applying the prototype tool to a small example problem, coordination of access to a coffee machine. The coffee machine is controlled by an Ada program, and the protocol of human users is modeled with Petri nets. Nets and process graph models are represented in the common internal form, and their composite behavior is analyzed by the prototype tool.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {172--179},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226314},
 doi = {http://doi.acm.org/10.1145/229000.226314},
 acmid = {226314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pezze:1996:GMS:226295.226314,
 author = {Pezz\`{e}, Mauro and Young, Michal},
 title = {Generation of multi-formalism state-space analysis tools},
 abstract = {As software evolves from early architectural sketches to final code, a variety of representations are appropriate. Moreover, at most points in development, different portions of a software system are at different stages in development, and consequently in different representations. State-space analysis techniques (reachability analysis, model checking, simulation, etc.) have been developed for several representations of concurrent systems, but each tool or technique has typically been targeted to a single design or program notation.We describe an approach to constructing space analysis tools using a core set of basic representations and components. Such a tool generation approach differs from translation to a common formalism. We need not map every supported design formalism to a single internal form that completely captures the original semantics; rather, a shared "inframodel" represents only the essential information for interpretation by tool components that can be customized to reflect the semantics of each formalism. This results in more natural and compact internal representations, and more efficient analysis, than a purely translational approach.We illustrate the approach by applying the prototype tool to a small example problem, coordination of access to a coffee machine. The coffee machine is controlled by an Ada program, and the protocol of human users is modeled with Petri nets. Nets and process graph models are represented in the common internal form, and their composite behavior is analyzed by the prototype tool.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {172--179},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226314},
 doi = {http://doi.acm.org/10.1145/226295.226314},
 acmid = {226314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sloane:1996:BTP:226295.226315,
 author = {Sloane, Anthony M. and Holdsworth, Jason},
 title = {Beyond traditional program slicing},
 abstract = {Traditional program slices are based on variables and statements. Slices consist of statements that potentially affect (or are affected by) the value of a particular variable at a given statement. Two assumptions are implicit in this definition: 1) that variables and statements are concepts of the programming language in which the program is written, and 2) that slices consist solely of statements.Generalised slicing is an extension of traditional slicing where variables are replaced by arbitrary named program entities and statements by arbitrary program constructs. A model of generalised slicing is presented that allows the essence of any slicing tool to be reduced to a node marking process operating on a program syntax tree. Slicing tools can thus be implemented in a straight-forward way using tree-based techniques such as attribute grammars.A variety of useful program decompositions are shown to be instances of generalised slicing including: call graph generation, interface extraction, slicing of object-oriented inheritance hierarchies and slices based on type dependences. Examples are also given of how slicing can enhance understanding of formal compiler specifications and aid the creation of subset language specifications.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {180--186},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/226295.226315},
 doi = {http://doi.acm.org/10.1145/226295.226315},
 acmid = {226315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sloane:1996:BTP:229000.226315,
 author = {Sloane, Anthony M. and Holdsworth, Jason},
 title = {Beyond traditional program slicing},
 abstract = {Traditional program slices are based on variables and statements. Slices consist of statements that potentially affect (or are affected by) the value of a particular variable at a given statement. Two assumptions are implicit in this definition: 1) that variables and statements are concepts of the programming language in which the program is written, and 2) that slices consist solely of statements.Generalised slicing is an extension of traditional slicing where variables are replaced by arbitrary named program entities and statements by arbitrary program constructs. A model of generalised slicing is presented that allows the essence of any slicing tool to be reduced to a node marking process operating on a program syntax tree. Slicing tools can thus be implemented in a straight-forward way using tree-based techniques such as attribute grammars.A variety of useful program decompositions are shown to be instances of generalised slicing including: call graph generation, interface extraction, slicing of object-oriented inheritance hierarchies and slices based on type dependences. Examples are also given of how slicing can enhance understanding of formal compiler specifications and aid the creation of subset language specifications.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {180--186},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/229000.226315},
 doi = {http://doi.acm.org/10.1145/229000.226315},
 acmid = {226315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Callahan:1996:AVV:229000.226316,
 author = {Callahan, John R. and Montgomery, Todd L.},
 title = {An approach to verification and validation of a reliable multicasting protocol},
 abstract = {This paper describes the process of implementing a complex communications protocol that provides reliable delivery of data in multicast-capable, packet-switching telecommunication networks. The protocol, called the Reliable Multicasting Protocol (RMP), was developed incrementally using a combination of formal and informal techniques in an attempt to ensure the correctness of its implementation. Our development process involved three concurrent activities: (1) the initial construction and incremental enhancement of a formal state model of the protocol machine; (2) the initial coding and incremental enhancement of the implementation; and (3) model-based testing of iterative implementations of the protocol. These activities were carried out by two separate teams: a design team and a V\&amp;amp;V team. The design team built the first version of RMP with limited functionality to handle only nominal requirements of data delivery. In a series of iterative steps, the design team added new functionality to the implementation while the V\&amp;amp;V team kept the state model in fidelity with the implementation. This was done by generating test cases based on suspected errant or off-nominal behaviors predicted by the current model. If the execution of a test was different between the model and implementation, then the differences helped identify inconsistencies between the model and implementation. The dialogue between both teams drove the co-evolution of the model and implementation. Testing served as the vehicle for keeping the model and implementation in fidelity with each other. This paper describes (1) our experiences in developing our process model; and (2) three example problems found during the development of RMP.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {187--194},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226316},
 doi = {http://doi.acm.org/10.1145/229000.226316},
 acmid = {226316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1996:AVV:226295.226316,
 author = {Callahan, John R. and Montgomery, Todd L.},
 title = {An approach to verification and validation of a reliable multicasting protocol},
 abstract = {This paper describes the process of implementing a complex communications protocol that provides reliable delivery of data in multicast-capable, packet-switching telecommunication networks. The protocol, called the Reliable Multicasting Protocol (RMP), was developed incrementally using a combination of formal and informal techniques in an attempt to ensure the correctness of its implementation. Our development process involved three concurrent activities: (1) the initial construction and incremental enhancement of a formal state model of the protocol machine; (2) the initial coding and incremental enhancement of the implementation; and (3) model-based testing of iterative implementations of the protocol. These activities were carried out by two separate teams: a design team and a V\&amp;amp;V team. The design team built the first version of RMP with limited functionality to handle only nominal requirements of data delivery. In a series of iterative steps, the design team added new functionality to the implementation while the V\&amp;amp;V team kept the state model in fidelity with the implementation. This was done by generating test cases based on suspected errant or off-nominal behaviors predicted by the current model. If the execution of a test was different between the model and implementation, then the differences helped identify inconsistencies between the model and implementation. The dialogue between both teams drove the co-evolution of the model and implementation. Testing served as the vehicle for keeping the model and implementation in fidelity with each other. This paper describes (1) our experiences in developing our process model; and (2) three example problems found during the development of RMP.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {187--194},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226316},
 doi = {http://doi.acm.org/10.1145/226295.226316},
 acmid = {226316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Offutt:1996:SMP:226295.226317,
 author = {Offutt, A. Jefferson and Hayes, J. Huffman},
 title = {A semantic model of program faults},
 abstract = {Program faults are artifacts that are widely studied, but there are many aspects of faults that we still do not understand. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas in testing. These include fault-based testing, testability, mutation testing, and the comparative evaluation of testing strategies. In this workshop paper, we explore the fundamental nature of faults by looking at the differences between a syntactic and semantic characterization of faults. We offer definitions of these characteristics and explore the differentiation. Specifically, we discuss the concept of "size" of program faults --- the measurement of size provides interesting and useful distinctions between the syntactic and semantic characterization of faults. We use the fault size observations to make several predictions about testing and present preliminary data that supports this model. We also use the model to offer explanations about several questions that have intrigued testing researchers.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {195--200},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/226295.226317},
 doi = {http://doi.acm.org/10.1145/226295.226317},
 acmid = {226317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Offutt:1996:SMP:229000.226317,
 author = {Offutt, A. Jefferson and Hayes, J. Huffman},
 title = {A semantic model of program faults},
 abstract = {Program faults are artifacts that are widely studied, but there are many aspects of faults that we still do not understand. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas in testing. These include fault-based testing, testability, mutation testing, and the comparative evaluation of testing strategies. In this workshop paper, we explore the fundamental nature of faults by looking at the differences between a syntactic and semantic characterization of faults. We offer definitions of these characteristics and explore the differentiation. Specifically, we discuss the concept of "size" of program faults --- the measurement of size provides interesting and useful distinctions between the syntactic and semantic characterization of faults. We use the fault size observations to make several predictions about testing and present preliminary data that supports this model. We also use the model to offer explanations about several questions that have intrigued testing researchers.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {195--200},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/229000.226317},
 doi = {http://doi.acm.org/10.1145/229000.226317},
 acmid = {226317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:1996:TSL:229000.226318,
 author = {Yang, Cheer-Sun D. and Pollock, Lori L.},
 title = {Towards a structural load testing tool},
 abstract = {Load sensitive faults cause a program to fail when it is executed under a heavy load or over a long period of time, but may have no detrimental effect under small loads or short executions. In addition to testing the functionality of these programs, testing how well they perform under stress</i> is very important. Current approaches to stress, or load, testing treat the system as a black box,</i> generating test data based on parameters specified by the tester within an operational profile. In this paper, we advocate a structural</i> approach to load testing. There exist many structural testing methods; however, their main goal is generating test data for executing all statements, branches, definition-use pairs, or paths of a program at least once, without consideration for executing any particular path extensively.Our initial work has focused on the identification of potentially load sensitive</i> modules based on a static analysis of the module's code, and then limiting the stress testing to the regions of the modules that could be the potential causes of the load sensitivity. This analysis will be incorporated into a testing tool for structural load testing which takes a program as input, and automatically determines whether that program needs to be load tested, and if so, automatically generates test data for structural load testing of the program.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {201--208},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226318},
 doi = {http://doi.acm.org/10.1145/229000.226318},
 acmid = {226318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yang:1996:TSL:226295.226318,
 author = {Yang, Cheer-Sun D. and Pollock, Lori L.},
 title = {Towards a structural load testing tool},
 abstract = {Load sensitive faults cause a program to fail when it is executed under a heavy load or over a long period of time, but may have no detrimental effect under small loads or short executions. In addition to testing the functionality of these programs, testing how well they perform under stress</i> is very important. Current approaches to stress, or load, testing treat the system as a black box,</i> generating test data based on parameters specified by the tester within an operational profile. In this paper, we advocate a structural</i> approach to load testing. There exist many structural testing methods; however, their main goal is generating test data for executing all statements, branches, definition-use pairs, or paths of a program at least once, without consideration for executing any particular path extensively.Our initial work has focused on the identification of potentially load sensitive</i> modules based on a static analysis of the module's code, and then limiting the stress testing to the regions of the modules that could be the potential causes of the load sensitivity. This analysis will be incorporated into a testing tool for structural load testing which takes a program as input, and automatically determines whether that program needs to be load tested, and if so, automatically generates test data for structural load testing of the program.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {201--208},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226318},
 doi = {http://doi.acm.org/10.1145/226295.226318},
 acmid = {226318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Korel:1996:ATD:229000.226319,
 author = {Korel, Bogdan},
 title = {Automated test data generation for programs with procedures},
 abstract = {Test data generation in program testing is the process of identifying a set of test data that satisfies a selected testing criterion, such as, statement coverage or branch coverage. The existing methods of test data generation are limited to unit testing and may not efficiently generate test data for programs with procedures. In this paper we present an approach for automated test data generation for programs with procedures. This approach builds on the current theory of execution-oriented test data generation. In this approach, test data are derived based on the actual execution of the program under test. For many programs, the execution of the selected statement may require prior execution of some other statements that may be part of some procedures. The existing methods use only control flow information of a program during the search process and may not efficiently generate test data for these types of programs because they are not able to identify statements that affect execution of the selected statement. Our approach uses data dependence analysis to guide the process of test data generation. Data dependence analysis automatically identifies statements (or procedures) that affect the execution of the selected statement and this information is used to guide the search process. The initial experiments have shown that this approach may improve the chances of finding test data.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {209--215},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/229000.226319},
 doi = {http://doi.acm.org/10.1145/229000.226319},
 acmid = {226319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Korel:1996:ATD:226295.226319,
 author = {Korel, Bogdan},
 title = {Automated test data generation for programs with procedures},
 abstract = {Test data generation in program testing is the process of identifying a set of test data that satisfies a selected testing criterion, such as, statement coverage or branch coverage. The existing methods of test data generation are limited to unit testing and may not efficiently generate test data for programs with procedures. In this paper we present an approach for automated test data generation for programs with procedures. This approach builds on the current theory of execution-oriented test data generation. In this approach, test data are derived based on the actual execution of the program under test. For many programs, the execution of the selected statement may require prior execution of some other statements that may be part of some procedures. The existing methods use only control flow information of a program during the search process and may not efficiently generate test data for these types of programs because they are not able to identify statements that affect execution of the selected statement. Our approach uses data dependence analysis to guide the process of test data generation. Data dependence analysis automatically identifies statements (or procedures) that affect the execution of the selected statement and this information is used to guide the search process. The initial experiments have shown that this approach may improve the chances of finding test data.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {209--215},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/226295.226319},
 doi = {http://doi.acm.org/10.1145/226295.226319},
 acmid = {226319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pomakis:1996:RAF:229000.226320,
 author = {Pomakis, Keith P. and Atlee, Joanne M.},
 title = {Reachability analysis of feature interactions: a progress report},
 abstract = {Features are added to an existing system to add functionality. A new feature interacts</i> with an existing feature if the behavior of the existing feature is changed by the presence of the new feature. Our research group has started to investigate how to detect feature interactions during the requirements phase of feature development. We have adopted a layered state-transition machine model that prioritizes features and avoids interactions due to non-determinism. We have a tabular notation for specifying behavioral requirements of services and features. Specifications are composed into a reachability graph, and the graph is searched for feature interactions. This paper demonstrates how reachability analysis has been used to automatically detect known control interactions, data interactions, and resource contentions among telephony features.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {216--223},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/229000.226320},
 doi = {http://doi.acm.org/10.1145/229000.226320},
 acmid = {226320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pomakis:1996:RAF:226295.226320,
 author = {Pomakis, Keith P. and Atlee, Joanne M.},
 title = {Reachability analysis of feature interactions: a progress report},
 abstract = {Features are added to an existing system to add functionality. A new feature interacts</i> with an existing feature if the behavior of the existing feature is changed by the presence of the new feature. Our research group has started to investigate how to detect feature interactions during the requirements phase of feature development. We have adopted a layered state-transition machine model that prioritizes features and avoids interactions due to non-determinism. We have a tabular notation for specifying behavioral requirements of services and features. Specifications are composed into a reachability graph, and the graph is searched for feature interactions. This paper demonstrates how reachability analysis has been used to automatically detect known control interactions, data interactions, and resource contentions among telephony features.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {216--223},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/226295.226320},
 doi = {http://doi.acm.org/10.1145/226295.226320},
 acmid = {226320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bultan:1996:CVM:226295.226321,
 author = {Bultan, Tevfik and Fischer, Jeffrey and Gerber, Richard},
 title = {Compositional verification by model checking for counter-examples},
 abstract = {Many concurrent systems are required to maintain certain safety and liveness properties. One emerging method of achieving confidence in such systems is to statically verify them using model checking.</i> In this approach an abstract, finite-state model of the system is constructed; then an automatic check is made to ensure that the requirements are satisfied by the model. In practice, however, this method is limited by the state space explosion problem.</i>We have developed a compositional method that directly addresses this problem in the context of multi-tasking programs. Our solution depends on three key space-saving ingredients: (1) checking for counter-examples, which leads to simpler search algorithms; (2) automatic extraction of interfaces, which allows a refinement of the finite model --- even before its communicating partners have been compiled; and (3) using propositional "strengthening assertions" for the sole purpose of reducing state space.In this paper we present our compositional approach, and describe the software tools that support it.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {224--238},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/226295.226321},
 doi = {http://doi.acm.org/10.1145/226295.226321},
 acmid = {226321},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bultan:1996:CVM:229000.226321,
 author = {Bultan, Tevfik and Fischer, Jeffrey and Gerber, Richard},
 title = {Compositional verification by model checking for counter-examples},
 abstract = {Many concurrent systems are required to maintain certain safety and liveness properties. One emerging method of achieving confidence in such systems is to statically verify them using model checking.</i> In this approach an abstract, finite-state model of the system is constructed; then an automatic check is made to ensure that the requirements are satisfied by the model. In practice, however, this method is limited by the state space explosion problem.</i>We have developed a compositional method that directly addresses this problem in the context of multi-tasking programs. Our solution depends on three key space-saving ingredients: (1) checking for counter-examples, which leads to simpler search algorithms; (2) automatic extraction of interfaces, which allows a refinement of the finite model --- even before its communicating partners have been compiled; and (3) using propositional "strengthening assertions" for the sole purpose of reducing state space.In this paper we present our compositional approach, and describe the software tools that support it.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {224--238},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/229000.226321},
 doi = {http://doi.acm.org/10.1145/229000.226321},
 acmid = {226321},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jackson:1996:ESA:226295.226322,
 author = {Jackson, Daniel and Damon, Craig A.},
 title = {Elements of style: analyzing a software design feature with a counterexample detector},
 abstract = {We illustrate the application of Nitpick, a specification checker, to the design of a style mechanism for a word processor. The design is cast, along with some expected properties, in a subset of Z. Nitpick checks a property by enumerating all possible cases within some finite bounds, displaying as a counterexample the first case for which the property fails to hold. Unlike animation or execution tools, Nitpick does not require state transitions to be expressed constructively, and unlike theorem provers, operates completely automatically without user intervention. Using a variety of reduction mechanisms, it can cover an enormous number of cases in a reasonable time, so that subtle flaws can be rapidly detected.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {239--249},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/226295.226322},
 doi = {http://doi.acm.org/10.1145/226295.226322},
 acmid = {226322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jackson:1996:ESA:229000.226322,
 author = {Jackson, Daniel and Damon, Craig A.},
 title = {Elements of style: analyzing a software design feature with a counterexample detector},
 abstract = {We illustrate the application of Nitpick, a specification checker, to the design of a style mechanism for a word processor. The design is cast, along with some expected properties, in a subset of Z. Nitpick checks a property by enumerating all possible cases within some finite bounds, displaying as a counterexample the first case for which the property fails to hold. Unlike animation or execution tools, Nitpick does not require state transitions to be expressed constructively, and unlike theorem provers, operates completely automatically without user intervention. Using a variety of reduction mechanisms, it can cover an enormous number of cases in a reasonable time, so that subtle flaws can be rapidly detected.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {239--249},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/229000.226322},
 doi = {http://doi.acm.org/10.1145/229000.226322},
 acmid = {226322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Corbett:1996:CAM:229000.226323,
 author = {Corbett, James C.},
 title = {Constructing abstract models of concurrent real-time software},
 abstract = {Concurrent real-time software is used in many safety-critical applications. Assuring the quality of such software requires the use of formal methods. Before a program can be analyzed formally, however, we must construct a mathematical model that captures the aspects of the program we want to verify. In this paper, we show how to construct mathematical models of concurrent real-time software that are suitable for analyzing the program's timing properties. Our approach differs from schedulability analysis in that we do not assume that the software has a highly restricted structure (e.g., a set of periodic tasks). Also, unlike most more abstract models of real-time systems, we account for essential properties of real implementations, such as resource constraints and run-time overhead.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {250--260},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/229000.226323},
 doi = {http://doi.acm.org/10.1145/229000.226323},
 acmid = {226323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Corbett:1996:CAM:226295.226323,
 author = {Corbett, James C.},
 title = {Constructing abstract models of concurrent real-time software},
 abstract = {Concurrent real-time software is used in many safety-critical applications. Assuring the quality of such software requires the use of formal methods. Before a program can be analyzed formally, however, we must construct a mathematical model that captures the aspects of the program we want to verify. In this paper, we show how to construct mathematical models of concurrent real-time software that are suitable for analyzing the program's timing properties. Our approach differs from schedulability analysis in that we do not assume that the software has a highly restricted structure (e.g., a set of periodic tasks). Also, unlike most more abstract models of real-time systems, we account for essential properties of real implementations, such as resource constraints and run-time overhead.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {250--260},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/226295.226323},
 doi = {http://doi.acm.org/10.1145/226295.226323},
 acmid = {226323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Godefroid:1996:UPM:229000.226324,
 author = {Godefroid, Patrice and Peled, Doron and Staskauskas, Mark},
 title = {Using partial-order methods in the formal validation of industrial concurrent programs},
 abstract = {We have developed a formal validation tool that has been used on several projects that are developing software for AT\&amp;amp;T's 5ESS<sup>\&amp;trade;</sup> telephone switching system. The tool uses Holzmann's supertrace algorithm to check for errors such as deadlock and livelock in networks of communicating processes. The validator invariably finds subtle errors that were missed during thorough simulation and testing; however, the brute-force search it performs can result in extremely long running times, which can be frustrating to users. Recently, a number of researchers have been investigating techniques known as partial-order methods</i> that can significantly reduce the running time of formal validation by avoiding redundant exploration of execution scenarios. In this paper, we describe the design of a partial-order algorithm for our validation tool and discuss its effectiveness. We show that a careful compile-time static analysis of process communication behavior yields information that can be used during validation to dramatically improve its performance. We demonstrate the effectiveness of our partial-order algorithm by presenting the results of experiments with actual industrial examples drawn from a variety of 5ESS<sup>\&amp;trade;</sup> application domains, including call processing, signalling, and switch maintenance.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {261--269},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/229000.226324},
 doi = {http://doi.acm.org/10.1145/229000.226324},
 acmid = {226324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Godefroid:1996:UPM:226295.226324,
 author = {Godefroid, Patrice and Peled, Doron and Staskauskas, Mark},
 title = {Using partial-order methods in the formal validation of industrial concurrent programs},
 abstract = {We have developed a formal validation tool that has been used on several projects that are developing software for AT\&amp;amp;T's 5ESS<sup>\&amp;trade;</sup> telephone switching system. The tool uses Holzmann's supertrace algorithm to check for errors such as deadlock and livelock in networks of communicating processes. The validator invariably finds subtle errors that were missed during thorough simulation and testing; however, the brute-force search it performs can result in extremely long running times, which can be frustrating to users. Recently, a number of researchers have been investigating techniques known as partial-order methods</i> that can significantly reduce the running time of formal validation by avoiding redundant exploration of execution scenarios. In this paper, we describe the design of a partial-order algorithm for our validation tool and discuss its effectiveness. We show that a careful compile-time static analysis of process communication behavior yields information that can be used during validation to dramatically improve its performance. We demonstrate the effectiveness of our partial-order algorithm by presenting the results of experiments with actual industrial examples drawn from a variety of 5ESS<sup>\&amp;trade;</sup> application domains, including call processing, signalling, and switch maintenance.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {261--269},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/226295.226324},
 doi = {http://doi.acm.org/10.1145/226295.226324},
 acmid = {226324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barjaktarovic:1996:FSV:226295.226325,
 author = {Barjaktarovic, Milica and Chin, Shiu-Kai and Jabbour, Kamal},
 title = {Formal specification and verification of the kernel functional unit of the OSI session layer protocol and service using CCS},
 abstract = {This paper describes an application of formal methods to protocol specification, validation and verification. Formal methods can be incorporated in protocol design and testing so that time and resources are saved on implementation, testing, and documentation. In this paper we show how formal methods can be used to write the control sequence, i.e. pseudo code, which can be formally tested using automated support. The formal specification serves as a blueprint for a correct implementation with desired properties.As a formal method we chose a process algebra called "plain" Calculus of Communicating Systems (CCS). Our specific objectives were to: 1) build a CCS model of the Kernel Functional Unit of OSI session layer service: 2) obtain a session protocol specification through stepwise refinement of the service specification; and 3) verify that the protocol specification satisfies the service specification. We achieved all of our objectives. Verification and validation were accomplished by using the CCS's model checker, the Edinburgh Concurrency Workbench (CWB). We chose plain CCS because of itssuccinct, abstract, and modular specifications, strong mathematical foundation which allows for formal reasoning and proofs, and existence of the automated support tool which supports temporal logic. The motivation for this work is: 1) testing the limits of CCS's succinct notation; 2) combining CCS and temporal logic; and 3) using a model-checker on a real-life example.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {270--279},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/226295.226325},
 doi = {http://doi.acm.org/10.1145/226295.226325},
 acmid = {226325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barjaktarovic:1996:FSV:229000.226325,
 author = {Barjaktarovic, Milica and Chin, Shiu-Kai and Jabbour, Kamal},
 title = {Formal specification and verification of the kernel functional unit of the OSI session layer protocol and service using CCS},
 abstract = {This paper describes an application of formal methods to protocol specification, validation and verification. Formal methods can be incorporated in protocol design and testing so that time and resources are saved on implementation, testing, and documentation. In this paper we show how formal methods can be used to write the control sequence, i.e. pseudo code, which can be formally tested using automated support. The formal specification serves as a blueprint for a correct implementation with desired properties.As a formal method we chose a process algebra called "plain" Calculus of Communicating Systems (CCS). Our specific objectives were to: 1) build a CCS model of the Kernel Functional Unit of OSI session layer service: 2) obtain a session protocol specification through stepwise refinement of the service specification; and 3) verify that the protocol specification satisfies the service specification. We achieved all of our objectives. Verification and validation were accomplished by using the CCS's model checker, the Edinburgh Concurrency Workbench (CWB). We chose plain CCS because of itssuccinct, abstract, and modular specifications, strong mathematical foundation which allows for formal reasoning and proofs, and existence of the automated support tool which supports temporal logic. The motivation for this work is: 1) testing the limits of CCS's succinct notation; 2) combining CCS and temporal logic; and 3) using a model-checker on a real-life example.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {270--279},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/229000.226325},
 doi = {http://doi.acm.org/10.1145/229000.226325},
 acmid = {226325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Atlee:1996:LSS:226295.226326,
 author = {Atlee, Joanne M. and Buckley, Michael A.},
 title = {A logic-model semantics for SCR software requirements},
 abstract = {This paper presents a simple logic-model semantics for Software Cost Reduction (SCR) software requirements. Such a semantics enables model-checking of native SCR requirements and obviates the need to transform the requirements for analysis. The paper also proposes modal-logic abbreviations for expressing conditioned events in temporal-logic formulae. The Symbolic Model Verifier (SMV) is used to verify that an SCR requirements specification enforces desired global requirements, expressed as formulae in the enhanced logic. The properties of a small system (an automobile cruise control system) are verified, including an invariant property that could not be verified previously. The paper concludes with a discussion of how other requirements notations for conditioned-event-driven systems could be similarly checked.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {280--292},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/226295.226326},
 doi = {http://doi.acm.org/10.1145/226295.226326},
 acmid = {226326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal semantics, model checking, reactive systems, software requirements},
} 

@inproceedings{Atlee:1996:LSS:229000.226326,
 author = {Atlee, Joanne M. and Buckley, Michael A.},
 title = {A logic-model semantics for SCR software requirements},
 abstract = {This paper presents a simple logic-model semantics for Software Cost Reduction (SCR) software requirements. Such a semantics enables model-checking of native SCR requirements and obviates the need to transform the requirements for analysis. The paper also proposes modal-logic abbreviations for expressing conditioned events in temporal-logic formulae. The Symbolic Model Verifier (SMV) is used to verify that an SCR requirements specification enforces desired global requirements, expressed as formulae in the enhanced logic. The properties of a small system (an automobile cruise control system) are verified, including an invariant property that could not be verified previously. The paper concludes with a discussion of how other requirements notations for conditioned-event-driven systems could be similarly checked.},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {280--292},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/229000.226326},
 doi = {http://doi.acm.org/10.1145/229000.226326},
 acmid = {226326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal semantics, model checking, reactive systems, software requirements},
} 

@article{Denney:1996:WSS:226295.226327,
 author = {Denney, Richard and Kemmerer, Dick and Leveson, Nancy and Savoia, Alberto},
 title = {Why state-of-the-art is not state-of-the-practice (panel)},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {21},
 issue = {3},
 month = {May},
 year = {1996},
 issn = {0163-5948},
 pages = {293--},
 url = {http://doi.acm.org/10.1145/226295.226327},
 doi = {http://doi.acm.org/10.1145/226295.226327},
 acmid = {226327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Denney:1996:WSS:229000.226327,
 author = {Denney, Richard and Kemmerer, Dick and Leveson, Nancy and Savoia, Alberto},
 title = {Why state-of-the-art is not state-of-the-practice (panel)},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '96},
 year = {1996},
 isbn = {0-89791-787-1},
 location = {San Diego, California, United States},
 pages = {293--},
 url = {http://doi.acm.org/10.1145/229000.226327},
 doi = {http://doi.acm.org/10.1145/229000.226327},
 acmid = {226327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chechik:1994:AVR:186258.186324,
 author = {Chechik, Marsha and Gannon, John},
 title = {Automatic verification of requirements implementation},
 abstract = {Requirements of event-based systems can be automatically analyzed to determine if certain safety properties hold. However, we lack comparable methods to verify that implementations maintain the properties guaranteed by the requirements. We have built a tool that compares implementations written in C with their requirements. Requirements describe events which cause state transitions. Implementations are annotated to describe changes in the values of their requirement's variables, and dataflow analysis techniques are used to determine the set of events which cause particular state changes. To show that an implementation is consistent with its requirements, we show that each event causing a change of state in the implementation appears in the requirements, and that all the events specified to cause state changes in the requirements appear in the implementation. The annotation language encourages programmers to describe local program behaviors. These behaviors are collected into system-level behaviors, which are compared to those in the requirements. Since our analysis is not based on program code, annotations can describe behaviors at any level of granularity. We illustrate the use of our tool with several different annotations of a temperature-control system.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/186258.186324},
 doi = {http://doi.acm.org/10.1145/186258.186324},
 acmid = {186324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Douglas:1994:ASE:186258.186487,
 author = {Douglas, Jeffrey and Kemmerer, Richard A.},
 title = {Aslantest: a symbolic execution tool for testing Aslan formal specifications},
 abstract = {This paper introduces Aslantest, a symbolic execution tool for the formal specification language Aslan. Aslan is a state-based specification language built on first-order predicate calculus with equality. Aslantest animates Aslan specifications and enables users to interactively run specific test cases or symbolically execute the specification. Testing the formal specifications early in the software life cycle allows one to assure a reliable system that also provides the desired functionality.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {15--27},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/186258.186487},
 doi = {http://doi.acm.org/10.1145/186258.186487},
 acmid = {186487},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kapur:1994:ATA:186258.186496,
 author = {Kapur, Deepak},
 title = {An automated tool for analyzing completeness of equational specifications},
 abstract = {Books on software engineering methodologies talk about the significance and need for designing consistent and complete specifications during the requirement analysis and design stages of a software development cycle. There is, however, little (or at best very limited) discussion of methods for ensuring these structural properties of specifications. In this paper, we discuss methods for checking completeness of equational specifications. Some of these methods were earlier proposed in somewhat different form in the context of developing the so-called inductionless induction method for automating proofs by induction using completion procedures. These methods are implemented in our theorem prover Rewrite Rule Laboratory (RRL), and have been tried on a number of examples  of specifications of data abstractions. In case a specification is incomplete, these methods can aid in making them complete by generating templates which are not specified. Templates can also be helpful in distinguishing between intentional and unintentional incompleteness in specifications. Further, these methods can be used to generate test cases for checking specifications and verifying implementations of specifications. These methods are illustrated on examples which exhibit their power as well as limitations.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {28--43},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/186258.186496},
 doi = {http://doi.acm.org/10.1145/186258.186496},
 acmid = {186496},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Avritzer:1994:GTS:186258.186507,
 author = {Avritzer, Alberto and Weyuker, Elaine J.},
 title = {Generating test suites for software load testing},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {44--57},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/186258.186507},
 doi = {http://doi.acm.org/10.1145/186258.186507},
 acmid = {186507},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peters:1994:GTO:186258.186508,
 author = {Peters, Dennis and Parnas, David L.},
 title = {Generating a test oracle from program documentation: work in progress},
 abstract = {A fundamental assumption of software testing is that there is some mechanism, an oracle, that will determine whether or not the results of a test execution are correct. In practice this is often done by comparing the output, either automatically or manually, to some pre-calculated, presumably correct, output [17]. However, if the program is formally documented it is possible to use the specification to determine the success or failure of a test execution, as in [1], for example. This paper discusses ongoing work to produce a tool that will generate a test oracle from formal program documentation.In [9], [10] and [11] Parnas et al. advocate the use of a relational model for documenting the intended behaviour of programs. In this method, tabular expressions are used to  improve readability so that formal documentation can replace conventional documentation. Relations are described by giving their characteristic predicate in terms of the values of concrete program variables. This documentation method has the advantage that the characteristic predicate can be used as the test oracle -- it simply must be evaluated for each test execution (input \&amp; output) to assign pass or fail. In contrast to [1], this paper discusses the testing of individual programs, not objects as used in [1]. Consequently, the method works with program documentation, written in terms of the concrete variables, and no representation function need be supplied. Documentation in this form, and the corresponding oracle, are illustrated by an example.Finally, some of the  implications of generating test oracles from relational specifications are discussed.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {58--65},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/186258.186508},
 doi = {http://doi.acm.org/10.1145/186258.186508},
 acmid = {186508},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Korel:1994:FCD:186258.186514,
 author = {Korel, Bogdan and Yalamanchili, Satish},
 title = {Forward computation of dynamic program slices},
 abstract = {A dynamic program slice is an executable part of the program whose behavior is identical, for the same program input, to that of the original program with respect to a variable(s) of interest at some execution position. It has been shown that dynamic slicing is useful for the purpose of debugging, testing and software maintenance. The existing methods of dynamic slice computation are based on ``backward" analysis, i.e., after the execution trace of the program is first recorded, the dynamic slice algorithm traces backwards the execution trace to derive dynamic dependence relations that are then used to compute dynamic slices. For many programs, during their execution extremely high volume of information may be recorded that may prevent accurate dynamic slice computation. In this paper we present a novel approach of dynamic slice computation, referred to as forward approach of dynamic slice computation. In this method, dynamic slices are computed during program execution without major recording of the execution trace. The major advantage of the forward approach is that space complexity is bounded as opposed to the backward methods of slice computation.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {66--79},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/186258.186514},
 doi = {http://doi.acm.org/10.1145/186258.186514},
 acmid = {186514},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goldberg:1994:AFP:186258.186523,
 author = {Goldberg, Allen and Wang, T. C. and Zimmerman, David},
 title = {Applications of feasible path analysis to program testing},
 abstract = {For certain structural testing criteria a significant proportion of tests instances are infeasible in the sense the semantics of the program implies that test data cannot be constructed that meet the test requirement. This paper describes the design and prototype implementation of a structural testing system that uses a theorem prover to determine feasibility of testing requirements and to optimize the number of test cases required to achieve test coverage. Using this approach, we were able to accurately and efficiently determine path feasibility for moderately-sized program units of production code written in a subset of Ada. On these problems, the computer solutions were obtained much faster and with greater accuracy than manual analysis. The paper describes how we formalize test criteria as control flow graph path expressions; how the criteria are mapped to logic formulas; and how we control the complexity of the inference task. It describes the limitations of the system and proposals for its improvement as well as other applications of the analysis.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {80--94},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/186258.186523},
 doi = {http://doi.acm.org/10.1145/186258.186523},
 acmid = {186523},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jasper:1994:TDG:186258.187150,
 author = {Jasper, Robert and Brennan, Mike and Williamson, Keith and Currier, Bill and Zimmerman, David},
 title = {Test data generation and feasible path analysis},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {95--107},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/186258.187150},
 doi = {http://doi.acm.org/10.1145/186258.187150},
 acmid = {187150},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bochmann:1994:PTR:186258.187153,
 author = {Bochmann, Gregor V. and Petrenko, Alexandre},
 title = {Protocol testing: review of methods and relevance for software testing},
 abstract = {Communication protocols are the rules that govern the communication between the different components within a distributed computer system. Since protocols are implemented in software and/or hardware, the question arises whether the existing hardware and software testing methods would be adequate for the testing of communication protocols. The purpose of this paper is to explain in which way the problem of testing protocol implementations is different from the usual problem of software testing. We review the major results in the area of protocol testing and discuss in which way these methods may also be relevant in the more general context of software testing.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {109--124},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/186258.187153},
 doi = {http://doi.acm.org/10.1145/186258.187153},
 acmid = {187153},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Karam:1994:VUT:186258.187157,
 author = {Karam, Gerald M.},
 title = {Visualization using timelines},
 abstract = {A timeline is a linear, graphical visualization of events over time. For example, in concurrent application, events would represent state changes for some system object (such as a task or variable). A timeline display generator creates the graphical visualization from some record of events. This paper reports on a model for timeline display generators based on a formal model of event history and the objectives of timeline visualization. In this model, any timeline display generator is completely described through the definition of a set of mathematical functions. The exact characteristics and flexibility of a particular implementation of a timeline display generator, depends on the way in which these functions have been implemented. The current prototype, xtg, (Timeline Display Generator for X-windows) serves as an example implementation of these ideas. Characteristics of xtg are presented, and its use in the analysis of a real-world client-server application is discussed. Xtg has been applied to several other applications to-date and is being applied by several telecommunications companies to areas ranging from software process analysis to call trace data analysis.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {125--137},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/186258.187157},
 doi = {http://doi.acm.org/10.1145/186258.187157},
 acmid = {187157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Richardson:1994:TTA:186258.187158,
 author = {Richardson, Debra J.},
 title = {TAOS: Testing with Analysis and Oracle Support},
 abstract = {Few would question that software testing is a necessary activity for assuring software quality, yet the typical testing process is a human intensive activity and as such, it is unproductive, error-prone, and often inadequately done. Moreover, testing is seldom given a prominent place in software development or maintenance processes, nor is it an integral part of them. Major productivity and quality enhancements can be achieved by automating the testing process through tool development and use and effectively incorporating it with development and maintenance processes.
},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {138--153},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/186258.187158},
 doi = {http://doi.acm.org/10.1145/186258.187158},
 acmid = {187158},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Siepmann:1994:TTC:186258.187168,
 author = {Siepmann, Ernst and Newton, A. Richard},
 title = {TOBAC: a test case browser for testing object-oriented software},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {154--168},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/186258.187168},
 doi = {http://doi.acm.org/10.1145/186258.187168},
 acmid = {187168},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rothermel:1994:STI:186258.187171,
 author = {Rothermel, Gregg and Harrold, Mary Jean},
 title = {Selecting tests and identifying test coverage requirements for modified software},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {169--184},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/186258.187171},
 doi = {http://doi.acm.org/10.1145/186258.187171},
 acmid = {187171},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fleyshgakker:1994:EMA:186258.187179,
 author = {Fleyshgakker, Vladimir N. and Weiss, Stewart N.},
 title = {Efficient mutation analysis: a new approach},
 abstract = {In previously reported research we designed and analyzed algorithms that improved upon the run time complexity of all known weak and strong mutation analysis methods at the expense of increased space complexity. Here we describe a new serial strong mutation algorithm whose running time is on the average much faster than the previous ones and that uses significantly less space than them also. Its space requirement is approximately the same as that of Mothra, a well-known and readily available implemented system. Moreover, while this algorithm can serve as basis for a new mutation system, it is designed to be consistent with the Mothra architecture, in the sense that, by replacing certain modules of that system with new ones, a much faster system will result. Such a Mothra-based implementation of the new work is in progress.Like the previous algorithms, this one, which we call Lazy Mutant Analysis or LMA, tries to determine whether a mutant is strongly killed by a given test only if it is already known that it is weakly killed by that test. Unlike those algorithms, LMA avoids executing many mutants by dynamically discovering classes of mutants that have the ``same" behavior, and executing representatives of those classes. The overhead it incurs is small in proportion to the time saved, and the algorithm has a very natural parallel implementation.In comparison to the fastest known algorithms for strong mutation analysis, in the best case, LMA can improve the speed by a factor proportional to the average number of mutants per program statement. In the worst case, there is no improvement in the running time, but such a case is hard to construct. This work enables us to apply mutation analysis to significantly larger programs than is currently possible.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {185--195},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/186258.187179},
 doi = {http://doi.acm.org/10.1145/186258.187179},
 acmid = {187179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Howden:1994:COS:186258.187194,
 author = {Howden, W. E. and Huang, Yudong},
 title = {Confidence oriented software dependability measurement},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {196--},
 url = {http://doi.acm.org/10.1145/186258.187194},
 doi = {http://doi.acm.org/10.1145/186258.187194},
 acmid = {187194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marcus:1994:ITV:186258.187196,
 author = {Marcus, Leo},
 title = {The incorporation of testing into verification (abstract): direct, modular, and hierarchical correctness degrees},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {197--},
 url = {http://doi.acm.org/10.1145/186258.187196},
 doi = {http://doi.acm.org/10.1145/186258.187196},
 acmid = {187196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Forgacs:1994:PFC:186258.187197,
 author = {Forg\'{a}cs, Istv\'{a}n},
 title = {The all program functions criterion for revealing computation errors},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {198--},
 url = {http://doi.acm.org/10.1145/186258.187197},
 doi = {http://doi.acm.org/10.1145/186258.187197},
 acmid = {187197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Knight:1994:TSA:186258.187198,
 author = {Knight, John C. and Cass, Aaron G. and Fern\'{a}ndez, Antonio M. and Wika, Kevin G.},
 title = {Testing a safety-critical application},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {199--},
 url = {http://doi.acm.org/10.1145/186258.187198},
 doi = {http://doi.acm.org/10.1145/186258.187198},
 acmid = {187198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Murrill:1994:EAA:186258.187200,
 author = {Murrill, Branson W. and Morell, Larry},
 title = {An experimental approach to analyzing software semantics using error flow information},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {200--},
 url = {http://doi.acm.org/10.1145/186258.187200},
 doi = {http://doi.acm.org/10.1145/186258.187200},
 acmid = {187200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pollock:1994:DOC:186258.187201,
 author = {Pollock, Lori and Bivens, Mary and Soffa, Mary Lou},
 title = {Debugging optimized code via tailoring},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {201--},
 url = {http://doi.acm.org/10.1145/186258.187201},
 doi = {http://doi.acm.org/10.1145/186258.187201},
 acmid = {187201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bertolino:1994:MBB:186258.187203,
 author = {Bertolino, Antonia and Marr\'{e}, Martina},
 title = {A meaningful bound for branch testing},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {202--},
 url = {http://doi.acm.org/10.1145/186258.187203},
 doi = {http://doi.acm.org/10.1145/186258.187203},
 acmid = {187203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Young:1994:SAA:186258.187204,
 author = {Young, Michal},
 title = {State-space analysis as an aid to testing},
 abstract = {Non-determinism makes testing concurrent software difficult. We consider how pre-run-time state-space analysis can be used to aid in testing implementations of concurrent software. State-space analysis techniques have the advantage in principle of exploring all possible execution histories, but they do not verify all properties of interest and in practice they may not accurately model program execution. Combining state-space analysis with testing can partially overcome the weaknesses of each. Using the state-space model in a test oracle is the simpler part: techniques based on classical automata theory are suitable for this. Covering all important non-deterministic executions is harder. We propose a pragmatic method for detecting unexecuted paths that are certainly executable and possibly important.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {203--},
 url = {http://doi.acm.org/10.1145/186258.187204},
 doi = {http://doi.acm.org/10.1145/186258.187204},
 acmid = {187204},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Corbett:1994:EET:186258.187206,
 author = {Corbett, James C.},
 title = {An empirical evaluation of three methods for deadlock analysis of Ada tasking programs},
 abstract = {Static analysis of Ada tasking programs has been hindered by the well known state explosion problem that arises in the verification of concurrent systems. Many different techniques have been proposed to combat this state explosion. All proposed methods excel on certain kinds of systems, but there is little empirical data comparing the performance of the methods. In this paper, we select one representative from each of three very different approaches to the state explosion problem: partial-orders (representing state-space reductions), symbolic model checking (representing OBDD-based approaches), and inequality necessary conditions (representing integer programming-based approaches). We apply the methods to several scalable concurrency examples from the literature and to one real Ada tasking program. The results of these experiments are presented and their significance is discussed.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {204--215},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/186258.187206},
 doi = {http://doi.acm.org/10.1145/186258.187206},
 acmid = {187206},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Damodaran-Kamal:1994:TRP:186258.187242,
 author = {Damodaran-Kamal, Suresh K. and Francioni, Joan M.},
 title = {Testing races in parallel programs with an OtOt strategy},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {216--227},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/186258.187242},
 doi = {http://doi.acm.org/10.1145/186258.187242},
 acmid = {187242},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Buy:1994:ARP:186258.187243,
 author = {Buy, Ugo and Sloan, Robert H.},
 title = {Analysis of real-time programs with simple time Petri nets},
 abstract = {We present a first report on our PARTS toolset for the automated static analysis of real-time systems. The PARTS toolset is based upon a timed extension of Petri nets.Our simple time Petri nets or STP nets are specifically aimed at facilitating real-time analysis. Our analysis approach uses the state space of an STP net in order to answer queries about the concurrency and timing behavior of the corresponding system. An attractive feature of STP nets is that they support a variety of techniques for controlling the number of states that must be explicitly enumerated. These techniques were originally defined for the analysis of concurrency properties of untimed systems, and in this paper we discuss the extension of each to the timed domain.We also report on some preliminary experimental results that we obtained by running our toolset on examples of real-time systems.},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/186258.187243},
 doi = {http://doi.acm.org/10.1145/186258.187243},
 acmid = {187243},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shimeall:1994:VST:186258.187250,
 author = {Shimeall, Timothy and Friedman, Michael and Chilenski, John and Voas, Jeffrey},
 title = {Views on software testability (panel)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '94},
 year = {1994},
 isbn = {0-89791-683-2},
 location = {Seattle, Washington, United States},
 pages = {240--},
 url = {http://doi.acm.org/10.1145/186258.187250},
 doi = {http://doi.acm.org/10.1145/186258.187250},
 acmid = {187250},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blum:1993:DPC:174146.154185,
 author = {Blum, Manuel},
 title = {Designing programs to check their work (abstract)},
 abstract = {
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/174146.154185},
 doi = {http://doi.acm.org/10.1145/174146.154185},
 acmid = {154185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blum:1993:DPC:154183.154185,
 author = {Blum, Manuel},
 title = {Designing programs to check their work (abstract)},
 abstract = {
},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/154183.154185},
 doi = {http://doi.acm.org/10.1145/154183.154185},
 acmid = {154185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Woit:1993:SOP:154183.154187,
 author = {Woit, Denise M.},
 title = {Specifying operational profiles for modules},
 abstract = {We describe a technique for specifying operational profiles for modules. The technique is more general than those of the current literature and allows more accurate specification of module usage. We also outline an algorithm for automatically generating random test cases from any such operational profile specification for a module, such that the test cases correspond to a random sampling of the module's input in actual operation. Operational-based statistical estimations, such as operational reliability, may be more meaningful when our specification method and generation algorithm are used, because our method permits more precise specifications than do other methods in the current literature.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {2--10},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/154183.154187},
 doi = {http://doi.acm.org/10.1145/154183.154187},
 acmid = {154187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Woit:1993:SOP:174146.154187,
 author = {Woit, Denise M.},
 title = {Specifying operational profiles for modules},
 abstract = {We describe a technique for specifying operational profiles for modules. The technique is more general than those of the current literature and allows more accurate specification of module usage. We also outline an algorithm for automatically generating random test cases from any such operational profile specification for a module, such that the test cases correspond to a random sampling of the module's input in actual operation. Operational-based statistical estimations, such as operational reliability, may be more meaningful when our specification method and generation algorithm are used, because our method permits more precise specifications than do other methods in the current literature.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {2--10},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/174146.154187},
 doi = {http://doi.acm.org/10.1145/174146.154187},
 acmid = {154187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stocks:1993:TTF:174146.154190,
 author = {Stocks, P. and Carrington, D.},
 title = {Test template framework: a specification-based testing case study},
 abstract = {A framework for specification-based testing is demonstrated on a symbol table case study, specified using the Z notation. Test derivation and structuring is discussed, as well as applications of the framework in deriving test oracles and aiding regressing testing during maintenance. Areas for further research and discussion are comparison of heuristics with regard to generated test suites and usability, formalising testing heuristics, and the discrepancy between functional testing and robustness testing.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {11--18},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/174146.154190},
 doi = {http://doi.acm.org/10.1145/174146.154190},
 acmid = {154190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stocks:1993:TTF:154183.154190,
 author = {Stocks, P. and Carrington, D.},
 title = {Test template framework: a specification-based testing case study},
 abstract = {A framework for specification-based testing is demonstrated on a symbol table case study, specified using the Z notation. Test derivation and structuring is discussed, as well as applications of the framework in deriving test oracles and aiding regressing testing during maintenance. Areas for further research and discussion are comparison of heuristics with regard to generated test suites and usability, formalising testing heuristics, and the discrepancy between functional testing and robustness testing.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {11--18},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/154183.154190},
 doi = {http://doi.acm.org/10.1145/154183.154190},
 acmid = {154190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jackson:1993:AAA:174146.154192,
 author = {Jackson, Daniel},
 title = {Abstract analysis with aspect},
 abstract = {Aspect is a static analysis technique for detecting bugs in code based on three forms of abstraction: declarative specification, data abstraction and partiality (ignoring some behavioural details). Together, they bring efficiency (the checker runs almost as fast as a type checker), modularity (a procedure can be analysed independently of the procedures it calls) and incrementality (allowing the checking of incomplete programs). Aspect can detect errors that are not detectable by other static means, especially errors of omission, which are pervasive but usually hard to detect.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {19--27},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/174146.154192},
 doi = {http://doi.acm.org/10.1145/174146.154192},
 acmid = {154192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jackson:1993:AAA:154183.154192,
 author = {Jackson, Daniel},
 title = {Abstract analysis with aspect},
 abstract = {Aspect is a static analysis technique for detecting bugs in code based on three forms of abstraction: declarative specification, data abstraction and partiality (ignoring some behavioural details). Together, they bring efficiency (the checker runs almost as fast as a type checker), modularity (a procedure can be analysed independently of the procedures it calls) and incrementality (allowing the checking of incomplete programs). Aspect can detect errors that are not detectable by other static means, especially errors of omission, which are pervasive but usually hard to detect.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {19--27},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/154183.154192},
 doi = {http://doi.acm.org/10.1145/154183.154192},
 acmid = {154192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Felder:1993:ARS:174146.154193,
 author = {Felder, Miguel and Ghezzi, Carlo and Pezz\`{e}, Mauro},
 title = {Analyzing refinements of state based specifications: the case of TB nets},
 abstract = {We describe how formal specifications given in terms of a high-level timed Petri net formalism (TB nets) can be analyzed to check the temporal properties of bounded invariance (the systems stays in a given state until time \&tgr;) and bounded response (the system will enter a given state within time \&tgr;). In particular, we concentrate on specifications given in a hierarchical, top-down manner, where one specification level refines a more abstract level.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {28--39},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/174146.154193},
 doi = {http://doi.acm.org/10.1145/174146.154193},
 acmid = {154193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Felder:1993:ARS:154183.154193,
 author = {Felder, Miguel and Ghezzi, Carlo and Pezz\`{e}, Mauro},
 title = {Analyzing refinements of state based specifications: the case of TB nets},
 abstract = {We describe how formal specifications given in terms of a high-level timed Petri net formalism (TB nets) can be analyzed to check the temporal properties of bounded invariance (the systems stays in a given state until time \&tgr;) and bounded response (the system will enter a given state within time \&tgr;). In particular, we concentrate on specifications given in a hierarchical, top-down manner, where one specification level refines a more abstract level.
},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {28--39},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/154183.154193},
 doi = {http://doi.acm.org/10.1145/154183.154193},
 acmid = {154193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levine:1993:MRS:174146.154196,
 author = {Levine, David L. and Taylor, Richard N.},
 title = {Metric-driven reengineering for static concurrency analysis},
 abstract = {An approach to statically analyzing a concurrent program not suited for analysis is described. The program is reengineered to reduce the complexity of concurrency-related activities, thereby reducing the size of the concurrency state space. The key to the reengineering process is a metric set that characterizes program task interaction complexity and provides guidance for restructuring. An initial version of a metric set is proposed and applied to two examples to demonstrate the utility of the reengineering-for-analysis process. The reengineering has potential benefits apart from supporting analyzability, following the dictum that if it is hard to analyze, it is hard to understand and maintain.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {40--50},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154196},
 doi = {http://doi.acm.org/10.1145/174146.154196},
 acmid = {154196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Levine:1993:MRS:154183.154196,
 author = {Levine, David L. and Taylor, Richard N.},
 title = {Metric-driven reengineering for static concurrency analysis},
 abstract = {An approach to statically analyzing a concurrent program not suited for analysis is described. The program is reengineered to reduce the complexity of concurrency-related activities, thereby reducing the size of the concurrency state space. The key to the reengineering process is a metric set that characterizes program task interaction complexity and provides guidance for restructuring. An initial version of a metric set is proposed and applied to two examples to demonstrate the utility of the reengineering-for-analysis process. The reengineering has potential benefits apart from supporting analyzability, following the dictum that if it is hard to analyze, it is hard to understand and maintain.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {40--50},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154196},
 doi = {http://doi.acm.org/10.1145/154183.154196},
 acmid = {154196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Duri:1993:USS:154183.154197,
 author = {Duri, S. and Buy, U. and Devarapalli, R. and Shatz, S. M.},
 title = {Using state space reduction methods for deadlock analysis in Ada tasking},
 abstract = {Over the past few years, a number of research investigations have been initiated for static analysis of concurrent and distributed software. In this paper we report on experiments with various optimization techniques for reachability-based deadlock detection in Ada programs using Petri net models. Our experimental results show that various optimization techniques are mutually beneficial with respect to the effectiveness of the analysis.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/154183.154197},
 doi = {http://doi.acm.org/10.1145/154183.154197},
 acmid = {154197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Duri:1993:USS:174146.154197,
 author = {Duri, S. and Buy, U. and Devarapalli, R. and Shatz, S. M.},
 title = {Using state space reduction methods for deadlock analysis in Ada tasking},
 abstract = {Over the past few years, a number of research investigations have been initiated for static analysis of concurrent and distributed software. In this paper we report on experiments with various optimization techniques for reachability-based deadlock detection in Ada programs using Petri net models. Our experimental results show that various optimization techniques are mutually beneficial with respect to the effectiveness of the analysis.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/174146.154197},
 doi = {http://doi.acm.org/10.1145/174146.154197},
 acmid = {154197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vogel:1993:IGP:174146.154200,
 author = {Vogel, Peter A.},
 title = {An integrated general purpose automated test environment},
 abstract = {As software systems become more and more complex, both the complexity of the testing effort and the cost of maintaining the results of that effort increase proportionately. Most existing test environments lack the power and flexibility needed to adequately test significant software systems. The CONVEX Integrated Test Environment (CITE) is discussed as an answer to the need for a more complete and powerful general purpose automated software test system.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {61--69},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/174146.154200},
 doi = {http://doi.acm.org/10.1145/174146.154200},
 acmid = {154200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vogel:1993:IGP:154183.154200,
 author = {Vogel, Peter A.},
 title = {An integrated general purpose automated test environment},
 abstract = {As software systems become more and more complex, both the complexity of the testing effort and the cost of maintaining the results of that effort increase proportionately. Most existing test environments lack the power and flexibility needed to adequately test significant software systems. The CONVEX Integrated Test Environment (CITE) is discussed as an answer to the need for a more complete and powerful general purpose automated software test system.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {61--69},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/154183.154200},
 doi = {http://doi.acm.org/10.1145/154183.154200},
 acmid = {154200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Belli:1993:IAT:154183.154203,
 author = {Belli, B. and Jack, O.},
 title = {Implementation-based analysis and testing of Prolog programs},
 abstract = {In this paper, we describe the PROTest II (PROlog Test Environment, Version 2) system to test logic programs in an interactive support environment. Logic programs are augmented with declarative information about the types and modes of the arguments of a predicate. Modes correspond to in, out, and in-out parameters. With this information PROTest II statically checks the types of Prolog programs, generates test cases, executes Prolog programs, and produces reports summarizing results including information about new test coverage metrics. Thus, PROTest II enables both static analysis and dynamic testing uniformly using a Prolog-based test language DTL/1. The strength of PROTest II stems from its idea of defining coverage in real logic programming terms, rather than adapting imperative programming ideas.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154203},
 doi = {http://doi.acm.org/10.1145/154183.154203},
 acmid = {154203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Belli:1993:IAT:174146.154203,
 author = {Belli, B. and Jack, O.},
 title = {Implementation-based analysis and testing of Prolog programs},
 abstract = {In this paper, we describe the PROTest II (PROlog Test Environment, Version 2) system to test logic programs in an interactive support environment. Logic programs are augmented with declarative information about the types and modes of the arguments of a predicate. Modes correspond to in, out, and in-out parameters. With this information PROTest II statically checks the types of Prolog programs, generates test cases, executes Prolog programs, and produces reports summarizing results including information about new test coverage metrics. Thus, PROTest II enables both static analysis and dynamic testing uniformly using a Prolog-based test language DTL/1. The strength of PROTest II stems from its idea of defining coverage in real logic programming terms, rather than adapting imperative programming ideas.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {70--80},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154203},
 doi = {http://doi.acm.org/10.1145/174146.154203},
 acmid = {154203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Modes:1993:SIA:154183.154205,
 author = {Modes, Ronald W.},
 title = {Structured IV \&amp; V for the space shuttle flight software},
 abstract = {},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {81--},
 url = {http://doi.acm.org/10.1145/154183.154205},
 doi = {http://doi.acm.org/10.1145/154183.154205},
 acmid = {154205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Modes:1993:SIA:174146.154205,
 author = {Modes, Ronald W.},
 title = {Structured IV \&amp; V for the space shuttle flight software},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {81--},
 url = {http://doi.acm.org/10.1145/174146.154205},
 doi = {http://doi.acm.org/10.1145/174146.154205},
 acmid = {154205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Avritzer:1993:LTS:174146.154244,
 author = {Avritzer, Alberto and Larson, Brian},
 title = {Load testing software using deterministic state testing},
 abstract = {In this paper we introduce a new load testing technique called Deterministic Markov State Testing and report on its application. Our approach is called ``deterministic" because the sequence of test case execution is set at planning time, and ``state testing" because each test case certifies a unique software state. There are four main advantages of Deterministic Markov State Testing for system testers: provision of precise software state information for root cause analysis in load test, accommodation for limitations of the system test lab configuration, higher acceleration ratios in system test, and simple management of distributed execution of test cases. System testers using the proposed method have great flexibility in dealing with common system test problems: limited access to the system test environment, unstable software, or changing operational conditions. Because each test case verifies correct execution on a path from the idle state to the software state under test, our method does not require the continuous execution of all test cases. Deterministic Markov State Testing is operational-profile-based, and allows for measurement of software reliability robustness when the operational profile changes.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {82--88},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/174146.154244},
 doi = {http://doi.acm.org/10.1145/174146.154244},
 acmid = {154244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Avritzer:1993:LTS:154183.154244,
 author = {Avritzer, Alberto and Larson, Brian},
 title = {Load testing software using deterministic state testing},
 abstract = {In this paper we introduce a new load testing technique called Deterministic Markov State Testing and report on its application. Our approach is called ``deterministic" because the sequence of test case execution is set at planning time, and ``state testing" because each test case certifies a unique software state. There are four main advantages of Deterministic Markov State Testing for system testers: provision of precise software state information for root cause analysis in load test, accommodation for limitations of the system test lab configuration, higher acceleration ratios in system test, and simple management of distributed execution of test cases. System testers using the proposed method have great flexibility in dealing with common system test problems: limited access to the system test environment, unstable software, or changing operational conditions. Because each test case verifies correct execution on a path from the idle state to the software state under test, our method does not require the continuous execution of all test cases. Deterministic Markov State Testing is operational-profile-based, and allows for measurement of software reliability robustness when the operational profile changes.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {82--88},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/154183.154244},
 doi = {http://doi.acm.org/10.1145/154183.154244},
 acmid = {154244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hamlet:1993:FSA:174146.154246,
 author = {Hamlet, Dick and Voas, Jeff},
 title = {Faults on its sleeve: amplifying software reliability testing},
 abstract = {Most of the effort that goes into improving the quality of software paradoxically does not lead to quantitative, measurable quality. Software developers and quality-assurance organizations spend a great deal of effort preventing, detecting, and removing ``defects"\&mdash;parts of software responsible for operational failure. But software quality can be measured only by statistical parameters like hazard rate and mean time to failure, measures whose connection with defects and with the development process is little understood.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {89--98},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/174146.154246},
 doi = {http://doi.acm.org/10.1145/174146.154246},
 acmid = {154246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure, fault, software reliability, testability},
} 

@inproceedings{Hamlet:1993:FSA:154183.154246,
 author = {Hamlet, Dick and Voas, Jeff},
 title = {Faults on its sleeve: amplifying software reliability testing},
 abstract = {Most of the effort that goes into improving the quality of software paradoxically does not lead to quantitative, measurable quality. Software developers and quality-assurance organizations spend a great deal of effort preventing, detecting, and removing ``defects"\&mdash;parts of software responsible for operational failure. But software quality can be measured only by statistical parameters like hazard rate and mean time to failure, measures whose connection with defects and with the development process is little understood.
},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {89--98},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/154183.154246},
 doi = {http://doi.acm.org/10.1145/154183.154246},
 acmid = {154246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure, fault, software reliability, testability},
} 

@article{Thevenod-Fosse:1993:SAS:174146.154262,
 author = {Th\'{e}venod-Fosse, P. and Waeselynck, H.},
 title = {STATEMATE applied to statistical software testing},
 abstract = {This paper is concerned with the use of statistical testing as a verification technique for complex software. <bold>Statistical testing</bold> involves exercising a program with random inputs, the test profile and the number of generated inputs being determined according to criteria based on program structure or software functionality. In case of complex programs, the probabilistic generation must be based on a black box analysis, the adopted criteria being defined from <bold>behavior models</bold> deduced from the specification. The proposed approach refers to a hierarchical specification produced in the <bold>STATEMATE</bold> environment. Its feasiblity is exemplified on a <bold>safety-critical module</bold> from the nuclear field, and the efficiency in revealing <bold>actual faults</bold> is investigated through experiments involving two versions of the module.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {99--109},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154262},
 doi = {http://doi.acm.org/10.1145/174146.154262},
 acmid = {154262},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thevenod-Fosse:1993:SAS:154183.154262,
 author = {Th\'{e}venod-Fosse, P. and Waeselynck, H.},
 title = {STATEMATE applied to statistical software testing},
 abstract = {This paper is concerned with the use of statistical testing as a verification technique for complex software. <bold>Statistical testing</bold> involves exercising a program with random inputs, the test profile and the number of generated inputs being determined according to criteria based on program structure or software functionality. In case of complex programs, the probabilistic generation must be based on a black box analysis, the adopted criteria being defined from <bold>behavior models</bold> deduced from the specification. The proposed approach refers to a hierarchical specification produced in the <bold>STATEMATE</bold> environment. Its feasiblity is exemplified on a <bold>safety-critical module</bold> from the nuclear field, and the efficiency in revealing <bold>actual faults</bold> is investigated through experiments involving two versions of the module.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {99--109},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154262},
 doi = {http://doi.acm.org/10.1145/154183.154262},
 acmid = {154262},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Corbett:1993:PTB:154183.154263,
 author = {Corbett, J. and Avrunin, G.},
 title = {A practical technique for bounding the time between events in concurrent real-time systems},
 abstract = {Showing that concurrent systems satisfy timing constraints on their behavior is difficult, but may be essential for critical applications. Most methods are based on some form of reachability analysis and require construction of a state space of size that is, in general, exponential in the number of components in the concurrent system. In an earlier paper with L. K. Dillon and J. E. Wileden, we described a technique for finding bounds on the time between events without enumerating the state space, but the technique applies chiefly to the case of logically concurrent systems executing on a uniprocessor, in which events do not overlap in time. In this paper, we extend that technique to obtain upper bounds on the time between events in maximally parallel concurrent systems. Our method does not require construction of the state space and the results of preliminary experiments show that, for at least some systems with large state spaces, it is quite tractable. We also briefly describe the application of our method to the case in which there are multiple processors, but several processes run on each processor.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {110--116},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/154183.154263},
 doi = {http://doi.acm.org/10.1145/154183.154263},
 acmid = {154263},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Corbett:1993:PTB:174146.154263,
 author = {Corbett, J. and Avrunin, G.},
 title = {A practical technique for bounding the time between events in concurrent real-time systems},
 abstract = {Showing that concurrent systems satisfy timing constraints on their behavior is difficult, but may be essential for critical applications. Most methods are based on some form of reachability analysis and require construction of a state space of size that is, in general, exponential in the number of components in the concurrent system. In an earlier paper with L. K. Dillon and J. E. Wileden, we described a technique for finding bounds on the time between events without enumerating the state space, but the technique applies chiefly to the case of logically concurrent systems executing on a uniprocessor, in which events do not overlap in time. In this paper, we extend that technique to obtain upper bounds on the time between events in maximally parallel concurrent systems. Our method does not require construction of the state space and the results of preliminary experiments show that, for at least some systems with large state spaces, it is quite tractable. We also briefly describe the application of our method to the case in which there are multiple processors, but several processes run on each processor.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {110--116},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/174146.154263},
 doi = {http://doi.acm.org/10.1145/174146.154263},
 acmid = {154263},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Atlee:1993:ATR:174146.154264,
 author = {Atlee, Joanne M. and Gannon, John},
 title = {Analyzing timing requirements},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {117--127},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154264},
 doi = {http://doi.acm.org/10.1145/174146.154264},
 acmid = {154264},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Atlee:1993:ATR:154183.154264,
 author = {Atlee, Joanne M. and Gannon, John},
 title = {Analyzing timing requirements},
 abstract = {},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {117--127},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154264},
 doi = {http://doi.acm.org/10.1145/154183.154264},
 acmid = {154264},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coen-Porisini:1993:CAR:154183.154271,
 author = {Coen-Porisini, Alberto and Kemmerer, Richard A.},
 title = {The composability of ASTRAL realtime specifications},
 abstract = {ASTRAL is a formal specification language for realtime systems. It is intended to support formal software development, and therefore has been formally defined. In ASTRAL a realtime system is modeled by a collection of state machine specifications and a single global specification.
},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {128--138},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154271},
 doi = {http://doi.acm.org/10.1145/154183.154271},
 acmid = {154271},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Coen-Porisini:1993:CAR:174146.154271,
 author = {Coen-Porisini, Alberto and Kemmerer, Richard A.},
 title = {The composability of ASTRAL realtime specifications},
 abstract = {ASTRAL is a formal specification language for realtime systems. It is intended to support formal software development, and therefore has been formally defined. In ASTRAL a realtime system is modeled by a collection of state machine specifications and a single global specification.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {128--138},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154271},
 doi = {http://doi.acm.org/10.1145/174146.154271},
 acmid = {154271},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Untch:1993:MAU:154183.154265,
 author = {Untch, Roland H. and Offutt, A. Jefferson and Harrold, Mary Jean},
 title = {Mutation analysis using mutant schemata},
 abstract = {Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram, which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300\% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {139--148},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/154183.154265},
 doi = {http://doi.acm.org/10.1145/154183.154265},
 acmid = {154265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault-based testing, mutation analysis, program schemata, software testing},
} 

@article{Untch:1993:MAU:174146.154265,
 author = {Untch, Roland H. and Offutt, A. Jefferson and Harrold, Mary Jean},
 title = {Mutation analysis using mutant schemata},
 abstract = {Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram, which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300\% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {139--148},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/174146.154265},
 doi = {http://doi.acm.org/10.1145/174146.154265},
 acmid = {154265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault-based testing, mutation analysis, program schemata, software testing},
} 

@article{Weiss:1993:ISA:174146.154266,
 author = {Weiss, Stewart N. and Fleyshgakker, Vladimir N.},
 title = {Improved serial algorithms for mutation analysis},
 abstract = {Existing serial algorithms to do mutation analysis are inefficient, and descriptions of parallel mutation systems pre-suppose that these serial algorithms are the best one can do serially. We present a universal mutation analysis data structure and new serial algorithms for both strong and weak mutation analysis that on average should perform much faster than existing ones, and can never do worse. We describe these algorithms as well as the results of our analysis of their run time complexities. We believe that this is the first paper in which analytical methods have been applied to obtain the run time complexities of mutation analysis algorithms.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {149--158},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/174146.154266},
 doi = {http://doi.acm.org/10.1145/174146.154266},
 acmid = {154266},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weiss:1993:ISA:154183.154266,
 author = {Weiss, Stewart N. and Fleyshgakker, Vladimir N.},
 title = {Improved serial algorithms for mutation analysis},
 abstract = {Existing serial algorithms to do mutation analysis are inefficient, and descriptions of parallel mutation systems pre-suppose that these serial algorithms are the best one can do serially. We present a universal mutation analysis data structure and new serial algorithms for both strong and weak mutation analysis that on average should perform much faster than existing ones, and can never do worse. We describe these algorithms as well as the results of our analysis of their run time complexities. We believe that this is the first paper in which analytical methods have been applied to obtain the run time complexities of mutation analysis algorithms.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {149--158},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/154183.154266},
 doi = {http://doi.acm.org/10.1145/154183.154266},
 acmid = {154266},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Miller:1993:EST:154183.154267,
 author = {Miller, Edward F.},
 title = {Exploitation of software test technology},
 abstract = {},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {159--},
 url = {http://doi.acm.org/10.1145/154183.154267},
 doi = {http://doi.acm.org/10.1145/154183.154267},
 acmid = {154267},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Miller:1993:EST:174146.154267,
 author = {Miller, Edward F.},
 title = {Exploitation of software test technology},
 abstract = {},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {159--},
 url = {http://doi.acm.org/10.1145/174146.154267},
 doi = {http://doi.acm.org/10.1145/174146.154267},
 acmid = {154267},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harrold:1993:ECP:174146.154268,
 author = {Harrold, Mary Jean and Malloy, Brian and Rothermel, Gregg},
 title = {Efficient construction of program dependence graphs},
 abstract = {We present a new technique for constructing a program dependence graph that contains a program's control flow, along with the usual control and data dependence information. Our algorithm constructs a program dependence graph while the program is being parsed. For programs containing only structured transfers of control, our algorithm does not require information provided by the control flow graph or post dominator tree and therefore obviates the construction of these auxiliary graphs. For programs containing explicit transfers of control, our algorithm adjusts the partial control dependence subgraph, constructed during the parse, to incorporate exact control dependence information. There are several advantages to our approach. For many programs, our algorithm may result in  substantial savings in time and memory since our construction of the program dependence graph does not require the auxiliary graph. Furthermore, since we incorporate control and data flow as well as exact control dependence information into the program dependence graph, our graph has a wide range of applicability. We have implemented our algorithm by incorporating it into the Free Software Foundation's GNU C compiler; currently we are performing experiments that compare our technique with the traditional approach.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154268},
 doi = {http://doi.acm.org/10.1145/174146.154268},
 acmid = {154268},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harrold:1993:ECP:154183.154268,
 author = {Harrold, Mary Jean and Malloy, Brian and Rothermel, Gregg},
 title = {Efficient construction of program dependence graphs},
 abstract = {We present a new technique for constructing a program dependence graph that contains a program's control flow, along with the usual control and data dependence information. Our algorithm constructs a program dependence graph while the program is being parsed. For programs containing only structured transfers of control, our algorithm does not require information provided by the control flow graph or post dominator tree and therefore obviates the construction of these auxiliary graphs. For programs containing explicit transfers of control, our algorithm adjusts the partial control dependence subgraph, constructed during the parse, to incorporate exact control dependence information. There are several advantages to our approach. For many programs, our algorithm may result in  substantial savings in time and memory since our construction of the program dependence graph does not require the auxiliary graph. Furthermore, since we incorporate control and data flow as well as exact control dependence information into the program dependence graph, our graph has a wide range of applicability. We have implemented our algorithm by incorporating it into the Free Software Foundation's GNU C compiler; currently we are performing experiments that compare our technique with the traditional approach.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154268},
 doi = {http://doi.acm.org/10.1145/154183.154268},
 acmid = {154268},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goradia:1993:DIA:154183.154269,
 author = {Goradia, Tarak},
 title = {Dynamic impact analysis: a cost-effective technique to enforce error-propagation},
 abstract = {This paper introduces dynamic impact analysis as a cost-effective technique to enforce the error-propagation condition for detecting a fault. The intuition behind dynamic impact analysis is as follows. In a specific test-case, if an execution of a syntactic component has a strong impact on the program output and if the output is correct, then the value of that component-execution is not likely to be erroneous. To capture this intuition in a theoretical framework the notion of impact is formally defined and the concept of impact strength is proposed as a quantitative measure of the impact. In order to provide an infrastructure supporting the computation of impact strengths, program impact graphs and execution impact  graphs are introduced. An empirical study validating the computation of impact strengths is presented. It is shown that the impact strengths computed by dynamic impact analysis provide reasonable estimates for the error-sensitivity with respect to the output except when the impact is via one or more error-tolerant components of the program. Potential applications of dynamic impact analysis in the area of mutation testing and dynamic program slicing are discussed.},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {171--181},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154269},
 doi = {http://doi.acm.org/10.1145/154183.154269},
 acmid = {154269},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goradia:1993:DIA:174146.154269,
 author = {Goradia, Tarak},
 title = {Dynamic impact analysis: a cost-effective technique to enforce error-propagation},
 abstract = {This paper introduces dynamic impact analysis as a cost-effective technique to enforce the error-propagation condition for detecting a fault. The intuition behind dynamic impact analysis is as follows. In a specific test-case, if an execution of a syntactic component has a strong impact on the program output and if the output is correct, then the value of that component-execution is not likely to be erroneous. To capture this intuition in a theoretical framework the notion of impact is formally defined and the concept of impact strength is proposed as a quantitative measure of the impact. In order to provide an infrastructure supporting the computation of impact strengths, program impact graphs and execution impact  graphs are introduced. An empirical study validating the computation of impact strengths is presented. It is shown that the impact strengths computed by dynamic impact analysis provide reasonable estimates for the error-sensitivity with respect to the output except when the impact is via one or more error-tolerant components of the program. Potential applications of dynamic impact analysis in the area of mutation testing and dynamic program slicing are discussed.},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {171--181},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154269},
 doi = {http://doi.acm.org/10.1145/174146.154269},
 acmid = {154269},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thompson:1993:IFM:174146.154270,
 author = {Thompson, Margaret C. and Richardson, Debra J. and Clarke, Lori A.},
 title = {An information flow model of fault detection},
 abstract = {RELAY is a model of how a fault causes a failure on execution of some test datum. This process begins with introduction of an original state potential failure at a fault location and continues as the potential failure(s) transfers to output. Here we describe the second stage of this process, transfer of an incorrect intermediate state from a faulty statement to output.
},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {18},
 issue = {3},
 month = {July},
 year = {1993},
 issn = {0163-5948},
 pages = {182--192},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/174146.154270},
 doi = {http://doi.acm.org/10.1145/174146.154270},
 acmid = {154270},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thompson:1993:IFM:154183.154270,
 author = {Thompson, Margaret C. and Richardson, Debra J. and Clarke, Lori A.},
 title = {An information flow model of fault detection},
 abstract = {RELAY is a model of how a fault causes a failure on execution of some test datum. This process begins with introduction of an original state potential failure at a fault location and continues as the potential failure(s) transfers to output. Here we describe the second stage of this process, transfer of an incorrect intermediate state from a faulty statement to output.
},
 booktitle = {Proceedings of the 1993 ACM SIGSOFT international symposium on Software testing and analysis},
 series = {ISSTA '93},
 year = {1993},
 isbn = {0-89791-608-5},
 location = {Cambridge, Massachusetts, United States},
 pages = {182--192},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/154183.154270},
 doi = {http://doi.acm.org/10.1145/154183.154270},
 acmid = {154270},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

