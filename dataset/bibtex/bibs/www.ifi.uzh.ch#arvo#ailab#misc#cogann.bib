%
% copied from /anonymous@alife.santafe.edu:/pub/USER-AREA/EC/refs/cogann.bib.gz
% file converted from bbt to bibtex format
% Nick Almassy Tue May 24 15:33:33 1994
%

@Conference{Ackley85,
  Author =	"David H. Ackley",
  Title =	"A Connectionist Algorithm for Genetic Search",
  Organization =	{GAC85},
  KEY =	{genetic algorithm, boltzmann, connectionism, cogann ref},
  PAGES =	{121-135} 
}

@Book{Ackley87,
  Author =	"David H. Ackley",
  Title =	"A Connectionist Machine for Genetic Hillclimbing",
  Publisher =	{Kluwer Academic Publishers},
  ADDRESS =	{Boston, MA},
  YEAR =	{1987},
  KEY =	{connectionism, genetic algorithm, sigh, stochastic iterated, cogann ref} 
}

@Inbook{Ackley92,
  Author =	"David H. Ackley and Michael L. Littman",
  EDITOR =	{Christopher G. Langton and Charles Taylor and J. Doyne Farmer and Steen Rasmussen},
  Title =	"Interactions between Learning and Evolution",
  Booktitle =	{Artificial Life II},
  PUBLISHER =	{ADDISON},
  PAGES =	{ 487-509},
  YEAR =	{1992},
  annote =	{connectionism genetic algorithm
neighborhood mate selection, cogann ref
animat} 
}

@Article{Ankenbrandt90,
  Author =	"Carol A. Ankenbrandt and B.P. Buckles and F.E. Petry",
  Title =	"Scene Recognition using Genetic Algorithms with Semantic Nets",
  Journal =	{Pattern Recognition Letters},
  VOLUME =	{11},
  YEAR =	{APR 1990},
  PAGES =	{285-293},
  PUBLISHER =	{North-Holland},
  KEY =	{Connectionism, fuzzy logic, pattern recognition} 
}

@Conference{Austin91,
  Author =	"Scott Austin",
  Title =	"Genetic Neurosynthesis",
  Organization =	{PROC of AIAA Aerospace VIII},
  ADDRESS =	{Baltimore, MD},
  YEAR =	{Oct 1991},
  KEY =	{algorithms connectionism, cogann ref} 
}

@Conference{Angeline,
  Author =	"Peter J. Angeline and Gregory M. Saunders and Jordan B. Pollack",
  Title =	"An Evolutionary Algorithm that Constructs Recurrent Neural Networks",
  Organization =	{IEEE Trans on Neural Networks},
  YEAR =	{to appear},
  KEY =	{genetic algorithms connectionism neural networks cogann programming} 
}

@Conference{Arena,
  Author =	"P. Arena and R. Caponetto and I. Fortuna and M.G. Xibilia",
  Title =	"MLP Optimal Topology via Genetic Algorithms",
  Pages =	{670-674},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Alba,
  Author =	"E. Alba and J.F. Aldana and J.M. Troya",
  Title =	"Genetic Algorithms as Heuristics for Optimizing ANN Design",
  Pages =	{683-690},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Article{Bergman87,
  Author =	"Aviv Bergman and Michel Kerszberg",
  JOURNAL =	{PROC IEEE CONF on Neural Networks},
  ADDRESS =	{San Diego, CA},
  YEAR =	{June 21-24, 1987},
  Title =	"Breeding Intelligent Automata",
  Volume =	{II},
  KEY =	{genetic algorithms, connectionism, evolution, cogann ref},
  PAGES =	{63-70} 
}

@Conference{Belew,
  Author =	"Richard K. Belew",
  Title =	"When Both Individuals and Populations Search: Adding Simple
		  Learning to the Genetic Algorithm", 
  Organization =	{ICGA89},
  KEY =	{hybrid learning, connectionism, cogann ref},
  PAGES =	{34-41} 
}

@Techreport{Belew89,
  Author =	"Richard K. Belew",
  Title =	"Evolution, Learning and Culture: Computational Metaphors for adaptive algorithms",
  Type =	{CSE Technical Report #CS89-156},
  PUBLISHER =	{University of California at San Deigo},
  ADDRESS =	{La Jolla, CA},
  YEAR =	{SEPT 1989},
  KEY =	{connectionism, genetic algorithms, cogann ref} 
}

@Techreport{Belew90,
  Author =	"Richard K. Belew and John McInerney and Nicol N. Schraudolph",
  Title =	"Evolving Networks: Using Genetic Algorithms with Connectionist Learning",
  Type =	{CSE Technical report CS90-174},
  YEAR =	{JUN 1990},
  PUBLISHER =	{Univeristy of California at Dan Diego},
  ADDRESS =	{La Jolla, CA},
  KEY =	{neural nets, cogann ref} 
}

@Techreport{Belew89,
  Author =	"Richard K. Belew and John McInerney",
  Title =	"Using the Genetic Algorithm to Wire Feed-forward Networks",
  Type =	{Technical abstract},
  PUBLISHER =	{Computer Science & Engineering Dept.,
University of California at San Deigo},
  ADDRESS =	{La Jolla, CA},
  YEAR =	{30 MAY 1989},
  KEY =	{connectionism, cogann ref},
  NOTE =	{Submitted to Neural Information Processing Systems 1989} 
}

@Article{Brill92,
  Author =	"F.Z. Brill and D.E. Brown and W.N. Martin",
  Title =	"Fast Genetic Selection of Features for Neural Network Classifiers",
  Journal =	{IEEE Transaction on Neural Networks},
  VOLUME =	{3},
  NUMBER =	{2},
  YEAR =	{MAR 1992},
  PAGES =	{324-328},
  KEY =	{genetic algorithms, connectionism, cogann ref},
  ABSTRACT =	{ABSTRACT - The task of classifiers is to determine the appropriate class
name when presented with a sample from one of several classes. In forming the
sample to present to the classifier, there may be a large number of
measurements one can make. Feature selection addresses the problem of determining
which of these measurements are the most useful for determining the pattern's
class. In this paper, we describe experiments using a genetic algorithm
for feature selection in the context of neural network classifiers,
specifically, counterpropagation networks. We present two novel techniques
in our application of genetic algorithms. First, we configure our genetic
algorithm to use an approximate evaluation in order to reduce significantly
the computation required. In particular, though our desired classifiers are
counterpropagation networks, we use a nearest-neighbor classifier to evaluate
feature sets. We show that the features selected by this method are effective
in the context of counterpropagation networks. Second, we propose a method we
call training set sampling, in which only a portion of the training set is
used on any given evaluation. Again, significant computational savings can be
made by using this method, i.e., evaluations can be made over an order of magnitude
faster. This method selects feature sets that are as good as and occasionally better
for counterpropagation than those chosen by an evaluation that uses the entire
training set.} 
}

@Conference{Ball90,
  Author =	"N. Ball",
  Title =	"Adaptive Signal Processing via Genetic Algorithms and
		  Self-organizing Neural Networks",
  Organization =	{PROC IEEE Workshop on Genetic Algorithms, Simulated Annealing and Neural
Networks},
  ADDRESS =	{University of Glasgow, Scotland},
  YEAR =	{1990},
  KEY =	{connectionism, cogann ref} 
}

@Inbook{Bergman89,
  Author =	"A. Bergman",
  EDITOR =	{E. Jen},
  Title =	"Self-Organization by Simulated Evolution",
  Booktitle =	{Lectures in Complex Systems, Proceedings of the 1989 Complex Systems Summerschool},
  ADDRESS =	{Santa Fe},
  YEAR =	{1989},
  KEY =	{genetic algorithms, connectionism recurrent neural networks, cogann ref} 
}

@Article{Bornholdt92,
  Author =	"S. Bornholdt and D. Graudenz",
  Title =	"General Assymetric Neural Networks and Structure Design by Genetic Algorithms",
  Journal =	{Neural Networks},
  VOLUME =	{5},
  NUMBER =	{2},
  YEAR =	{1992},
  PAGES =	{327-334},
  NOTE =	{DESSY 91-046, Deutsches Electronen-Synchrotron, Hamburg, Germany, MAY 1991},
  KEY =	{connectionism, cogann ref} 
}

@Article{Beer92,
  Author =	"Randall D. Beer and John C. Gallagher",
  Title =	"Evolving Dynamical Neural Networks for Adaptive Behavior",
  Journal =	{Adaptive Behavior},
  VOLUME =	{1},
  NUMBER =	{1},
  YEAR =	{1992},
  KEY =	{genetic algorithms GENESIS, connectionism} 
}

@Article{Brassinne93,
  Author =	"P. de la Brassinne",
  Title =	"Generic algorithms and learning of neural nets",
  Journal =	{Bulletin Scientifique de l'Association des Ingenieurs
Electriciens sortis de l'Institut Electrotechnique Montefiore},
  VOLUME =	{106},
  NUMBER =	{1},
  PAGES =	{41-,58},
  YEAR =	{1993},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Author sought "" apply genetic algorithms to two concrete
industrial problems which caused trouble to classical optimization
techniques (they were usually trapped into local minima), without positive
results. One of the reasons was that the solutions among the population
were too close to one another too early in the search process. Another was
the unsuitability of the operators employed to create new solutions for the
neural network optimization problem. Attempts at application to control
problems, where backpropagation could not be used, yielded disappointing
results except for very simple problems such as the inverted pendulum. An
explanation of these findings is suggested.} 
}

@Book{Bessiere92,
  Author =	"P. Bessiere",
  EDITOR =	{F.J. Varela and P. Bourgine},
  Title =	"Genetic algorithms applied to formal neural networks: parallel genetic
	   implementation of a Boltzmann machine and associated robotic experimentations",
  Journal =	{Toward a Practice of Autonomous Systems. Proceedings of the First European
Conference on Artificial Life},
  PAGES =	{310-314},
  PUBLISHER =	{MIT Press},
  ADDRESS =	{Cambridge, MA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Describes a possible application of computing techniques
inspired by natural life mechanisms to an artificial life creature, namely
a small mobile robot, called KitBorg. Probabilistic inference suggests that
any cognitive problem may be split in two optimization problems. The first
one called the dynamic inference problem is an abstraction of learning,
the second one, namely, the static inference problem, being a
mathematical metaphor of pattern association. Other optimization technics
should be considered in that context and especially genetic algorithms. The
purpose of this paper is to describe the state of the art of the
investigations which the Author is "akin" about that question using a
parallel genetic algorithm. The Author first "ecall" the principles of
probabilistic inference, then he presents briefly the parallel genetic
algorithm and the ways it is used to deal with both optimization problems,
to finally conclude about ongoing robotic experimentations and future
planned extensions.} 
}

@Conference{Bengio,
  Author =	"Yoshua Bengio and Samy Bengio and Jocelyn Cloutier",
  Title =	"Learning a synaptic learning rule",
  Organization =	{IJCNN-91},
  PAGES =	{969},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Summary form only given, as follows. The Authors discuss ""
original approach to neural modeling based on the idea of searching, with
learning methods, for a synaptic learning rule which is biologically
plausible and yields networks that are able to learn to perform difficult
tasks. The proposed method of automatically finding the learning rule
relies on the idea of considering the synaptic modification rule as a
parametric function. This function has local inputs and is the same in many
neurons. The parameters that define this function can be estimated with
known learning methods. For this optimization, particular attention is
given to gradient descent and genetic algorithms. In both cases, estimation
of this function consists of a joint global optimization of the synaptic
modification function and the networks that are learning to perform some
tasks. Both network architecture and the learning function can be designed
within constraints derived from biological knowledge.} 
}

@Article{Bukatova92,
  Author =	"I.L. Bukatova",
  Title =	"Evolutionary Computer",
  Journal =	{Proceedings of the RNNS/IEEE Symposium on Neuroinformatics
and Neurocomputers.},
  ADDRESS =	{Rostov-on-Don, Russia},
  YEAR =	{7-10 OCT 1992},
  VOLUME =	{I},
  PAGES =	{467-477},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Braun,
  Author =	"H. Braun and J. Weisbrod",
  Title =	"Evolving Neural Feedforward Networks",
  Pages =	{25-32},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Ball,
  Author =	"N.R. Ball",
  Title =	"Towards the Development of Cognitive Maps in Classifier Systems",
  Pages =	{712-718},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Bishop,
  Author =	"J.M. Bishop and M.J. Bushnell and A. Usher and S. Westland",
  Title =	"Genetic Optimization of Neural Network Architectures for
		  Colour Recipe Prediction",
  Pages =	{719-725},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
application paint industry} 
}

@Techreport{Bornholdt93,
  Author =	"Stephan Bornholdt and Dirk Graudenz",
  Title =	"General Assymetric Neural Networks and Structure Design by
		  Genetic Algorithms: A Learning Rule for Temporal Patterns",
  Type =	{HD-THEP-93-26 LBL-34384},
  PUBLISHER =	{Lawrence Berkeley Laboratory
University of California},
  ADDRESS =	{Berkeley, CA},
  YEAR =	{JUL 1993},
  KEY =	{connectionism, cogann},
  ABSTRACT =	{ABSTRACT
A learning algorithm based on genetic algorithms for asymmetric neural networks with
an arbitrary structure is presented. It is suited for the learning of temporal
patterns and leads to stable neural networks with feedback.} 
}

@Conference{Caudell,
  Author =	"Thomas P. Caudell and Charles P. Dolan",
  Title =	"Parametric Connectivity: Training of Constrained Networks using Genetic Algorithms",
  Key =	{neural networks, connectionism, constrained weights, implementation of neural networks,
electro-optical systems, rms error minimization, convoluted error surfaces,
problem: parity, parametric connectivity, cogann ref},
  ORGANIZATION =	{ICGA89},
  ABSTRACT =	{Uses constrained (linked) weights (ie, spread networks) trained via genetic search.},
  PAGES =	{370-374} 
}

@Conference{Caudill91,
  Author =	"Maureen Caudill",
  Title =	"Evolutionary Neural Networks",
  Organization =	{AI Expert},
  YEAR =	{MAR 1991},
  PAGES =	{28-33},
  KEY =	{genetic algorithm connectionism, cogann ref} 
}

@Techreport{Carugo91,
  Author =	"Marcelo H. Carugo",
  Title =	"Optimization of Parameters of a Neural Network, applied to
		  Document Recognition, using Genetic Algorithms",
  Type =	{Nat. Lab. Technical Note No. 049/91},
  PUBLISHER =	{N.V. Philips},
  ADDRESS =	{Eindhoven, The Netherlands},
  YEAR =	{1991},
  KEY =	{connectionism, backpropagation, cogann ref} 
}

@Inbook{Chalmers90,
  Author =	"David J. Chalmers",
  EDITOR =	{D.S. Touretsky and J.L. Elman and T.J Sejnowski and G.E. Hinton},
  Title =	"The Evolution of Learning: An Experiment in Genetic Connectionism",
  Booktitle =	{PROC of the 1990 Connectionist Summer School},
  PUBLISHER =	{MKAUF},
  YEAR =	{1990},
  PAGES =	{81-90},
  KEY =	{cogann ref} 
}

@Inbook{Chang,
  Author =	"E. Chang and R. Lippmann",
  Title =	"Using Genetic Algorithms to Improve Pattern Classification Performance",
  Booktitle =	{NIPS3},
  KEY =	{connectionism},
  PAGES =	{797-803} 
}

@Conference{Collins,
  Author =	"R. Collins and D. Jefferson",
  Title =	"An Artificial Neural Network Representation for Artificial Organisms",
  Pages =	{259-263},
  ORGANIZATION =	{PPSN-90},
  KEY =	{cogann ref, genetic algorithms, connectionism} 
}

@Article{Caudell,
  Author =	"Thomas P. Caudell",
  Title =	"Parametric Connectivity: Feasibility of Learning in Constrained Weight Space",
  Key =	{genetic algorithms, neural networks, connectionism, constrained weights,
implementation of neural networks, electro-optical systems, rms error minimization,
convoluted error surfaces, problem: parity, parametric connectivity, cogann ref},
  JOURNAL =	{IJCNN-90},
  ABSTRACT =	{Uses constrained (linked) weights (ie, spread networks) trained via genetic search.},
  VOLUME =	{I},
  PAGES =	{667-675} 
}

@Conference{Carpenter,
  Author =	"Gail A. Carpenter and Stephen Grossberg and Natalya Markuzon and John H. Reynolds and David B. Rosen",
  Title =	"Fuzzy {ARTMAP}: An Adaptive Resonance Architecture for Incremental Learning of Analog Maps",
  Abstract =	{ABSTRACT
A neural network architecture is introduced for incremental supervised
learning of recognition categories and multidimensional maps in response
to arbitrary sequences of analog or binary input vectors. The
architecture, called Fuzzy ARTMAP, achieves a synthesis of fuzzy logic
and Adaptive Resonance Theory (ART) neural networks. Fuzzy ARTMAP
realizes a new Minimax Learning Rule that conjointly minimizes predictive
error and maximizes code compressions, or generalization. This is
achieved by a match tracking process that increases the ART vigilance
parameter by the minimum amount needed to correct a predictive error. As
a result, the system automatically learns a minimal number of recognition
categories, or "hidden units," to meet accuracy criteria. A
normalization procedure called complement coding leads to a symmetric
theory in which the MIN operator and the MAX operator of fuzzy logic play
complementary roles. Improved prediction is achieved by training the
system several times using different orderings of the input set, then
voting. This voting strategy can also be used to assign probability
estimates to competing predictions given small, noisy, or incomplete
training sets. Simulations illustrate Fuzzy ARTMAP performance as
compared to benchmark back propagation and genetic algorithm systems.
These simulations include (i) finding points inside versus outside a
circle; (ii) learing to tell two spirals apart; (iii) incremental
approximation of a piecewise continuous function; (iv) a letter
recognition database; and (v) a medical database.},
  PAGES =	{III-309 - III-314},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Chen,
  Author =	"Qi Chen and W. A. Weigand",
  Title =	"Neural Net Model of Batch Processes and Optimization Based
		  on an Extended Genetic Algorithm",
  Abstract =	{ABSTRACT
This paper investigates the use of neural network for modeling the batch
processes. The consideration of the dynamics of batch processes, a
cascade neural network which is the combination of BPN and Euler's
numerical integration method, is successfully used to model of batch
processes. In terms of this neural network model, an extended genetic
algorithm is adopted to generate the optimal trajectory for improving the
desired process performance. The genetic algorithm is a general
methodology for searching a solution space in a manner analogous to the
natural selection procedure in biological evolution. With the motivation
of modern genetic techonology, the rule-inducer genetic algorithm is
proposed for dynamic optimization of batch processes. The simulation
study of a typical biochemical process shows this neural network modeling
technique has a good generalization of the batch process and the extended
real-value genetic algorithm has a good capability to solve the
complicated dynamical optimization problems.},
  PAGES =	{IV-519 - IV-524},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Caudell,
  Author =	"Thomas P. Caudell",
  Title =	"Genetic Algorithms as a Tool for the Analysis of Adaptive
		  Resonance Theory Network Training Sets.",
  Organization =	{COGANN92},
  PAGES =	{184-200},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Chu,
  Author =	"C. H. Chu and C. R. Chow",
  Title =	"A Genetic Algorithm Approach to Supervised Learning for Multilayered Networks",
  Organization =	{WCNN93},
  PAGES =	{IV744 - IV747},
  KEY =	{Connectionism, Genetic Algorithms, cogann},
  ABSTRACT =	{ABSTRACT
A neural network learning algorithm based on genetic algorithms (GAs) for
multilayered networks is described. The present method does not require that
the input-output pairs for each layer to be known "a priori", since all
modules are trained concurrently. For an N-module system, N separate pools of
chromosomes are maintained and updated. The algorithm is tested using the
4-bit parity problem and a classification problem. Experiment results are
presented and discussed.} 
}

@Conference{Dolan,
  Author =	"Charles P. Dolan and Michael G. Dyer",
  Title =	"Toward the Evolution of Symbols",
  Pages =	{123-131},
  ORGANIZATION =	{GAC87},
  KEY =	{genetic algorithm, connectionism, evolve neural net architecture
competitive learning, Hebbian learning, CRAM, cogann ref} 
}

@Inbook{Dress87,
  Author =	"W. B. Dress and J. R. Knisley",
  Booktitle =	"1987 IEEE Conference on Systems, Man, and Cybernetics",
  Year =	{October 1987},
  Title =	"A Darwinian Approach to Artificial Neural Systems",
  Key =	{connectionism, genetic algorithms, cogann ref} 
}

@Conference{Davis,
  Author =	"Lawrence Davis",
  Title =	"Adapting Operator Probabilities in Genetic Algorithms",
  Key =	{novel operators, adaptive parameter optimization, neural networks, connectionism, cogann ref},
  ORGANIZATION =	{ICGA89},
  PAGES =	{61-69} 
}

@Conference{Davis,
  Author =	"Lawrence Davis",
  Title =	"Mapping Classifier Systems into Neural Networks",
  Key =	{connectionism, neural networks, formal equivalence, classifier systems, mapping networks to classifiers
genetic algorithms},
  ORGANIZATION =	{NIPS1},
  PAGES =	{49-56} 
}

@Conference{Dress87,
  Author =	"W. B. Dress",
  Title =	"Darwinian Optimization of Synthetic Neural Systems",
  Organization =	{Proceeding of the IEEE First Annual International Conference on Neural Networks},
  YEAR =	{1987},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Conference{Dress89,
  Author =	"W. B. Dress",
  Title =	"Genetic Optimization in Synthetic Systems",
  Year =	{1989},
  KEY =	{connectionism, cogann ref} 
}

@Book{Dress90,
  Author =	"W. B. Dress",
  Title =	"Electronic Life and Synthetic Intelligent Systems",
  Publisher =	{Instrumentation and Controls Division, Oak Ridge Natuional Laboratory},
  YEAR =	{1990},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Conference{Dress90,
  Author =	"W. B. Dress",
  Title =	"In-Silico Gene Expression:  A Specific Example and Possible Generalizations",
  Organization =	{Proceedings of Emergence and Evolution of Life-Forms},
  YEAR =	{1990},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Techreport{Dolan87,
  Author =	"Charles P. Dolan and Michael G. Dyer",
  Title =	"Symbolic Schemata in Connectionist Memories: Role Binding and the Evolution of Structure",
  Type =	{Technical Report UCLA-AI-87-11},
  PUBLISHER =	{UCLA AI Laboratory},
  YEAR =	{1987},
  KEY =	{genetic algorithm, connectionism, evolve neural net architecture
competitive learning, Hebbian learning, CRAM, cogann ref} 
}

@Conference{Dominic92,
  Author =	"S. Dominic and R. Das and Darrell Whitley and C. Anderson",
  Title =	"Genetic Reinforcement Learning for Neural Networks",
  Organization =	{IJCNN-92},
  PAGES =	{II-71 - II-76},
  YEAR =	{1992},
  KEY =	{genetic algorithms, connectionism, hill-climbing, mutation only, cogann ref},
  ABSTRACT =	{Abstract
The genetic algorithms which have been shown to yield good performance for neural network
weight optimization are really genetic hill-climbers, with a strong reliance on mutation
rather than hyperplane sampling. Neural control problems are more appropriate for these
genetic hill-climbers than supervised learning applications because in reinforcement learning
applications gradient information is not directly available. Genetic reinforcement learning
produces competitive results with AHC, another reinforcement learning paradigm for neural
networks that employs temporal difference methods. The genetic hill-climbing algorithm appears
to be robust over a wide range of learning conditions.} 
}

@Inbook{Dodd91,
  Author =	"N. Dodd",
  EDITOR =	{G. Rzevski and R.A. Adey},
  Title =	"Optimization of Network Structure using Genetic Techniques",
  Booktitle =	{Applications of Artificial Intelligence in Engineering VI},
  YEAR =	{1991},
  KEY =	{genetic algorithms, connectionism, dolphin vocalization} 
}

@Conference{Dodd91,
  Author =	"N. Dodd and D. Macfarlane and C. Marland",
  Title =	"Optimisation of artificial neural network structure using
		  genetic techniques implemented on multiple
		  transputers",
  Organization =	{PROC of Transputing '91},
  YEAR =	{1991},
  KEY =	{connectionism} 
}

@Conference{Dasgupta,
  Author =	"Dipankar Dasgupta and Douglas McGregor",
  Title =	"Designing Application-Specific Neural Networks using the Structured Genetic Algorithm.",
  Organization =	{COGANN92},
  PAGES =	{87-96},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Das,
  Author =	"Rajarshi Das and Darrell Whitley",
  Title =	"Genetic Sparse Distributed Memories.",
  Organization =	{COGANN92},
  PAGES =	{97-107},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Dessert92,
  Author =	"P.E. Dessert",
  Title =	"Anomaly detection in data using neural networks with natural selection",
  Journal =	{Proceedings of the SPIE - The International Society for Optical Engineering},
  VOLUME =	{1710, pt.1},
  PAGES =	{II-725--II-733},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Frequently, time series data taken off machines contains
erroneous data points due to errors in the measurement of the data. One
such instance of measuring devices recording anomalies occurs in the crash
testing of vehicles. Force and acceleration data is collected which an
engineer  inspects  for anomalies, correcting those that are found.
Artificial Neural Network (ANN) technology was successfully applied to this
problem to eliminate the cost and delay of this manual process. The Author
employed " machine learning algorithm that simulates the Darwinian concept
of survival of the fittest known as the Genetic Learning Algorithm (GLA).
By combining the strength of the GLA and ANNs, a network architecture was
created that optimized the size, speed, and accuracy of the ANN. This
hybridized system also used the GLA to determine the smallest number of
inputs into the ANN that were necessary to detect anomalies in data. This
algorithm is known as GENENET, and is described in the paper.} 
}

@Article{DeRouin92,
  Author =	"E. DeRouin and J. Brown",
  Title =	"Alternative   learning  methods  for  training  neural  network classifiers",
  Journal =	{Proceedings of the SPIE - The International Society for Optical Engineering},
  VOLUME =	{1710, pt.1},
  PAGES =	{II-474-II-483},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Neural networks have proven very useful in the field of pattern
classification  by  mapping  input patterns into one of several categories.
Rather  than being specifically programmed, backpropagation networks (BPNs)
'learn'  this  mapping by exposure to a training set, a collection of input
pattern samples matched with their corresponding output classification. The
proper  construction of this training set is crucial to successful training
of  a  BPN.  One  of  the  criteria  to be met for proper construction of a
training  set is that each of the classes must be adequately represented. A
class  that  is  represented  less  often  in  the training data may not be
learned  as completely or correctly, impairing the network's discrimination
ability.  The  degree  of impairment is a function of (among other factors)
the  relative  number of samples of each class used for training. The paper
addresses  the  problem  of  unequal  representation  in  training  sets by
proposing  two  alternative  methods  of learning. One adjusts the learning
rate  for  each class to achieve user-specified goals. The other utilizes a
genetic  algorithm  to  set  the connection weights with a fitness function
based  on  these same goals. These methods are tested using both artificial
and real-world training data.} 
}

@Conference{Eberhart,
  Author =	"R. C. Eberhart and R. W. Dobbins",
  Title =	"Designing Neural Network Explanation Facilities Using Genetic Algorithms",
  Organization =	{PROC IJCNN-91-S},
  PAGES =	{1758-1763},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Eberhart,
  Author =	"Russell C. Eberhart",
  Title =	"The Role of Genetic Algorithms in Neural Network Query-Based Learning and Explanation Facilities",
  Organization =	{COGANN92},
  PAGES =	{169-183},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Elias,
  Author =	"John G. Elias",
  Title =	"Genetic Generation of Connection Patterns for a Dynamic Artificial Neural Network",
  Organization =	{COGANN92},
  PAGES =	{38-54},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Elias92,
  Author =	"J.G. Elias",
  Title =	"Target tracking using impulsive analog circuits",
  Journal =	{Proceedings of the SPIE - The International Society for Optical Engineering},
  VOLUME =	{1709, pt.1},
  PAGES =	{338-350},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The  electronic  architecture and silicon implementation of an
artificial neuron which can be used to process and classify dynamic signals
is  described. The electrical circuit architecture is modeled after complex
neurons  in  the  vertebrate brain which have spatially extensive dendritic
tree  structures  that  support  large  numbers of synapses. The circuit is
primarily  analog  and,  as  in  the  biological model system, is virtually
immune  to  process  variations  and  other factors which often plague more
conventional  circuits. The nonlinear circuit is sensitive to both temporal
and   spatial   signal  characteristics  but  does  not  make  use  of  the
conventional  neural  network  concept of weights, and as such does not use
multipliers,  adders,  look-up-tables,  microprocessors  or  other  complex
computational  devices.  The  Author  shows "ha" artificial neural networks
with  passive dendritic tree structures can be trained, using a specialized
genetic  algorithm,  to  produce control signals useful for target tracking
and other dynamic signal processing applications.} 
}

@Book{Fenanzo86,
  Author =	"A. J. Fenanzo~Jr.",
  Title =	"Darwinian Evolution as a Paradigm for AI Research",
  Publisher =	{Harding Lawson Associates},
  JOURNAL =	{SIGART Newsletter},
  YEAR =	{July 1986},
  NUMBER =	{97},
  PAGES =	{22-23},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Article{Fontanari91,
  Author =	"J.F. Fontanari and R. Meir",
  Title =	"Evolving a Learning Algorithm for the Binary Perceptron",
  Journal =	{Network},
  VOLUME =	{2},
  PAGES =	{353-359},
  YEAR =	{1991},
  KEY =	{genetic algorithm connectionism} 
}

@Techreport{Fogel93,
  Author =	"David B. Fogel and Lawrence J. Fogel",
  Title =	"Method and Apparatus for Training a Neural Network Using Evolutionary Programming",
  Type =	{Patent 5214746},
  PUBLISHER =	{United States},
  YEAR =	{25 MAY 1993},
  KEY =	{genetic algorithms, connectionism, COGANN} 
}

@Book{Fullmer92,
  Author =	"B. Fullmer and R. Miikkulainen",
  EDITOR =	{F.J. Varela and P. Bourgine},
  Title =	"Using marker-based genetic encoding of neural networks to
		  evolve finite-state behaviour",
  Pages =	{255-262},
  JOURNAL =	{Toward a Practice of Autonomous Systems. Proceedings of the First European
Conference on Artificial Life},
  PAGES =	{310-314},
  PUBLISHER =	{MIT Press},
  ADDRESS =	{Cambridge, MA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
A new mechanism for genetic encoding of neural networks is
proposed, which is loosely based on the marker structure of biological DNA.
The mechanism allows all aspects of the network structure, including the
number of nodes and their connectivity, to be evolved through genetic
algorithms. The effectiveness of the encoding scheme is demonstrated in an
object recognition task that requires artificial creatures (whose behavior
is driven by a neural network) to develop high-level finite-state
exploration and discrimination strategies. The task requires solving the
sensory-motor grounding problem, i.e., developing a functional
understanding of the effects that a creature's movement has on its sensory
input.} 
}

@Conference{Feldman,
  Author =	"David S. Feldman",
  Title =	"Fuzzy Network Synthesis and Genetic Algorithms",
  Organization =	{ICGA93},
  KEY =	{connectionism cogann} 
}

@Conference{Fekadu,
  Author =	"A. Fekadu and E.L. Hines and J.W. Gardner",
  Title =	"Genetic Algorithm Design of Neural Net Based Electronic Nose",
  Pages =	{691-698},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
Warwick electronic nose, smell odor discrimination} 
}

@Conference{Fielder,
  Author =	"D. Fielder and C.O. Alford",
  Title =	"Counting and Naming Connection Islands on a Grid of Conductors",
  Pages =	{731},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Book{deGaris90,
  Author =	"Hugo de Garis",
  Title =	"Genetic Programming:  Building Nanobrains with GEnetically Programmed Neural Network Modules.",
  Publisher =	{CADEPS AI Research Unit, Universitye Libre de Bruxelles, CP 194/7, B-1050 Brussels, Belgium},
  YEAR =	{1990},
  KEY =	{connectionsm, cogann ref} 
}

@Article{Gierer88,
  Author =	"A. Gierer",
  Title =	"Spatial organization and genetic information in brain development",
  Year =	{1988},
  JOURNAL =	{Biological Cybernetics},
  PAGES =	{13-21},
  VOLUME =	{59},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Techreport{deGaris89,
  Author =	"H. de Garis",
  YEAR =	{1989},
  Title =	"WALKER, A Genetically Programmed, Time Dependent, Neural
		  Net Which Teaches a Pair of Sticks to Walk", 
  Type =	{Technical Report},
  PUBLISHER =	{Center for AI, George Mason Univ, Virginia},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Conference{deGaris90,
  Author =	"Hugo de Garis",
  Title =	"Genetic Programming: Evolution of a Time Dependent Neural
		  Network Module which Teaches a Pair of Stick Legs to Walk",
  Organization =	{PROC of the 9th European Conference on Artificial Intelligence},
  ADDRESS =	{Stockholm, Sweden},
  YEAR =	{AUG 6-10, 1990},
  PAGES =	{204-206},
  KEY =	{genetic algorithm GenNets connectionism robot control LIZZY, cogann ref} 
}

@Article{deGaris90,
  Author =	"Hugo de Garis",
  Title =	"BRAIN building with GenNets",
  Journal =	{PROC INNC-90},
  ADDRESS =	{Paris},
  VOLUME =	{2},
  PAGES =	{1036-1039},
  YEAR =	{1990},
  PUBLISHER =	{Kluwer Academic Publishers},
  KEY =	{genetic algorithm GenNets connectionism, cogann ref} 
}

@Conference{deGaris,
  Author =	"Hugo de~Garis",
  Title =	"Exploring GenNet Behaviors Using Genetic Programming to
		  Explore Qualitatively New Behaviors in Recurrent Neural Networks",
  Abstract =	{ABSTRACT
The neural network research community's preoccupation with convergent
networks (until the recent rise of 'recurrent backpropagation' algorithms
[e.g. WILLIAMS & ZIPSER 1989ab]) has not been unreasonable.  Relatively
little analytical work had been done on neural networks whose inputs
and/or outputs are time-dependent, hence few guidelines existed on how to
train such networks.  Consequently, research concentrated on more
restrictive 'static' neural nets such as 'feedforward' (Backprop)
[RUMELHART & McCLELLAND 1986] and "Hopfield" (clamped inputs, convergent
outputs) [HOPFIELD 1982].  This emphasis on convergence was unfortunate,
because the true richness of neural network dynamics is to be found when
inputs and/or outputs are time-dependent.  This paper shows that Genetic
Programming techniques (i.e. using Genetic Algorithms to build/evolve
complex systems) can be applied successfully to training nonconvergent
networks, and presents some examples of their extraordinary behavioral
versatility.  This paper terminates by comparing GenNet behaviors with
those generated by the new 'recurrent backpropagation' algorithms
[WILLIAMS & ZIPSER 1989ab].  It is claimed that the GenNet behaviors are a
lot more flexible and interesting because they do not requ ire the
training process to be "closely supervised".},
  PAGES =	{III-547 - III-552},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Gonzalez-Seco,
  Author =	"Jose Gonzalez-Seco",
  Title =	"A Genetic Algorithm as the Learning Procedure for Neural Networks",
  Abstract =	{ABSTRACT
The relationship between genetic algorithms and neural networks has been
somewhat one directional.  In most cases a genetic algorithm has been used
to generate better neural networks.  In this paper we combine the use of
genetics algorithms and neural networks, but from a conceptually
different point of view.  We show that it is possible to use a genetics
algorithm as the learning algorithm for a neural network.  In our model
the neural network has a fixed architecture and processes binary strings
using genetic operators.},
  PAGES =	{I-835 - I-840},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism, GLANN} 
}

@Conference{Gruau,
  Author =	"Frederic Gruau",
  Title =	"Genetic Synthesis of Boolean Neural Networks with a Cell Rewriting Developmental Process",
  Organization =	{COGANN92},
  PAGES =	{55-74},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Guo,
  Author =	"Zhichao Guo and Robert Uhrig",
  Title =	"Using Genetic Algorithms to Select Inputs for Neural Networks",
  Organization =	{COGANN92},
  PAGES =	{223-234},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Techreport{Guha92,
  Author =	"Aloke Guha and Steven A. Harp and Tariq Samad",
  Title =	"Genetic Algorithm Synthesis of Neural Networks",
  Type =	{Patent 5140530},
  PUBLISHER =	{United States},
  YEAR =	{18 AUG 1992},
  KEY =	{connectionism, neural networks, cogann} 
}

@Article{Gruau93,
  Author =	"Fredric Gruau",
  Title =	"Cellular encoding as a graph grammar",
  Journal =	{IEE Colloquium on Grammatical Inference: Theory,
Applications and Alternatives},
  VOLUME =	{(Digest No.092)},
  PAGES =	{17/1-10},
  PUBLISHER =	{IEE},
  ADDRESS =	{London},
  YEAR =	{22-23 April 1993},
  KEY =	{genetic algorithm connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Cellular encoding is a method for encoding a family of neural
networks into a set of labeled trees. Such sets of trees can be evolved by
the genetic algorithm so as to find a particular set of trees that encodes
a family of Boolean neural networks for computing a family of Boolean
functions. Cellular encoding is presented as a graph grammar. A method is
proposed for translating a cellular encoding into a set of graph grammar
rewriting rules of the kind used in the Berlin algebraic approach to
graph rewriting. The genetic search of neural networks via cellular
encoding appears as a grammatical inference process where the language to
parse is implicitly specified, instead of explicitly by positive and
negative examples. Experimental results shows that the genetic algorithm
can infer grammars that derive neural networks for the parity, symmetry and
decoder Boolean function of arbitrary large size.} 
}

@Book{deGaris92,
  Author =	"Hugo de Garis",
  EDITOR =	{F.J. Varela and P. Bourgine},
  Title =	"Steerable GenNets: the genetic programming of steerable
		  behaviors in GenNets",
  Pages =	{272-281},
  JOURNAL =	{Toward a Practice of Autonomous Systems. Proceedings of the First European
Conference on Artificial Life},
  PAGES =	{310-314},
  PUBLISHER =	{MIT Press},
  ADDRESS =	{Cambridge, MA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Shows how genetic programming techniques (i.e. the art of
applied evolution, or building complex systems using the genetic
algorithm) can be used to evolve dynamic behaviors in neural systems which
are controllable or steerable. The genetic algorithm evolves the weights of
a fully-connnected time-dependent neural network (called a GenNet), such
that the same GenNet is capable of generating two separate time-dependent
behaviors, depending upon the setting of two different values of a clamped
input control variable. By freezing these weights in the GenNet and then
applying intermediate control values, one obtains intermediate behaviors,
showing that the GenNet has generalized its behavioral learning. It has
become controllable or steerable. This principle is applicable to the
evolution of many controllable neural behaviors and is useful in the
construction of artificial creatures (with artificial nervous systems)
based on neural modules. One simply evolves two behaviors at different
settings of the control input so that the GenNet generalizes its behavioral
learning. In this paper, a concrete example of this process is given in the
form of the genetic programming of a variable frequency generator GenNet.
This paper ends with a discussion on the handcrafters vs. evolutionists
controversy, concerning future approaches to artificial creature (biot)
building.} 
}

@Conference{Gruau,
  Author =	"Frederic Gruau",
  Title =	"Genetic Synthesis of Modular Neural Networks",
  Organization =	{ICGA93},
  KEY =	{genetic algorithms, connectionism cogann} 
}

@Conference{deGaris,
  Author =	"H. de Garis",
  Title =	"Circuits of Production Rule GenNets.  The Genetic
		  Programming of Artificial Nervous Systems",
  Pages =	{699-705},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{de,
  Author =	"Hugo de Garis",
  Title =	"INCREMENTAL EVOLUTION of NEURAL NETS Genetic Programming in Incremental Steps",
  Organization =	{WCNN93},
  PAGES =	{II447 - II450},
  KEY =	{connectionism, genetic algorithms, cogann, Incremental Evolution,
Genetic Programming, GenNets Genetically Programmed Neural Network Modules,
Artificial Nervous Systems, Biots Biological Robots, Darwinian Robotics,
1000-GenNet Biots, GenNet Accelerators, GenNet Shaping.},
  ABSTRACT =	{ABSTRACT
This paper addresses itself to the question of Incremental Evolution of neural
networks, which is defined to be the art of evolving neural networks in
incremental steps, using Genetic Algorithms. One evolves the weight values
of a fully connected neural network (called a GenNet [de Garis 1990, 1993])
containing N neurons to perform T tasks, and then takes the result (i.e. the
evolved weights of the N neurons) and adds a few more neurons dN, to evolve
the performance of a few more tasks dT. This paper investigates (a) whether
this can be done at all, (b) whether is is faster to evolve an N + dN GenNet
performing T + dT tasks from scratch or to do it incrementally (1.e. [N,T]
then [N+dN,T+dT]), and (c) how the two approaches (i.e. from scratch or
incremental) compare in task performance quality. Incremental Evolution will
become an important issue when the various brain builder groups around the
world (i.e.groups using evolved neural network modules to build artificial
nervous systems for biological robots (biots), e.g. Beer's group at Case
Western Reserve University USA, Cliff et al's group at Sussex University UK,
and the Author's group "" ATR Japan [de Garis 1993] are confronted with the
decision whether to start from scratch when desiring to evolve biots with a
greater number of behaviors, or to increment their already evolved nervous
systems. Nature obviously had to increment.} 
}

@Article{Hinton87,
  Author =	"Geoffrey E. Hinton and Stephen J. Nowlan",
  Title =	"How Learning Can Guide Evolution",
  Journal =	{Complex Systems},
  VOLUME =	{1},
  NUMBER =	{1},
  YEAR =	{JUN 1987},
  PAGES =	{495-502},
  KEY =	{Neural nets genetic algorithms connectionism, cogann ref} 
}

@Conference{Harp,
  Author =	"Steven Alex Harp and Tariq Samad and Aloke Guha",
  Title =	"Towards the Genetic Synthesis of Neural Networks",
  Key =	{neural networks, connectionism, back-propagation, GENESYS, representation: network architectures,
cogann ref},
  ORGANIZATION =	{ICGA89},
  PAGES =	{360-369} 
}

@Conference{Harp,
  Author =	"Steven Alex Harp and Tariq Samad and Aloke Guha",
  Title =	"Designing Application-Specific Neural Networks Using the Genetic Algorithm",
  Organization =	{NIPS2},
  PAGES =	{447-454},
  KEY =	{connectionism, cogann ref} 
}

@Article{Harp,
  Author =	"Steven Alex Harp and Tariq Samad",
  Title =	"Genetic Optimization of Self-Organizing Feature Maps",
  Journal =	{IJCNN-91},
  VOLUME =	{I},
  PAGES =	{341-346},
  KEY =	{algorithms, connectionism, Kohonen,
clustering, vector quantization, cogann ref} 
}

@Conference{Hancock90,
  Author =	"P. Hancock",
  Title =	"{GANNET}: Design of a Neural Network for Face Recognition
		  by Genetic Algorithm",
  Organization =	{PROC IEEE Workshop on Genetic Algorithms, Simulated Annealing and Neural
Networks},
  ADDRESS =	{University of Glasgow, Scotland},
  YEAR =	{1990},
  KEY =	{connectionism, cogann ref} 
}

@Inbook{Heistermann90,
  Author =	"J. Heistermann",
  EDITOR =	{R. Eckmiller and G. Hartmann and G. Hauske},
  Title =	"Learning in Neural Nets by Genetic Algorithms",
  Booktitle =	{Parallel Processing in Neural Systems and Computers},
  PUBLISHER =	{Elsevier Science Publishers},
  PAGES =	{165-168},
  YEAR =	{1990},
  KEY =	{connectionism} 
}

@Article{Heistermann89,
  Author =	"J. Heistermann",
  Title =	"Parallel Algorithms for Learning in Neural Networks with Evolution Strategy",
  Journal =	{Parallel Computing},
  VOLUME =	{12},
  YEAR =	{1989},
  KEY =	{connectionism, genetic algorithms} 
}

@Inbook{Harp91,
  Author =	"Steven Alex Harp and Tariq Samad",
  EDITOR =	{Lawrence Davis},
  Title =	"The Genetic Synthesis of Neural Network Architecture",
  Booktitle =	{Handbook of Genetic Algorithms},
  PAGES =	{202-221},
  KEY =	{connectionism, genetic algorithms, cogann ref},
  PUBLISHER =	{VNR},
  YEAR =	{1991} 
}

@Book{Hintz90,
  Author =	"K.J. Hintz and J.J. Spofford",
  EDITOR =	{A. Meystel},
  Title =	"Evolving a Neural Network",
  Journal =	{PROC 5th IEEE International Symposium on Intelligent Control},
  PAGES =	{479-484},
  PUBLISHER =	{IEEE Computer Society Press},
  ADDRESS =	{Los Alamitos, CA},
  YEAR =	{SEPT 1990},
  KEY =	{connectionism, genetic algorithm (?)} 
}

@Conference{Hancock,
  Author =	"Peter J.B. Hancock",
  Title =	"Genetic Algorithms and Permutation Problems: A Comparison
		  of Recombination Operators for Neural Net Structure Specification",
  Organization =	{COGANN92},
  PAGES =	{108-122},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Hemker93,
  Author =	"A. Hemker",
  Title =	"A data parallel problem solving architecture for the
		  reconstruction of physical events",
  Journal =	{International Journal of Modern Physics C (Physics and Computers)},
  VOLUME =	{4},
  NUMBER =	{1},
  PAGES =	{143-9},
  YEAR =	{Feb. 1993},
  KEY =	{Genetic algorithm quarks subatomic physics neural network connectionism},
  ABSTRACT =	{ABSTRACT
A system is presented which is able to reconstruct the physical
processes  after  the annihilation of particles in collider rings like LEP.
It  can  be  shown  that  the  reconstruction  task belongs to the class of
NP-complete  problems.  Neither the conventional symbol processing approach
of  artificial  intelligence  nor  connectionist  systems alone satisfy the
requirements  of  the application. A hybrid system is introduced in which a
genetic  algorithm acts together with a model-based hillclimbing component.
The  power  of this method is based on the interaction of a dynamically and
an  inferentially  working  module.  A data parallel implementation of this
algorithm  on  the  Connection Machine Model CM-2 is described. Lastly, the
reconstruction   system   is   applied   to   a  classification  task,  the
identification  of  the initial quark pair. The results are compared with a
conventional analysis method and a neural network.} 
}

@Article{Hsu92,
  Author =	"Loke Soo Hsu and Zhibiao Wu",
  Title =	"Chinese character prediction by recurrent network",
  Journal =	{Computer Processing of Chinese & Oriental Languages},
  VOLUME =	{6},
  NUMBER =	{2},
  PAGES =	{179-194},
  YEAR =	{Dec 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The speedy input of Chinese characters from a keyboard is a
problem that receives much attention. Many different input methods have
been implemented and put into use. For all these input methods, the
prediction aids are provided. This requires the system to predict the next
most likely Chinese character based on the previous input. The conventional
approach is to use the frequencies of the Chinese word phrases. The Authors
present "" approach by using the combination of the simple recurrent neural
network and the genetic algorithm. The simple recurrent network makes the
prediction result change dynamically for different contexts, even the
previous Chinese character may be the same. The genetic algorithm is used
to do a generalized adaptive search in the solution space for a better
input pattern encoding scheme to ensure that the network size is not too
large. Experiments show that the method is promising.} 
}

@Book{Heistermann92,
  Author =	"J. Heistermann",
  EDITOR =	{P. Dewilde and J. Vandewalle},
  Title =	"A mixed genetic approach to the optimization of neural controllers",
  Journal =	{CompEuro 1992 Proceedings. Computer Systems and Software Engineering},
  PAGES =	{459-464},
  PUBLISHER =	{IEEE Comput. Soc. Press},
  ADDRESS =	{Los Alamitos, CA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Author discusses "om" of the capabilities of genetic
algorithms (GAs). GAs are compared with other standard optimization methods
like gradient descent or simulated annealing (SA). It is shown that SA is
just a special case of GA. The role of a population in the optimization
process is demonstrated by an example. GA was applied as a learning
algorithm to neural networks.} 
}

@Book{Ho92,
  Author =	"A.W. Ho and G.C. Fox",
  EDITOR =	{P. Messina and A. Murli},
  Title =	"Competitive-cooperative system of distributed artificial neural agents",
  Journal =	{Parallel Computing: Problems, Methods and Applications.
Selection of Papers Presented at the Conference on Parallel Computing:
Achievements, Problems and Prospects},
  PAGES =	{499-507},
  PUBLISHER =	{Elsevier, Amsterdam, Netherlands},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
A framework for simulations of hierarchical organizations of
interacting, distributed artificial agents on distributed-memory, MIMD
computers is presented. Interactions among aggregates of intelligent agents
in an organization are restricted to obey competition and cooperation
criteria. Each intelligent agent in an organization is a parallel
implementation of a feedforward multilayer perceptrons neural network using
error backpropagation (BP) as the learning rule. In this preliminary study,
domination, viewed as a type of deterministic genetic algorithm (GA,) is
chosen to be the preferred form of interaction. The framework exploits the
hierarchical nature intrinsic in an organizational approach to
problem-solving. It takes advantage of parallelism at different levels of
granularity, from domain decomposition within the agents to coarse grain
team-level interaction. Transputer-based simulation results for a test
problem of learning the solution to a parity function of predicate order 10
is discussed.} 
}

@Article{Harp92,
  Author =	"Steven A. Harp and Tario Samad",
  Title =	"Optimizing neural networks with genetic algorithms",
  Journal =	{Proceedings of the American Power Conference},
  VOLUME =	{54 pt 2},
  PUBLISHER =	{Illinois Inst of Technology},
  ADDRESS =	{Chicago, IL, USA.},
  PAGES =	{1138-1143},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
We describe an approach to application-specific neural network
design using genetic algorithms. A genetic algorithm is a robust
optimization method particularly well suited for search spaces that are
high-dimensional, discontinuous and noisy - features that typify the neural
network design problem. Our approach is relevant to virtually all neural
network applications: it is network-model independent and it permits
optimization for arbitrary, user-defined criteria. We have developed an
experimental system, NeuroGENESYS, and have conducted several experiments
on small-scale problems. Performance improvements over manual designs have
been observed, the interplay between performance criteria and network
design aspects has been demonstrated, and general design principles have
been uncovered.} 
}

@Article{Holland92,
  Author =	"O.E. Holland and M.A. Snaith",
  Title =	"Neural control of locomotion in a quadrupedal robot",
  Journal =	{IEE  Proceedings  Part F: Radar and Signal Processing},
  VOLUME =	{139},
  NUMBER =	{6},
  PAGES =	{431-436},
  YEAR =	{DEC 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann robotics},
  ABSTRACT =	{ABSTRACT
The Authors present "esult" of a first study demonstrating that
the apparently complex task of controlling walking in a real quadrupedal
robot with highly nonlinear interactions between the control elements can
be learned quickly by a crude and simple reinforcement learning algorithm.
They can as yet say little that is useful about the contribution of
reflexes to learned walking, and nothing about the quality of evolved
solutions other than that their discovery by applying genetic algorithms to
real robots is likely to take a prohibitively long time. However, they hope
that their experiences will point the way to more controlled studies of the
applications of reinforcement learning to real-world problems, especially
to control problems associated with autonomous mobile robots.} 
}

@Conference{Hassoun,
  Author =	"Mohamad H. Hassoun and Jing Song",
  Title =	"Multilayer Perceptron Learning Via Genetic Search for Hidden Layer Activations",
  Organization =	{WCNN93},
  PAGES =	{III437 - III444},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
A new learning technique is proposed for multilayer neural networks based on
genetic search, in hidden target space, and gradient descent learning
strategies. Our simulations show that the new algorithm combines the global
optimization capabilities of genetic algorithms with the speed of gradient
descent local search in order to outperform pure descent-based algorithms such
as backpropagation. In addition, we show that genetic search in hidden target
space is less complex than that of weight space.} 
}

@Article{Ichikawa92,
  Author =	"Yoshiaki Ichikawa and Toshiyuki Sawa",
  Title =	"Neural network application for direct feedback controllers",
  Journal =	{IEEE Transactions on Neural Networks},
  VOLUME =	{3},
  NUMBER =	{2},
  YEAR =	{MAR 1992},
  PAGES =	{224-231},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Author presents " learning algorithm and capabilities of
perceptron-like neural networks whose outputs and inputs are directly
connected to plants just like ordinary feedback controllers. This simple
configuration includes the difficulty of teaching the network. In addition,
it is preferable to let the network learn so that a global and arbitrary
evaluation of the total responses of the plant will be optimized
eventually. In order to satisfy these needs, genetic algorithms are
modified to accommodate the network learning procedure. This procedure is a
kind of simulated evolution process in which a group of networks gradually
improves as a whole, by crossing over connection weights among them, or by
mutational changes of the weights, according to fitness values assigned to
each network by a global evaluation. Simulations demonstrate that these
networks can be optimized in terms of various evaluations, and they can
discover schemes by themselves, such as state estimation and nonlinear
control.} 
}

@Article{Janson92,
  Author =	"D.J. Janson and J.F. Frenzel",
  Title =	"Application of genetic algorithms to the training of higher
		  order neural networks",
  Journal =	{Journal of Systems Engineering},
  VOLUME =	{2},
  NUMBER =	{4},
  PAGES =	{272-276},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Product unit neural networks are a new form of feedforward
learning networks in which several summing units are replaced by units
capable of calculating a weighted product of inputs. While such networks
can be trained using traditional backpropagation, the solution involves the
manipulation of complex-valued expressions. As an alternative, this paper
investigates the training of product networks using genetic algorithms.
Results are presented on the training of a neural network to calculate the
optimum width of transistors in a CMOS switch given desired operating
parameters. It is shown how local minima affect the performance of the
genetic algorithm, and one method of overcoming this is presented.} 
}

@Conference{Jakob,
  Author =	"C. Jakob and J. Rehder",
  Title =	"Evolution of Neural Net Architectures by a Hierarchical Grammar-Based
Genetic System",
  Pages =	{72-79},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Book{Kerszberg,
  Author =	"Michel Kerszberg",
  PUBLISHER =	{Institut fur Festkorperforschung der},
  Title =	"Genetic and Epigenetic Factors in Neural Circuit Wiring (preliminary)",
  Key =	{connectionism, cogann ref} 
}

@Inbook{Kerszberg86,
  Author =	"Michel Kerszberg and Aviv Bergman",
  Booktitle =	"Computer Simulation in Brain Science, Copenhagen, Denmark",
  Year =	{August 1986},
  Title =	"The Evolution of Data Processing Abilities in Competing Automata",
  Key =	{genetic algorithms, connectionism, cogann ref} 
}

@Conference{Kadaba90,
  Author =	"Nagesh Kadaba and Kendall E. Nygard and Paul L. Juell",
  Title =	"Integration of Adaptive Machine Learning and Knowledge-Based
Systems for Routing and Scheduling Applications",
  Key =	{Connectionism, genetic algorithms, XROUTE, expert system, neural network, cogann ref},
  ORGANIZATION =	{Expert Systems with Applications: An International Journal},
  YEAR =	{1990},
  NOTE =	{NDSU-CS-TR-89-25} 
}

@Book{Kadaba90,
  Author =	"Nagesh Kadaba and Kendall E. Nygard",
  Title =	"Improving the Performance of Genetic Algorithms in Automated
Discovery of Parameters",
  Key =	{vehicle routing, Connectionism, genetic algorithms, XROUTE, expert system, neural network, cogann
ref},
  PUBLISHER =	{Dept. of SC and OR, North Dakota State University},
  YEAR =	{JAN 25, 1990},
  NOTE =	{Draft} 
}

@Conference{Kadaba90,
  Author =	"Nagesh Kadaba and Kendall E. Nygard and Paul L. Juell and Lars Kangas",
  Title =	"Modular Back_Propagation Neural Networks for Large
Domain Patter Classification",
  Key =	{TSP, traveling salesperson, Connectionism, genetic algorithms, neural network},
  ORGANIZATION =	{PROC IJCNN-90},
  YEAR =	{JAN 15-19, 1990},
  ADDRESS =	{Washington, D.C.} 
}

@Article{Koza,
  Author =	"John R. Koza and James P. Rice",
  Title =	"Genetic Generation of Both the Weights and Architecture
for a Neural Network",
  Journal =	{IJCNN-91},
  VOLUME =	{II},
  PAGES =	{397-404},
  KEY =	{genetic algorithms, connectionism, one-bit adder, cogann ref} 
}

@Conference{Kargupta,
  Author =	"Hillol Kargupta and Robert E. Smith",
  ORGANIZATION =	{ICGA91},
  PAGES =	{370-376},
  Title =	"System Identification with Evolving Polynomial Networks",
  Abstract =	{Abstract: The construction of models for prediction and control of
initially unknown, potentially nonlinear
systems is a difficult, fundamental problem in
machine learning and engineering control.
In this paper, a {\em genetic algorithm}
(GA) based technique is used
to iteratively form polynomial networks that model the behavior of
nonlinear systems.
This approach is motivated by the {\em group method of data
handling} (GMDH) (Ivakhnenko, 1971), but attempts to
overcome the computational overhead and locality associated with the
original GMDH.
The approach presented here uses a multi-modal GA (Deb, 1989) to
select nodes for a network
based on an information-theoretic fitness measure.
Preliminary results show that the GA is successful in
modeling continuous-time and discrete-time
chaotic systems. Implications and extensions of this work are
discussed.},
  KEY =	{genetic algorithms selection crowding; relation AI machine learning connectionist networks
genetic algorithms, cogann ref} 
}

@Article{Kitano90,
  Author =	"Hiroaki Kitano",
  Title =	"Designing Neural Network Using Genetic Algorithm with
Graph Generation System",
  Journal =	{Complex Systems},
  VOLUME =	{4},
  PAGES =	{461-476},
  YEAR =	{1990},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Kitano90,
  Author =	"Hiroaki Kitano",
  Title =	"Empirical Studies on the Speed of Convergence of Neural Network Training
Using Genetic Algorithms",
  Organization =	{PROC AAAI-90},
  YEAR =	{1990},
  KEY =	{connectionism, cogann ref} 
}

@Inbook{Keesing,
  Author =	"R. Keesing and D. Stork",
  Title =	"Evolution and Learning in Neural Networks:
The Number and Distribution of Learning Trials Affect the Rate of Evolution",
  Booktitle =	{NIPS3},
  PAGES =	{804-810},
  KEY =	{connectionism, genetic algorithms} 
}

@Techreport{Kitano92,
  Author =	"Hiroaki Kitano",
  Title =	"Neurogenetic Learning: An Integrated Method of Designing
and Training Neural Networks using Genetic Algorithms",
  Type =	{CMU-CMT-92-134},
  YEAR =	{MAR 1992},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Kirby89,
  Author =	"K. G. Kirby and Michael Conrad and R.R. Kampfner",
  Title =	"Evolutionary learning in reaction-diffusion neurons",
  Organization =	{SUBMITTED TO Bull. Math. Biol.},
  YEAR =	{1989},
  KEY =	{genetic algorithms,  connectionism, cogann ref} 
}

@Techreport{Kirby88,
  Author =	"K. G. Kirby",
  Title =	"Intraneuronal dynamics and evolutionary learning",
  Type =	{Ph.D. DISS},
  PUBLISHER =	{Dept. of Computer Science
   Wayne State University},
  YEAR =	{1988},
  KEY =	{genetic algorithms,  connectionism, cogann ref} 
}

@Article{Kirby86,
  Author =	"K.G. Kirby and Michael Conrad",
  Title =	"Intraneuronal dynamics as a substrate for evolutionary learning",
  Journal =	{Physica D},
  VOLUME =	{22},
  PAGES =	{205-215},
  YEAR =	{1986},
  KEY =	{genetic algorithms,  connectionism, cogann ref} 
}

@Conference{Koza,
  Author =	"John R. Koza",
  Title =	"A Genetic Approach to the Truck Backer Upper Problem and the Inter-Twined
Spiral Problem",
  Abstract =	{ABSTRACT
Neural networks are a biologically motivated problem-solving paradigm that
has proven successful in robustly solving a variety of problems.  This
paper describes another biologically motivated paradigm, namely genetic
programming, which can also solve a variety of problems.  This paper
explains genetic programming and applies it to two well-know benchmark
problems from the field of neural networks.  The truck backer upper
problem is a multi-dimensional control problem and the inter-twined
spirals p roblem is a challenging classification problem.},
  PAGES =	{IV-310 - IV-318},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{KrishnaKumar,
  Author =	"K. KrishnaKumar",
  Title =	"Immunized Neurocontrol--Concepts and Initial Results",
  Organization =	{COGANN92},
  PAGES =	{146-168},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Karunanithi,
  Author =	"Nachimuthu Karunanithi and Rajarshi Das and Darrell Whitley",
  Title =	"Genetic Cascade Learning for Neural Networks",
  Organization =	{COGANN92},
  PAGES =	{134-145},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Karim92,
  Author =	"M.N. Karim and S.L. Rivera",
  Title =	"Use of recurrent neural networks for bioprocess identification in
on-line optimization by micro-genetic algorithms",
  Journal =	{Proceedings of the American Control Conference},
  VOLUME =	{3},
  PUBLISHER =	{American Automatic Control Council},
  ADDRESS =	{Green Valley, AZ,},
  PAGES =	{1931-1932},
  YEAR =	{1992},
  KEY =	{process control, connectionism, genetic},
  ABSTRACT =	{ABSTRACT
The use of recurrent neural networks in bioprocess
identification and optimization is investigated. A recurrent neural network
is trained on a set of fermentation data, and thereafter used as a
nonlinear process model to estimate nonmeasurable process states at
different conditions. With the bioprocess state variable information
available, an optimization technique can be used to generate optimum
controls settings to improve the process performance. This paper explores
the use of Micro-Genetic Algorithms as a technique for bioreactor
optimization. Simulation results will be discussed based in the
fermentative ethanol production by the anaerobic bacteria Zymomonas
mobilis.} 
}

@Article{Kitano93,
  Author =	"H. Kitano",
  Title =	"Continuous generation genetic algorithms",
  Journal =	{Journal of the Society of Instrument and Control Engineers},
  VOLUME =	{32},
  NUMBER =	{1},
  PAGES =	{31-8},
  YEAR =	{Jan. 1993},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Presents a continuous generation genetic algorithm. Most
genetic algorithms use a discrete generation model in which all individuals
in a population synchronize mating period. The discrete generation model,
however, wastes processor time in parallel implementations when the fitness
of each individual (proportionally or reversely) correlates with the
computational cost of its evaluation. An example of such a task is neural
network design and training. In some cases, over 80% of total CPU time has
been wasted. The continuous generation model mitigates this problem by
introducing asynchronous mating, the continuous generation model increases
the number of reproduction per a unit-time over 500% over the traditional
discrete model. CPU idle time has been minimized to 1/25. Also, a
significant improvement in convergence speed has been estimated.} 
}

@Article{Kouchi92,
  Author =	"M. Kouchi and H. Inayoshi and T. Hoshino",
  Title =	"Optimization of neural-net structure by genetic algorithm with
diploidy and geographical isolation model",
  Journal =	{Journal of Japanese Society for Artificial Intelligence},
  VOLUME =	{7},
  NUMBER =	{3},
  PAGES =	{509-517},
  YEAR =	{May 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The structure of a simple neural network is optimized by the
use of a genetic-algorithm. The neural network is a perceptron, which has
three outputs; the logical AND, OR and XOR of two inputs The evaluation
function for optimization is a linear combination of the correctness, the
network sizes, and an auxiliary term inducing the optimum solution The
chromosome is a vector of the link weights of the network. The genetic
operators used are crossing-over and point-mutation on the parent
chromosomes Two genetic rules were tested. In the haploidy rule, each
individual has single chromosome, and the offspring is generated by
crossing-over the parents' chromosomes at a randomly chosen locus and
taking one of those crossed-over chromosomes. In the diploidy rule, each
individual has a pair of chromosomes, and the offspring's chromosomes are
generated by combining the gamete produced through the meiosis of the
parents' chromosomes. The other model used in the genetic algorithm is the
geographical isolation model, where the entire population is divided into
four sub-populations, in which the local selection and reproduction are
carried out, though, in some time interval, randomly sampled individuals
are exchanged among sub-populations. Comparison was made among four
combinations of haploid or diploid, and single-population or multiple
sub-populations. Diploidy together with the sub-population model was proved
to be the best for this optimization problem. Thus, the optimum structure
of network was found.} 
}

@Article{Klimasauskas92,
  Author =	"C.C. Klimasauskas",
  Title =	"Neural networks: an engineering perspective",
  Journal =	{IEEE Communications Magazine},
  VOLUME =	{30},
  NUMBER =	{9},
  PAGES =	{II-50--II-53},
  YEAR =	{SEPT 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The development and application of neural networks are
presented from an engineering perspective. It is stated that neural
computing is a collection of mathematical techniques that have been gaining
growing acceptance as plug-compatible replacements for statistical and
other data-modeling techniques. Two of these techniques, function
approximation and clustering, are discussed. The forces shaping the future
of neural networking systems, including plug compatibility, hybrid
systems-neural computing concepts integrated with expert systems, fuzzy
logic, and genetic algorithms-and application specification systems, are
reviewed.} 
}

@Conference{Kwiatkowski,
  Author =	"L. Kwiatkowski and J-P Stromboni",
  Title =	"Neuromimetic Algorithms Processing: Tools for Design of Dedicated
Architectures",
  Pages =	{706-711},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Littman,
  Author =	"Michael L. Littman and David H. Ackley",
  ORGANIZATION =	{ICGA91},
  PAGES =	{136-142},
  Title =	"Adaptation in constant utility non-stationary environments",
  Abstract =	{Abstract: Environments that vary over time present
special challenges to adaptive systems.  Although in the worst case
there may be no hope of effective adaptation, not all forms of
environmental variability need be so disabling.  We consider a broad
class of non-stationary environments, those which combine a variable
*result function* with an invariant *utility function*, and
demonstrate via simulation that an adaptive strategy employing both
evolution and learning can tolerate a much higher rate of
environmental variation than an evolution-only strategy.  We suggest
that in many cases where stability has previously been assumed, the
constant utility non-stationary environment may in fact be a more
robust description.},
  KEY =	{genetic algorithms environment fitness functions dynamic; biological modeling evolution and learning; hillclimbing, cogann ref
ERL, Evolutionary reinforcement,
non-stationary environment, dynamic , neural networks,
connectionism} 
}

@Conference{Lai,
  Author =	"W.K. Lai and G.G. Coghill",
  Title =	"Genetic Breeding of Control Parameters for the Hopfield/Tank Neural Net",
  Abstract =	{ABSTRACT
Artificial neural networks, especially the Hopfield/Tank neural net
have been used to solve the travelling salesman problem. These networks
usually require a set of parameters to be carefully selected and tuned
to produce sensible solutions.  Genetic Algorithms are basically
adaptive systems that transform a population of individuals into new
populations, using relatively simple mechanisms. It has the ability to
efficiently explore the problem sub-space to produce approximate
solutions that are globally competitive.  This paper will show how
Genetic Algorithms may be used in conjunction with the Hopfield/Tank
neural net by breeding an effective set of control parameters in the
parameter sub-space to be used by the artificial neural network.},
  PAGES =	{IV-618 - IV-623},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Lindgren,
  Author =	"Kristian Lindgren and Anders Nilsson and Mats Nordahl and Ingrid Rade",
  Title =	"Regular Language Inference Using Evolving Neural Networks",
  Organization =	{COGANN92},
  PAGES =	{75-86},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Lewis92,
  Author =	"M. Anthony Lewis and Andrew H. Fagg and Alan Solidum",
  Title =	"Genetic  programming  approach  to  the construction of a neural
network for control of a walking robot",
  Journal =	{Proceedings of IEEE  International  Conference  on Robotics and Automation},
  VOLUME =	{3},
  PUBLISHER =	{IEEE},
  ADDRESS =	{Piscataway, NJ, USA},
  PAGES =	{2618-2623},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann programming},
  ABSTRACT =	{ABSTRACT
The Authors describe "h" staged evolution of a complex motor
pattern generator (MPG) for the control of a walking robot. The experiments
were carried out on a six-legged, Brooks-style insect robot. The MPG was
composed of a network of neurons with weights determined by genetic
algorithm optimization. Staged evolution was used to improve the
convergence rate of the algorithm. First, an oscillator for the individual
leg movements was evolved. Then, a network of these oscillators was evolved
to coordinate the movements of the different legs. By introducing a staged
set of manageable challenges, the algorithm's performance was improved.} 
}

@Conference{Lindgren,
  Author =	"K. Lindgren and A. Nelsson and M.G. Nordahl and I. Rade",
  Title =	"Evolving Recurrent Neural Networks",
  Pages =	{55-62},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Mjolsness86,
  Author =	"E. Mjolsness and D. H. Sharp",
  YEAR =	{1986},
  ORGANIZATION =	{PROC American Institute of Physics (Special Issue on Neural Nets)},
  Title =	"A Preliminary Analysis of Recursively Generated Networks",
  Key =	{connectionism, genetic algorithms, cogann ref} 
}

@Techreport{Mjolsness88,
  Author =	"Eric Mjolsness and David H. Sharp and Bradley K. Alpert",
  YEAR =	{March 1988},
  TYPE =	{YALEU/DCS/TR-613, Yale},
  TYPE =	{LA-UR-88-142, Los Alamos},
  Title =	"Scaling, Machine Learning, and Genetic Neural Nets",
  Key =	{connectionism, genetic algorithms, cogann ref} 
}

@Inbook{Mjolsness,
  Author =	"Eric Mjolsness and Gene Gindi and Tony Zador and P. Anandan",
  Booktitle =	"Snowbird 1988",
  Title =	"Objective Functions for Visual Recognition: A Neural
	   Network that Incorporates Inheritance and Abstraction",
  KEY =	{genetic connectionism, cogann ref} 
}

@Inbook{Mjolsness,
  Author =	"Eric Mjolsness and David H. Sharp and Bradley K. Alpert",
  Booktitle =	"Snowbird 1988",
  Title =	{Genetic Parsimony in Neural Nets},
  KEY =	{genetic connectionism, cogann ref} 
}

@Conference{Montana,
  Author =	"David J. Montana and Lawrence Davis",
  Title =	"Training Feedforward Neural Networks Using Genetic Algorithms",
  Organization =	{IJCAI-89},
  PAGES =	{762-767},
  KEY =	{connectionism, new operators, adaptive parameters, cogann ref} 
}

@Conference{Miller,
  Author =	"Geoffrey F. Miller and Peter M. Todd and Shailesh U. Hegde",
  Title =	"Designing Neural Networks using Genetic Algorithms",
  Key =	{connectionism, neural networks, learning, evolution, representation: network architectures, INNERVATOR},
  ORGANIZATION =	{ICGA89},
  PAGES =	{379-384} 
}

@Conference{Maricic90,
  Author =	"Borut Maricic and Zoran Nikolov",
  Title =	"GENNET - System for Computer Aided Neural
Network Design Using Genetic Algorithms",
  Organization =	{PROC IJCNN-90},
  ADDRESS =	{Washington, DC},
  YEAR =	{JAN 1990},
  PAGES =	{I-102 - I-105},
  KEY =	{connectionism} 
}

@Article{Mjolsness89,
  Author =	"Eric Mjolsness and David H. Sharp and Bradley K. Alpert",
  Title =	"Scaling, Machine Learning, and Genetic Neural Nets",
  Year =	{1989},
  JOURNAL =	{Advances in Applied Mathematics},
  VOLUME =	{10},
  KEY =	{connectionism, cogann ref} 
}

@Book{Muhlenbein89,
  Author =	"Heinz Muhlenbein and J{\"o}rg Kindermann",
  Title =	"The Dynamics of Evolution and Learning - Towards Genetic Neural Networks",
  Year =	{pre-1989},
  PUBLISHER =	{German National Research Center for Computer Science},
  KEY =	{connectionism, cogann ref} 
}

@Inbook{Muhlenbein89,
  Author =	"Heinz Muhlenbein and J{\"o}rg Kindermann",
  EDITOR =	{R. Pfeifer and Z. Schreter and F. Fogelman-Soulie and L. Steels},
  Title =	"The Dynamics of Evolution and Learning: Towards Genetic Neural Networks",
  Booktitle =	{Connectionism in Perspective},
  YEAR =	{1989},
  PUBLISHER =	{Elsevier Science Publishers B.V. (North-Holland)},
  PAGES =	{173-197},
  KEY =	{connectionism, cogann ref} 
}

@Article{Moed90,
  Author =	"Michael C. Moed and George N. Saridis",
  Title =	"A Boltzman Machine for the Optimization of
Intelligent Machines",
  Journal =	{IEEE SMC},
  VOLUME =	{29},
  NUMBER =	{5},
  PAGES =	{1094-1102},
  YEAR =	{SEP/OCT 1990},
  KEY =	{genetic algorithm, simulated annealing, connectionism, cogann ref, machine learning} 
}

@Conference{Montana,
  Author =	"David Montana",
  Title =	"A Weighted Probabilistic Neural Network",
  Organization =	{NIPS4},
  ABSTRACT =	{Abstract: The Probabilistic Neural Network (PNN) algorithm represents
the likelihood function of a given class as the sum of identical,
isotropic Gaussians.  In practice, PNN is often an excellent pattern
classifier, outperforming other classifiers including backpropagation.
However, it is not robust with respect to affine transformations of
feature space, and this can lead to poor performance on certain data.
We have derived an extension of PNN called Weighted PNN (WPNN) which
compensates for this flaw by allowing anisotropic Gaussians, i.e.
Gaussians whose covariance is not a multiple of the identity matrix.
The covariance is optimized using a genetic algorithm, some
interesting features of which are its redundant, logarithmic encoding
and large population size.  Experimental results validate our claims.},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Conference{Marti,
  Author =	"Leonardo Mart{\'i}",
  Title =	"Genetically  Generated  Neural  Networks  II: Searching  for  an Optimal
Representation",
  Abstract =	{ABSTRACT
 Genetic Algorithms (GAs) make use of an internal representation of
a given system in order to perform optimization functions.  The actual
structural layout of this representation, called a genome, has a
crucial impact on the outcome of the optimization process.  The
purpose of this paper is to study the effects of different internal
representations in a GA, which generates neural networks.  A second GA
was used to optimize the genome structure.  This structure produces an
optimized system within a shorter time interval.},
  PAGES =	{II-221 - II-226},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Marti,
  Author =	"Leonardo Mart{\'i}",
  Title =	"Genetically Generated Neural Networks I:  Representational Effects",
  Abstract =	{ABSTRACT
This paper studies several applications of genetic algorithms (GAs) within
the neural networks field.  After generating a robust GA engine, the
system was used to generate neural network circuit architectures.  This
was accomplished by using the GA to determine the weights in a fully
interconnected network.  The importance of the internal genetic
representation was shown by testing different approaches.  The effects in
speed of optimization of varying the constraints imposed upon the desired
network were also studied.  It was observed that relatively loose
constraints provided results comparable to a fully constrained system.
The typeof neural network circuits generated were recurrent competitive
fields as described by Grossberg (1982).},
  PAGES =	{IV-537 - IV-542},
  ORGANIZATION =	{IJCNN-92},
  KEY =	{genetic algorithms, connectionism} 
}

@Book{Mansour92,
  Author =	"N. Mansour and G.C. Fox",
  EDITOR =	{L. Bouge and M. Cosnard and Y. Robert and D. Trystram},
  Title =	"Parallel physical optimization algorithms for data mapping",
  Booktitle =	{Conference on Vector and Parallel Processing},
  PAGES =	{91-96},
  PUBLISHER =	{Springer-Verlag},
  ADDRESS =	{Berlin, Germany},
  YEAR =	{1992},
  KEY =	{genetic algorithms, simulated annealing neural networks, connectionism
comparisons},
  ABSTRACT =	{ABSTRACT
Parallel algorithms, based on simulated annealing, neural
networks and genetic algorithms, for mapping irregular data to
multicomputers are presented and compared. The three algorithms deviate
from the sequential versions in order to achieve acceptable speed-ups. The
parallel annealing and neural algorithms include communication schemes
adapted to the properties of the mapping problem and of the algorithms
themselves. These schemes are found useful for providing both good
solutions and reasonable execution times. The parallel genetic algorithm is
based on a model of natural evolution. The three algorithms preserve the
high quality solutions and the non-bias properties of their sequential
counterparts. Further, the comparison results show their suitability for
different requirements of mapping time and quality.} 
}

@Article{Mansour92,
  Author =	"N. Mansour and G.C. Fox",
  Title =	"Allocating data to multicomputer nodes by physical optimization
algorithms for loosely synchronous computations",
  Journal =	{Concurrency: Practice and Experience},
  VOLUME =	{4},
  NUMBER =	{7},
  PAGES =	{557-574},
  YEAR =	{Oct. 1992},
  KEY =	{genetic algorithms, simulated annealing neural networks, connectionism
comparisons},
  ABSTRACT =	{ABSTRACT
Three optimization methods derived from natural sciences are
considered for allocating data to multicomputer nodes (of a distributed
memory parallel computer). These are simulated annealing, genetic
algorithms and neural networks. A number of design choices and the addition
of preprocessing and postprocessing steps lead to versions of the
algorithms which differ in solution qualities and execution times. The
performances of these versions are critically evaluated and compared for
test cases with different features. The performance criteria are solution
quality, execution time, robustness, bias and parallelizability.
Experimental results show that the physical algorithms produce better
solutions than those of recursive bisection methods and that they have
diverse properties. Hence, different algorithms would be suitable for
different applications. For example, the annealing and genetic algorithms
produce better solutions and do not show a bias towards particular problem
structures, but they are slower than the neural network algorithms.
Preprocessing graph contraction is one of the additional steps suggested
for the physical methods. It produces a significant reduction in execution
time, which is necessary for their applicability to large problems.} 
}

@Book{Muhlenbein92,
  Author =	"Heinz Muhlenbein",
  EDITOR =	{D. J. Evans and G. R. Joubert and H. Liddell},
  Title =	"Parallel genetic algorithms and neural networks as learning machines",
  Pages =	{91-103},
  JOURNAL =	{Parallel computing '91 Proceedings of the International Conference},
  PUBLISHER =	{North-Holland Publishing Co.},
  ADDRESS =	{Amsterdam},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Book{Montgomery92,
  Author =	"L. Montgomery and K. KrishnaKumar and G. Weeks",
  Title =	"Structural control using connectionist learning principles",
  Journal =	{AIAA Guidance, Navigation and Control Conference},
  NOTE =	{Hilton Head Island, SC, Aug. 10-12, 1992},
  ADDRESS =	{Washington},
  PUBLISHER =	{American Institute of Aeronautics and Astronautics},
  YEAR =	{1992},
  PAGES =	{771-785},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
This paper presents an evaluation of using genetic algorithms (GAs) for
exploration and artificial neural networks (ANNs) for learning in
structural control problems. First, a GA is used to explore and optimize an
actuator/sensor placement for a simple structure. Secondly, an ANN system
is synthesized for controlling a 50 DOF model of ACES (advanced control
evaluation for systems). Finally, a laboratory experiment was constructed
to verify the capabilities of GAs and ANNs observed in the above two
simulation cases.} 
}

@Article{Maeda92,
  Author =	"Y. Maeda and Y. Kanata",
  Title =	"A genetic algorithm for an unsupervised learning of neural networks",
  Journal =	{Engineering & Technology},
  VOLUME =	{10},
  NUMBER =	{2},
  PAGES =	{1-7},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors deal "it" a genetic algorithm for an unsupervised
learning rule of neural networks. The genetic algorithm consists of four
operations: selection; reproduction; crossover; and mutation. They look
into the learning efficiency of two kinds of the crossover for the
unsupervised learning rule. Moreover, they investigate the learning rate
with respect to the mutation rate.} 
}

@Article{Menczer92,
  Author =	"F. Menczer and D. Parisi",
  Title =	"Recombination and unsupervised learning: effects of crossover in
the genetic optimization of neural networks",
  Journal =	{Network: Computation in Neural Systems},
  VOLUME =	{3},
  NUMBER =	{4},
  PAGES =	{423-442},
  YEAR =	{Nov. 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Genetic algorithms have been successfully used for optimizing
complex functions over multidimensional domains, such as the space of the
connection weights in a neural network. A feed-forward layered network is
used to simulate the life cycle of a synthetic animal that moves in an
environment and captures food objects. The adaptation of the animal (i.e.
of the network's weight matrix) to the environment can be measured by the
amount of reached food objects in a given lifetime. The Authors consider
"hi" amount as a fitness function to be optimized by a genetic algorithm
over the space of the connection weights. The network can learn the weights
that solve the survival task only by means of its genetic evolution. The
recombination genetic operator (crossover) can be seen as a model of sexual
recombination for the population, while mutation models agamic
reproduction. The central problem in trying to apply crossover is the
difficult mapping between the genetic code string (genotype) and the
network's weight matrix (phenotype). For this reason crossover has been
considered unsuitable for this kind of problem in the past. The Authors
propose " simple mapping and compare the effects of sexual versus agamic
reproduction in such a problem. The results of several parametric
simulations are outlined, showing that crossover actually helps to speed up
the genetic learning.} 
}

@Book{Menczer92,
  Author =	"F. Menczer and D. Parisi",
  EDITOR =	{F.J. Varela and P. Bourgine},
  Title =	"A model for the emergence of sex in evolving networks: adaptive
advantage or random drift?",
  Journal =	{Toward a Practice of Autonomous Systems. Proceedings of the First European
Conference on Artificial Life},
  PAGES =	{337-345},
  PUBLISHER =	{MIT Press},
  ADDRESS =	{Cambridge, MA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The evolution of sex is an intriguing problem in evolutionary
biology: most high organisms use some form of sexual recombination of the
genetic material in the process of reproduction, thus there should be an
adaptive advantage in recombination if sex was selected in the course of
evolution. One might hope that the new tools offered by the simulation
methods of artificial life, genetic algorithms, (GA) and neural networks,
might help the investigation by allowing the study of simplified models and
of their detailed consequences. The Authors start "ro" some results on the
effects of introducing crossover in a GA used for evolving a population of
artificial animals trained on a simple task. Since there is a clear
advantage in applying crossover versus simple mutations alone, this
advantage could be retained by the population through selection: this
hypothesis is tested in a model with local, individual genetic operators'
probabilities by studying the emergent recombination frequencies. It is
unexpectedly hard to analyze the results of the simulations, as the
operator probabilities do not enter directly in the computation of fitness,
while they have a well-known indirect influence on the 'behaviour' of
fitness. The Authors are "onitorin" a trait that is not directly selected,
thus being subject to the strong action of random drift.} 
}

@Article{Menczer92,
  Author =	"F. Menczer and D. Parisi",
  Title =	"Evidence of hyperplanes in the genetic learning of neural networks",
  Journal =	{Biological Cybernetics},
  VOLUME =	{66},
  NUMBER =	{3},
  PAGES =	{283-289},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Genetic algorithms (GA) have been successfully applied to the
learning process of neural networks simulating artificial life. In previous
research the Authors (1990) "ompare" mutation and crossover as genetic
operators on neural networks directly encoded as real vectors. With
reference to crossover they were actually testing the building blocks
hypothesis, as the effectiveness of recombination relies on the validity of
such hypothesis. Even with the real genotype used, it was found that the
average fitness of the population of neural networks is optimized much more
quickly by crossover than it is by mutation. This indicated that the
intrinsic parallelism of crossover is not reduced by the high cardinality.
In this paper the Authors first "ummariz" such findings and then propose an
interpretation in terms of the spatial correlation of the fitness function
with respect to the metric defined by the average steps of the genetic
operators. Some numerical evidence of such interpretation is given, showing
that the fitness surface appears smoother to crossover than it does to
mutation. This confirms indirectly that crossover moves along privileged
directions, and at the same time provides a geometric rationale for
hyperplanes.} 
}

@Article{McDonnell92,
  Author =	"John R. McDonnell and Don E. Waagen",
  Title =	"Evolving neural network architecture",
  Journal =	{Proceedings  of  SPIE  -  The  International Society for Optical Engineering},
  VOLUME =	{1766},
  PUBLISHER =	{Society for Optical Engineering},
  ADDRESS =	{Bellingham, WA USA},
  PAGES =	{690-701},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann
evolutionary programming},
  ABSTRACT =	{ABSTRACT
This work investigates the application of a stochastic search
technique, evolutionary programming, for developing self-organizing neural
networks. The chosen stochastic search method is capable of simultaneously
evolving both network architecture and weights. The number of synapses and
neurons are incorporated into an objective function so that network
parameter optimization is done with respect to computational costs as well
as mean pattern error. Experiments are conducted using feedforward networks
for simple binary mapping problems.} 
}

@Book{Machado92,
  Author =	"Ricardo Jose Machado and Armando Freitas da Rocha",
  Title =	"Evolutive fuzzy neural networks",
  Journal =	{1992 IEEE INT CONF Fuzzy Syst FUZZ-IEEE},
  PUBLISHER =	{IEEE},
  ADDRESS =	{Piscataway, NJ},
  PAGES =	{493-500},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors describe "h" combination of fuzzy neural networks
with genetic algorithms, producing a flexible and powerful learning
paradigm, called evolutive learning. Evolutive learning combines as
complementary tools both inductive learning through synaptic weight
adjustment and deductive learning through the modification of the network
topology to obtain the automatic adaptation of system knowledge to the
problem domain environment. Algorithms for the development of an evolutive
learning machine are presented. A fuzzy criterion based on entropy is
proposed to select the architecture for a fuzzy neural network best suited
to a specific problem domain.} 
}

@Mastersthesis{Mayer93,
  Author =	"Erik Mayer",
  Title =	"Genetic Algorithm Approach to Neural Network Optimization",
  Type =	{Masters Thesis},
  PUBLISHER =	{University of Toledo},
  ADDRESS =	{Toledo, Ohio},
  YEAR =	{August 1993},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Munro,
  Author =	"P. Munro",
  Title =	"Genetic Search for Optimal Representations in Neural Networks",
  Pages =	{628-634},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Maniezzo,
  Author =	"V. Maniezzo",
  Title =	"Searching Among Search Spaces: Hastening the Genetic Evolution of
Feedforward Neural Networks",
  Pages =	{635-642},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
adaptive representation, variable length chromosomes} 
}

@Conference{Mandischer,
  Author =	"M. Mandischer",
  Title =	"Representation and Evolution of Neural Networks",
  Pages =	{643-649},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Mitchell,
  Author =	"R.J. Mitchell and J.M. Bishop and W. Low",
  Title =	"Using a Genetic Algorithm to Find the Rules of A Neural Network",
  Pages =	{664-669},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{McDonnell,
  Author =	"John R. McDonnell and Don Waagen",
  Title =	"Determining Neural Network Hidden Layer Size using Evolutionary Programming",
  Organization =	{WCNN93},
  PAGES =	{III564 - III567},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
This work investigates the application of evolutionary programming, a
stochastic search technique, for simultaneously determining the weights and the
number of hidden units in a fully-connected, multi-layer neural network. The
simulated evolution search paradigm provides a means for optimizing both
network structure and weight coefficients. Orthogonal learning is implemented
by independently modifying network structure and weight parameters. Different
structural level search strategies are investigated by comparing the training
processes for the 3-bit parity problem. The results indicate that evolutionary
programming provides a robust framework for evolving neural networks.} 
}

@Conference{McInerney,
  Author =	"Michael McInerney and Atam P. Dhawan",
  Title =	"Use of Genetic Algorithms with Back Propagation in
Training of Feed-Forward Neural Networks",
  Organization =	{preprint from the Author; don't "no" about publication},
  KEY =	{connectionism cogann xor encoder} 
}

@Book{Nygard89,
  Author =	"Kendall E. Nygard and Paul L. Juell and Nagesh Kadaba",
  Title =	"Artificial Intelligence in Routing and Scheduling Applications",
  Publisher =	{Dept. of CS and OR, North Dacota State University},
  YEAR =	{1989},
  KEY =	{genetic algorithms, expert systems, connectionism, neural networks, cogann ref} 
}

@Book{Nygard89,
  Author =	"Kendall E. Nygard and Nagesh Kadaba",
  Title =	"Modular Neural Networks and Distributed Adaptive Search for
Traveling Salesman Algorithms",
  Publisher =	{Dept. of CS and OR, North Dacota State University},
  YEAR =	{1989},
  KEY =	{genetic algorithms, connectionism, neural networks, cogann ref},
  JOURNAL =	{Technical Symposium on Optical Engineering and Photonics in Aerospace Sensing},
  PUBLISHER =	{SPIE} 
}

@Techreport{Nolfi90,
  Author =	"Stefano Nolfi and Jeffrey L. Elman and Domenico Parisi",
  Title =	"Learning and Evolution in Neural Networks",
  Type =	{CRL Technical Report 9019},
  YEAR =	{JUL 1990},
  KEY =	{Genetic Algorithms, Baldwin effect, connectionism, cogann ref} 
}

@Article{Narayanan93,
  Author =	"M.N. Narayanan and S.B. Lucas",
  Title =	"A genetic algorithm to improve a neural network to predict a
patient's response to Warfarin",
  Journal =	{Methods of Information in Medicine},
  VOLUME =	{32},
  NUMBER =	{1},
  PAGES =	{55-8},
  YEAR =	{Feb. 1993},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The ability of neural networks to predict the international
normalised ratio (INR) for patients treated with Warfarin was investigated.
Neural networks were obtained by using all the predictor variables in the
neural network, or by using a genetic algorithm to select an optimal subset
of predictor variables in a neural network. The use of a genetic algorithm
gave a marked and significant improvement in the prediction of the INR in
two of the three cases investigated. The mean error in these cases,
typically, reduced from 1.02+or-0.29 to 0.28+or-0.25 (paired t-test,
t=-4.71, p<0.001, n=30). The use of a genetic algorithm with Warfarin data
offers a significant enhancement of the predictive ability of a neural
network with Warfarin data, identifies significant predictor variables,
reduces the size of the neural network and thus the speed at which the
reduced network can be trained, and reduces the sensitivity of a network to
over-training.} 
}

@Article{Nagao92,
  Author =	"T. Nagao and T. Agui and H. Nagahashi",
  Title =	"Structural evolution of neural networks by a genetic method",
  Journal =	{Transactions of the Institute of Electronics, Information and Communication
Engineers D-II},
  VOLUME =	{J75D-II},
  NUMBER =	{9},
  PAGES =	{1634-1637},
  YEAR =	{SEPT 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
A method of neural networks construction by a genetic algorithm
is proposed. Each network has mutual connections and is assumed to be a
living thing whose genes denote the connections among its units. In order
to find out a network available to the current task, the simulation of
evolution processes of the networks is executed.} 
}

@Article{Oliker92,
  Author =	"S. Oliker and M. Furst and O. Maimon",
  Title =	"A  distributed  genetic  algorithm  for neural network design and training",
  Journal =	{Complex Systems},
  VOLUME =	{6},
  NUMBER =	{5},
  PAGES =	{459-477},
  YEAR =	{Oct. 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
A  new  approach for designing and training neural networks is
developed  using  a distributed genetic algorithm. A search for the optimal
architecture  and  weights  of  a  neural network comprising binary, linear
threshold  units  is  performed. For each individual unit, the Authors look
"o"  the  optimal  set  of  connections  and  associated  weights under the
restriction  of  a feedforward network structure. This is accomplished with
the  modified  genetic  algorithm, using an objective function-fitness-that
considers,  primarily,  the  overall network error; and, secondarily, using
the  unit's  possible  connections  and  weights  that  are  preferable for
continuity  of  the  convergence  process.  Examples  are given showing the
potential of the proposed approach.} 
}

@Article{Neill92,
  Author =	"A.W. O'Neill",
  Title =	"Genetic  based  training  of  two-layer,  optoelectronic  neural network",
  Journal =	{Electronics Letters},
  VOLUME =	{28},
  NUMBER =	{1},
  YEAR =	{2 JAN 1992},
  PAGES =	{47-48},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
For the first time, the supervised training of a high-speed,
two-layer, optoelectronic neural network using a genetic algorithm is
demonstrated, and results for the 3 bit exclusive-or function are
presented.} 
}

@Conference{Porto90,
  Author =	"Vincent W. Porto and David B. Fogel",
  Title =	"Neural Network Techniques for Navigation of AUVs",
  Organization =	{PROC of the IEEE Symposium on Autonomous Underwater Vehicle Technology},
  YEAR =	{JUN 5-6, 1990},
  ADDRESS =	{Washington, DC},
  PAGES =	{137-141},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Article{Prados92,
  Author =	"D.L. Prados",
  Title =	"New Learning Algorithm for Training Multilayered Neural Networks that Uses
Genetic-Algorithm Techniques",
  Pages =	{1560-1561},
  JOURNAL =	{Electronics Letters},
  VOLUME =	{28},
  NUMBER =	{16},
  YEAR =	{30 JUL 1992},
  KEY =	{genetic algorithms connectionism} 
}

@Conference{Potter,
  Author =	"Mitchell A. Potter",
  Title =	"A Genetic Cascade-Correlation Learning Algorithm",
  Organization =	{COGANN92},
  PAGES =	{123-133},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Prados92,
  Author =	"Donald L. Prados",
  Title =	"Training multilayered neural networks by replacing the least fit
hidden neurons",
  Journal =	{Proceedings of IEEE SOUTHEASTCON},
  VOLUME =	{2},
  PUBLISHER =	{IEEE},
  ADDRESS =	{Piscataway, NJ},
  PAGES =	{634-637},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Author discusses " supervised-learning algorithm, called
GenLearn, for training multilayered neural networks. GenLearn uses
techniques from the field of genetic algorithms to perform a global search
of weight space and, thereby, to avoid the common problem of getting stuck
in local minima. GenLearn is based on survival of the fittest hidden
neuron. In searching for the most fit hidden neurons, GenLearn searches for
a globally optimal internal representation of the input data. A big
advantage of the GenLearn procedure over the generalized delta rule (GDR)
in training three-layered neural nets is that, during each iteration of
GenLearn, each weight in the first matrix is modified only once, whereas,
in the GDR procedure, each weight in the first matrix is modified once for
each output-layer neuron. What makes this such a big advantage is that,
although GenLearn often reaches the desired mean square error in about the
same number of iterations as the GDR, each iteration takes considerably
less time.} 
}

@Article{Prados92,
  Author =	"D.L. Prados",
  Title =	"New Learning Algorithm for Training Multilayered Neural Networks that Uses
Genetic-Algorithm Techniques",
  Pages =	{1560-1561},
  JOURNAL =	{Electronics Letters},
  VOLUME =	{28},
  NUMBER =	{16},
  YEAR =	{30 JUL 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Inbook{Paredis,
  Author =	"Jan Paredis",
  Title =	"The Evolution of Behavior: Some Experiments",
  Booktitle =	{SAB90},
  KEY =	{genetic algorithms, connectionism cogann} 
}

@Conference{Philipsen,
  Author =	"W.J.M. Philipsen and L.J.M. Cluitmans",
  Title =	"Using a Genetic Algorithm to Tune Potts Neural Networks",
  Pages =	{650-657},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
Hopfield cooling optimization} 
}

@Techreport{Rudnick90,
  Author =	"Mike Rudnick",
  Title =	"A Bibliography:  The Intersection of Genetic Search and Artificial Neural Networks",
  Year =	{1990},
  TYPE =	{CS/E 90-001},
  PUBLISHER =	{Department of Computer Science and Engineering, Oregon Graduate Institute},
  KEY =	{connectionism, genetic algorithms, cogann ref} 
}

@Techreport{Radcliffe90,
  Author =	"Nicholas J. Radcliffe",
  Title =	"Genetic Neural Networks on MIMD Computers",
  Type =	{Ph.D. DISS},
  PUBLISHER =	{Dept. of Theoretical Physics
University of Edinburgh},
  ADDRESS =	{Edinburgh, Scotland},
  YEAR =	{1990},
  KEY =	{genetic algorithms, connectionism, cogann ref} 
}

@Techreport{Rudnick92,
  Author =	"W. Michael Rudnick",
  Title =	"Genetic Algorithms and Fitness Variance with an Application
to the Automatic Design of Artificial Neural Networks",
  Type =	{Unpublished Ph.D. DISS},
  PUBLISHER =	{Oregon Graduate Institute of Science and Technology},
  ADDRESS =	{Portland, OR},
  YEAR =	{1992},
  KEY =	{connectionism, contiguity problem} 
}

@Book{Renders92,
  Author =	"J.M. Renders and R. Hanus",
  Title =	"Biological learning metaphors for adaptive process control: a
general strategy",
  Journal =	{Proceedings of the 1992 IEEE International Symposium on Intelligent Control},
  PAGES =	{469-474},
  PUBLISHER =	{IEEE},
  ADDRESS =	{New York, NY, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors propose " general strategy for applying biological
adaptive metaphors to nonlinear process control. The metaphors considered
consists of a mixture of neural networks, immune networks, and genetic
algorithms. Issues regarding the fundamental limitations of these metaphors
in process control are raised. An approach aimed at overcoming these
limitations as far as possible is proposed. In particular, it is shown that
the requirement that control be exercised by poorly adapted regimes can be
circumvented, and a certain quality control guaranteed. The approach allows
current controllers, whether conventional or of novel design (e.g., fuzzy
or neural), to be integrated naturally into a coherent control scheme.} 
}

@Article{Rehm92,
  Author =	"W. Rehm and V. Sterzing",
  Title =	"An optimization method for multilayer perceptron based on
evolution-theoretic principles",
  Journal =	{Informationstechnik - IT},
  VOLUME =	{34},
  NUMBER =	{5},
  PAGES =	{307-312},
  YEAR =	{OCT 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Pattern classification tasks can be addressed successfully
using multilayer perceptrons, i.e. a simple form of feed forward neural
nets. The training of a neural net can be regarded to be a parametric
optimization problem, for which several possible algorithms are known.
These differ in efficiency, given a fixed complexity of structure and
classification task, and in the implementation constrains on parallel
hardware. The Authors introduce " new method based on evolution-theoretic
principles using operators from genetic algorithms, that is well suited for
a parallel implementation on MIMD architectures. Finally they provide some
results on parity problems.} 
}

@Book{Reeves92,
  Author =	"C. Reeves and N. Steele",
  EDITOR =	{M.H. Hamza},
  Title =	"Problem-solving by simulated genetic processes: a review and
application to neural networks",
  Journal =	{Proceedings of the Tenth IASTED International
Conference. Applied Informatics},
  PAGES =	{269-272},
  PUBLISHER =	{Acta Press},
  ADDRESS =	{Zurich, Switzerland},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
In the past decade, researchers have become aware of the value
of simulating natural processes in order to solve large and difficult
problems. One example which is attracting increasing attention is the idea
of a genetic algorithm (GA). The first part of this paper provides a review
of the basic concepts underlying genetic algorithms. The methodology is
illustrated by a simple example, and some of the issues involved in more
advanced GAs are discussed. Finally, it describes some of their
applications. The second part describes in some detail research carried out
in applying genetic algorithms to the field of neural networks, in
particular to the multi-layer perceptron (MLP). This work falls into two
main areas. The first of these deals with the question of the design of a
neural network architecture, and the choice of a training regime for a
particular problem. The second area of application is to the basic learning
process itself. Traditionally, the MLP has been trained by a process called
back-propagation. This paper reports on an alternative method based on a
GA, and it is argued that such an approach has many advantages over
back-propagation.} 
}

@Conference{Romaniuk,
  Author =	"Steve G. Romaniuk",
  Title =	"Evolutionary Growth Perceptrons",
  Organization =	{ICGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Robbins,
  Author =	"P. Robbins and A. Soper and K. Rennolls",
  Title =	"Use of Genetic Algorithms for Optimal Topology Determination in Back
Propagation Neural Networks",
  Pages =	{726-730},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann} 
}

@Conference{Rudnick,
  Author =	"Mike Rudnick",
  Title =	"Evolutionary Network Design & the Contiguity Problem",
  Organization =	{WCNN93},
  PAGES =	{IV135 - IV138},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
Given a particular problem to solve using an artificial neural network,
we wish to find a superior network architecture; this is called the network
design problem. One approach is to use evolutionary methods, or evolutionary
network design (END).
.PP
The contiguity problem consists of counting the number of clumps of 1's in
a binary input field. It is a good test problem for END because, for back-
propagation networks, the space of network architectures has been characterized
with respect to network generalization ability. We present experience gained
using END to find superior network architectures for the contiguity problem.} 
}

@Article{Sebald,
  Author =	"A. V. Sebald and David B. Fogel",
  Title =	"Using Evolutionary Neural Networks for Arterial Waveform Discriminiation",
  Journal =	{IJCNN-91},
  VOLUME =	{II},
  PAGES =	{A-955},
  KEY =	{genetic algorithms, connectionism, cogann ref},
  ABSTRACT =	{abstract only} 
}

@Article{Stacey,
  Author =	"Deborah A. Stacey and Stefan Kremer",
  Title =	"The Guelph Darwin Project:
The Evolution of Neural Networks by Genetic Algorithms",
  Journal =	{IJCNN-91},
  VOLUME =	{II},
  PAGES =	{A-957},
  KEY =	{genetic algorithms, connectionism, cogann ref},
  ABSTRACT =	{abstract only} 
}

@Conference{Suzuki,
  Author =	"Keiji Suzuki and Yukinori Kakazu",
  ORGANIZATION =	{ICGA91},
  PAGES =	{539-546},
  Title =	"An Approach to the Analysis of the Basins of the Associative Memory Model Using Genetic Algorithms",
  Abstract =	{Abstract: In this paper, an approach to the analysis of the brain
of a correlational associative memory model using the Genetic
Algorithms and a new training algorithm for this model is described.
The recalling process of a model described by direction cosine is insufficient
for the better understanding of the dynamical behavior of the model.
In order to know the characteristics of memorized states,
the methodology of the Genetic Algorithms applied to analyze the recalling process
concerned with each memorized state is proposed.
Furthermore, before the analyzing, the LU-algorithm is proposed to give
the model the ability of keeping a wide basin in both highly memorized rates
and mutually non-orthogonal states.
Finally, results of experiments related to the basin analysis are shown.},
  KEY =	{genetic algorithms applications pattern classification categorization; relation AI machine learning connectionist networks
basin; associative memory model; analysis, neural, cogann ref} 
}

@Inbook{Schiffmann90,
  Author =	"W. Schiffmann and K. Mecklenburg",
  EDITOR =	{R. Eckmiller and G. Hartmann and G. Hauske},
  Title =	"Genetic Generation of Backpropagation Trained Neural Networks",
  Booktitle =	{Parallel Processing in Neural Systems and Computers},
  PUBLISHER =	{Elsevier Science Publishers},
  PAGES =	{205-208},
  YEAR =	{1990},
  KEY =	{connectionism} 
}

@Conference{Stork,
  Author =	"D.G. Stork and S. Walker and M. Burns and B. Jackson",
  Title =	"Preadaptation in Neural Circuits",
  Organization =	{IJCNN-90},
  PAGES =	{I-202 - I-205},
  KEY =	{genetic algorithms, connectionism} 
}

@Conference{Schaffer,
  Author =	"J. David Schaffer and Darrell Whitley and Larry J. Eshelman",
  Title =	"Combinations of Genetic Algorithms and Neural Networks:
A Survey of the State of the Art",
  Organization =	{COGANN92},
  PAGES =	{1-37},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Shonkwiler,
  Author =	"R. Shonkwiler and Kenyon R. Miller",
  Title =	"Genetic Algorithm/Neural Network Synergy
For Nonlinear Constrained Optimization Problems",
  Organization =	{COGANN92},
  PAGES =	{248-257},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Hsu,
  Author =	"Loke Soo Hsu and Zhi Biao Wu",
  Title =	"Input Pattern Encoding Though Generalized Adaptive Search",
  Organization =	{COGANN92},
  PAGES =	{235-247},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Conference{Schizas,
  Author =	"C.N. Schizas and C.S. Pattichis and L.T. Middleton",
  Title =	"Neural Networks, Genetic Algorithms and K-Means Algorithm:
In Search of Data Classification",
  Organization =	{COGANN92},
  PAGES =	{201-222},
  KEY =	{genetic algorithms, connectionism, neural networks} 
}

@Article{Smith93,
  Author =	"Robert E. Smith",
  Title =	"Genetic learning in rule-based and neural systems",
  Journal =	{Proceedings of the Third International
Workshop on Neural Networks and Fuzzy Logic},
  VOLUME =	{1},
  PAGES =	{183},
  PUBLISHER =	{NASA. Johnson Space Center},
  YEAR =	{JAN 1993},
  KEY =	{cogann genetic algorithms, connectionism, neural networks, classifier systems},
  ABSTRACT =	{ABSTRACT
The design of neural networks and fuzzy systems can involve complex,
nonlinear, and ill-conditioned optimization problems. Often, traditional
optimization schemes are inadequate or inapplicable for such tasks. Genetic
Algorithms (GA's) are a class of optimization procedures whose mechanics
are based on those of natural genetics. Mathematical arguments show how GAs
bring substantial computational leverage to search problems, without
requiring the mathematical characteristics often necessary for traditional
optimization schemes (e.g., modality, continuity, availability of
derivative information, etc.). GA's have proven effective in a variety of
search tasks that arise in neural networks and fuzzy systems. This
presentation begins by introducing the mechanism and theoretical
underpinnings of GA's. GA's are then related to a class of rule-based
machine learning systems called learning classifier systems (LCS's). An LCS
implements a low-level production-system that uses a GA as its primary rule
discovery mechanism. This presentation illustrates how, despite its
rule-based framework, an LCS can be thought of as a competitive neural
network. Neural network simulator code for an LCS is presented. In this
context, the GA is doing more than optimizing and objective function. It is
searching for an ecology of hidden nodes with limited connectivity. The GA
attempts to evolve this ecology such that effective neural network
performance results. The GA is particularly well adapted to this task,
given its naturally-inspired basis. The LCS/neural network analogy extends
itself to other, more traditional neural networks. Conclusions to the
presentation discuss the implications of using GA's in ecological search
problems that arise in neural and fuzzy systems.} 
}

@Techreport{Smieja92,
  Author =	"F.J. Smieja",
  Title =	"Evolution of Intelligent Systems in a Changing Environment",
  Type =	{GMD-623},
  PUBLISHER =	{Gesellschaft fuer Mathematik und Datenverarbeitung m.b.H.},
  ADDRESS =	{Bonn, Germany},
  PAGES =	{24},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
In the report a first version of a simulated robot is described, which
will embody both neural network and genetic algorithm optimization
procedures. The system is modularly structured, with neural networks at the
lower (recognition) level of the simple brain of the robot, and at the
higher level prescribed decision behaviors are followed. It is the higher
level parameters determining the nature of the decisions made that are to
be optimized via genetic algorithms. Having sketched the structure and
method of operation of the prototype robot, a community of robots situation
is introduced as the next stage, for optimization within a robot-inhabited
world.} 
}

@Article{Scherf92,
  Author =	"A.V. Scherf and L.D. Voelz",
  Title =	"Training neural networks with genetic algorithms for target detection",
  Journal =	{Proceedings of the SPIE - The International Society for Optical Engineering},
  VOLUME =	{1710, pt.1},
  PAGES =	{II-734--II-41},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Algorithms for training artificial neural networks, such as
backpropagation, often employ some form of gradient descent in their search
for an optimal weight set. The problem with such algorithms is their
tendency to converge to local minima, or not to converge at all. Genetic
algorithms simulate evolutionary operators in their search for optimality.
The techniques of genetic search are applied to training a neural network
for target detection in infrared imagery. The algorithm design, parameters,
and experimental results are detailed. Testing verifies that genetic
algorithms are a useful and effective approach for neural network training.} 
}

@Book{Spiessens92,
  Author =	"P. Spiessens and J. Torreele",
  EDITOR =	{F.J. Varela and P. Bourgine},
  Title =	"Massively parallel evolution of recurrent networks: an approach to
temporal processing",
  Pages =	{70-77},
  JOURNAL =	{Toward a Practice of Autonomous Systems. Proceedings of the First European
Conference on Artificial Life},
  PAGES =	{310-314},
  PUBLISHER =	{MIT Press},
  ADDRESS =	{Cambridge, MA, USA},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors investigate "" evolutionary approach to the problem
of time-dependent processing with recurrent networks. Both structure and
weights of these networks are evolved by a fine-grained parallel genetic
algorithm. The parallel nature of this algorithm, which enables the
co-evolution of clusters of networks, made it possible to successfully
solve three non-trivial temporal processing problems. One of these problems
consists of evolving a trail-following behaviour for an artificial ant.} 
}

@Article{Sano92,
  Author =	"C. Sano",
  Title =	"Hybrid of (ID3 extension+backpropagation) hybrid & (case-based
reasoner+Grossberg net) hybrid with economics modeling, controlled by
genetic algorithm",
  Journal =	{Proceedings of the SPIE - The International Society for Optical Engineering},
  VOLUME =	{1707},
  PAGES =	{180-194},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The article shows an appropriate architecture of the AI/ANN/GA
hybrid system that does problem solving in the economic world, based on the
necessity or unnecessity of economic forecasting for each task. The AI
components of the hybrid system are explanation-based learning,
theory-driven learning, similarity-based learning, and genetic-based
learning along with a cased-based reasoner. The artificial neural network
components of the hybrid system are a feed-forward net with
backpropagation, BAM and Carpenter-Grossberg net. In order to model and
analyze the economic world, the system makes use of theories of qualitative
physics, Basian causality nets, lattice algebra theory, of qualitative
physics, Basian Causality nets, lattice algebra theory, conceptual
dependency theory, BDS statistics, the Von Neumann economic growth model,
the Von Neumann zero-sum and nonzero sum game theory (as the model of
conflictual competition).} 
}

@Conference{Schwarz,
  Author =	"M. Schwarz and B.J. Hosticka and M. Kesper and P. Richert and M. Scholles",
  Title =	"A  CMOS-array-computer  with  on-chip  communication  hardware
developed for massively parallel applications",
  Organization =	{IJCNN-91-S},
  PAGES =	{89-94},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors present " scalable MIMD computer system which was
designed to be used as a neurocomputer. It is capable of emulating
different types of neurons, including complex biologically motivated models
based on activity pulses, variable pulse transmission times, and multiple
threshold learning rules. It is constructed as an array consisting of nodal
computer chips, each containing an on-chip communication processor to
realize a full global communication. Hence, not only neural networks
featuring arbitrary topologies can be built, but also a wide range of
nonneural processing applications can be implemented. As an example, the
Authors show "o" to use the system in solving optimization problems using
genetic algorithms, and how to program it for real-time image processing
using a combination of neural nets, genetic algorithms, and classical image
processing techniques.} 
}

@Article{Sebald92,
  Author =	"A. V. Sebald and J. Schlenzig and David B. Fogel",
  Title =	"Minimax  design of CMAC encoded neural network controllers using
evolutionary programming",
  Journal =	{Asilomar Conference on Circuits, Systems & Computers},
  VOLUME =	{1},
  PUBLISHER =	{Maple Press, Inc},
  ADDRESS =	{San Jose, CA, USA},
  PAGES =	{551-555},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors describe "h" use of evolutionary programming for
computer-aided design and testing of cerebellar model arithmetic computer
(CMAC) encoded neural network regulators. The design and testing problem is
viewed as a game in that the controller parameters are to be chosen with a
minimax criterion, i.e. to minimize the loss associated with their use on
the worst possible plant parameters. The technique permits analysis of
neural strategies against a set of plants. This gives both the best choice
of control parameters and identification of the plant configuration which
is most difficult for the best controller to handle.} 
}

@Book{Schlager92,
  Author =	"  Kenneth J. Schlager",
  Title =	"On-line  fluorometric  microbiological analysis for life support systems",
  Journal =	{SAE  Technical  Paper  Series},
  PUBLISHER =	{SAE},
  PUBLISHER =	{Warrendale, PA, USA,},
  PAGES =	{1-10},
  YEAR =	{1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Both spectral and time-resolved fluorometry have important
potential applications in on-line microbial monitoring for chemical and
biological life support systems. Spectral fluorometers operating with
remote fiber-optic probes have demonstrated their potential for detecting,
identifying and quantifying bacteria and fungi in recirculated plant
nutrient solutions. This same spectral fluorometry can play a similar role
in detecting and identifying pathogenic bacteria in water supplies.
Time-resolved fluorometers, also operating in the on-line fiber-optic mode,
have exceptional sensitivity and are capable of detecting extremely low
bacteria population densities in both air and water in life support
applications. Fluorometric data, both spectral and time-resolved, are
characterized by background luminescence and interfering, overlapping
spectra from various living and non-living fluorometric sources.
Sophisticated pattern recognition techniques are required to identify and
quantify microbial species of interest in this complex measurement
environment. Advanced pattern recognition methods employing neural networks
supervised by genetic algorithms have been quite successful in identifying
and quantifying bacteria and fungi in continuous on-line monitoring
applications.} 
}

@Conference{Schiffmann,
  Author =	"W. Schiffmann and M. Joost and R. Werner",
  Title =	"Application of Genetic Algorithms to the Construction of Topologies for
Multilayer Perceptrons",
  Pages =	{675-682},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
application classification of thyroid tests} 
}

@Conference{Spears,
  Author =	"W.M. Spears and K.A. De Jong",
  Title =	"Using Neural Networks and Genetic Algorithms as Heuristics for NP-Complete Problems",
  Organization =	{IJCNN90},
  PAGES =	{118 - 121},
  KEY =	{COGANN connectionism} 
}

@Techreport{Sharp91,
  Author =	"David H. Sharp and John Reinitz and Eric Mjolsness",
  Title =	"Genetic Algorithms for Genetic Neural Nets",
  Year =	{JAN 1991},
  TYPE =	{Research Report YALEU/DCS/TR-845},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Torreele,
  Author =	"Jan Torreele",
  ORGANIZATION =	{ICGA91},
  PAGES =	{555-561},
  Title =	"Temporal Processing with Recurrent Networks:
An Evolutionary Approach",
  Key =	{genetic algorithms relation AI machine learning connectionist networks;
neural networks architectures, connectionism, cogann ref},
  ABSTRACT =	{Abstract: In this paper we present an evolutionary approach to the problem of temporal
processing with recurrent networks.  A genetic algorithm is used to evolve
both structure and weights, so as to alleviate the design and learning problem
recurrent networks suffer from.  The viability of this approach is
demonstrated by successfully solving two nontrivial temporal processing
problems.  The important technique of teacher forcing is identified and its
influence on the performance of the algorithm is empirically demonstrated.} 
}

@Inbook{Todd,
  Author =	"Peter M. Todd and Geoffery F. Miller",
  EDITOR =	{S. Wilson and J.-A. Meyer},
  Title =	"Exploring Adaptive Agency II: Simulating the Evolution of Associative Learning",
  Booktitle =	{PROC of the International CONF on Simulation of Adaptive Behavior:
From Animals to Animats},
  PUBLISHER =	{MITP},
  PAGES =	{306-315},
  KEY =	{genetic algorithm, connectionism, Hebbian learning} 
}

@Article{Tamburino92,
  Author =	"Louis A. Tamburino and Mateen M. Rizki",
  Title =	"Performance-driven  autonomous  design  of  pattern-recognition
systems",
  Journal =	{Applied Artificial Intelligence},
  VOLUME =	{6},
  NUMBER =	{1},
  YEAR =	{JAN-MAR 1992},
  PAGES =	{59-77},
  KEY =	{genetic algorithms connectionism neural networks cogann
cooperative application},
  ABSTRACT =	{ABSTRACT
The closed-loop design experiment described in this paper
demonstrates a three-phase automated design approach to pattern
recognition. The experiment generates morphological feature detectors and
then uses a novel application of genetic algorithms to select cooperative
sets of features to pass to a neural net classifier. The self-organizing
hybrid learning approach embodied in this closed-loop design methodology is
complementary to conventional artificial intelligence (AI) expert systems
that utilize rule-based approaches and a specific set of design elements.
This experiment is part of a study directed to emulating the nondirected
processes of biological evolution. The approach we discuss is semiautomatic
in that initialization of computer programs requires human experience and
expertise to select representations, develop search strategies, choose
performance measures, and devise resource-allocation strategies. The hope
is that these tasks will become easier with experience and will provide the
means to exploit parallel processing without the need to analyze or program
an entire design solution.} 
}

@Conference{Thierens,
  Author =	"D. Thierens and J. Suykens and J. Vandewalle and B. De Noor",
  Title =	"Genetic Weight Optimization of a Feedforward Neural Network Controller",
  Pages =	{658-663},
  ORGANIZATION =	{ANNGA93},
  KEY =	{genetic algorithms connectionism neural networks cogann
pole inversion},
  ABSTRACT =	{ABSTRACT
The optimization of the weights of a feedforward neural network
with a genetic algorithm is discussed. The search by the
recombination operator is hampered by the existence of two functionally
equivalent symmetries in feedforward neural networks. To sidestep
these representation redundancies we reorder the hidden neurons
on the genotype before recombination according to a weight sign
matching criterion, and flip the weight signs of a hidden neuron's connections
whenever there are more inhibitory than excitatory incoming and
outgoing links. As an example we optimize a feedforward neural network
that implements a nonlinear optimal control law. The neural
controller has to swing up the inverted pendulum from its
lower equilibrium point to its upper equilibrium point and
stabilize it there. Finding weights of the network represents a
nonlinear optimization problem which is solved by the genetic algorithm.} 
}

@Conference{Tackett,
  Author =	"Walter Alden Tackett",
  Title =	"Genetic Generation of"Dendritic" Trees for Image Classification}",
  ORGANIZATION =	{WCNN93},
  PAGES =	{IV646 - IV649},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
Genetic Programming (GP) is an adaptive method for generating executable
programs from labeled training data. It differs from the conventional methods
of Genetic Algorithms because it manipulates tree structures of arbitrary size
and shape rather than fixed length binary strings. We apply GP to the
development of a processing tree with a dendritic, or neuron-like structure:
measurements from a set of input nodes are weighted and combined through linear
and nonlinear operations to form an output response. Unlike conventional neural
methods, no constraints are placed upon size, shape, or order of processing
withing the network. This network is used to classify feature vectors extracted
from IR imagery into target/nontarget catagories using a database of 2000
training samples. Performance is tested against a separate database of 7000
samples. For purposes of comparison, the same training and test sets are used
to train two other adaptive classifier systems, the binary tree classifier and
the Backpropagation neural network. The GP network acheives higher performance
with reduced computational requirements.} 
}

@Conference{Takagi,
  Author =	"Hideyuki Takagi",
  Title =	"Neural Network and Genetic Algorithm Techniques for Fuzzy Systems",
  Organization =	{WCNN93},
  PAGES =	{II631 - II634},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
This paper introduces (1) how neural networks and genetic algorithms have been
used for auto-designing fuzzy systems, (2) how neural networks are combined
with fuzzy systems in commecial applications, and (3) how fuzzy systems are
used to improve the performance of neural networks and genetic algorithms.} 
}

@Conference{Toth,
  Author =	"Gabor J. Toth and Andras Lorincz",
  Title =	"Genetic Algorithm With Migration on Topology Conserving Maps",
  Organization =	{WCNN93},
  PAGES =	{III168 - III171},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
Optimization problems depending on external variables (parameters) are treated
with the help of a Kohonen network extended by a genetic algorithm (GA). The
optimal solution is assumed to have continuous dependence on the external
variables. The GA was generalized to organize individuals into subpopulations,
which were allocated in the space of the external variables in an optimal
fashion by Kohnonen digitization. Individuals were allowed to breed within
their own subpopulations and in neighboring ones (migration). To illustrate
the strength of the modified GA the optimal control of a simulated robot-arm
is treated: a falling ping-pong ball has to be caught by a bat without
bouncing. It is shown that the simultaneous optimization problem (for different
values of the external parameter) can be solved successfully, and the migration
can considerably reduce computation time.} 
}

@Conference{Uhrig92,
  Author =	"R. E. Uhrig",
  Title =	"Use of neural networks in the analysis of complex systems",
  Organization =	{WNN-1992:  Workshop  on  Neural Networks},
  ADDRESS =	{Alburn, AL},
  PAGES =	{10-12},
  YEAR =	{FEB. 1992},
  KEY =	{cogann genetic algorithms connectionism neural networks},
  ABSTRACT =	{ABSTRACT
The  application  of  neural networks, alone or in conjunction with other
advanced   technologies   (expert  systems,  fuzzy  logic,  and/or  genetic
algorithms)  to some of the problems of complex engineering systems has the
potential  to  enhance  the  safety  reliability  and  operability of these
systems. Complex systems or parts of such systems that can be isolated from
the  total system are addressed. Typically, the measured variables from the
systems  are  analog  variables  that  must  be  sampled  and normalized to
expected peak values before they are introduced into neural networks. Often
data  must be processed to put it into a form more acceptable to the neural
network.  The  neural  networks  are usually simulated on modern high-speed
computers that carry out the calculations serially. However, it is possible
to  implement neural networks using specially designed microchips where the
network  calculations  are truly carried out in parallel, thereby providing
virtually   instantaneous   outputs   for  each  set  of  inputs.  Specific
applications  described  include:  diagnostics:  state of the plant; hybrid
system for transient identification; detection of change of mode in complex
systems;   sensor   validation;   plant-wide   monitoring;   monitoring  of
performance  and  efficiency;  and  analysis  of  vibrations.  Although the
specific  examples  described  deal  with  nuclear  power  plants  or their
subsystems,  the  techniques  described can be applied to a wide variety of
complex engineering systems.} 
}

@Conference{Uthmann,
  Author =	"T. Uthmann and D. Polani",
  Title =	"Training Kohonen Feature Maps in Different Topologies:
An analysis using Genetic Algorithms",
  Organization =	{ICGA93},
  KEY =	{connectionism cogann} 
}

@Article{Vico92,
  Author =	"F.J. Vico and F. Sandoval",
  Title =	"Neural networks definition algorithm",
  Journal =	{Microprocessing & Microprogramming},
  VOLUME =	{34},
  NUMBER =	{1-5},
  PAGES =	{251-254},
  YEAR =	{FEB 1992},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
There is not a general methodology for neural network
definition. The Authors propose "" algorithm highly inspired on biological
concepts for generating neural networks oriented to solve particular
problems given on terms of input and output. With this algorithm they
intend to specify formal tools of general use for network definition, and
to disclose underlying processing structures of the living organisms. The
concepts of genetic code, embryogenesis and evolution are the main keys in
the development of the algorithm they propose.} 
}

@Conference{Whitley,
  Author =	"Darrell Whitley",
  Title =	"Applying Genetic Algorithms to Neural Network Problems:
A Preliminary Report",
  Organization =	{PROC INNS88},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Whitley,
  Author =	"Darrell Whitley and Thomas Hanson",
  Title =	"Optimizing Neural Networks Using Faster, More Accurate Genetic Search",
  Key =	{GENITOR, neural networks, connectionism, ranking, one-at-a-time reproduction (also see steady state), optimization},
  ORGANIZATION =	{ICGA89},
  PAGES =	{391-396} 
}

@Techreport{Gerhard90,
  Author =	"Gerhard Weiss",
  Title =	"Combining Neural and Evolutionary Learning:
Aspects and Approaches",
  Type =	{Report FKI-132-90},
  YEAR =	{May 1990},
  PUBLISHER =	{Technische Universitat Munchen},
  KEY =	{connectionism, genetic algorithms, cogann ref} 
}

@Inbook{Wilson90,
  Author =	"Stewart W. Wilson",
  EDITOR =	{Stephanie Forrest},
  Title =	"Perceptron Redux: Emergence of Structure",
  Booktitle =	{Emergent Computation},
  PUBLISHER =	{North Holland},
  YEAR =	{1990},
  PAGES =	{249-256},
  ADDRESS =	{Amsterdam},
  KEY =	{genetic algorithm, connectionism, cogann ref} 
}

@Article{Whitley90,
  Author =	"Darrell Whitley and Timothy Starkweather and Christopher Bogart",
  Title =	"Genetic Algorithms and Neural Networks:
Optimizing Connections and Connectivity",
  Journal =	{Parallel Computing},
  VOLUME =	{14},
  YEAR =	{1990},
  PAGES =	{347-361},
  KEY =	{connectionism, cogann ref} 
}

@Book{Whitley89c,
  Author =	"Darrell Whitley",
  Title =	"Applying Genetic Algorithms to Neural Network Learning",
  Journal =	{PROC Seventh CONF of the Society of Artificial Intelligence
and Simulation of Behavior},
  PUBLISHER =	{Pitman Publishing},
  ADDRESS =	{Sussex, England},
  PAGES =	{137-144},
  YEAR =	{APR 1989},
  KEY =	{connectionism, cogann ref} 
}

@Conference{Whitley89b,
  Author =	"Darrell Whitley",
  Title =	"Genetic Algorithm Applications: Neural Nets,
Traveling Salesmen and Schedules",
  Organization =	{1989 Rocky Mountain CONF on Artificial Intelligence},
  ADDRESS =	{Denver, CO},
  YEAR =	{1989},
  KEY =	{connectionism, cogann ref} 
}

@Book{Whitley89,
  Author =	"Darrell Whitley",
  Title =	"Optimizing Neural Networks Using Genetic Algorithms",
  Journal =	{Special Neurocomputing Issue of Design and Electronik},
  PUBLISHER =	{Markt and Technik},
  ADDRESS =	{Munich, Germany},
  YEAR =	{1989},
  KEY =	{cogann ref, connectionism} 
}

@Article{Wieland,
  Author =	"Alexis P. Wieland",
  Title =	"Evolving Neural Network Controllers for Unstable Systems",
  Journal =	{IJCNN-91},
  VOLUME =	{II},
  PAGES =	{667-673},
  KEY =	{genetic algorithms, connectionism, pole balancing problems, cogann ref} 
}

@Conference{Whitley,
  Author =	"Darrell Whitley and Stephen Dominic and Rajarshi Das",
  ORGANIZATION =	{ICGA91},
  PAGES =	{562-5769},
  Title =	"Genetic Reinforcement Learning with Multilayer Neural Networks",
  Key =	{genetic algorithms relation AI machine learning connectionist networks;
reinforcement learning, connectionism, training, real coding,
pole balancing, cogann ref},
  ABSTRACT =	{Abstract: Empirical tests indicate that the genetic algorithms which have produced good
performance for neural network weight optimization are really genetic
hill-climbers, with a strong reliance on mutation rather than hyperplane
sampling.  Initial results are presented using genetic hill-climbers for
reinforcement learning with multilayer neural networks for the control of a
simulated cart-centering and pole-balancing dynamical system.  "Genetic
reinforcement learning" produces competitive results with AHC, a well-known
reinforcement learning paradigm for neural networks that employs temporal
difference methods.} 
}

@Conference{Whitley92,
  Author =	"Darrell Whitley and S. Dominic and R. Das and C. Anderson",
  Title =	"Genetic Reinforcement Learning for Neurocontrol Problems",
  Organization =	{Machine Learning},
  YEAR =	{1992},
  KEY =	{genetic algorithms, connectionism, hill-climbing, mutation only, cogann ref},
  ABSTRACT =	{Abstract
Empirical tests indicate that the class of genetic algorithms which have been
shown to yield good performance for neural network weight optimization are
really genetic hill-climbers, with a strong reliance on mutation rather than
hyperplane sampling. These results are consistent with the theoretical results
of Goldberg (1991) analyzing real-coded genetic algorithms. We argue that
neural network learning applications such as neurocontrol problems are perhaps
more appropriate for these genetic hill-climbers than supervised learning
applications because in reinforcement learning applications gradient
information is not directly available. On an inverted pendulum control problem
reinforcement learning produces competitive results with AHC, another
well-known reinforcement learning paradigm for neural networks that employs
temporal difference methods. The genetic hill-climbing algorithm appears to be
robust over a wide range of learning conditions. We also discuss several
approaches for evaluating neural network performance.} 
}

@Inbook{Werner,
  Author =	"G.M. Werner and M.G. Dyer",
  Title =	"Evolution of Communication in Artificial Organisms",
  Booktitle =	{AlifeII},
  PAGES =	{659-687},
  KEY =	{genetic algorithm connectionism neural networks} 
}

@Article{Whitaker93,
  Author =	"Kevin W. Whitaker and Ravi K. Prasanth and Robert E. Markin",
  Title =	"Specifying exhaust nozzle contours with a neural network",
  Journal =	{AIAA Journal},
  VOLUME =	{31},
  NUMBER =	{2},
  YEAR =	{Feb 1993},
  PAGES =	{273-277},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Thrust vectoring is continuing to become an important issue in
future military aircraft system designs. A recently developed concept of
vectoring aircraft thrust makes use of flexible exhaust nozzles. Subtle
modifications in the nozzle wall contours produce a nonuniform flowfield
containing a complex pattern of shock and expansion waves. The end result,
due to the asymmetric velocity and pressure distributions, is vectored
thrust. Specification of the nozzle contours required for a desired thrust
vector angle (an inverse design problem) has been achieved with genetic
algorithms. However, this approach is computationally intensive, preventing
nozzles from being designed on demand, which is necessary for an
operational aircraft system. An investigation was conducted into using
genetic algorithms to train a neural network in an attempt to obtain, in
real time, two-dimensional nozzle contours. Results show that
genetic-algorithm-trained neural networks provide a viable, time-efficient
alternative for designing thrust vectoring nozzle contours. Thrust vector
angles up to 20 deg were obtained within an average error of 0.0914 deg.
The error surfaces encountered were highly degenerate and thus the
robustness of genetic algorithms was well suited for minimizing global
errors.} 
}

@Conference{Wilke,
  Author =	"Peter Wilke",
  Title =	"Simulation of Neural Network and Genetic Algorithms in a Distributed
Computing Environment Using NeuroGraph",
  Organization =	{WCNN93},
  PAGES =	{I269 - I272},
  KEY =	{connectionism, genetic algorithms, cogann},
  ABSTRACT =	{ABSTRACT
NeuroGraph is a simulation environment for design, construction and execution
of neural networks and genetic algorithms in a distributed computing
environment. The simulator parts either run on single computers or as
distributed applications on Unix/X-based networks, consisting of personal
computers, workstations, or multi-processors. The parallelization component
offers the possibility to divide computational tasks into concurrently
executable modules, according to restrictions due to the neural net topology
and computer net capabilities, ie. NeuroGraph tries to select the best
configuration out of the available distributed hardware environment to fit
performance requirements.} 
}

@Book{Yao92,
  Author =	"Xin Yao",
  YEAR =	{1992},
  Title =	"A Review of Evolutionary Artificial Neural Networks",
  Publisher =	{Commonwealth Scientific and Industrial Research Organization.},
  ADDRESS =	{Victoria, Australia},
  KEY =	{genetic algorithms, connectionism} 
}

@Article{Yao93,
  Author =	"Xin Yao",
  Title =	" A review of evolutionary artificial neural networks",
  Journal =	{International Journal of Intelligent Systems},
  VOLUME =	{8},
  NUMBER =	{4},
  PAGES =	{539-67},
  YEAR =	{April 1993},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
Research  on  potential  interactions  between  connectionist
learning systems, i.e., artificial neural networks (ANNs), and evolutionary
search  procedures,  like  genetic algorithms (GAs), has attracted a lot of
attention.  Evolutionary  ANNs (EANNs) can be considered as the combination
of   ANNs   and   evolutionary   search   procedures.  This  article  first
distinguishes  among three kinds of evolution in EANNs, i.e., the evolution
of  connection  weights,  of  architectures, and of learning rules. Then it
reviews  each  kind  of  evolution  in  detail and analyzes critical issues
related  to  different  evolutions. The review shows that although a lot of
work   has   been   done   on  the  evolution  of  connection  weights  and
architectures,  few  attempts have been made to understand the evolution of
learning   rules.   Interactions  among  different  evolutions  are  seldom
mentioned in current research. However, the evolution of learning rules and
its interactions with other kinds of evolution, play a vital role in EANNs.
Finally,  this  article  briefly  describes  a general framework for EANNs,
which  not  only  includes the aforementioned three kinds of evolution, but
also considers interactions among them.} 
}

@Conference{Zhang,
  Author =	"Byoung-Tak Zhang and Gerd Veenker",
  Title =	"Neural networks that teach themselves through genetic discovery of
novel examples",
  Organization =	{IJCNN-91-S},
  PAGES =	{690-695},
  KEY =	{genetic algorithms connectionism neural networks cogann},
  ABSTRACT =	{ABSTRACT
The Authors introduce "" active learning paradigm for neural
networks. In contrast to the passive paradigm, the learning in the active
paradigm is initiated by the machine learner instead of its environment or
teacher. The Authors present " learning algorithm that uses a genetic
algorithm for creating novel examples to teach multilayer feedforward
networks. The creative learning networks, based on their own knowledge,
discover new examples, criticize and select useful ones, train themselves,
and thereby extend their existing knowledge. Experiments on function
extrapolation show that the self-teaching neural networks not only reduce
the teaching efforts of the human, but the genetically created examples
also contribute robustly to the improvement of generalization performance
and the interpretation of the connectionist knowledge.} 
}

@Conference{Zhang,
  Author =	"Byoung-Tak Zhang and H. Muhlenbein",
  Title =	"Genetic Programming of Minimal Neural Nets Using Occam's Razor",
  Organization =	{ICGA93},
  KEY =	{connectionism, cogann} 
}

@Conference{Byoung-Tak,
  Author =	"Byoung-Tak Zhang" 
}

