%%% ----------------------------------------------------------------------
%%% BibTeX-file {
%%%    author	 = "{Electronic Visualization Library Service}",
%%%    filename  = "EVL-2001.bib",
%%%    address   = "Konrad-Zuse-Zentrum f{\"u}r
%%%                 Informationstechnik Berlin (ZIB)
%%%                 Scientific Visualization Department
%%%                 Takustr. 7
%%%                 14195 Berlin
%%%                 Germany",
%%%    URL       = "http://visinfo.zib.de/EVlib/",
%%%    email     = "davis@zib.de",
%%%    supported = "yes",
%%%    docstring = "This file contains the complete bibliography of
%%%                 references submitted to the 
%%%                 Electronic Visualization Library for the year 2001.",
%%% }


@InProceedings{EVL-2001-1,
  year =         "2001",
  title =        "An Application Model for Visualization of Natural
                 Resources Management",
  author =       "Elizabeth Simao Carvalho and Jose Carlos Teixeira",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-1",
  abstract =     "Natural resources data is usually geo-referenced. It
                 contains mostly 1-n dimension scalar or vector values,
                 and can be associated with meteorology, geology, water,
                 weather, etc. Scientists apply natural resources data
                 in different models in order to predict or model the
                 real world. Computer applications for this area have to
                 be based on a powerful, adaptable and flexible
                 architecture, besides using GIS and Scientific
                 Visualization technologies for development. Other
                 important feature is the correlation that has to be
                 established between the data model and the
                 visualization model. This paper introduces a conceptual
                 model of a computer application to natural resources
                 management. It uses visualization as the main key for i
                 information deliverance.",
  editor =       "V. Skala",
  keywords =     "Scientific visualization, GIS, system analysis.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-10,
  year =         "2001",
  title =        "An Alternative Approach for Pattern Detection Applied
                 to Materials Characterization",
  author =       "Raul Queiroz Feitosa and Guilherme Mota1 and Sidnei
                 Paciornik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-10",
  abstract =     "The problem of detecting specific patterns in images
                 of materials obtained through High Resolution
                 Transmission Electron Microscopy is addressed. A
                 supervised classification method is proposed using an
                 extension of Principal Component Analysis and a new a
                 procedure for building the training set. Experiments on
                 two different types of images indicate that the
                 proposed method is superior to the conventional
                 cross-correlation approach. Moreover, using the same
                 number of components, the new dimensionality reduction
                 approach shows a better performance than the standard
                 PCA method.",
  editor =       "V. Skala",
  keywords =     "Pattern recognition, dimensionality reduction,
                 materials characterization.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-100,
  pages =        "129--140",
  year =         "2001",
  title =        "Wire{GL}: {A} Scalable Graphics System for Clusters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-100",
  author =       "Greg Humphreys and Matthew Eldridge and Ian Buck and
                 Gordon Stoll and Matthew Everett and Pat Hanrahan",
  abstract =     "We describe WireGL, a system for scalable interactive
                 rendering on a cluster of workstations. WireGL provides
                 the familiar OpenGL API to each node in a cluster,
                 virtualizing multiple graphics accelerators into a
                 sort-first parallel renderer with a parallel interface.
                 We also describe techniques for reassembling an output
                 image from a set of tiles distributed over a cluster.
                 Using flexible display management, WireGL can drive a
                 variety of output devices, from standalone displays to
                 tiled display walls. By combining the power of virtual
                 graphics, the familiarity and ordered semantics of
                 OpenGL, and the scalability of clusters, we are able to
                 create time-varying visualizations that sustain
                 rendering performance over 70,000,000 triangles per
                 second at interactive refresh rates using 16 compute
                 nodes and 16 rendering nodes.",
  editor =       "Eugene Fiume",
  keywords =     "Scalable Rendering, Cluster Rendering, Parallel
                 Rendering, Tiled Displays, Remote Graphics, Virtual
                 Graphics",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-101,
  pages =        "141--148",
  year =         "2001",
  title =        "Lightning-2: {A} High-Performance Display Subsystem
                 for {PC} Clusters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-101",
  author =       "Gordon Stoll and Matthew Eldridge and Dan Patterson
                 and Art Webb and Steven Berman and Richard Levy and
                 Chris Caywood and Milton Taveira and Stephen Hunt and
                 Pat Hanrahan",
  abstract =     "Clusters of PCs are increasingly popular as
                 cost-effective platforms for supercomputer-class
                 applications. Given recent performance improvements in
                 graphics accelerators, clusters are similarly
                 attractive for demanding graphics applications. We
                 describe the design and implementation of Lightning-2,
                 a display subsystem for such a cluster. The system
                 scales in both the number of rendering nodes and the
                 number of displays supported, and allows any pixel data
                 generated from any node to be dynamically mapped to any
                 location on any display. A number of image-compositing
                 functions are supported, including color-keying and
                 depth-compositing. A distinguishing feature of the
                 system is its platform independence: it connects to
                 graphics accelerators via an industry-standard digital
                 video port and requires no modifications to accelerator
                 hardware or device drivers. As a result, rendering
                 clusters that utilize Lightning-2 can be upgraded
                 across multiple generations of graphics accelerators
                 with little effort. We demonstrate a renderer that
                 achieves 106 Mtri/s on an 8-node cluster using
                 Lightning-2 to perform sort-last depth compositing.",
  editor =       "Eugene Fiume",
  keywords =     "Graphics Hardware, Graphics Systems, Parallel
                 Computing, Rendering Hardware, Rendering Systems",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-102,
  pages =        "149--158",
  year =         "2001",
  title =        "A User-Programmable Vertex Engine",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-102",
  author =       "Erik Lindholm and Mark J. Kilgard and Henry Moreton",
  abstract =     "In this paper we describe the design, programming
                 interface, and implementation of a very efficient
                 user-programmable vertex engine. The vertex engine of
                 NVIDIA's GeForce3 GPU evolved from a highly tuned
                 fixed-function pipeline requiring considerable
                 knowledge to program. Programs operate only on a stream
                 of independent vertices traversing the pipe. Embedded
                 in the broader fixed function pipeline, our approach
                 preserves parallelism sacrificed by previous
                 approaches. The programmer is presented with a
                 straightforward programming model, which is supported
                 by transparent multi-threading and bypassing to
                 preserve parallelism and performance. In the remainder
                 of the paper we discuss the motivation behind our
                 design and contrast it with previous work. We present
                 the programming model, the instruction set selection
                 process, and details of the hardware implementation.
                 Finally, we discuss important API design issues
                 encountered when creating an interface to such a
                 device. We close with thoughts about the future of
                 programmable graphics devices.",
  editor =       "Eugene Fiume",
  keywords =     "Graphics Hardware, Graphics Systems",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-103,
  pages =        "159--170",
  year =         "2001",
  title =        "A Real-Time Procedural Shading System for Programmable
                 Graphics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-103",
  author =       "Kekoa Proudfoot and William R. Mark and Svetoslav
                 Tzvetkov and Pat Hanrahan",
  abstract =     "Real-time graphics hardware is becoming programmable,
                 but this programmable hardware is complex and difficult
                 to use given current APIs. Higher-level abstractions
                 would both increase programmer productivity and make
                 programs more portable. However, it is challenging to
                 raise the abstraction level while still providing high
                 performance. We have developed a real-time procedural
                 shading language system designed to achieve this goal
                 Our system is organized around multiple computation
                 frequencies. For example, computations may be
                 associated with vertices or with fragments/pixels. Our
                 system's shading language provides a unified interface
                 that allows a single procedure to include operations
                 from more than one computation frequency. Internally,
                 our system virtualizes limited hardware resources to
                 allow for arbitrarily-complex computations. We map
                 operations to graphics hardware if possible, or to the
                 host CPU as a last resort. This mapping is performed by
                 compiler back-end modules associated with each
                 computation frequency. Our system can map vertex
                 operations to either programmable vertex hardware or to
                 the host CPU, and can map fragment operations to either
                 programmable fragment hardware or to multipass OpenGL.
                 By carefully designing all the components of the
                 system, we are able to generate highly-optimized code.
                 We demonstrate our system running in real-time on a
                 variety of hardware.",
  editor =       "Eugene Fiume",
  keywords =     "Graphics hardware, graphics systems, shading
                 languages, rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-104,
  pages =        "179--184",
  year =         "2001",
  title =        "Consistent Mesh Parameterizations",
  author =       "Emil Praun and Wim Sweldens and Peter Schr{\"{o}}der",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-104",
  abstract =     "A basic element of Digital Geometry Processing
                 algorithms is the establishment of a smooth
                 parameterization for a given model. In this paper we
                 propose an algorithm which establishes
                 parameterizations for a set of models. The
                 parameterizations are called consistent because they
                 share the same base domain and respect features. They
                 give immediate correspondences between models and allow
                 remeshes with the same connectivity. Such remeshes form
                 the basis for a large class of algorithms, including
                 principal component analysis, wavelet transforms,
                 detail and texture transfer between models, and n-way
                 shape blending. We demonstrate the versatility of our
                 algorithm with a number of examples.",
  editor =       "Eugene Fiume",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-105,
  pages =        "185--194",
  year =         "2001",
  title =        "Approximate Boolean Operations on Free-Form Solids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-105",
  author =       "Daniel Kristjansson and Henning Biermann and Denis
                 Zorin",
  abstract =     "In this paper we describe a method for computing
                 approximate results of boolean operations (union,
                 intersection, difference) applied to free-form solids
                 bounded by multiresolution subdivision surfaces. We
                 present algorithms for generating a control mesh for a
                 multiresolution surface approximating the result,
                 optimizing the parameterization of the new surface with
                 respect to the original surfaces, and fitting the new
                 surface to the geometry of the original surfaces. Our
                 algorithms aim to minimize the size and optimize the
                 quality of the new control mesh. The original control
                 meshes are modified only in a neighborhood of the
                 intersection. While the main goal is to obtain
                 approximate results, high-accuracy approximations are
                 also possible at additional computational expense, if
                 the topology of the intersection curve is resolved
                 correctly.",
  editor =       "Eugene Fiume",
  keywords =     "Subdivision surfaces, Multiresolution surfaces,
                 Geometric modeling",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-106,
  pages =        "195--202",
  year =         "2001",
  title =        "Progressive Compression for Lossless Transmission of
                 Triangle Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-106",
  author =       "Pierre Alliez and Mathieu Desbrun",
  abstract =     "Lossless transmission of 3D meshes is a very
                 challenging and timely problem for many applications,
                 ranging from collaborative design to engineering.
                 Additionally, frequent delays in transmissions call for
                 progressive transmission in order for the end user to
                 receive useful successive refinements of the final
                 mesh. In this paper, we present a novel, fully
                 progressive encoding approach for lossless transmission
                 of triangle meshes with a very fine granularity. A new
                 valence-driven decimating conquest, combined with patch
                 tiling and an original strategic retriangulation is
                 used to maintain the regularity of valence. We
                 demonstrate that this technique leads to good mesh
                 quality, near-optimal connectivity encoding, and
                 therefore a good rate-distortion ratio throughout the
                 transmission. We also improve upon previous lossless
                 geometry encoding by decorrelating the normal and
                 tangential components of the surface. For typical
                 meshes, our method compresses connectivity down to less
                 than 3.7 bits per vertex, 40% better in average than
                 the best methods previously reported [5, 18]; we
                 further reduce the usual geometry bit rates by 20% in
                 average by exploiting the smoothness of meshes.
                 Concretely, our technique can reduce an ascii VRML 3D
                 model down to 1.7% of its size for a 10-bit
                 quantization (2.3% for a 12-bit quantization) while
                 providing a very progressive reconstruction.",
  editor =       "Eugene Fiume",
  keywords =     "Triangle Mesh Compression, Progressive Transmission,
                 Connectivity Encoding, Geometry Encoding, Levels of
                 Details, Mesh Decimation",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-107,
  pages =        "185--194",
  year =         "2001",
  title =        "Homomorphic Factorization of {BRDF}s for
                 High-Performance Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-107",
  author =       "Michael D. McCool and Jason Ang and Anis Ahmad",
  abstract =     "A bidirectional reflectance distribution function
                 (BRDF) describes how a material reflects light from its
                 surface. To use arbitrary BRDFs in real-time rendering,
                 a compression technique must be used to represent BRDFs
                 using the available texture-mapping and computational
                 capabilities of an accelerated graphics pipeline. We
                 present a numerical technique, homomorphic
                 factorization, that can decompose arbitrary BRDFs into
                 products of two or more factors of lower
                 dimensionality, each factor dependent on a different
                 interpolated geometric parameter. Compared to an
                 earlier factorization technique based on the singular
                 value decomposition, this new technique generates a
                 factorization with only positive factors (which makes
                 it more suitable for current graphics hardware
                 accelerators), provides control over the smoothness of
                 the result, minimizes relative rather than absolute
                 error, and can deal with scattered, sparse data without
                 a separate resampling and interpolation algorithm.",
  editor =       "Eugene Fiume",
  keywords =     "Hardware accelerated rendering and shading",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-108,
  pages =        "203--212",
  year =         "2001",
  title =        "Topology Matching for Fully Automatic Similarity
                 Estimation of 3{D} Shapes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-108",
  author =       "Masaki Hilaga and Yoshihisa Shinagawa and Taku Kohmura
                 and Tosiyasu L. Kunii",
  abstract =     "There is a growing need to be able to accurately and
                 efficiently search visual data sets, and in particular,
                 3D shape data sets. This paper proposes a novel
                 technique, called Topology Matching, in which
                 similarity between polyhedral models is quickly,
                 accurately, and automatically calculated by comparing
                 Multiresolutional Reeb Graphs (MRGs). The MRG thus
                 operates well as a search key for 3D shape data sets.
                 In particular, the MRG represents the skeletal and
                 topological structure of a 3D shape at various levels
                 of resolution. The MRG is constructed using a
                 continuous function on the 3D shape, which may
                 preferably be a function of geodesic distance because
                 this function is invariant to translation and rotation
                 and is also robust against changes in connectivities
                 caused by a mesh simplification or subdivision. The
                 similarity calculation between 3D shapes is processed
                 using a coarse-to-fine strategy while preserving the
                 consistency of the graph structures, which results in
                 establishing a correspondence between the parts of
                 objects. The similarity calculation is fast and
                 efficient because it is not necessary to determine the
                 particular pose of a 3D shape, such as a rotation, in
                 advance. Topology Matching is particularly useful for
                 interactively searching for a 3D object because the
                 results of the search fit human intuition well.",
  editor =       "Eugene Fiume",
  keywords =     "Computer Vision, Shape Recognition, 3D Search",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-109,
  pages =        "213--220",
  year =         "2001",
  title =        "Measuring and Predicting Visual Fidelity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-109",
  author =       "Benjamin Watson and Alinda Friedman and Aaron
                 McGaffey",
  abstract =     "This paper is a study of techniques for measuring and
                 predicting visual fidelity. As visual stimuli we use
                 polygonal models, and vary their fidelity with two
                 different model simplification algorithms. We also
                 group the stimuli into two object types: animals and
                 man made artifacts. We examine three different
                 experimental techniques for measuring these fidelity
                 changes: naming times, ratings, and preferences. All
                 the measures were sensitive to the type of
                 simplification and level of simplification. However,
                 the measures differed from one another in their
                 response to object type. We also examine several
                 automatic techniques for predicting these experimental
                 measures, including techniques based on images and on
                 the models themselves. Automatic measures of fidelity
                 were successful at predicting experimental ratings,
                 less successful at predicting preferences, and largely
                 failures at predicting naming times. We conclude with
                 suggestions for use and improvement of the experimental
                 and automatic measures of visual fidelity.",
  editor =       "Eugene Fiume",
  keywords =     "Visual fidelity, model simplification, image quality,
                 naming time, human vision, perception",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-11,
  year =         "2001",
  title =        "A Multichannel Variational Model for Robust Image
                 Segmentation under Noise",
  author =       "R. Romano and D. Vitulano",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-11",
  abstract =     "This paper presents a novel model for image
                 segmentation under noise: EVRIST (Efficient Variational
                 Representation for Image Segmentation Technique). In
                 order to segment general images containing both quite
                 smooth regions and textures, EVRIST, based on a
                 hierarchical representation obtained by the classical
                 weak membrane, utilizes two channels: one relative to
                 the mean and another for the edges density. The results
                 achieved on both manual compositions and on real
                 images, show the high robustness to the noise along
                 with a low computational effort.",
  editor =       "V. Skala",
  keywords =     "Image Segmentation, Variational Models, Textures",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-110,
  pages =        "221--230",
  year =         "2001",
  title =        "Perception-Guided Global Illumination Solution for
                 Animation Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-110",
  author =       "Karol Myszkowski and Takehiro Tawara and Hiroyuki
                 Akamine and Hans-Peter Seidel",
  abstract =     "We present a method for efficient global illumination
                 computation in dynamic environments by taking advantage
                 of temporal coherence of lighting distribution. The
                 method is embedded in the framework of stochastic
                 photon tracing and density estimation techniques. A
                 locally operating energy-based error metric is used to
                 prevent photon processing in the temporal domain for
                 the scene regions in which lighting distribution
                 changes rapidly. A perception-based error metric
                 suitable for animation is used to keep noise inherent
                 in stochastic methods below the sensitivity level of
                 the human observer. As a result a
                 perceptually-consistent quality across all animation
                 frames is obtained. Furthermore, the computation cost
                 is reduced compared to the traditional approaches
                 operating solely in the spatial domain.",
  editor =       "Eugene Fiume",
  keywords =     "Animation, Human Factors, Illumination, Monte Carlo
                 Techniques, Temporal Aliasing",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-111,
  pages =        "231--240",
  year =         "2001",
  title =        "Interactive Stereoscopic Display for Three or More
                 Users",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-111",
  author =       "Yoshifumi Kitamura and Takashige Konishi and Sumihiko
                 Yamamoto and Fumio Kishino",
  abstract =     "An ideal stereoscopic display system for multiple
                 users is proposed. It allows three or more people to
                 simultaneously observe individual stereoscopic image
                 pairs from their own viewpoints. The system tracks the
                 head positions of all of the users and generates
                 distortion-free images for each eye of each user. The
                 system consists of a normal display and a display mask,
                 which has a hole in its center. The display mask is
                 placed over the display surface at a suitable distance
                 from it. By controlling the position of the image
                 drawing area for each user according to the
                 corresponding user's viewpoint, each user can observe
                 the stereoscopic image pairs shown in an individual
                 area of the display system with shutter glasses. On the
                 other hand, no user is able to see the image drawing
                 areas of the other users because these areas are
                 adequately occluded by the display mask. Accordingly,
                 the display system can simultaneously provide
                 intelligible 3D stereoscopic images for three or more
                 moving observers without flicker or distortion.",
  editor =       "Eugene Fiume",
  keywords =     "Interactive, 3D display, multiple users, motion
                 parallax, without flicker, without distortion,
                 visualization, collaborative work",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-112,
  pages =        "241--250",
  year =         "2001",
  title =        "Rendering Effective Route Maps: Improving Usability
                 Through Generalization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-112",
  author =       "Maneesh Agrawala and Chris Stolte",
  abstract =     "Route maps, which depict a path from one location to
                 another, have emerged as one of the most popular
                 applications on the Web. Current computer-generated
                 route maps, however, are often very difficult to use.
                 In this paper we present a set of cartographic
                 generalization techniques specifically designed to
                 improve the usability of route maps. Our generalization
                 techniques are based both on cognitive psychology
                 research studying how route maps are used and on an
                 analysis of the generalizations commonly found in
                 handdrawn route maps. We describe algorithmic
                 implementations of these generalization techniques
                 within LineDrive, a real-time system for automatically
                 designing and rendering route maps. Feedback from over
                 2200 users indicates that almost all believe LineDrive
                 maps are preferable to using standard
                 computer-generated route maps alone.",
  editor =       "Eugene Fiume",
  keywords =     "Information Visualization, Non-Realistic Rendering,
                 WWW Applications, Human Factors",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-113,
  pages =        "251--260",
  year =         "2001",
  title =        "Composable Controllers for Physics-Based Character
                 Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-113",
  author =       "Petros Faloutsos and Michiel van de Panne and Demetri
                 Terzopoulos",
  abstract =     "An ambitious goal in the area of physics-based
                 computer animation is the creation of virtual actors
                 that autonomously synthesize realistic human motions
                 and possess a broad repertoire of lifelike motor
                 skills. To this end, the control of dynamic,
                 anthropomorphic figures subject to gravity and contact
                 forces remains a difficult open problem. We propose a
                 framework for composing controllers in order to enhance
                 the motor abilities of such figures. A key contribution
                 of our composition framework is an explicit model of
                 the {"}pre-conditions{"} under which motor controllers
                 are expected to function properly. We demonstrate
                 controller composition with pre-conditions determined
                 not only manually, but also automatically based on
                 Support Vector Machine (SVM) learning theory. We
                 evaluate our composition framework using a family of
                 controllers capable of synthesizing basic actions such
                 as balance, protective stepping when balance is
                 disturbed, protective arm reactions when falling, and
                 multiple ways of standing up after a fall. We
                 furthermore demonstrate these basic controllers working
                 in conjunction with more dynamic motor skills within a
                 prototype virtual stuntperson. Our composition
                 framework promises to enable the community of
                 physics-based animation practitioners to easily
                 exchange motor controllers and integrate them into
                 dynamic characters.",
  editor =       "Eugene Fiume",
  keywords =     "Computer Animation, Character Animation, Physics-Based
                 Animation Control, Physics-Based Modeling",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-114,
  pages =        "261--270",
  year =         "2001",
  title =        "Automating Gait Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-114",
  author =       "Harold C. Sun and Dimitris N. Metaxas",
  abstract =     "One of the most routine actions humans perform is
                 walking. To date, however, an automated tool for
                 generating human gait is not available. This paper
                 addresses the gait generation problem through three
                 modular components. We present ElevWalker, a new
                 low-level gait generator based on sagittal elevation
                 angles, which allows curved locomotion - walking along
                 a curved path - to be created easily; ElevInterp, which
                 uses a new inverse motion interpolation algorithm to
                 handle uneven terrain locomotion; and MetaGait, a
                 high-level control module which allows an animator to
                 control a figure's walking simply by specifying a path.
                 The synthesis of these components is an easy-to-use,
                 real-time, fully automated animation tool suitable for
                 off-line animation, virtual environments and
                 simulation.",
  editor =       "Eugene Fiume",
  keywords =     "Animation, animation systems, animation w/constraints,
                 human body simulation",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-115,
  pages =        "271--276",
  year =         "2001",
  title =        "Expressive Expression Mapping With Ratio Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-115",
  author =       "Zicheng Liu and Ying Shan and Zhengyou Zhang",
  abstract =     "Facial expressions exhibit not only facial feature
                 motions, but also subtle changes in illumination and
                 appearance (e.g., facial creases and wrinkles).These
                 details are important visual cues, but they are
                 difficult to synthesize.Traditional expression mapping
                 techniques consider feature motions while the details
                 in illumination changes are ignored. In this paper, we
                 present a novel technique for facial expression
                 mapping.We capture the illumination change of one
                 person's expression in what we call an expression ratio
                 image (ERI). Together with geometric warping,we map an
                 ERI to any other person's face image to generate more
                 expressive facial expressions.",
  editor =       "Eugene Fiume",
  keywords =     "Facial animation, Morphing, Animation",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-116,
  pages =        "277--288",
  year =         "2001",
  title =        "Expression Cloning",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-116",
  author =       "Jun-yong Noh and Ulrich Neumann",
  abstract =     "We present a novel approach to producing facial
                 expression animations for new models. Instead of
                 creating new facial animations from scratch for each
                 new model created, we take advantage of existing
                 animation data in the form of vertex motion vectors.
                 Our method allows animations created by any tools or
                 methods to be easily retargeted to new models. We call
                 this process expression cloning and it provides a new
                 alternative for creating facial animations for
                 character models. Expression cloning makes it
                 meaningful to compile a high-quality facial animation
                 library since this data can be reused for new models.
                 Our method transfers vertex motion vectors from a
                 source face model to a target model having different
                 geometric proportions and mesh structure (vertex number
                 and connectivity). With the aid of an automated
                 heuristic correspondence search, expression cloning
                 typically requires a user to select fewer than ten
                 points in the model. Cloned expression animations
                 preserve the relative motions, dynamics, and character
                 of the original facial animations.",
  editor =       "Eugene Fiume",
  keywords =     "Deformations, Facial animation, Morphing, Neural
                 Nets",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-117,
  pages =        "289--300",
  year =         "2001",
  title =        "The Use of Positional Information in the Modeling of
                 Plants",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-117",
  author =       "Przemyslaw Prusinkiewicz and Lars M{\"{u}}ndermann and
                 Radoslaw Karwowski and Brendan Lane",
  abstract =     "We integrate into plant models three elements of plant
                 representation identified as important by artists:
                 posture (manifested in curved stems and elongated
                 leaves), gradual variation of features, and the
                 progression of the drawing process from overall
                 silhouette to local details. The resulting algorithms
                 increase the visual realism of plant models by offering
                 an intuitive control over plant form and supporting an
                 interactive modeling process. The algorithms are united
                 by the concept of expressing local attributes of plant
                 architecture as functions of their location along the
                 stems.",
  editor =       "Eugene Fiume",
  keywords =     "Realistic image synthesis, interactive procedural
                 modeling, plant, positional information, phyllotaxis,
                 Chomsky grammar, L-system, differential turtle
                 geometry, generalized cylinder",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-118,
  pages =        "301--308",
  year =         "2001",
  title =        "Procedural Modeling of Cities",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-118",
  author =       "Yoav I. H. Parish and Pascal M{\"{u}}ller",
  abstract =     "Modeling a city poses a number of problems to computer
                 graphics. Every urban area has a transportation network
                 that follows population and environmental influences,
                 and often a superimposed pattern plan. The buildings
                 appearances follow historical, aesthetic and statutory
                 rules. To create a virtual city, a roadmap has to be
                 designed and a large number of buildings need to be
                 generated. We propose a system using a procedural
                 approach based on L-systems to model cities. From
                 various image maps given as input, such as land-water
                 boundaries and population density, our system generates
                 a system of highways and streets, divides the land into
                 lots, and creates the appropriate geometry for the
                 buildings on the respective allotments. For the
                 creation of a city street map, L-systems have been
                 extended with methods that allow the consideration of
                 global goals and local constraints and reduce the
                 complexity of the production rules. An L-system that
                 generates geometry and a texturing system based on
                 texture elements and procedural methods compose the
                 buildings.",
  editor =       "Eugene Fiume",
  keywords =     "L-system, software design, developmental models,
                 modeling, urban development, architecture",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-119,
  pages =        "309--316",
  year =         "2001",
  title =        "Feature-Based Cellular Texturing for Architectural
                 Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-119",
  author =       "Justin Legakis and Julie Dorsey and Steven J.
                 Gortler",
  abstract =     "Cellular patterns are all around us, in masonry,
                 tiling, shingles, and many other materials. Such
                 patterns, especially in architectural settings, are
                 influenced by geometric features of the underlying
                 shape. Bricks turn corners, stones frame windows and
                 doorways, and patterns on disconnected portions of a
                 building align to achieve a particular aesthetic goal.
                 We present a strategy for feature-based cellular
                 texturing, where the resulting texture is derived from
                 both patterns of cells and the geometry to which they
                 are applied. As part of this strategy, we perform
                 texturing operations on features in a well-defined
                 order that simplifies the interdependence between cells
                 of adjacent patterns. Occupancy maps are used to
                 indicate which regions of a feature are already
                 occupied by cells of its neighbors, and which regions
                 remain to be textured. We also introduce the notion of
                 a pattern generator - the cellular texturing analogy of
                 a shader used in local illumination - and show how
                 several can be used together to build complex textures.
                 We present results obtained with an implementation of
                 this strategy and discuss details of some example
                 pattern generators.",
  editor =       "Eugene Fiume",
  keywords =     "Cellular texturing, computer-aided design, procedural
                 modeling, texturing Copyright",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-12,
  year =         "2001",
  title =        "An Application of Combined Neural Networks to Remotely
                 Sensed Images",
  author =       "R. V. Santos and M. R. Velasco and R. Q. Feitosa and
                 M. Simoes and R. Tanscheitd",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-12",
  abstract =     "Studies in the area of Pattern Recognition have
                 indicated that in most cases a classifier performs
                 differently from one pattern class to another. This
                 observation gave birth to the idea of combining the
                 individual results from different classifiers to derive
                 a consensus decision. This work investigates the
                 potential of combining neural networks to remotely
                 sensed images. A classifier system is built by
                 integrating the results of a plurarity of feed-forward
                 neural networks, each of them designed to have the best
                 performance for one class. Fuzzy Integrals are used as
                 the combining strategy. Experiments carried out to
                 evaluate the system, using a satellite image of an area
                 undergoing a rapid degradation process, have shown that
                 the combination may yield a better performance than
                 that of a single neural network.",
  editor =       "V. Skala",
  keywords =     "Combining Classifiers, Remote Sensing, Pattern
                 Recognition.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-120,
  pages =        "317--326",
  year =         "2001",
  title =        "Integrating Shape and Pattern in Mammalian Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-120",
  author =       "Marcelo Walter and Alain Fournier and Daniel
                 Menevaux",
  abstract =     "The giraffe and its patches, the leopard and its
                 spots, the tiger and its stripes are spectacular
                 examples of the integration of a pattern and a body
                 shape. We present an approach that integrates a
                 biologically-plausible pattern generation model, which
                 can effectively deliver a variety of patterns
                 characteristic of mammalian coats, and a body growth
                 and animation system that uses experimental growth data
                 to produce individual bodies and their associated
                 patterns automatically. We use the example of the
                 giraffe to illustrate how our approach takes us from a
                 canonical embryo to a full adult giraffe in a
                 continuous way, with results that are not only
                 realistic looking, but also objectively validated. The
                 flexibility of the approach is demonstrated by examples
                 of big cat patterns, including an interpolation between
                 patterns. The approach also allows a considerable
                 amount of user control to fine-tune the results and to
                 animate the resulting body with the pattern.",
  editor =       "Eugene Fiume",
  keywords =     "Natural Phenomena, Texture Synthesis, Clonal Mosaic
                 Patterns, Integration, Animal Models, Animal Patterns,
                 Growth",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-121,
  pages =        "327--340",
  year =         "2001",
  title =        "Image Analogies",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-121",
  author =       "Aaron Hertzmann and Charles E. Jacobs and Nuria Oliver
                 and Brian Curless and David H. Salesin",
  abstract =     "This paper describes a new framework for processing
                 images by example, called {"}image analogies.{"} The
                 framework involves two stages: a design phase, in which
                 a pair of images, with one image purported to be a
                 {"}filtered{"} version of the other, is presented as
                 {"}training data{"}; and an application phase, in which
                 the learned filter is applied to some new target image
                 in order to create an {"}analogous{"} filtered result.
                 Image analogies are based on a simple multi-scale
                 autoregression, inspired primarily by recent results in
                 texture synthesis. By choosing different types of
                 source image pairs as input, the framework supports a
                 wide variety of {"}image filter{"} effects, including
                 traditional image filters, such as blurring or
                 embossing; improved texture synthesis, in which some
                 textures are synthesized with higher quality than by
                 previous approaches; super-resolution, in which a
                 higher-resolution image is inferred from a
                 low-resolution source; texture transfer, in which
                 images are {"}texturized{"} with some arbitrary source
                 texture; artistic filters, in which various drawing and
                 painting styles are synthesized based on scanned
                 real-world examples; and texture-by-numbers, in which
                 realistic scenes, composed of a variety of textures,
                 are created using a simple painting interface.",
  editor =       "Eugene Fiume",
  keywords =     "Example-based rendering, texture synthesis,
                 non-photorealistic rendering, Markov random fields,
                 autoregression, texture-by-numbers, texture transfer",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-122,
  pages =        "341--346",
  year =         "2001",
  title =        "Image Quilting for Texture Synthesis and Transfer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-122",
  author =       "Alexei A. Efros and William T. Freeman",
  abstract =     "We present a simple image-based method of generating
                 novel visual appearance in which a new image is
                 synthesized by stitching together small patches of
                 existing images. We call this process image quilting.
                 First, we use quilting as a fast and very simple
                 texture synthesis algorithm which produces surprisingly
                 good results for a wide range of textures. Second, we
                 extend the algorithm to perform texture transfer -
                 rendering an object with a texture taken from a
                 different object. More generally, we demonstrate how an
                 image can be re-rendered in the style of a different
                 image. The method works directly on the images and does
                 not require 3D information.",
  editor =       "Eugene Fiume",
  keywords =     "Texture Synthesis, Texture Mapping, Image-based
                 Rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-123,
  pages =        "347--354",
  year =         "2001",
  title =        "Texture Synthesis on Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-123",
  author =       "Greg Turk",
  abstract =     "Many natural and man-made surface patterns are created
                 by interactions between texture elements and surface
                 geometry. We believe that the best way to create such
                 patterns is to synthesize a texture directly on the
                 surface of the model. Given a texture sample in the
                 form of an image, we create a similar texture over an
                 irregular mesh hierarchy that has been placed on a
                 given surface. Our method draws upon texture synthesis
                 methods that use image pyramids, and we use a mesh
                 hierarchy to serve in place of such pyramids. First, we
                 create a hierarchy of points from low to high density
                 over a given surface, and we connect these points to
                 form a hierarchy of meshes. Next, the user specifies a
                 vector field over the surface that indicates the
                 orientation of the texture. The mesh vertices on the
                 surface are then sorted in such a way that visiting the
                 points in order will follow the vector field and will
                 sweep across the surface from one end to the other.
                 Each point is then visited in turn to determine its
                 color. The color of a particular point is found by
                 examining the color of neighboring points and finding
                 the best match to a similar pixel neighborhood in the
                 given texture sample. The color assignment is done in a
                 coarse-to-fine manner using the mesh hierarchy. A
                 texture created this way fits the surface naturally and
                 seamlessly.",
  editor =       "Eugene Fiume",
  keywords =     "Texture synthesis, texture mapping",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-124,
  pages =        "355--360",
  year =         "2001",
  title =        "Texture Synthesis Over Arbitrary Manifold Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-124",
  author =       "Li-Yi Wei and Marc Levoy",
  abstract =     "Algorithms exist for synthesizing a wide variety of
                 textures over rectangular domains. However, it remains
                 difficult to synthesize general textures over arbitrary
                 manifold surfaces. In this paper, we present a solution
                 to this problem for surfaces defined by dense polygon
                 meshes. Our solution extends Wei and Levoy's texture
                 synthesis method [25] by generalizing their definition
                 of search neighborhoods. For each mesh vertex, we
                 establish a local parameterization surrounding the
                 vertex, use this parameterization to create a small
                 rectangular neighborhood with the vertex at its center,
                 and search a sample texture for similar neighborhoods.
                 Our algorithm requires as input only a sample texture
                 and a target model. Notably, it does not require
                 specification of a global tangent vector field; it
                 computes one as it goes - either randomly or via a
                 relaxation process. Despite this, the synthesized
                 texture contains no discontinuities, exhibits low
                 distortion, and is perceived to be similar to the
                 sample texture. We demonstrate that our solution is
                 robust and is applicable to a wide range of textures.",
  editor =       "Eugene Fiume",
  keywords =     "Texture Synthesis, Texture Mapping, Curves &
                 Surfaces",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-125,
  pages =        "361--370",
  year =         "2001",
  title =        "The Randomized z-Buffer Algorithm: Interactive
                 Rendering of Highly Complex Scenes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-125",
  author =       "Michael Wand and Matthias Fischer and Ingmar Peter and
                 Friedhelm Meyer auf der Heide and Wolfgang
                 Stra{\ss{}}er",
  abstract =     "We present a new output-sensitive rendering algorithm,
                 the randomized z-buffer algorithm. It renders an image
                 of an arbitrary three-dimensional scene consisting of
                 triangular primitives by reconstruction from a
                 dynamically chosen set of random surface sample points.
                 This approach is independent of mesh connectivity and
                 topology. The resulting rendering time grows only
                 logarithmically with the numbers of triangles in the
                 scene. We were able to render walkthroughs of scenes of
                 up to 1014 triangles at interactive frame rates.
                 Automatic identification of low detail scene components
                 ensures that the rendering speed of the randomized
                 z-buffer cannot drop below that of conventional
                 z-buffer rendering. Experimental and analytical
                 evidence is given that the image quality is comparable
                 to that of common approaches like z-buffer rendering.
                 The precomputed data structures employed by the
                 randomized z-buffer allow for interactive dynamic
                 updates of the scene. Their memory requirements grow
                 only linearly with the number of triangles and allow
                 for a scene graph based instantiation scheme to further
                 reduce memory consumption.",
  editor =       "Eugene Fiume",
  keywords =     "Rendering Systems, Level of Detail Algorithms, Monte
                 Carlo Techniques",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-126,
  pages =        "371--378",
  year =         "2001",
  title =        "Surface Splatting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-126",
  author =       "Matthias Zwicker and Hanspeter Pfister and Jeroen van
                 Baar and Markus Gross",
  abstract =     "Modern laser range and optical scanners need rendering
                 techniques that can handle millions of points with high
                 resolution textures. This paper describes a point
                 rendering and texture filtering technique called
                 surface splatting which directly renders opaque and
                 transparent surfaces from point clouds without
                 connectivity. It is based on a novel screen space
                 formulation of the Elliptical Weighted Average (EWA)
                 filter. Our rigorous mathematical analysis extends the
                 texture resampling framework of Heckbert to irregularly
                 spaced point samples. To render the points, we develop
                 a surface splat primitive that implements the screen
                 space EWA filter. Moreover, we show how to optimally
                 sample image and procedural textures to irregular point
                 data during pre-processing. We also compare the optimal
                 algorithm with a more efficient view-independent EWA
                 pre-filter. Surface splatting makes the benefits of EWA
                 texture filtering available to point-based rendering.
                 It provides high quality anisotropic texture filtering,
                 hidden surface removal, edge anti-aliasing, and
                 order-independent transparency.",
  editor =       "Eugene Fiume",
  keywords =     "Rendering Systems, Texture Mapping, Antialiasing,
                 Image-Based Rendering, Frame Buffer Algorithms",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-127,
  pages =        "379--386",
  year =         "2001",
  title =        "Spectral Processing of Point-Sampled Geometry",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-127",
  author =       "Mark Pauly and Markus Gros",
  abstract =     "We present a new framework for processing
                 point-sampled objects using spectral methods. By
                 establishing a concept of local frequencies on
                 geometry, we introduce a versatile spectral
                 representation that provides a rich repository of
                 signal processing algorithms. Based on an adaptive
                 tesselation of the model surface into regularly
                 resampled displacement fields, our method computes a
                 set of windowed Fourier transforms creating a spectral
                 decomposition of the model. Direct analysis and
                 manipulation of the spectral coefficients supports
                 effective filtering, resampling, power spectrum
                 analysis and local error control. Our algorithms
                 operate directly on points and normals, requiring no
                 vertex connectivity information. They are
                 computationally efficient, robust and amenable to
                 hardware acceleration. We demonstrate the performance
                 of our framework on a selection of example applications
                 including noise removal, enhancement, restoration and
                 subsampling.",
  editor =       "Eugene Fiume",
  keywords =     "Signal processing, spectral filtering, subsampling,
                 Fourier transform, point-based representations",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-128,
  pages =        "387--390",
  year =         "2001",
  title =        "Adaptive Shadow Maps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-128",
  author =       "Randima Fernando and Sebastian Fernandez and Kavita
                 Bala and Donald P. Greenberg",
  abstract =     "Shadow maps provide a fast and convenient method of
                 identifying shadows in scenes but can introduce
                 aliasing. This paper introduces the Adaptive Shadow Map
                 (ASM) as a solution to this problem. An ASM removes
                 aliasing by resolving pixel size mismatches between the
                 eye view and the light source view. It achieves this
                 goal by storing the light source view (i.e., the shadow
                 map for the light source) as a hierarchical grid
                 structure as opposed to the conventional flat
                 structure. As pixels are transformed from the eye view
                 to the light source view, the ASM is refined to create
                 higher-resolution pieces of the shadow map when needed.
                 This is done by evaluating the contributions of shadow
                 map pixels to the overall image quality. The
                 improvement process is view-driven, progressive, and
                 confined to a user-specifiable memory footprint. We
                 show that ASMs enable dramatic improvements in shadow
                 quality while maintaining interactive rates.",
  editor =       "Eugene Fiume",
  keywords =     "Rendering, Shadow Algorithms",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-129,
  pages =        "391--398",
  year =         "2001",
  title =        "Photo-Realistic Rendering of Knitwear Using the
                 Lumislice",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-129",
  author =       "Ying-Qing Xu and Yanyun Chen and Stephen Lin and Hua
                 Zhong and Enhua Wu and Baining Guo and Heung-Yeung
                 Shum",
  abstract =     "We present a method for efficient synthesis of
                 photorealistic free-form knitwear. Our approach is
                 motivated by the observation that a single
                 cross-section of yarn can serve as the basic primitive
                 for modeling entire articles of knitwear. This
                 primitive, called the lumislice, describes radiance
                 from a yarn cross-section based on fine-level
                 interactions - such as occlusion, shadowing, and
                 multiple scattering - among yarn fibers. By
                 representing yarn as a sequence of identical but
                 rotated cross-sections, the lumislice can effectively
                 propagate local microstructure over arbitrary stitch
                 patterns and knitwear shapes. This framework
                 accommodates varying levels of detail and capitalizes
                 on hardware-assisted transparency blending. To further
                 enhance realism, a technique for generating soft
                 shadows from yarn is also introduced.",
  editor =       "Eugene Fiume",
  keywords =     "Knitwear, Image-based Rendering, Transparency
                 Blending, Parametric Surfaces, Photorealistic
                 Rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-13,
  year =         "2001",
  title =        "Consistent Orientation of Segmented Models Recovered
                 from Digitized Data",
  author =       "M. Vanco and Guido Brunett",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-13",
  abstract =     "Reverse Engineering addresses the problem of creating
                 a CAD model for an existing physical object. In the
                 data acquisition phase of the reconstruction process a
                 discrete set of points on the boundary of the original
                 object is created that serves as the database for all
                 consecutive steps. The author has developed a method to
                 collect these points into segments that from the basic
                 elements of a B-rep model for the 3D object. In this
                 paper we consider the important question of how to
                 obtain a consistent orientation of the segmented model.
                 For this three different methods are presented that are
                 based on the following principles: orientation
                 propagation, orientation via the boundary curves and
                 orientation induction from surrounding 3D space. For
                 these strategies a detailed description of the
                 corresponding algorithm on the segmented models are
                 given. The performance of these methods for several
                 benchmark objects is discussed.",
  editor =       "V. Skala",
  keywords =     "Reverse Engineering, Surface Reconstruction,
                 Consistent normal orientation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-130,
  pages =        "399--408",
  year =         "2001",
  title =        "A Physically-Based Night Sky Model",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-130",
  author =       "Henrik Wann Jensen and Fr{\'{e}}do Durand and Michael
                 M. Stark and Simon Premoze and Julie Dorsey and Peter
                 Shirley",
  abstract =     "This paper presents a physically-based model of the
                 night sky for realistic image synthesis. We model both
                 the direct appearance of the night sky and the
                 illumination coming from the Moon, the stars, the
                 zodiacal light, and the atmosphere. To accurately
                 predict the appearance of night scenes we use
                 physically-based astronomical data, both for position
                 and radiometry. The Moon is simulated as a geometric
                 model illuminated by the Sun, using recently measured
                 elevation and albedo maps, as well as a specialized
                 BRDF. For visible stars, we include the position,
                 magnitude, and temperature of the star, while for the
                 Milky Way and other nebulae we use a processed
                 photograph. Zodiacal light due to scattering in the
                 dust covering the solar system, galactic light, and
                 airglow due to light emission of the atmosphere are
                 simulated from measured data. We couple these
                 components with an accurate simulation of the
                 atmosphere. To demonstrate our model, we show a variety
                 of night scenes rendered with a Monte Carlo ray
                 tracer.",
  editor =       "Eugene Fiume",
  keywords =     "Natural Phenomena, Atmospheric Effects, Illumination,
                 Rendering, Ray Tracing Copyright",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-131,
  pages =        "409--416",
  year =         "2001",
  title =        "Texture Mapping Progressive Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-131",
  author =       "Pedro V. Sander and John Snyder and Steven J. Gortler
                 and Hugues Hoppe",
  abstract =     "Given an arbitrary mesh,we present a method to
                 construct a progressive mesh (PM) such that all meshes
                 in the PM sequence share a common texture
                 parametrization. Our method considers two important
                 goals simultaneously.It minimizes texture stretch
                 (small texture distances mapped onto large surface
                 distances) to balance sampling rates over all locations
                 and directions on the surface. It also minimizes
                 texture deviation ({"}slippage{"} error based on
                 parametric correspondence) to obtain accurate textured
                 mesh approximations. The method begins by partitioning
                 the mesh into charts using planarity and compactness
                 heuristics. It creates a stretch-minimizing
                 parametrization within each chart, and resizes the
                 charts based on the resulting stretch. Next,it
                 simplifies the mesh while respecting the chart
                 boundaries. The parametrization is re-optimized to
                 reduce both stretch and deviation over the whole PM
                 sequence. Finally,the charts are packed into a texture
                 atlas. We demonstrate using such atlases to sample
                 color and normal maps over several models.",
  editor =       "Eugene Fiume",
  keywords =     "Mesh simplification, surface flattening, surface
                 parametrization, texture stretch",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-132,
  pages =        "417--424",
  year =         "2001",
  title =        "Constrained Texture Mapping for Polygonal Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-132",
  author =       "Bruno L{\'{e}}vy",
  abstract =     "Recently, time and effort have been devoted to
                 automatic texture mapping. It is possible to study the
                 parameterization function and to describe the texture
                 mapping process in terms of a functional optimization
                 problem. Several methods of this type have been
                 proposed to minimize deformations. However, these
                 existing methods suffer from several limitations. For
                 instance, it is difficult to put details of the texture
                 in correspondence with features of the model, since
                 most of the existing methods can only constrain
                 iso-parametric curves. We introduce in this paper a new
                 optimization-based method for parameterizing polygonal
                 meshes with minimum deformations, while enabling the
                 user to interactively define and edit a set of
                 constraints. Each user-defined constraint consists of a
                 relation linking a 3D point picked on the surface and a
                 2D point of the texture. Moreover, the non-deformation
                 criterion introduced here can act as an extrapolator,
                 thus making it unnecessary to constrain the border of
                 the surface, in contrast with classic methods. To
                 minimize the criterion, a conjugate gradient algorithm
                 is combined with a compressed representation of sparse
                 matrices, making it possible to achieve a fast
                 convergence.",
  editor =       "Eugene Fiume",
  keywords =     "Texture Mapping, Paint Systems, Polygonal Modeling",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-133,
  pages =        "425--432",
  year =         "2001",
  title =        "Unstructured Lumigraph Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-133",
  author =       "Chris Buehler and Michael Bosse and Leonard McMillan
                 and Steven J. Gortler and Michael F. Cohen",
  abstract =     "We describe an image based rendering approach that
                 generalizes many current image based rendering
                 algorithms, including light field rendering and
                 view-dependent texture mapping. In particular, it
                 allows for lumigraph-style rendering from a set of
                 input cameras in arbitrary configurations (i.e., not
                 restricted to a plane or to any specific manifold). In
                 the case of regular and planar input camera positions,
                 our algorithm reduces to a typical lumigraph approach.
                 When presented with fewer cameras and good approximate
                 geometry, our algorithm behaves like view-dependent
                 texture mapping. The algorithm achieves this
                 flexibility because it is designed to meet a set of
                 specific goals that we describe. We demonstrate this
                 flexibility with a variety of examples.",
  editor =       "Eugene Fiume",
  keywords =     "Image-Based Rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-134,
  pages =        "433--442",
  year =         "2001",
  title =        "Image-Based Modeling and Photo Editing",
  author =       "Byong Mok Oh and Max Chen and Julie Dorsey and
                 Fr{\'{e}}do Durand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-134",
  abstract =     "We present an image-based modeling and editing system
                 that takes a single photo as input. We represent a
                 scene as a layered collection of depth images, where
                 each pixel encodes both color and depth. Starting from
                 an input image, we employ a suite of user-assisted
                 techniques, based on a painting metaphor, to assign
                 depths and extract layers. We introduce two specific
                 editing operations. The first, a {"}clone brushing
                 tool,{"} permits the distortion-free copying of parts
                 of a picture, by using a parameterization optimization
                 technique. The second, a {"}texture-illuminance
                 decoupling filter,{"} discounts the effect of
                 illumination on uniformly textured areas, by decoupling
                 large- and small-scale features via bilateral
                 filtering. Our system enables editing from different
                 viewpoints, extracting and grouping of image-based
                 objects, and modifying the shape, color, and
                 illumination of these objects.",
  editor =       "Eugene Fiume",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-136,
  pages =        "443--450",
  year =         "2001",
  title =        "Plenoptic Stitching: {A} Scalable Method for
                 Reconstructing 3{D} Interactive Walkthroughs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-136",
  author =       "Daniel G. Aliaga and Ingrid Carlbom",
  abstract =     "Interactive walkthrough applications require detailed
                 3D models to give users a sense of immersion in an
                 environment. Traditionally these models are built using
                 computer-aided design tools to define geometry and
                 material properties. But creating detailed models is
                 time-consuming and it is also difficult to reproduce
                 all geometric and photometric subtleties of real-world
                 scenes. Computer vision attempts to alleviate this
                 problem by extracting geometry and photogrammetry from
                 images of the real-world scenes. However, these models
                 are still limited in the amount of detail they recover.
                 Image-based rendering generates novel views by
                 resampling a set of images of the environment without
                 relying upon an explicit geometric model. Current such
                 techniques limit the size and shape of the environment,
                 and they do not lend themselves to walkthrough
                 applications. In this paper, we define a
                 parameterization of the 4D plenoptic function that is
                 particularly suitable for interactive walkthroughs and
                 define a method for its sampling and reconstructing.
                 Our main contributions are: 1) a parameterization of
                 the 4D plenoptic function that supports walkthrough
                 applications in large, arbitrarily shaped environments;
                 2) a simple and fast capture process for complex
                 environments; and 3) an automatic algorithm for
                 reconstruction of the plenoptic function.",
  editor =       "Eugene Fiume",
  keywords =     "Virtual environments, plenoptic function, image-based
                 rendering, interactive walkthroughs, omnidirectional",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-137,
  pages =        "451--460",
  year =         "2001",
  title =        "Hybrid Stereo Camera: An {IBR} Approach for Synthesis
                 of Very High Resolution Stereoscopic Image Sequences",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-137",
  author =       "Harpreet S. Sawhney and Yanlin Guo and Keith Hanna and
                 Rakesh Kumar and Sean Adkins and Samuel Zhou",
  abstract =     "This paper introduces a novel application of IBR
                 technology for efficient rendering of high quality CG
                 and live action stereoscopic sequences. Traditionally,
                 IBR has been applied to render novel views using image
                 and depth based representations of the plenoptic
                 functions. In this work, we present a restricted form
                 of IBR in which lower resolution images for the views
                 to be generated at a very high resolution are assumed
                 to be available. Specifically, the paper addresses the
                 problem of synthesizing stereo IMAX(R)1 3D motion
                 picture images at a standard resolution of 4-6K. At
                 such high resolutions, producing CG content is
                 extremely time consuming and capturing live action
                 requires bulky cameras. We propose a Hybrid Stereo
                 Camera concept in which one view is rendered at the
                 target high resolution but the other is rendered at a
                 much lower resolution. Methods for synthesizing the
                 second view sequence at the target resolution using
                 image analysis and IBR techniques are the focus of this
                 work. The high quality results from the techniques
                 presented in this paper have been visually evaluated in
                 the IMAX 3D large screen projection environment. The
                 paper also highlights generalizations and extensions of
                 the hybrid stereo camera concept.",
  editor =       "Eugene Fiume",
  keywords =     "Image-based Rendering, Stereo Sequence Synthesis,
                 Image Analysis",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-138,
  pages =        "469--476",
  year =         "2001",
  title =        "Project {FEELEX}: Adding Haptic Surface to Graphics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-138",
  author =       "Hiroo Iwata and Hiroaki Yano and Fumitaka Nakaizumi
                 and Ryo Kawamura",
  abstract =     "This paper presents work carried out for a project to
                 develop a new interactive technique that combines
                 haptic sensation with computer graphics. The project
                 has two goals. The first is to provide users with a
                 spatially continuous surface on which they can
                 effectively touch an image using any part of their bare
                 hand, including the palm. The second goal is to present
                 visual and haptic sensation simultaneously by using a
                 single device that doesn't oblige the user to wear any
                 extra equipment. In order to achieve these goals, we
                 designed a new interface device comprising of a
                 flexible screen, an actuator array and a projector. The
                 actuator deforms the flexible screen onto which the
                 image is projected. The user can then touch the image
                 directly and feel its shape and rigidity. Initially we
                 fabricated two prototypes, and their effectiveness is
                 examined by studying the observations made by anonymous
                 users and a performance evaluation test for spatial
                 resolution.",
  address =      "ACM Press / ACM SIGGRAPH",
  editor =       "Eugene Fiume",
  keywords =     "Haptics, interactive graphics, deformable screen,
                 actuator array",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-139,
  pages =        "477--486",
  year =         "2001",
  title =        "{BEAT}: The Behavior Expression Animation Toolkit",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-139",
  author =       "Justine Cassell and Hannes H{\"{o}}gni
                 Vilhj{\'{a}}lmsson and Timothy Bickmore",
  abstract =     "The Behavior Expression Animation Toolkit (BEAT)
                 allows animators to input typed text that they wish to
                 be spoken by an animated human figure, and to obtain as
                 output appropriate and synchronized nonverbal behaviors
                 and synthesized speech in a form that can be sent to a
                 number of different animation systems. The nonverbal
                 behaviors are assigned on the basis of actual
                 linguistic and contextual analysis of the typed text,
                 relying on rules derived from extensive research into
                 human conversational behavior. The toolkit is
                 extensible, so that new rules can be quickly added. It
                 is designed to plug into larger systems that may also
                 assign personality profiles, motion characteristics,
                 scene constraints, or the animation styles of
                 particular animators.",
  editor =       "Eugene Fiume",
  keywords =     "Animation Systems, Facial Animation, Speech Synthesis,
                 Gesture",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-14,
  year =         "2001",
  title =        "An Improved Refinement and Decimation Method for
                 Adaptive Terrain Surface Approximation",
  author =       "Helio Pedrini",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-14",
  abstract =     "An improved method for adaptively constructing a
                 terrain surface representation from a set of data
                 points is presented. Refinement and decimation steps
                 are repeatedly applied to triangular meshes,
                 incrementally determining a better distribution of the
                 data points, while a specified error tolerance is
                 preserved. Even though not asymptotically optimal or
                 monotonically convergent, it produces approximations
                 that are, experimentally, significantly better than
                 those generated by straightforward greedy insertion
                 algorithms. A new local error metric is used to select
                 points to be inserted into the triangulation, based on
                 the maximum vertical error weighted by the standard
                 deviation calculated in a neighborhood of the candidate
                 point. Conversely, a measure of angle between surface
                 normals is used to determine whether a vertex should be
                 removed from the triangulation. The method has been
                 implemented and tested on both synthetic test cases and
                 real terrain data sets.",
  editor =       "V. Skala",
  keywords =     "Surface simplification, triangulated irregular
                 network, mesh optimization.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-140,
  pages =        "487--496",
  year =         "2001",
  title =        "WordsEye: An Automatic Text-to-Scene Conversion
                 System",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-140",
  author =       "Bob Coyne and Richard Sproat",
  abstract =     "Natural language is an easy and effective medium for
                 describing visual ideas and mental images. Thus, we
                 foresee the emergence of language-based 3D scene
                 generation systems to let ordinary users quickly create
                 3D scenes without having to learn special software,
                 acquire artistic skills, or even touch a desktop
                 window-oriented interface. WordsEye is such a system
                 for automatically converting text into representative
                 3D scenes. WordsEye relies on a large database of 3D
                 models and poses to depict entities and actions. Every
                 3D model can have associated shape displacements,
                 spatial tags, and functional properties to be used in
                 the depiction process. We describe the linguistic
                 analysis and depiction techniques used by WordsEye
                 along with some general strategies by which more
                 abstract concepts are made depictable.",
  editor =       "Eugene Fiume",
  keywords =     "Applications, HCI (Human-Computer Interface),
                 Multimedia, Text-to-Scene Conversion, Scene
                 Generation",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-141,
  pages =        "501--510",
  year =         "2001",
  title =        "Applying Appearance Standards to Light Reflection
                 Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-141",
  author =       "Harold B. Westlund and Gary W. Meyer",
  abstract =     "Appearance standards for gloss, haze, and
                 goniochromatic color are applied to computer graphic
                 reflection models. Correspondences are derived between
                 both the gloss and haze standards and the specular
                 exponent of the Phong model, the surface roughness of
                 the Ward model, and the surface roughness of the
                 Cook-Torrance model. Metallic and pearlescent colors
                 are rendered using three aspecular measurements defined
                 in a proposed standard for goniochromatic color. The
                 reflection models for gloss and goniochromatic color
                 are combined to synthesize pictures of clear coated
                 automotive paint. Advantages of using appearance
                 standards to select reflection model parameters include
                 the small number of required measurements and the
                 inexpensive commercially available instruments
                 necessary to acquire the data. The use of a standard
                 appearance scale also provides a more intuitive way of
                 selecting the reflection model parameters and a
                 reflection model independent method of specifying
                 appearance.",
  editor =       "Eugene Fiume",
  keywords =     "Color, optics, reflection and shading models,
                 rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-143,
  pages =        "511--518",
  year =         "2001",
  title =        "A Practical Model for Subsurface Light Transport",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-143",
  author =       "Henrik Wann Jensen and Stephen R. Marschner and Marc
                 Levoy and Pat Hanrahan",
  abstract =     "This paper introduces a simple model for subsurface
                 light transport in translucent materials. The model
                 enables efficient simulation of effects that BRDF
                 models cannot capture, such as color bleeding within
                 materials and diffusion of light across shadow
                 boundaries. The technique is efficient even for
                 anisotropic, highly scattering media that are expensive
                 to simulate using existing methods. The model combines
                 an exact solution for single scattering with a dipole
                 point source diffusion approximation for multiple
                 scattering. We also have designed a new, rapid
                 image-based measurement technique for determining the
                 optical properties of translucent materials. We
                 validate the model by comparing predicted and measured
                 values and show how the technique can be used to
                 recover the optical properties of a variety of
                 materials, including milk, marble, and skin. Finally,
                 we describe sampling techniques that allow the model to
                 be used within a conventional ray tracer.",
  editor =       "Eugene Fiume",
  keywords =     "Subsurface scattering, BSSRDF, reflection models,
                 light transport, diffusion theory, realistic image
                 synthesis",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-144,
  pages =        "519--528",
  year =         "2001",
  title =        "Polynomial Texture Maps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-144",
  author =       "Tom Malzbender and Dan Gelb and Hans Wolters",
  abstract =     "In this paper we present a new form of texture mapping
                 that produces increased photorealism. Coefficients of a
                 biquadratic polynomial are stored per texel, and used
                 to reconstruct the surface color under varying lighting
                 conditions. Like bump mapping, this allows the
                 perception of surface deformations. However, our method
                 is image based, and photographs of a surface under
                 varying lighting conditions can be used to construct
                 these maps. Unlike bump maps, these Polynomial Texture
                 Maps (PTMs) also capture variations due to surface
                 self-shadowing and interreflections, which enhance
                 realism. Surface colors can be efficiently
                 reconstructed from polynomial coefficients and light
                 directions with minimal fixed-point hardware. We have
                 also found PTMs useful for producing a number of other
                 effects such as anisotropic and Fresnel shading models
                 and variable depth of focus. Lastly, we present several
                 reflectance function transformations that act as
                 contrast enhancement operators. We have found these
                 particularly useful in the study of ancient
                 archeological clay and stone writings.",
  editor =       "Eugene Fiume",
  keywords =     "Graphics Hardware, Illumination, Image Processing,
                 Image-Based Rendering, Reflectance & Shading Models,
                 Texture Mapping",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-145,
  pages =        "497--500",
  year =         "2001",
  title =        "An Efficient Representation for Irradiance Environment
                 Maps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-145",
  author =       "Ravi Ramamoorthi and Pat Hanrahan",
  abstract =     "We consider the rendering of diffuse objects under
                 distant illumination, as specified by an environment
                 map. Using an analytic expression for the irradiance in
                 terms of spherical harmonic coefficients of the
                 lighting, we show that one needs to compute and use
                 only 9 coefficients, corresponding to the
                 lowest-frequency modes of the illumination, in order to
                 achieve average errors of only 1%. In other words, the
                 irradiance is insensitive to high frequencies in the
                 lighting, and is well approximated using only 9
                 parameters. In fact, we show that the irradiance can be
                 procedurally represented simply as a quadratic
                 polynomial in the cartesian components of the surface
                 normal, and give explicit formulae. These observations
                 lead to a simple and efficient procedural rendering
                 algorithm amenable to hardware implementation, a
                 prefiltering method up to three orders of magnitude
                 faster than previous techniques, and new
                 representations for lighting design and image-based
                 rendering.",
  editor =       "Eugene Fiume",
  keywords =     "Environment Maps, Rendering Hardware, Signal
                 Processing, Irradiance, Radiance, Illumination,
                 Lambertian Reflectance, Prefiltering, Spherical
                 Harmonics",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-146,
  pages =        "529--536",
  year =         "2001",
  title =        "Synthesizing Sounds From Physically Based Motion",
  author =       "James F. O'Brien and Perry R. Cook and Georg Essl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-146",
  abstract =     "This paper describes a technique for approximating
                 sounds that are generated by the motions of solid
                 objects. The technique builds on previous work in the
                 field of physically based animation that uses
                 deformable models to simulate the behavior of the solid
                 objects. As the motions of the objects are computed,
                 their surfaces are analyzed to determine how the motion
                 will induce acoustic pressure waves in the surrounding
                 medium. Our technique computes the propagation of those
                 waves to the listener and then uses the results to
                 generate sounds corresponding to the behavior of the
                 simulated objects.",
  editor =       "Eugene Fiume",
  keywords =     "Sound modeling, physically based modeling, simulation,
                 surface vibrations, dynamics, animation techniques,
                 finite element method",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-147,
  pages =        "537--544",
  year =         "2001",
  title =        "FoleyAutomatic: Physically-Based Sound Effects for
                 Interactive Simulation and Animation",
  author =       "Kees van den Doel and Paul G. Kry and Dinesh K. Pai",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-147",
  abstract =     "We describe algorithms for real-time synthesis of
                 realistic sound effects for interactive simulations
                 (e.g., games) and animation. These sound effects are
                 produced automatically, from 3D models using dynamic
                 simulation and user interaction. We develop algorithms
                 that are efficient, physically-based, and can be
                 controlled by users in natural ways. We develop
                 effective techniques for producing high quality
                 continuous contact sounds from dynamic simulations
                 running at video rates which are slow relative to audio
                 synthesis. We accomplish this using modal models driven
                 by contact forces modeled at audio rates, which are
                 much higher than the graphics frame rate. The contact
                 forces can be computed from simulations or can be
                 custom designed. We demonstrate the effectiveness with
                 complex realistic simulations.",
  editor =       "Eugene Fiume",
  keywords =     "Animation Systems, Computer Games, Multimedia,
                 Physically Based Animation, Physically Based Modeling,
                 Sound Visualization, Virtual Reality, Head Mounted
                 Displays",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-148,
  pages =        "545--552",
  year =         "2001",
  title =        "Modeling Acoustics in Virtual Environments Using the
                 Uniform Theory of Diffraction",
  author =       "Nicolas Tsingos and Thomas Funkhouser and Addy Ngan
                 and Ingrid Carlbom",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-148",
  abstract =     "Realistic modeling of reverberant sound in 3D virtual
                 worlds provides users with important cues for
                 localizing sound sources and understanding spatial
                 properties of the environment. Unfortunately, current
                 geometric acoustic modeling systems do not accurately
                 simulate reverberant sound. Instead, they model only
                 direct transmission and specular reflection, while
                 diffraction is either ignored or modeled through
                 statistical approximation. However, diffraction is
                 important for correct interpretation of acoustic
                 environments, especially when the direct path between
                 sound source and receiver is occluded The Uniform
                 Theory of Diffraction (UTD) extends geometrical
                 acoustics with diffraction phenomena: illuminated edges
                 become secondary sources of diffracted rays that in
                 turn may propagate through the environment. In this
                 paper, we propose an efficient way for computing the
                 acoustical effect of diffraction paths using the UTD
                 for deriving secondary diffracted rays and associated
                 diffraction coefficients. Our main contributions are:
                 1) a beam tracing method for enumerating sequences of
                 diffracting edges efficiently and without aliasing in
                 densely occluded polyhedral environments; 2) a
                 practical approximation to the simulated sound field in
                 which diffraction is considered only in shadow regions;
                 and 3) a real-time auralization system demonstrating
                 that diffraction dramatically improves the quality of
                 spatialized sound in virtual environments.",
  editor =       "Eugene Fiume",
  keywords =     "Spatialized Sound, Virtual Environments, Sound
                 Visualization, Uniform Theory of Diffraction, Beam
                 Tracing",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-149,
  pages =        "553--560",
  year =         "2001",
  title =        "An Immersive, Multi-User, Musical Stage Environment",
  author =       "Matthew Reynolds and Bernd Schoner and Joey Richards
                 and Kelly Dobson and Neil Gershenfeld",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-149",
  abstract =     "A multi-user, polyphonic sensor stage environment that
                 maps position and gestures of up to four performers to
                 the pitch and articulation of distinct notes is
                 presented. The design seeks to provide multiple players
                 on a stage with the feeling of a traditional acoustic
                 instrument by giving them complete control over the
                 instrument's expressive parameters and a clear causal
                 connection between their actions and the resulting
                 sound. The positions of the performers are determined
                 by a custom ultrasonic tracking system, while hand
                 motions are measured by custom-made gloves containing
                 accelerometer units. Furthermore, juggling clubs are
                 illuminated dynamically to make complex juggling
                 patterns more apparent. The system is currently on tour
                 with the Flying Karamazov Brothers juggling troupe.",
  editor =       "Eugene Fiume",
  keywords =     "Applications, HCI (Human-Computer Interface), Object
                 Tracking, Spatialized Sound, User Interface Design",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-15,
  year =         "2001",
  title =        "The Relation between {RATS}-Splines and the Catmull
                 and Clark {B}-Splines",
  author =       "Andreas Savva and Gordon J. Clapworthy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-15",
  abstract =     "This paper presents the relationship between the
                 Recursive Arbitrary Topology Splines (RATS) method,
                 derived by the authors, and the Catmull and Clark
                 recursive B-Spline method. Both methods are capable of
                 defining surfaces of any arbitrary topology of control
                 points. They {"}fill-in{"} n-sided regions with
                 four-sided patches. The Catmull & Clark method is
                 derived from the midpoint subdivision of B-splines
                 whereas the RATS method is derived from the midpoint
                 subdivision of Bezier splines. RATS generates an
                 additional set of patches defining the border of the
                 surface but the RATS inner surface is identical to the
                 Catmull and Clark surface. This paper illustrates this
                 relationship between the two methods.",
  editor =       "V. Skala",
  keywords =     "Surface modelling, recursive splines, geometric
                 design, arbitrary topology figures.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-150,
  pages =        "561--566",
  year =         "2001",
  title =        "Image-Based Motion Blur for Stop Motion Animation",
  author =       "Gabriel J. Brostow and Irfan Essa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-150",
  abstract =     "Stop motion animation is a well-established technique
                 where still pictures of static scenes are taken and
                 then played at film speeds to show motion. A major
                 limitation of this method appears when fast motions are
                 desired; most motion appears to have sharp edges and
                 there is no visible motion blur. Appearance of motion
                 blur is a strong perceptual cue, which is automatically
                 present in live-action films, and synthetically
                 generated in animated sequences. In this paper, we
                 present an approach for automatically simulating motion
                 blur. Ours is wholly a post-process, and uses image
                 sequences, both stop motion or raw video, as input.
                 First we track the frame-to-frame motion of the objects
                 within the image plane. We then integrate the scene's
                 appearance as it changed over a period of time. This
                 period of time corresponds to shutter speed in
                 live-action filming, and gives us interactive control
                 over the extent of the induced blur. We demonstrate a
                 simple implementation of our approach as it applies to
                 footage of different motions and to scenes of varying
                 complexity. Our photorealistic renderings of these
                 input sequences approximate the effect of capturing
                 moving objects on film that is exposed for finite
                 periods of time.",
  editor =       "Eugene Fiume",
  keywords =     "Animation, computer vision, image-based rendering,
                 motion blur, stop motion animation, temporal
                 antialiasing, video post-processing",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-151,
  pages =        "567--572",
  year =         "2001",
  title =        "A Simple and Efficient Error-Diffusion Algorithm",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-151",
  author =       "Victor Ostromoukhov",
  abstract =     "In this contribution, we introduce a new
                 error-diffusion scheme that produces higher quality
                 results. The algorithm is faster than the universally
                 used Floyd-Steinberg algorithm, while maintaining its
                 original simplicity. The efficiency of our algorithm is
                 based on a deliberately restricted choice of the
                 distribution coefficients. Its pleasing nearly
                 artifact-free behavior is due to the off-line
                 minimization process applied to the basic algorithm's
                 parameters (dis-tribution coefficients). This
                 minimization brings the Fourier spectra of the selected
                 key intensity levels as close as possible to the
                 corresponding {"}blue noise{"} spectra. The continuity
                 of the algorithm's behavior across the full range of
                 intensity levels is achieved thanks to smooth
                 interpolation between the distribution coefficients
                 corresponding to key levels. This algorithm is
                 applicable in a wide range of computer graphics
                 applications, where a color quantization algorithm with
                 good visual properties is needed.",
  editor =       "Eugene Fiume",
  keywords =     "Halftoning, Error-Diffusion, Image Quality, Color
                 Quantization",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-152,
  pages =        "573--578",
  year =         "2001",
  title =        "Simulating Decorative Mosaics",
  author =       "Alejo Hausner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-152",
  abstract =     "This paper presents a method for simulating decorative
                 tile mosaics. Such mosaics are challenging because the
                 square tiles that comprise them must be packed tightly
                 and yet must follow orientations chosen by the artist.
                 Based on an existing image and user-selected edge
                 features, the method can both reproduce the image's
                 colours and emphasize the selected edges by placing
                 tiles that follow the edges. The method uses centroidal
                 voronoi diagrams which normally arrange points in
                 regular hexagonal grids. By measuring distances with an
                 manhattan metric whose main axis is adjusted locally to
                 follow the chosen direction field, the centroidal
                 diagram can be adapted to place tiles in curving square
                 grids instead. Computing the centroidal voronoi diagram
                 is made possible by leveraging the z-buffer algorithm
                 available in many graphics cards.",
  editor =       "Eugene Fiume",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InProceedings{EVL-2001-153,
  pages =        "579--584",
  year =         "2001",
  title =        "Real-Time Hatching",
  author =       "Emil Praun and Hugues Hoppe and Matthew Webb and Adam
                 Finkelstein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-153",
  abstract =     "Drawing surfaces using hatching strokes simultaneously
                 conveys material, tone, and form. We present a
                 real-time system for non-photorealistic rendering of
                 hatching strokes over arbitrary surfaces. During an
                 automatic preprocess, we construct a sequence of
                 mip-mapped hatch images corresponding to different
                 tones, collectively called a tonal art map. Strokes
                 within the hatch images are scaled to attain
                 appropriate stroke size and density at all resolutions,
                 and are organized to maintain coherence across scales
                 and tones. At runtime, hardware multitexturing blends
                 the hatch images over the rendered faces to locally
                 vary tone while maintaining both spatial and temporal
                 coherence. To render strokes over arbitrary surfaces,
                 we build a lapped texture parametrization where the
                 overlapping patches align to a curvature-based
                 direction field. We demonstrate hatching strokes over
                 complex surfaces in a variety of styles.",
  editor =       "Eugene Fiume",
  keywords =     "Non-photorealistic rendering, line art,
                 multitexturing, chicken-and-egg problem",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
}

@InCollection{EVL-2001-154,
  pages =        "xv--xv(1)",
  year =         "2001",
  title =        "Rendering: Input and Output",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-154",
  author =       "H. Rushmeier",
  abstract =     "Rendering is the process of creating an image from
                 numerical input data. In the past few years our ideas
                 about methods for acquiring the input data and the form
                 of the output have expanded. The availability of
                 inexpensive cameras and scanners has influenced how we
                 can obtain data needed for rendering. Input for
                 rendering ranges from sets of images to complex
                 geometric descriptions with detailed BRDF data. The
                 images that are rendered may be simply arrays of RGB
                 images, or they may be arrays with vectors or matrices
                 of data defined for each pixel. The rendered images may
                 not be intended for direct display, but may be textures
                 for geometries that are to be transmitted to be
                 rendered on another system. A broader range of
                 parameters now need to be taken into account to render
                 images that are perceptually consistent across displays
                 that range from CAVEs to personal digital assistants.
                 This presentation will give an overview of how new
                 hardware and new applications have changed traditional
                 ideas of rendering input and output.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-155,
  pages =        "xvi--xvi(1)",
  year =         "2001",
  title =        "Why Games Will Be the Preeminent Art Form of the 21st
                 Century",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-155",
  author =       "C. Hecker",
  abstract =     "Computer games share many artistic and technical
                 characteristics with films of the early 1900s. Games'
                 artistic evolution is hampered by the lack of artistic
                 respect from society at large, and the lack of
                 technical standards that would allow artistic
                 innovation. The same problems affected cinema during
                 its birth. During the early 20th century, film managed
                 to find its way from popular diversion to highly
                 respected art form. Will games follow the same course,
                 or will they be stuck forever in the ghetto of pop
                 culture? What technological and artistic changes need
                 to occur in the medium for games to evolve beyond
                 merely shooting aliens and into an art form worthy of
                 association with painting, music, writing, and film?
                 This talk will pose some of those questions, if not
                 attempt to answer them.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-156,
  pages =        "xvii--xvii(1)",
  year =         "2001",
  title =        "Are Points the Better Graphics Primitives?",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-156",
  author =       "M. Gross",
  abstract =     "Since the early days of graphics the computer based
                 representation of three-dimensional geometry has been
                 one of the core research fields. Today, various
                 sophisticated geometric modelling techniques including
                 NURBS or implicit surfaces allow the creation of 3D
                 graphics models with increasingly complex shape. In
                 spite of these methods the triangle has survived over
                 decades as the king of graphics primitives meeting the
                 right balance between descriptive power and
                 computational burden. As a consequence, today's
                 consumer graphics hardware is heavily tailored for high
                 performance triangle processing. In addition, a new
                 generation of geometry processing methods including
                 hierarchical representations, geometric filtering, or
                 feature detection fosters the concept of triangle
                 meshes for graphics modelling. Unlike triangles, points
                 have amazingly been neglected as a graphics primitive.
                 Although being included in APIs since many years, it is
                 only recently that point samples experience a
                 renaissance in computer graphics. Conceptually, points
                 provide a mere discretization of geometry without
                 explicit storage of topology. Thus, point samples
                 reduce the representation to the essentials needed for
                 rendering and enable us to generate highly optimized
                 object representations. Although the loss of topology
                 poses great challenges for graphics processing, the
                 latest generation of algorithms features high
                 performance rendering, point/pixel shading, anisotropic
                 texture mapping, and advanced signal processing of
                 point sampled geometry. This talk will give an overview
                 of how recent research results in the processing of
                 triangles and points are changing our traditional way
                 of thinking of surface representations in computer
                 graphics - and will discuss the question: Are Points
                 the Better Graphics Primitives?",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-157,
  pages =        "1--8",
  year =         "2001",
  title =        "Reflective Interaction in Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-157",
  author =       "J. Lane and V. Lalioti",
  abstract =     "The proliferation of projection-based systems in
                 recent years resulted in a variety of specialised
                 interaction techniques that make Virtual Environments a
                 better man machine interface. However, the success of
                 these techniques and metaphors is directly linked to
                 the interaction and tracking devices used in their
                 implementation. Users find that devices such as the
                 data glove, stylus or joystick can be expensive and
                 cumbersome especially for the inexperienced. A variety
                 of approaches exist that make use of computer vision
                 for tracking gestures or for achieving wireless
                 interaction. Typically these approaches involve the use
                 of a two- camera pair, or a stereoscopic camera. Our
                 approach uses only one camera and one or more
                 reflective surfaces, to effectively and accurately
                 calculate 3D information. The calibration time is
                 minimal and it allows for a very flexible positioning
                 of the camera and reflecting surfaces. Wireless
                 interaction and natural interaction metaphors in the
                 user's physical space can be created using our method.
                 The method can be combined easily and effectively with
                 projection-based systems as well as with standard and
                 stereoscopic monitors, or extended for the use in
                 augmented spaces. It is an inexpensive method that uses
                 commonly available hardware and therefore its
                 application areas as an interaction and tracking
                 device, include games and use of virtual environments
                 in education. In this paper, we describe the method and
                 its use as an interaction device in two applications,
                 and conclude with a discussion on its advantages and
                 limitations.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  booktitle =    "EG 2001 Proceedings",
  serires =      "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-159,
  pages =        "8--17",
  year =         "2001",
  title =        "{JAPE}: {A} Prototyping System for Collaborative
                 Virtual Environments",
  author =       "O. G. Staadt and M. N{\"{a}}f and E. Lamboray and S.
                 W{\"{u}}rmlin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-159",
  abstract =     "We present JAPE, a flexible prototyping system to
                 support the design of a new advanced collaborative
                 virtual environment. We describe the utilization of
                 different hard- and software components to quickly
                 build a flexible, yet powerful test bed for application
                 and algorithm development. These components include a
                 3-D rendering toolkit, live video acquisition, speech
                 transmission, and the control of tracking and
                 interaction devices. To facilitate the simultaneous
                 design of applications and algorithms that take
                 advantage of unique features of new collaborative
                 virtual environments, we provide the developer with a
                 flexible prototyping toolkit which emulates the
                 functionality of the final system. The applicability of
                 JAPE is demonstrated with several prototype
                 applications and algorithms.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-16,
  year =         "2001",
  title =        "Exploiting Eigenvalues of the Hessian Matrix for
                 Volume Decimation",
  author =       "Jiri Hladuvka and Andreas K{\"o}nig and Eduard
                 Gr{\"o}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-16",
  abstract =     "In recent years the Hessian matrix and its eigenvalues
                 became important in pattern recognition. Several
                 algorithms based on the information they provide have
                 been introduced. We recall the relationship between the
                 eigenvalues of Hessian matrix and the 2nd order edge
                 detection filter, show the usefulness of treating them
                 separately and exploit these facts to design a combined
                 threshold operation to generate sparse data sets.",
  editor =       "V. Skala",
  keywords =     "Volume Rendering, Sparse data, Hessian matrix,
                 Eigenvalues, Laplacian filter.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-160,
  pages =        "17--25",
  year =         "2001",
  title =        "Streaming of Complex 3{D} Scenes for Remote
                 Walkthroughs",
  author =       "E. Teler and D. Lischinski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-160",
  abstract =     "We describe a new 3D scene streaming approach for
                 remote walkthroughs. In a remote walkthrough, a user on
                 a client machine interactively navigates through a
                 scene that resides on a remote server. Our approach
                 allows a user to walk through a remote 3D scene,
                 without ever having to download the entire scene from
                 the server. Our algorithm achieves this by selectively
                 transmitting only small parts of the scene and lower
                 quality representations of objects, based on the user's
                 viewing parameters and the available connection
                 bandwidth. An online optimization algorithm selects
                 which object representations to send, based on the
                 integral of a benefit measure along the predicted path
                 of movement. The rendering quality at the client
                 depends on the available bandwidth, but practical
                 navigation of the scene is possible even when bandwidth
                 is low.",
  sieries =      "Computer Graphics Forum",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-161,
  pages =        "26--35",
  year =         "2001",
  title =        "Implementation and Complexity of the
                 Watershed-from-Markers Algorithm",
  author =       "P. Felkel and M. Bruckschwaiger and R. Wegenkittl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-161",
  abstract =     "The watershed algorithm belongs to classical
                 algorithms in mathematical morphology. Lotufo et al.1
                 published a principle of the watershed computation by
                 means of an iterative forest transform (IFT), which
                 computes a shortest path forest from given markers. The
                 algorithm itself was described for a 2D case (image)
                 without a detailed discussion of its computation and
                 memory demands for real datasets. As IFT cleverly
                 solves the problem of plateaus and as it gives precise
                 results when thin objects have to be segmented, it is
                 obvious to use this algorithm for 3D datasets taking in
                 mind the minimizing of a higher memory consumption for
                 the 3D case without loosing low asymptotical time
                 complexity of O(m + C) (and also the real computation
                 speed). The main goal of this paper is an
                 implementation of the IFT algorithm with a priority
                 queue with buckets and careful tuning of this
                 implementation to reach as minimal memory consumption
                 as possible. The paper presents five possible
                 modifications and methods of implementation of the IFT
                 algorithm. All presented implementations keep the time
                 complexity of the standard priority queue with buckets
                 but the best one minimizes the costly memory allocation
                 and needs only 19-45% of memory for typical 3D medical
                 imaging datasets. Memory saving was reached by an IFT
                 algorithm simplification, which stores more elements in
                 temporary structures but these elements are simpler and
                 thus need less memory.The best presented modification
                 allows segmentation of large 3D medical datasets (up to
                 512  512  680 voxels) with 12- or 16-bits per voxel
                 on currently available PC based workstations.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-162,
  pages =        "36--48",
  year =         "2001",
  title =        "3{D} Metamorphosis Between Different Types of
                 Geometric Models",
  author =       "D. E. Breen and S. Mauch and R. T. Whitaker and J
                 Mao",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-162",
  abstract =     "We present a powerful morphing technique based on
                 level set methods, that can be combined with a variety
                 of scan conversion/model processing techniques.
                 Bringing these techniques together creates a general
                 morphing approach that allows a user to morph a number
                 of geometric model types in a single animation. We have
                 developed techniques for converting several types of
                 geometric models (polygonal meshes, CSG models and MRI
                 scans) into distance volumes, the volumetric
                 representation required by our level set morphing
                 approach. The combination of these two capabilities
                 allows a user to create a morphing sequence regardless
                 of the model type of the source and target objects,
                 freeing him/her to use whatever model type is
                 appropriate for a particular animation.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-163,
  pages =        "49--57",
  year =         "2001",
  title =        "Rapid high quality compression of volume data for
                 visualization",
  author =       "N. Ky Giang and D. Saupe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-163",
  abstract =     "Volume data sets resulting from, e.g., computerized
                 tomography (CT) or magnetic resonance (MR) imaging
                 modalities require enormous storage capacity even at
                 moderate resolution levels. Such large files may
                 require compression for processing in CPU memory which,
                 however, comes at the cost of decoding times and some
                 loss in reconstruction quality with respect to the
                 original data. For many typical volume visualization
                 applications (rendering of volume slices, subvolumes of
                 interest, or isosurfaces) only a part of the volume
                 data needs to be decoded. Thus, efficient compression
                 techniques are needed that provide random access and
                 rapid decompression of arbitrary parts the volume data.
                 We propose a technique which is block based and
                 operates in the wavelet transformed domain. We report
                 performance results which compare favorably with
                 previously published methods yielding large
                 reconstruction quality gains from about 6 to 12 dB in
                 PSNR for a5123 -volume extracted from the Visible Human
                 data set. In terms of compression our algorithm
                 compressed the data 6 times as much as the previous
                 state-of-the-art block based coder for a given PSNR
                 quality.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-164,
  pages =        "57--66",
  year =         "2001",
  title =        "Fast simulation and rendering techniques for fluid
                 objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-164",
  author =       "A. Kunimatsu and T. Watanabe and H. Fujii and T. Saito
                 and K. Hiwada and T. Takahashi and H. Ueki",
  abstract =     "Movies with actions and light effects of fluid objects
                 are aesthetically pleasing and interesting. Until now,
                 the calculation costs of simulation and rendering of
                 fluid objects have been very high. Using a modern PC
                 system and appropriate methods, we achieved a time of
                 10-20 seconds per frame for this application. Our
                 system uses a full Navier-Stokes equation solver with
                 uniform Eulerian mesh, marching cube isosurface
                 techniques, Catmull-Clark subdivision surface
                 techniques, ray tracing techniques on each vertex and
                 conventional polygon base rendering by HW accelerator.
                 In this paper, we describe the components of our system
                 and the reasons for choosing them. By measuring CPU
                 times of each process for some movie scenes of fluid
                 objects, we evaluate this system. We consider what
                 factors are important for creating movies of fluid
                 objects with short TAT",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-165,
  pages =        "67--76",
  year =         "2001",
  title =        "Animation of Soap Bubble Dynamics, Cluster Formation
                 and Collision",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-165",
  author =       "R. Durikovic",
  abstract =     "What is happening when a soap bubble floats on the
                 air? How do bubbles coalesce to form beautiful
                 three-dimensional clusters? The physical-based model
                 and animation described herein provide the answers.
                 This paper deals with a complete computer simulation of
                 soap bubbles from a dynamic perspective, which should
                 prove to be of great interest to physicists and
                 mathematicians. We discuss the dynamic formation of
                 irregular bubble clusters and how to animate bubbles.
                 The resulting model takes into account surface tension,
                 film elasticity, and shape variations due to gravity
                 and external wind forces.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-166,
  pages =        "76--84",
  year =         "2001",
  title =        "Real-Time Cloud Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-166",
  author =       "M. J. Harris and A. Lastra",
  abstract =     "This paper presents a method for realistic real-time
                 rendering of clouds suitable for flight simulation and
                 games. It provides a cloud shading algorithm that
                 approximates multiple forward scattering in a
                 preprocess, and first order anisotropic scattering at
                 runtime. Impostors are used to accelerate cloud
                 rendering by exploiting frame-to-frame coherence in an
                 interactive flight simulation. Impostors are shown to
                 be particularly well suited to clouds, even in
                 circumstances under which they cannot be applied to the
                 rendering of polygonal geometry. The method allows
                 hundreds of clouds and hundreds of thousands of
                 particles to be rendered at high frame rates, and
                 improves interaction with clouds by reducing artifacts
                 introduced by direct particle rendering techniques.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-167,
  pages =        "85--94",
  year =         "2001",
  title =        "Horizon Map Capture",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-167",
  author =       "H. Rushmeier and L. Balmelli and F. Bernardini",
  abstract =     "We present a method for computing horizon maps from
                 captured images of a bumpy surface. 1Horizon maps
                 encode surface self-shadowing effects, and can be used
                 with bump or normals maps to realistically render
                 surfaces with small height perturbations. The method
                 does not rely on complete surface reconstruction, and
                 requires only eight captured images as input. In this
                 paper we discuss how shadow information is extrapolated
                 from the eight captured images to compute the horizon
                 map. Our implementation accounts for the noise and
                 uncertainties in physically acquired data.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-168,
  pages =        "95--104",
  year =         "2001",
  title =        "Texture Mapping with Hard Constraints",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-168",
  author =       "I. Eckstein and V. Surazhsky and C. Gotsman",
  abstract =     "We show how to continuously map a texture onto a 3D
                 triangle mesh when some of the mesh vertices are
                 constrained to have given (u, v) coordinates. This
                 problem arises frequently in interactive texture
                 mapping applications and, to the best of our knowledge,
                 a complete and efficient solution is not available. Our
                 techniques always guarantee a solution by introducing
                 extra (Steiner) vertices in the triangulation if
                 needed. We show how to apply our methods to texture
                 mapping in multi-resolution scenarios and image warping
                 and morphing.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-169,
  pages =        "105--113",
  year =         "2001",
  title =        "{PC}-based Real-time Texture Painting on Real World
                 Objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-169",
  author =       "Y. Iwakiri and T. Kaneko",
  abstract =     "The problem of texture mapping on real world objects
                 has attracted attention11,8 recently. A work by Lensch
                 et al.9 addressed the problem of locating a camera
                 position in the celestial sphere and then mapping the
                 acquired pictures on a real world object. The entire
                 process took a half hour to one hour to map 10 to 15
                 pictures. In this paper, we propose a new innovative
                 algorithm to speed up the texture mapping or painting
                 process in real-time. We built a PC-based system using
                 a commonly available video card with a geometry engine.
                 Mapping of a picture required about 20 seconds. It is
                 successful in giving an illusion to the operator to
                 paint a colorless real world object with a color
                 texture brush.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-17,
  year =         "2001",
  title =        "Image-Based Rendering and General Relativity",
  author =       "Daniel Kobras and Daniel Weiskopf and Hanns Ruder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-17",
  abstract =     "Imaged-based rendering is a well-known method in
                 computer graphics to achieve photo-realistic images. In
                 this paper we show how conventional image-based
                 rendering algorithms can be extended to visualize
                 general relativistic effects in a restricted class of
                 spacetimes. We propose a generalized aberration formula
                 in order to treat the visualization of special and
                 general relativistic effects on the same footing. In
                 this way, image-based general relativistic rendering
                 can be regarded as an extension of special relativistic
                 rendering. As an example, we present snapshots from the
                 viewpoint of an observer traveling at warp speed.",
  editor =       "V. Skala",
  keywords =     "General relativity, image-based rendering, scientific
                 visualization, warp speed.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-170,
  pages =        "114--122",
  year =         "2001",
  title =        "Drawing for Illustration and Annotation in 3{D}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-170",
  author =       "D. Bourguignon and M.-P. Cani and G. Drettakis",
  abstract =     "We present a system for sketching in 3D, which strives
                 to preserve the degree of expression, imagination, and
                 simplicity of use achieved by 2D drawing. Our system
                 directly uses user-drawn strokes to infer the sketches
                 representing the same scene from different viewpoints,
                 rather than attempting to reconstruct a 3D model. This
                 is achieved by interpreting strokes as indications of a
                 local surface silhouette or contour. Strokes thus
                 deform and disappear progressively as we move away from
                 the original viewpoint. They may be occluded by objects
                 indicated by other strokes, or, in contrast, be drawn
                 above such objects. The user draws on a plane which can
                 be positioned explicitly or relative to other objects
                 or strokes in the sketch. Our system is interactive,
                 since we use fast algorithms and graphics hardware for
                 rendering. We present applications to education,
                 design, architecture and fashion, where 3D sketches can
                 be used alone or as an annotation of an existing 3D
                 model.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-171,
  pages =        "123--131",
  year =         "2001",
  title =        "The Synthesis of Rock Textures in Chinese Landscape
                 Painting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-171",
  author =       "D.-L. Way and Z.-C. Shih",
  abstract =     "In Chinese landscape painting, rock textures portray
                 the orientation of mountains and contribute to the
                 atmosphere. Many landscape-painting skills are required
                 according to the type of rock. Landscape painting is
                 the major theme of Chinese painting. Over the
                 centuries, masters of Chinese landscape painting
                 developed various texture strokes. Hemp-fiber and
                 axe-cut are two major types of texture strokes. A
                 slightly sinuous and seemingly broken line, the
                 hemp-fiber stroke is used for describing the gentle
                 slopes of rock formations whereas the axe-cut stroke
                 best depicts hard, rocky surfaces. This paper presents
                 a novel method of synthesizing rock textures in Chinese
                 landscape painting, useful not only to artists who want
                 to paint interactively, but also in automated rendering
                 of natural scenes. The method proposed underwrites the
                 complete painting process after users have specified
                 only the contour and parameters.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "2001",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-172,
  title =        "Tour Into the Picture using a Vanishing Line and its
                 Extension to Panoramic Images",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "132--141",
  year =         "2001",
  author =       "Hyung Woo Kang and Soon Hyoung Pyo and Ken-ichi Anjyo
                 and Sung Yong Shin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-172",
  abstract =     "Tour into the picture (TIP) proposed by Horry et al.is
                 a method for generating a sequence of walk-through
                 images from a single reference picture (or image). By
                 navigating a 3D scene model constructed from the
                 picture, TIP produces convincing 3D effects. Assuming
                 that the picture has one vanishing point, they proposed
                 the scene modeling scheme called spidery mesh. However,
                 this scheme has to go through major modification when
                 the picture contains multiple vanishing points or does
                 not have any well-defined vanishing point. Moreover,
                 the spidery mesh is hard to generalize for other types
                 of images such as panoramic images. In this paper, we
                 propose a new scheme for TIP which is based on a single
                 vanishing line instead of a vanishing point. Based on
                 projective geometry, our scheme is simple and yet
                 general enough to address the problems faced with the
                 previous method. We also show that our scheme can be
                 naturally extended to a panoramic image.",
  keywords =     "Image-based modeling/rendering, projective geometry,
                 vanishing line, panoramic image",
  volume =       "20(3)",
}

@InCollection{EVL-2001-173,
  pages =        "142--152",
  year =         "2001",
  title =        "Perceptually Guided Corrective Splatting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-173",
  author =       "J{\"{o}}rg Haber and Karol Myszkowski and Hitoshi
                 Yamauchi and Hans-Peter Seidel",
  abstract =     "One of the basic difficulties with interactive
                 walkthroughs is the high quality rendering of object
                 surfaces with non-diffuse light scattering
                 characteristics. Since full ray tracing at interactive
                 rates is usually impossible, we render a precomputed
                 global illumination solution using graphics hardware
                 and use remaining computational power to correct the
                 appearance of non-diffuse objects on-the-fly. The
                 question arises, how to obtain the best image quality
                 as perceived by a human observer within a limited
                 amount of time for each frame. We address this problem
                 by enforcing corrective computation for those
                 non-diffuse objects that are selected using a
                 computational model of visual attention. We consider
                 both the saliency- and task-driven selection of those
                 objects and benefit from the fact that shading
                 artifacts of {"}unattended{"} objects are likely to
                 remain unnoticed. We use a hierarchical image-space
                 sampling scheme to control ray tracing and splat the
                 generated point samples. The resulting image converges
                 progressively to a ray traced solution if the viewing
                 parameters remain unchanged. Moreover, we use a sample
                 cache to enhance visual appearance if the time budget
                 for correction has been too low for some frame. We
                 check the validity of the cached samples using a novel
                 criterion suited for non-diffuse surfaces and reproject
                 valid samples into the current view.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-174,
  pages =        "153--164",
  year =         "2001",
  title =        "Interactive Rendering with Coherent Ray Tracing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-174",
  author =       "Ingo Wald and Philipp Slusallek and Carsten Benthin
                 and Markus Wagner",
  abstract =     "For almost two decades researchers have argued that
                 ray tracing will eventually become faster than the
                 rasterization technique that completely dominates
                 todays graphics hardware. However, this has not
                 happened yet. Ray tracing is still exclusively being
                 used for off-line rendering of photorealistic images
                 and it is commonly believed that ray tracing is simply
                 too costly to ever challenge rasterization-based
                 algorithms for interactive use. However, there is
                 hardly any scientific analysis that supports either
                 point of view. In particular there is no evidence of
                 where the crossover point might be, at which ray
                 tracing would eventually become faster, or if such a
                 point does exist at all. This paper provides several
                 contributions to this discussion: We first present a
                 highly optimized implementation of a ray tracer that
                 improves performance by more than an order of magnitude
                 compared to currently available ray tracers. The new
                 algorithm make better use of computational resources
                 such as caches and SIMD instructions and better
                 exploits image and object space coherence. Secondly, we
                 show that this software implementation can challenge
                 and even outperform high-end graphics hardware in
                 interactive rendering performance for complex
                 environments. We also provide an brief overview of the
                 benefits of ray tracing over rasterization algorithms
                 and point out the potential of interactive ray tracing
                 both in hardware and software.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-175,
  pages =        "165--173",
  year =         "2001",
  title =        "On-the-Fly Processing of Generalized Lumigraphs",
  author =       "Hartmut Schirmacher and Li Ming and Hans-Peter
                 Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-175",
  abstract =     "We introduce a flexible and powerful concept for
                 reconstructing arbitrary views from multiple source
                 images on the fly. Our approach is based on a Lumigraph
                 structure with per-pixel depth values, and generalizes
                 the classical two-plane parameterized light fields and
                 Lumigraphs. With our technique, it is possible to
                 render arbitrary views of time-varying, non-diffuse
                 scenes at interactive frame rates, and it allows using
                 any kind of sensor that yields images with dense depth
                 information. We demonstrate the flexibility and
                 efficiency of our approach through various examples.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-176,
  pages =        "174--183",
  year =         "2001",
  title =        "A Camera Engine for Computer Games: Managing the
                 Trade-Off Between Constraint Satisfaction and Frame
                 Coherence",
  author =       "Nicolas Halper and Ralf Helbing and Thomas
                 Strothotte",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-176",
  abstract =     "Many computer games treat the user in the {"}1st
                 person{"} and bind the camera to his or her view. More
                 sophistication in a game can be achieved by enabling
                 the camera to leave the users' viewpoint. This,
                 however, requires new methods for automatic, dynamic
                 camera control. In this paper we present methods and
                 tools for such camera control. We emphasize guiding
                 camera control by constraints; however, optimal
                 constraint satisfaction tends to lead to the camera
                 jumping around too much. Thus, we pay particular
                 attention to a trade-off between constraint
                 satisfaction and frame coherence. We present a new
                 algorithm for dynamic consideration of the visibility
                 of objects which are deemed to be important in a given
                 game context.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-177,
  pages =        "184--191",
  year =         "2001",
  title =        "Walk-Through Illustrations: Frame-Coherent Pen-and-Ink
                 Style in a Game Engine",
  author =       "Bert Freudenberg and Maic Masuch and Thomas
                 Strothotte",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-177",
  abstract =     "In this paper we show how a game engine designed to
                 generate photorealistic images can be extended to
                 produce non-photorealistic and hybrid renditions. We
                 introduce new hardware-based methods to accomplish
                 pen-and-ink illustrations. The combination of the
                 highly optimized processing of a game engine and the
                 use of hardware for NPR algorithms yields real-time
                 animation of pen-and-ink illustrations. The particular
                 advance of this method is that it yields the first
                 real-time, frame-coherent pen-and-ink animations which
                 maintain both tone and texture.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-178,
  pages =        "192--202",
  year =         "2001",
  title =        "A Dynamic Motion Control Technique for Human-like
                 Articulated Figures",
  author =       "Masaki Oshita and Akifumi Makinouchi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-178",
  abstract =     "This paper presents a dynamic motion control technique
                 for human-like articulated figures in a physically
                 based character animation system. This method controls
                 a figure such that the figure tracks input motion
                 specified by a user. When environmental physical input
                 such as an external force or a collision impulse are
                 applied to the figure, this method generates
                 dynamically changing motion in response to the physical
                 input. We have introduced comfort and balance control
                 to compute the angular acceleration of the figure's
                 joints. Our algorithm controls the several parts of a
                 human-like articulated figure separetely through the
                 minimum number of degrees-of-freedom. Using this
                 approach, our algorithm simulates realistic human
                 motions at efficient computational cost. Unlike
                 existing dynamic simulation systems, our method assumes
                 that input motion is already realistic, and is aimed at
                 dynamically changing the input motion in real-time only
                 when unexpected physical input is applied to the
                 figure. As such, our method works efficiently in the
                 framework of current computer games.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-179,
  pages =        "203--214",
  year =         "2001",
  title =        "Flexible Image-Based Photometric Reconstruction using
                 Virtual Light Sources",
  author =       "Simon Gibson and Toby Howard and Roger Hubbold",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-179",
  abstract =     "Photometric reconstruction is the process of
                 estimating the illumination and surface reflectance
                 properties of an environment, given a geometric model
                 of the scene and a set of photographs of its surfaces.
                 For mixed-reality applications, such data is required
                 if synthetic objects are to be correctly illuminated or
                 if synthetic light sources are to be used to re-light
                 the scene. Current methods of estimating such data are
                 limited in the practical situations in which they can
                 be applied, due to the fact that the geometric and
                 radiometric models of the scene which are provided by
                 the user must be complete, and that the position (and
                 in some cases, intensity) of the light sources must
                 also be specified a-priori. In this paper, a novel
                 algorithm is presented which overcomes these
                 constraints, and allows photometric data to be
                 reconstructed in less restricted situations. This is
                 achieved through the use of virtual light sources which
                 mimic the effect of direct illumination from unknown
                 luminaires, and indirect illumination reflected off
                 unknown geometry. The intensity of these virtual light
                 sources and the surface material properties are
                 estimated using an iterative algorithm which attempts
                 to match calculated radiance values to those observed
                 in photographs. Results are presented for both
                 synthetic and real scenes that show the quality of the
                 reconstructed data and its use in off-line
                 mixed-reality applications.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-18,
  year =         "2001",
  title =        "{B2LIC}: An Algorithm for Mapping Two Scalar Values on
                 Texture-Based Representations of Vector Fields",
  author =       "A. Sanna and B. Montrucchio and P. Montuschi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-18",
  abstract =     "Visualization of vector data produced from
                 applications such as computational fluid dynamics
                 (CFD), environmental sciences, and material engineering
                 is a challenging task. Texture-based methods reveal to
                 be effective, versatile, and suitable for a large
                 spectrum of applications since they allow to obtain
                 high resolution output textures where direction,
                 orientation, and magnitude of the flow can be
                 displayed. In this Paper we present a new method called
                 B^{2}LIC, which allows both to characterize and
                 visualize interesting structures in the flow and to map
                 two additional scalar values in output textures by
                 bumps, depressions, and shadows, leaving colors for
                 further information mapping. B^{2}LIC is the natural
                 and direct evolution and the improvement of the BLIC
                 (Bumped LIC) algorithm, which is able to map just one
                 scalar value by the bump mapping technique. Some
                 examples show how the proposed method can effectively
                 allow to map two additional scalar values such as
                 temperature, vorticity, pressure, and so on, adding new
                 and additional representation capabilities to dense
                 texture-based visualization methodologies.",
  editor =       "V. Skala",
  keywords =     "Scientific visualization, multivariate visualization
                 techniques, texture-based methods, LIC.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-180,
  pages =        "215--226",
  year =         "2001",
  title =        "Automatic Lighting Design using a Perceptual Quality
                 Metric",
  author =       "Ram Shacked and Dani Lischinski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-180",
  abstract =     "Lighting has a crucial impact on the appearance of 3D
                 objects and on the ability of an image to communicate
                 information about a 3D scene to a human observer. This
                 paper presents a new automatic lighting design approach
                 for comprehensible rendering of 3D objects. Given a
                 geometric model of a 3D object or scene, the material
                 properties of the surfaces in the model, and the
                 desired viewing parameters, our approach automatically
                 determines the values of various lighting parameters by
                 optimizing a perception-based image quality objective
                 function. This objective function is designed to
                 quantify the extent to which an image of a 3D scene
                 succeeds in communicating scene information, such as
                 the 3D shapes of the objects, fine geometric details,
                 and the spatial relationships between the objects. Our
                 results demonstrate that the proposed approach is an
                 effective lighting design tool, suitable for users
                 without expertise or knowledge in visual perception or
                 in lighting design.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-181,
  pages =        "227--238",
  year =         "2001",
  title =        "Rendering Pearlescent Appearance Based On
                 Paint-Composition Modelling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-181",
  author =       "Sergey Ershov and Konstantin Kolchin and Karol
                 Myszkowski",
  abstract =     "We describe a new approach to modelling pearlescent
                 paints based on decomposing paint layers into stacks of
                 imaginary thin sublayers. The sublayers are chosen so
                 thin that multiple scattering can be considered across
                 different sublayers, while it can be neglected within
                 each of the sublayers. Based on this assumption, an
                 efficient recursive procedure of assembling the layers
                 is developed, which enables to compute the paint BRDF
                 at interactive speeds. Since the proposed paint model
                 connects fundamental optical properties of multi-layer
                 pearlescent and metallic paints with their microscopic
                 structure, interactive prediction of the paint
                 appearance based on its composition becomes possible.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-182,
  title =        "Artist-Directed Inverse-Kinematics Using Radial Basis
                 Function Interpolation",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "239--250",
  year =         "2001",
  author =       "Charles F. Rose III and Peter-Pike J. Sloan and
                 Michael F. Cohen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-182",
  abstract =     "One of the most common tasks in computer animation is
                 inverse-kinematics, or determining a joint
                 configuration required to place a particular part of an
                 articulated character at a particular location in
                 global space. Inverse-kinematics is required at
                 design-time to assist artists using commercial 3D
                 animation packages, for motion capture analysis, and
                 for run-time applications such as games. We present an
                 efficient inverse-kinematics methodology based on the
                 interpolation of example motions and positions. The
                 technique is demonstrated on a number of
                 inverse-kinematics positioning tasks for a human
                 figure. In addition to simple positioning tasks, the
                 method provides complete motion sequences that satisfy
                 an inverse-kinematic goal. The interpolation at the
                 heart of the algorithm allows an artist's influence to
                 play a major role in ensuring that the system always
                 generates plausible results. Due to the lightweight
                 nature of the algorithm, we can position a character at
                 extremely high frame rates, making the technique useful
                 for time-critical run-time applications such as
                 games.",
  keywords =     "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
}

@InCollection{EVL-2001-183,
  pages =        "239--250",
  year =         "2001",
  title =        "Artist-Directed Inverse-Kinematics Using Radial Basis
                 Function Interpolation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-183",
  author =       "Charles F. Rose III and Peter-Pike J. Sloan and
                 Michael F. Cohen",
  abstract =     "One of the most common tasks in computer animation is
                 inverse-kinematics, or determining a joint
                 configuration required to place a particular part of an
                 articulated character at a particular location in
                 global space. Inverse-kinematics is required at
                 design-time to assist artists using commercial 3D
                 animation packages, for motion capture analysis, and
                 for run-time applications such as games. We present an
                 efficient inverse-kinematics methodology based on the
                 interpolation of example motions and positions. The
                 technique is demonstrated on a number of
                 inverse-kinematics positioning tasks for a human
                 figure. In addition to simple positioning tasks, the
                 method provides complete motion sequences that satisfy
                 an inverse-kinematic goal. The interpolation at the
                 heart of the algorithm allows an artist's influence to
                 play a major role in ensuring that the system always
                 generates plausible results. Due to the lightweight
                 nature of the algorithm, we can position a character at
                 extremely high frame rates, making the technique useful
                 for time-critical run-time applications such as
                 games.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-184,
  pages =        "251--259",
  year =         "2001",
  title =        "Interactive Computer Animation of Hand Gestures using
                 Status Estimation with Multiple Regression Analysis",
  author =       "Yoshifumi Kitamura and Tomohiko Higashi and Takayuki
                 Iida and Fumio Kishino",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-184",
  abstract =     "This paper presents a method of interactively
                 generating natural hand gesture animation using reduced
                 dimensionality from multiple captured data sequences of
                 finger motions conducting specific tasks. This method
                 is achieved by introducing an estimation with multiple
                 regression analysis. Even when the skeletal structure
                 of the user who inputs the motion is different from
                 that of the shape model in the computer, the motion
                 that a user imagines is generated. Experimental results
                 obtained from the interface applied to virtual object
                 manipulation showed that the proposed method generates
                 animation naturally, just as users would expect. This
                 method enables us to make input devices that require
                 minimal user training and computer calibration, and
                 helps to make the user interface intuitive and easy to
                 use.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-185,
  pages =        "260--267",
  year =         "2001",
  title =        "Fast Cloth Animation on Walking Avatars",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-185",
  author =       "T. Vassilev and B. Spanlang and Y. Chrysanthou",
  abstract =     "This paper describes a fast technique for animating
                 clothing on walking humans. It exploits a mass-spring
                 cloth model but applies a new velocity directional
                 modification approach to overcome its super-elasticity.
                 The algorithm for cloth-body collision detection and
                 response is based on image-space interference tests,
                 unlike the existing ones that use object-space checks.
                 The modern workstations' graphics hardware is used not
                 only to compute the depth maps of the body but also to
                 interpolate the body normal vectors and velocities of
                 each vertex. As a result the approach is very fast and
                 makes it possible to produce animation at a rate of
                 three to four frames per second.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-186,
  title =        "Incremental Updates for Rapid Glossy Global
                 Illumination",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "268--277",
  year =         "2001",
  author =       "Xavier Granier and George Drettakis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-186",
  abstract =     "We present an integrated global illumination algorithm
                 including non-diffuse light transport which can handle
                 complex scenes and enables rapid incremental updates.
                 We build on a unified algorithm which uses hierarchical
                 radiosity with clustering and particle tracing for
                 diffuse and non-diffuse transport respectively. We
                 present a new algorithm which chooses between
                 reconstructing specular effects such as caustics on the
                 diffuse radiosity mesh, or special purpose caustic
                 textures, when high frequencies are present. Algorithms
                 are presented to choose the resolution of these
                 textures and to reconstruct the high-frequency
                 non-diffuse lighting effects. We use a dynamic spatial
                 data structure to restrict the number of particles
                 re-emitted during the local modifications of the scene.
                 By combining this incremental particle trace with a
                 line-space hierarchy for incremental update of diffuse
                 illumination, we can locally modify complex scenes
                 rapidly. We also develop an algorithm which, by
                 permitting slight quality degradation during motion,
                 achieves quasi-interactive updates. We present an
                 implementation of our new method and its application to
                 indoors and outdoors scenes",
  volume =       "20(3)",
  keywords =     "Global Illumination, Rapid/Interactive Updates,
                 Radiosity, Particle Tracing, Textures, Reconstruction",
}

@InCollection{EVL-2001-187,
  pages =        "278--287",
  year =         "2001",
  title =        "An Adaptive Method for Indirect Illumination Using
                 Light Vectors",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-187",
  author =       "Xavier Serpaggi and Bernard P{\'{e}}roche",
  abstract =     "In computer graphics, several phenomema need to be
                 taken into account when it comes to the field of
                 photo-realism. One of the most relevant is obviously
                 the notion of global, and more precisely indirect,
                 illumination. In {"}classical{"} ray-tracing if you are
                 not under the light, then you are in a shadow. A great
                 amount of work has been carried out which proposes
                 ray-tracing based solutions to take into account the
                 fact that {"}there is a certain amount of light in
                 shadows{"}. All of these methods carry the same
                 weaknesses: high computation time and a lot of
                 parameters you need to manage to get something out of
                 the method. This paper proposes a generic computation
                 method of indirect illumination based on Monte Carlo
                 sampling and on the sequential analysis theory, which
                 is faster and more automatic than classical methods.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-188,
  title =        "Global Illumination as a Combination of Continuous
                 Random Walk and Finite-Element Based Iteration",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "288--298",
  year =         "2001",
  author =       "L{\'{a}}szl{\'{o}} Szirmay-Kalos and Ferenc Csonka and
                 Gy{\"{o}}rgy Antal",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-188",
  abstract =     "The paper introduces a global illumination method that
                 combines continuous and finite-element approaches,
                 preserving the speed of finite-element based iteration
                 and the accuracy of continuous random walks. The basic
                 idea is to decompose the radiance function to a
                 finite-element component that is only a rough estimate
                 and to a difference component that is obtained by
                 Monte-Carlo techniques. Iteration and random walk are
                 handled uniformly in the framework of stochastic
                 iteration. This uniform treatment allows the
                 finite-element component to be built up adaptively
                 aiming at minimizing the Monte-Carlo component. The
                 method is also suited for interactive walkthrough
                 animation in glossy scenes since when the viewpoint
                 changes, only the small Monte-Carlo component needs to
                 be recomputed.",
  volume =       "20(3)",
  keywords =     "Rendering equation, global radiance, Monte-Carlo Carlo
                 integration, light-tracing, global ray-bundle
                 tracing.",
}

@InCollection{EVL-2001-189,
  pages =        "299--308",
  year =         "2001",
  title =        "A low cost 3{D} scanner based on structured light",
  author =       "C. Rocchini and Paulo Cignoni and C. Montani and P.
                 Pingi and Roberto Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-189",
  abstract =     "Automatic 3D acquisition devices (often called 3D
                 scanners) allow to build highly accurate models of real
                 3D objects in a cost- and time-effective manner. We
                 have experimented this technology in a particular
                 application context: the acquisition of Cultural
                 Heritage artefacts. Specific needs of this domain are:
                 medium-high accuracy, easy of use, affordable cost of
                 the scanning device, self-registered acquisition of
                 shape and color data, and finally operational safety
                 for both the operator and the scanned artefacts.
                 According to these requirements, we designed a low-cost
                 3D scanner based on structured light which adopts a
                 new, versatile colored stripe pattern approach. We
                 present the scanner architecture, the software
                 technologies adopted, and the first results of its use
                 in a project regarding the 3D acquisition of an
                 archeological statue.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-19,
  year =         "2001",
  title =        "Hypertexturing Complex Volume Objects",
  author =       "R. A. Satherley and M. W. Jones",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-19",
  abstract =     "This paper will examine hypertexture rendering
                 techniques and will demonstrate how volume may be
                 adapted in order for hypertexture to be applied.
                 Details are given of a process for the conversion of
                 complex objects, such as CT scans, into accurate
                 distance fields. Hypertexture is applied to these
                 objects and example renderings include the UNC CT head,
                 a chess piece, a dodecahedron and a tank. Additional
                 information is given about soft objects, density
                 modulation functions, ray marching and controlling
                 hypertexture applications.",
  editor =       "V. Skala",
  keywords =     "Hypertexture, Distance Transform, Distance Field.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-190,
  title =        "Integrating Behavioural Animation Techniques",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "309--318",
  year =         "2001",
  author =       "Jean-S{\'{e}}bastien Monzani and Angela Caicedo and
                 Daniel Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-190",
  abstract =     "Our research focuses on animating autonomous virtual
                 humans which are able to take decisions by themselves.
                 We especially address in this paper the technical
                 problem of integrating altogether the physical
                 simulation of agents (represented as virtual humans in
                 a 3D environment) and their behaviours and motivations,
                 driven by a Beliefs, Desires and Intentions
                 architecture. We also explain how goals drive plans,
                 and how an agent can coherently handle concurrent
                 tasks.",
  volume =       "20(3)",
  keywords =     "Behavioural animation and planning, virtual humans
                 animation, inter-agents communication",
}

@InCollection{EVL-2001-191,
  pages =        "319--328",
  year =         "2001",
  title =        "A High Performance Solver for the Animation of
                 Deformable Objects using Advanced Numerical Methods",
  author =       "M. Hauth and O. Etzmuss",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-191",
  abstract =     "Physically based modelling of deformable objects has
                 become the most popular technique to model textiles,
                 skin or human tissue. The crucial problem in the
                 animation of deformable objects is the solution of the
                 resulting differential equations. Recently fast
                 solutions have been presented. In this work we will
                 first give a theoretical analysis and then exploit
                 special properties of the system and advanced numerical
                 techniques to achieve further speed-ups of the
                 simulations. Also, higher accuracy, leading to higher
                 quality animations, will be achieved and an error bound
                 is enforced",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-192,
  pages =        "329--338",
  year =         "2001",
  title =        "Modeling Dynamic Hair as a Continuum",
  author =       "Sunil Hadap and Nadia Magnenat-Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-192",
  abstract =     "In this paper we address the difficult problem of hair
                 dynamics, particularly hair-hair and hair-air
                 interactions. To model these interactions, we propose
                 to consider hair volume as a continuum. Subsequently,
                 we treat the interaction dynamics to be fluid dynamics.
                 This proves to be a strong as well as viable approach
                 for an otherwise very complex phenomenon. However, we
                 retain the individual character of hair, which is vital
                 to visually realistic rendering of hair animation. For
                 that, we develop an elaborate model for stiffness and
                 inertial dynamics of individual hair strand. Being a
                 reduced coordinate formulation, the stiffness dynamics
                 is numerically stable and fast. We then unify the
                 continuum interaction dynamics and the individual
                 hair's stiffness dynamics.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-193,
  pages =        "339--348",
  year =         "2001",
  title =        "Modeling Stochastic Dynamical Systems for Interactive
                 Simulation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-193",
  author =       "L. M. Reissell and Dinesh K. Pai",
  abstract =     "We present techniques for constructing approximate
                 stochastic models of complicated dynamical systems for
                 applications in interactive computer graphics. The
                 models are designed to produce realistic interaction at
                 low cost. We describe two kinds of stochastic models:
                 continuous state (ARX) models and discrete state
                 (Markov chains) models. System identi cation techniques
                 are used for learning the input-output dynamics
                 automatically, from either measurements of a real
                 system or from an accurate simulation. The synthesis of
                 behavior in this manner is several orders of magnitude
                 faster than physical simulation.We demonstrate the
                 techniques with two examples: (1) the dynamics of
                 candle ame in the wind, modeled using data from a real
                 candle and (2) the motion of a falling leaf, modeled
                 using data from a complex simulation. We have
                 implemented an interactive Java program which
                 demonstrates real-time interaction with a realistically
                 behaving simulation of a cartoon candle ame. The user
                 makes the ame animation icker by blowing into a
                 microphone.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-194,
  pages =        "349--358",
  year =         "2001",
  title =        "Adaptive Nonlinear Finite Elements for Deformable Body
                 Simulation Using Dynamic Progressive Meshe",
  author =       "Xunlei Wu and Michael S. Downes and Tolga Goktekin and
                 Frank Tendick",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-194",
  abstract =     "Realistic behavior of deformable objects is essential
                 for many applications such as simulation for surgical
                 training. Existing techniques of deformable modeling
                 for real time simulation have either used approximate
                 methods that are not physically accurate or linear
                 methods that do not produce reasonable global behavior.
                 Nonlinear finite element methods (FEM) are globally
                 accurate, but conventional FEM is not real time. In
                 this paper, we apply nonlinear FEM using mass lumping
                 to produce a diagonal mass matrix that allows real time
                 computation. Adaptive meshing is necessary to provide
                 sufficient detail where required while minimizing
                 unnecessary computation. We propose a scheme for mesh
                 adaptation based on an extension of the progressive
                 mesh concept, which we call dynamic progressive
                 meshes.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-195,
  pages =        "359--367",
  year =         "2001",
  title =        "Generalized Stochastic Sampling Method for
                 Visualization and Investigation of Implicit Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-195",
  author =       "Satoshi Tanaka and Akihiro Shibata and Hiroaki
                 Yamamoto and Hisakiyo Kotsuru",
  abstract =     "Recently we proposed the stochastic sampling method
                 (SSM), which can numerically generate sample points on
                 complicated implicit surfaces quickly and uniformly. In
                 this paper we generalize the method in two aspects: (1)
                 We introduce two kinds of boundary conditions, so that
                 we can sample a finite part of an open surface
                 spreading infinitely. (2) We generalize the stochastic
                 differential equation used in the SSM, so that its
                 solutions can satisfy plural constraint conditions
                 simultaneously. The first generalization enables us to
                 visualize cut views of open surfaces. The second
                 generalization enables us to visualize intersections of
                 static and moving implicit surfaces, which leads to
                 detailed investigation of intersections and other
                 interesting applications such as visualization of
                 contour maps.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-196,
  pages =        "368--376",
  year =         "2001",
  title =        "Mesh Optimization for Polygonized Isosurfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-196",
  author =       "Yutaka Ohtake and Alexander G. Belyaev",
  abstract =     "In this paper, we propose a method for improvement of
                 isosurface polygonizations. Given an initial
                 polygonization of an isosurface, we introduce a mesh
                 evolution process initialized by the polygonization.
                 The evolving mesh converges quickly to its limit mesh
                 which provides with a high quality approximation of the
                 isosurface even if the isosurface has sharp features,
                 boundary, complex topology. To analyze how close the
                 evolving mesh approaches its destined isosurface, we
                 introduce error estimators measuring the deviations of
                 the mesh vertices from the isosurface and mesh normals
                 from the isosurface normals. A new technique for mesh
                 editing with isosurfaces is also proposed. In
                 particular, it can be used for creating carving
                 effects.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-197,
  pages =        "377--384",
  year =         "2001",
  title =        "Visualization of Isosurfaces with Parametric Cubes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-197",
  author =       "B. Mora and J. P. Jessel and R. Caubet",
  abstract =     "To render images from volume datasets, an
                 interpolation method also called reconstruction is
                 needed. The level of details of the resultant image
                 closely depends on the filter used for reconstruction.
                 We propose here a new filter producing C1 continue
                 surfaces. The provided image quality is better than
                 current high-quality algorithms, like splatting or
                 trilinear raycasting, where tiny details are often
                 eliminated. In contrast with other studied high quality
                 filters that are practically unusable, our algorithm
                 has been implemented interactively on a modest platform
                 thanks to an efficient implementation using parametric
                 cubes. We also demonstrate the interest of a min-max
                 octree in the visualization of isosurfaces
                 interactively thresholded.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-198,
  pages =        "385--392",
  year =         "2001",
  title =        "Detection of Salient Curvature Features on Polygonal
                 Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-198",
  author =       "Kouki Watanabe and Alexander G. Belyaev",
  abstract =     "We develop an approach for stable detection of
                 perceptually salient curvature features on surfaces
                 approximated by dense triangle meshes. The approach
                 explores an {"}area degenerating{"} effect of the focal
                 surface near its singularities and combines together a
                 new approximations of the mean and Gaussian curvatures,
                 nonlinear averaging of curvature maps, histogram-based
                 curvature extrema filtering, and an image processing
                 skeletonization procedure adapted for triangular
                 meshes. Finally we use perceptually significant
                 curvature extrema triangles to enhance the
                 Garland-Heckbert mesh decimation method.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-199,
  pages =        "393--401",
  year =         "2001",
  title =        "Feature Sensitive Remeshing",
  author =       "J. Vorsatz and C. R{\"{o}}ssl and Leif P. Kobbelt and
                 Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-199",
  abstract =     "Remeshing artifacts are a fundamental problem when
                 converting a given geometry into a triangle mesh. We
                 propose a new remeshing technique that is sensitive to
                 features. First, the resolution of the mesh is
                 iteratively adapted by a global restructuring process
                 which additionally optimizes the connectivity. Then a
                 particle system approach evenly distributes the
                 vertices across the original geometry. To exactly find
                 the features we extend this relaxation procedure by an
                 effective mechanism to attract the vertices to feature
                 edges. The attracting force is imposed by means of a
                 hierarchical curvature field and does not require any
                 thresholding parameters to classify the features.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-2,
  year =         "2001",
  title =        "Coloured Visualization for Numerical Modelling",
  author =       "Rossella Cossu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-2",
  abstract =     "In this paper scalar and vector data are visualised by
                 suited colour scales based on perceptive and uniform
                 colour models. Using opportune colour scales, colour
                 information is created from the two-dimensional scalar
                 data computed at different time steps. Direction and
                 magnitude of computed vector data are represented
                 employing circular colour look-up tables (LKT). In a
                 scientific computing environment focused on analysis
                 and interpretation of physical phenomena, the coloured
                 visualisation of data generated by numerical
                 simulations represents a fundamental fashion of
                 knowledge. The colour, in fact, can help the researcher
                 to analyse and interpret information present in
                 computed data in a fast and immediate way. The colour
                 human perception is a complex process, which includes
                 physiological, psychophysical, psychological and
                 physical aspects. A colour model (colour space) is a
                 way adopted to represent and describe a colour using
                 three coordinates. We visualise the results obtained by
                 a finite difference method applied to the solution of
                 2D shallow water equations for the simulation of water
                 circulation in natural basin: the San Pablo Bay. We
                 show solutions of the two-dimensional shallow water
                 equations (SWEs), that is, solutions of quasi linear
                 hyperbolic partial differential equations, governing
                 the water circulation in a basin with spatial
                 dimensions significantly greater than the water
                 depth.",
  editor =       "V. Skala",
  keywords =     "Colour, colour models, scientific visualisation,
                 scalar data, vector data.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-20,
  year =         "2001",
  title =        "An Animation System for User Interface Agents",
  author =       "Marc Alexa and Uwe Berner and Michael Hellenschmidt
                 and Thomas Rieger",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-20",
  abstract =     "With the advent of software agents and assistants, the
                 concept of so called conversational user interfaces
                 evolved, incorporating natural language interaction,
                 dialogue management, and anthropomorphic
                 representations. Today's challenge is to build a
                 suitable visualization architecture for anthropomorphic
                 conversational user interfaces, and to design
                 believable and appropriate face-to-face interaction
                 imitating human attributes such as emotions. The system
                 is designed as an autonomous agent enabling easy
                 integration into a variety of scenarios. Architecture,
                 protocols, and graphical output are discussed.",
  editor =       "V. Skala",
  keywords =     "Agents, Anthropomorphic Conversational Interfaces,
                 Avatars, Facial Animation, KQML, XML.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-200,
  pages =        "402--410",
  year =         "2001",
  title =        "Resampling Feature and Blend Regions in Polygonal
                 Meshes for Surface Anti-Aliasing",
  author =       "Mario Botsch and Leif P. Kobbelt",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-200",
  abstract =     "Efficient surface reconstruction and reverse
                 engineering techniques are usually based on a polygonal
                 mesh representation of the geometry: the resulting
                 models emerge from piecewise linear interpolation of a
                 set of sample points. The quality of the reconstruction
                 not only depends on the number and density of the
                 sample points but also on their alignment to sharp and
                 rounded features of the original geometry. Bad
                 alignment can lead to severe alias artifacts. In this
                 paper we present a sampling pattern for feature and
                 blend regions which minimizes these alias errors. We
                 show how to improve the quality of a given polygonal
                 mesh model by resampling its feature and blend regions
                 within an interactive framework. We further demonstrate
                 sophisticated modeling operations that can be
                 implemented based on this resampling technique.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-201,
  pages =        "411--421",
  year =         "2001",
  title =        "Instant Visibility",
  author =       "Peter Wonka and Michael Wimmer and Fran{\c{c}}ois X.
                 Sillion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-201",
  abstract =     "We present an online occlusion culling system which
                 computes visibility in parallel to the rendering
                 pipeline. We show how to use point visibility
                 algorithms to quickly calculate a tight potentially
                 visible set (PVS) which is valid for several frames, by
                 shrinking the occluders used in visibility calculations
                 by an adequate amount. These visibility calculations
                 can be performed on a visibility server, possibly a
                 distinct computer communicating with the display host
                 over a local network. The resulting system essentially
                 combines the advantages of online visibility processing
                 and region-based visibility calculations, allowing
                 asynchronous processing of visibility and display
                 operations. We analyze two different types of
                 hardware-based point visibility algorithms and address
                 the problem of bounded calculation time which is the
                 basis for true real-time behavior. Our results show
                 reliable, sustained 60 Hz performance in a walkthrough
                 with an urban environment of nearly 2 million polygons,
                 and a terrain flyover.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-202,
  pages =        "422--430",
  year =         "2001",
  title =        "Ray Tracing Triangular {B}{\'{e}}zier Patches",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-202",
  author =       "S. H. Martin Roth and Patrick Diezi and Markus H.
                 Gross",
  abstract =     "We present a new approach to finding ray-patch
                 intersections with triangular Bernstein-B{\'{e}}zier
                 patches of arbitrary degree. This paper extends and
                 complements on the short presentation. Unlike a
                 previous approach which was based on a combination of
                 hierarchical subdivision and a Newton-like iteration
                 scheme, this work adapts the concept of B{\'{e}}zier
                 clipping to the triangular domain. The problem of
                 reporting wrong intersections, inherent to the original
                 B{\'{e}}zier clipping algorithm, is investigated and
                 opposed to the triangular case. It turns out that
                 reporting wrong hits is very improbable, even close to
                 impossible, in the triangular set-up. A combination of
                 B{\'{e}}zier clipping and a simple hierarchy of nested
                 bounding volumes offers a reliable and accurate
                 solution to the problem of ray tracing triangular
                 B{\'{e}}zier patches.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-204,
  pages =        "431--442",
  year =         "2001",
  title =        "Hoops: 3{D} Curves as Conservative Occluders for
                 Cell-Visibility",
  author =       "Pere Brunet and Isabel Navazo and Jarek Rossignac and
                 Carlos Saona-V{\'{a}}zquez",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-204",
  abstract =     "Most visibility culling algorithms require convexity
                 of occluders. Occluder synthesis algorithms attempt to
                 construct large convex occluders inside bulky
                 non-convex sets. Occluder fusion algorithms generate
                 convex occluders that are contained in the umbra cast
                 by a group of objects given an area light. In this
                 paper we prove that convexity requirements can be
                 shifted from the occluders to their umbra with no loss
                 of efficiency, and use this property to show how some
                 special non-planar, non-convex closed polylines that we
                 call {"}hoops{"} can be used to compute occlusion
                 efficiently for objects that have no large interior
                 convex sets and were thus rejected by previous
                 approaches.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-205,
  pages =        "443--451",
  year =         "2001",
  title =        "Real-Time Volume Deformation",
  author =       "R{\"{u}}diger Westermann and Christof Rezk-Salama",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-205",
  abstract =     "Real-time free-form deformation tools are primarily
                 based on surface or particle representations to allow
                 for interactive modification and fast rendering of
                 complex models. The efficient handling of volumetric
                 representations, however, is still a challenge and has
                 not yet been addressed sufficiently. Volumetric models,
                 on the other hand, form an important class of
                 representation in many applications. In this paper we
                 present a novel approach to the real-time deformation
                 of scalar volume data sets taking advantage of hardware
                 supported 3D texture mapping. In a prototype
                 implementation a modeling environment has been designed
                 that allows for interactive manipulation of arbitrary
                 parts of volumetric objects. In this way, any desired
                 shape can be modeled and used subsequently in various
                 applications. The underlying algorithms have wide
                 applicability and can be exploited effectively for
                 volume morphing and medical data processing.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-206,
  title =        "Fast Visualization of Object Contours by
                 Non-Photorealistic Volume Rendering",
  editor =       "A. Chalmers and T.-M. Rhyne",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
  pages =        "452--460",
  year =         "2001",
  author =       "Bal{\'{a}}zs Cs{\'{e}}bfalvi and Lukas Mroz and Helwig
                 Hauser and Andreas K{\"{o}}nig and Eduard
                 Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-206",
  abstract =     "In this paper we present a fast visualization
                 technique for volumetric data, which is based on a
                 recent non-photorealistic rendering technique. Our new
                 approach enables alternative insights into 3D data sets
                 (compared to traditional approaches such as direct
                 volume rendering or iso-surface rendering). Object
                 contours, which usually are characterized by locally
                 high gradient values, are visualized regardless of
                 their density values. Cumbersome tuning of transfer
                 functions, as usually needed for setting up DVR views
                 is avoided. Instead, a small number of parameters is
                 available to adjust the non-photorealistic display.
                 Based on the magnitude of local gradient information as
                 well as on the angle between viewing direction and
                 gradient vector, data values are mapped to visual
                 properties (color, opacity), which then are combined to
                 form the rendered image (MIP is proposed as the default
                 compositing stragtegy here). Due to the fast
                 implementation of this alternative rendering approach,
                 it is possible to interactively investigate the 3D
                 data, and quickly learn about internal structures.
                 Several further extensions of our new approach, such as
                 level lines are also presented in this paper.",
  volume =       "20(3)",
  keywords =     "Interactive volume rendering, non-photorealistic
                 rendering, shear-warp projection",
}

@InCollection{EVL-2001-207,
  pages =        "461--470",
  year =         "2001",
  title =        "Tensor Topology Tracking: {A} Visualization Method for
                 Time-Dependent 2{D} Symmetric Tensor Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-207",
  author =       "X. Tricoche and G. Scheuermann and Hans Hagen",
  abstract =     "Topological methods produce simple and meaningful
                 depictions of symmetric, second order two-dimensional
                 tensor fields. Extending previous work dealing with
                 vector fields, we propose here a scheme for the
                 visualization of time-dependent tensor fields. Basic
                 notions of unsteady tensor topology are discussed.
                 Topological changes - known as bifurcations - are
                 precisely detected and identified by our method which
                 permits an accurate tracking of degenerate points and
                 related structures.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-208,
  pages =        "471--479",
  year =         "2001",
  title =        "A Unified Subdivision Scheme for Polygonal Modeling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-208",
  author =       "J{\'{e}}r{\^{o}}me Maillot and Jos Stam",
  abstract =     "Subdivision rules have traditionally been designed to
                 generate smooth surfaces from polygonal meshes. In this
                 paper we propose to employ subdivision rules as a
                 polygonal modeling tool, specifically to add additional
                 level of detail to meshes. However, existing
                 subdivision schemes have several undesirable properties
                 making them ill suited for polygonal modeling. In this
                 paper we propose a general set of subdivision rules
                 which provides users with more control over the
                 subdivision process. Most existing subdivision schemes
                 are special cases. In particular, we provide
                 subdivision rules which blend approximating spline
                 based schemes with interpolatory ones. Also, we
                 generalize subdivision to allow any number of
                 refinements to be performed in a single step.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-209,
  pages =        "480--489",
  year =         "2001",
  title =        "Valence-Driven Connectivity Encoding for 3{D} Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-209",
  author =       "Pierre Alliez and Mathieu Desbrun",
  abstract =     "In this paper, we propose a valence-driven,
                 single-resolution encoding technique for lossless
                 compression of triangle mesh connectivity. Building
                 upon a valence-based approach pioneered by Touma and
                 Gotsman, we design a new valence-driven conquest for
                 arbitrary meshes that always guarantees smaller
                 compression rates than the original method.
                 Furthermore, we provide a novel theoretical entropy
                 study of our technique, hinting the optimality of the
                 valence-driven approach. Finally, we demonstrate the
                 practical efficiency of this approach (in agreement
                 with the theoretical prediction) on a series of test
                 meshes, resulting in the lowest compression ratios
                 published so far, for both irregular and regular
                 meshes, small or large.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-21,
  year =         "2001",
  title =        "Compound Media Streaming in Time",
  author =       "B. Feustel and T. C. Schmidt and D. Marpe and M.
                 Palkow and H. L. Cycon",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-21",
  abstract =     "The widespread availability of networked multimedia
                 potentials embedded in an infrastructure of qualitative
                 superior kind gives rise to new approaches in the areas
                 of teleteaching and internet presentation: The
                 distribution of professionally styled multimedia
                 streams has fallen in the realm of possibility. This
                 paper presents a prototype - both model and runtime
                 environment - of a time directed media system treating
                 any kind of presentational contribution as reusable
                 media object components. The plug-in free runtime
                 system is based on a database and allows for a flexible
                 support of static media types as well as for easy
                 extensions by streaming media servers. The technique of
                 pluggable subservers is used to include a real-time
                 video codec, based on a fast low complexity wavelet
                 transformation and an highly efficient lossless
                 precoding scheme using partitioning, aggregation, and
                 conditional coding (PACC). The JAVA implementation
                 enables real-time video streaming in CIF format on IP
                 connections with low bandwidth (= 200 kb/s).",
  editor =       "V. Skala",
  keywords =     "Synchronized Media, Multimedia Modeling, Video
                 Streaming.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-210,
  pages =        "490--499",
  year =         "2001",
  title =        "Coarse-to-fine surface simplification with geometric
                 guarantees",
  author =       "Jean-Daniel Boissonnat and Fr{\'{e}}d{\'{e}}ric
                 Cazals",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-210",
  abstract =     "Let PC be a 3D point cloud and {"}epsilon{"} be a
                 positive value called tolerance. We aim at constructing
                 a triangulated surface S based on a subset PCU of PC
                 such that all the points in PCL = PC\PCU are at
                 distance at most {"}epsilon{"} from a facet of S. (PCU
                 and PCL respectively stand for Point Cloud Used and
                 Point Cloud Left.) We call this problem simplification
                 with geometric guarantees. This paper presents a new
                 framework to simplify with geometric guarantees. The
                 approach relies on two main ingredients. First an
                 oracle providing information on the surface being
                 reconstructed even though the triangulated surface
                 itself has not been computed. Second, a reconstruction
                 algorithm providing incremental updates of the
                 reconstructed surface, as well as a fast
                 point-to-triangles distance computation. The oracle is
                 used to guess a subset of the point cloud from which a
                 triangulated surface is reconstructed. It relies on an
                 implicit surface the triangulated surface is an
                 approximation of, and is therefore available before the
                 triangle mesh. The point-to-triangles distance
                 computation and the local updates are then invoked to
                 insert new vertices until the tolerance is met. We also
                 present a detailed experimental study which shows the
                 efficiency of the simplification process both in terms
                 of simplification rate and running time. To the best of
                 our knowledge, this algorithm is the first one
                 performing coarse-to-fine surface simplification with
                 geometric guarantees.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-211,
  pages =        "500--510",
  year =         "2001",
  title =        "Accurate and Fast Proximity Queries Between Polyhedra
                 Using Convex Surface Decomposition",
  author =       "Stephan A. Ehmann and Ming C. Lin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-211",
  abstract =     "The need to perform fast and accurate proximity
                 queries arises frequently in physically-based modeling,
                 simulation, animation, real-time interaction within a
                 virtual environment, and game dynamics. The set of
                 proximity queries include intersection detection,
                 tolerance verification, exact and approximate minimum
                 distance computation, and (disjoint) contact
                 determination. Specialized data structures and
                 algorithms have often been designed to perform each
                 type of query separately. We present a unified approach
                 to perform any of these queries seamlessly for general,
                 rigid polyhedral objects with boundary representations
                 which are orientable 2-manifolds. The proposed method
                 involves a hierarchical data structure built upon a
                 surface decomposition of the models. Furthermore, the
                 incremental query algorithm takes advantage of
                 coherence between successive frames. It has been
                 applied to complex benchmarks and compares very
                 favorably with earlier algorithms and systems.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-212,
  pages =        "511--521",
  year =         "2001",
  title =        "Spatial Patches - {A} Primitive for 3{D} Model
                 Representation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-212",
  author =       "D. Ivanov and Ye Kuzmin",
  abstract =     "The commonly used solution for real-life 3D model
                 representation is polygonal spatially consistent
                 geometry, with texture, and, optionally, bump or
                 displacement maps attached. Although the idea of
                 displacement mapping is well known, there are just a
                 few approaches to its efficient implementation. In this
                 paper we develop a technique that allows for efficient
                 representation and rendering of 3D models by getting a
                 new angle on the displacement mapping concept. We
                 introduce a new primitive that is defined as the range
                 image of a small part of the model's surface;
                 therefore, it is called a spatial patch. The whole
                 model is just a collection of patches with no
                 connectivity information between them. Such a
                 representation can be directly acquired by 3D scanning
                 machinery, and stored in a compact uniform form. It
                 also allows for efficient visualization, which is the
                 major focus of this paper. Thus, we present the logical
                 structure of a rendering unit based on conventional
                 z-buffering, and discuss the involved algorithms in
                 detail. These algorithms benefit from modern features
                 of computing units for which we believe the proposed
                 technique can be used in a wide range of applications
                 dealing with real-life 3D data.",
  editor =       "A. Chalmers and T.-M. Rhyne",
  volume =       "20(3)",
  series =       "Computer Graphics Forum",
  booktitle =    "EG 2001 Proceedings",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-213,
  pages =        "5--11",
  year =         "2001",
  title =        "Antialiasing of Environment Maps",
  author =       "Andreas Schilling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-213",
  abstract =     "Environment maps, like texture maps or any other maps
                 consisting of discretely stored data have to be
                 properly filtered, if they are being resampled in the
                 process of rendering an image. For environment maps,
                 this is especially important, as the sampling rate is
                 subject to extreme changes due to the curvature of the
                 reflecting surfaces. However, for the same reason, the
                 antialiasing is especially difficult to perform, as the
                 sampling rate has to be determined for each pixel. We
                 introduce a method to perform this calculation and
                 determine the parameters for anisotropic filtering of
                 the environment map. Instead of the BRDF, we employ
                 roughness pyramids to account for the properties of the
                 reflecting surface. The principles that are used to
                 perform this antialiasing can be applied for
                 antialiasing reflected textures in general e.g. in ray
                 tracing.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(1)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-214,
  pages =        "13--26",
  year =         "2001",
  title =        "Displacement Mapping using Scan Conversion Hardware
                 Architectures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-214",
  author =       "Michael Doggett and Anders Kugler and Wolfgang
                 Stra{\ss{}}er",
  abstract =     "This paper presents a novel algorithm and
                 architectures for perspective correct displacement of
                 the surface geometry of a polygonal model using a
                 displacement map. This new displaced surface geometry
                 is passed onto a traditional rendering pipeline. The
                 algorithm uses a multiple pass approach in which the
                 geometry is displaced in the first pass and then the
                 displaced geometry is rendered. The significant
                 features of the algorithm are that the surface is
                 displaced after its triangle mesh is transformed into
                 screen space and that it uses only bi-linear
                 interpolation for calculating the displaced geometry
                 allowing a cheap incremental scan-line implementation.
                 A hardware architecture based on this algorithm is
                 presented along with possible alternative
                 implementations. The technique presented here allows
                 greater photorealism by using increased detail without
                 an increase in bandwidth for geometry or calculation
                 time for transformation",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(1)",
  keywords =     "Displacement mapping; graphics hardware; hardware
                 architectures; image synthesis",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-215,
  pages =        "27--34",
  year =         "2001",
  title =        "Visualizing Stars and Emission Nebulas",
  author =       "David R. Nadeau and Jon D. Genetti and Steve Napear
                 and Bernard Pailthorpe and Carter Emmart and Erik
                 Wesselak and Dennis Davidson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-215",
  abstract =     "We describe star and nebula visualization techniques
                 used to create a 3D volumetric visualization of the
                 Orion Nebula. The nebula's ionization layer is modeled
                 first as a surface model, derived from infrared and
                 visible light observations. The surface model is
                 imported into a volume scene graph-based visualization
                 system that uses procedural volume modeling to simulate
                 the nebula's emissive gas layers. Additional scene
                 graphs model proplyds and shock fronts within the
                 nebula. Stars are rendered using Gaussian spots that
                 are attenuated with distance. Finally, eighty-six
                 separate volumes are voxelized from these scene graphs,
                 then simultaneously volume rendered.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(1)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-216,
  pages =        "35--46",
  year =         "2001",
  title =        "A Recursive Subdivision Algorithm for Piecewise
                 Circular Spline",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-216",
  author =       "Ahmad H. Nasri and C. W. A. M. van Overveld and Brian
                 Wyvill",
  abstract =     "We present an algorithm for generating a piecewise G1
                 circular spline curve from an arbitrary given control
                 polygon. For every corner, a circular biarc is
                 generated with each piece being parameterized by its
                 arc length. This is the first subdivision scheme that
                 produces a piecewise biarc curve that can interpolate
                 an arbitrary set of points. It is easily adopted in a
                 recursive subdivision surface scheme to generate
                 surfaces with circular boundaries with pieces
                 parameterized by arc length, a property not previously
                 available. As an application, a modified version of
                 Doo-Sabin subdivision algorithm is outlined making it
                 possible to blend a subdivision surface with other
                 surfaces having circular boundaries such as
                 cylinders.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(1)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-217,
  pages =        "47--59",
  year =         "2001",
  title =        "Multiresolution for Algebraic Curves and Surfaces
                 using Wavelets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-217",
  author =       "Jordi Esteve and Pere Brunet and Alvar Vinacua",
  abstract =     "his paper describes a multiresolution method for
                 implicit curves and surfaces. The method is based on
                 wavelets, and is able to simplify the topology. The
                 implicit curves and surfaces are defined as the
                 zero-valued piece-wise algebraic isosurface of a
                 tensor-product uniform cubic B-spline. A wavelet
                 multiresolution method that deals with uniform cubic
                 B-splines on bounded domains is proposed. In order to
                 handle arbitrary domains the proposed algorithm
                 dynamically adds appropriate control points and deletes
                 them in the synthesis phase.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(1)",
  keywords =     "Geometric modeling; algebraic surfaces; wavelets;
                 multiresolution; topological simplification; conversion
                 algorithms",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-218,
  pages =        "67--80",
  year =         "2001",
  title =        "Adaptive Implicit Surface Polygonization Using
                 Marching Triangles",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-218",
  author =       "Samir Akkouche and Eric Galin",
  abstract =     "This paper presents several improvements to the
                 marching triangles algorithm for general implicit
                 surfaces. The original method generates equilateral
                 triangles of constant size almost everywhere on the
                 surface. We present several modifications to adapt the
                 size of the triangles to the curvature of the surface.
                 As cracks may arise in the resulting polygonization, we
                 propose a specific crack-closing method invoked at the
                 end of the mesh growing step. Eventually, we show that
                 the marching triangles can be used as an incremental
                 meshing technique in an interactive modeling
                 environment. In contrast to existing incremental
                 techniques based on spatial subdvision, no extra
                 data-structure is needed to incrementally edit skeletal
                 implicit surfaces, which saves both memory and
                 computation time.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  keywords =     "Implicit Surfaces; Incremental Meshing; Marching
                 Triangles; Polygonization",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-219,
  pages =        "81--91",
  year =         "2001",
  title =        "Fast and Controllable Simulation of the Shattering of
                 Brittle Objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-219",
  author =       "Jeffrey Smith and Andrew Witkin and David Baraff",
  abstract =     "We present a method for the rapid and controllable
                 simulation of the shattering of brittle objects under
                 impact. An object is represented as a set of point
                 masses connected by distance-preserving linear
                 constraints. This use of constraints, rather than stiff
                 springs, gains us a significant advantage in speed
                 while still retaining fine control over the fracturing
                 behavior. The forces exerted by these constraints
                 during impact are computed using Lagrange multipliers.
                 These constraint forces are then used to determine when
                 and where the object will break, and to calculate the
                 velocities of the newly created fragments. We present
                 the details of our technique together with examples
                 illustrating its use. An earlier version of this paper
                 was presented at Graphics Interface 2000, held in
                 Montreal, Canada.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  keywords =     "Physically based modeling; computer animation; impact;
                 brittle materials",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-22,
  year =         "2001",
  title =        "Development of an {XML} Web Based Motion Capture Data
                 Warehousing and Translation System for Collaborative
                 Animation Projects",
  author =       "Carlos R. Morales",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-22",
  abstract =     "While motion capture has been hailed as a way to
                 achieve extremely realistic animation in a cost
                 effective manner, many animators are reluctant to use
                 it or to intermix it with keyframing techniques in
                 situations where they are required to collaborate with
                 other production personnel due to the lack of
                 consistent software performance. The myriad of
                 different file formats and the performance of those
                 formats across different animation packages have
                 limited the penetration of this technology into the
                 production environment. The author details the
                 development of an ASP web based application which
                 archives segmented motion capture data into an
                 intermediate XML format and then allows the users of
                 the system to dynamically retrieve the data in standard
                 Biovision segmented data format (BVA), Lightwave 3D
                 motion dump format, and Adaptive Optics Associates
                 (AOA) motion capture format. The net result is a
                 dynamic web enabled systems that allows users of Avid
                 SoftImage 3D, Alias Wavefront Maya, Newteck LightWave
                 3D, and Kinetix 3D Studio Max to exchange segmented
                 motion capture data.",
  editor =       "V. Skala",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-220,
  year =         "2001",
  title =        "Triangle Strip Compression",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-220",
  author =       "Martin Isenburg",
  abstract =     "In this paper we introduce a simple and efficient
                 scheme for encoding the connectivity and the
                 stripification of a triangle mesh. Since generating a
                 good set of triangle strips is a hard problem, it is
                 desirable to do this just once and store the computed
                 strips with the triangle mesh. However, no previously
                 reported mesh encoding scheme is designed to include
                 triangle strip information into the compressed
                 representation. Our algorithm encodes the
                 stripification and the connectivity in an interwoven
                 fashion, that exploits the correlation existing between
                 the two.",
  chapter =      "91-101",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Mesh compression; connectivity coding; triangle
                 strips; triangle fans; stripification.",
  volume =       "20(2)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-221,
  pages =        "103--113",
  year =         "2001",
  title =        "Smoothing Normal Vectors on Discrete Surfaces While
                 Preserving Slope Discontinuities",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-221",
  author =       "Grit Th{\"{u}}rmer",
  abstract =     "A new method is proposed which smoothes normal vectors
                 over a discrete surface, preserving slope
                 discontinuities and small details. Assume an estimate
                 of the normal vector at each surface point is known and
                 these estimates are computed from small neighbourhoods
                 such that slope discontinuities and small details are
                 still reflected by these normals. To smooth these
                 normals, the normal vectors at points in a certain
                 neighbourhood are averaged. The size of the
                 neighbourhood considered for the smoothing at a point
                 is adapted according the local surface configuration.
                 The adaptation is performed, depending on the tangent
                 plane at the point considered as well as the angles
                 between the normals at neighbouring points and the
                 normal at the point in question.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  keywords =     "Discrete Normals; Discrete Shading; Discrete Surfaces;
                 Visualization",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-222,
  pages =        "115--122",
  year =         "2001",
  title =        "Constrained Fairing for Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-222",
  author =       "Xinguo Liu and Hujun Bao and PhengAnn Heng and
                 TienTsin Wong and Qunsheng Peng",
  abstract =     "n this paper, we present a novel fairing algorithm for
                 the removal of noise from uniform triangular meshes
                 without shrinkage and serious distortion. The key
                 feature of this algorithm is to keep all triangle
                 centers invariant at each smoothing step by including
                 some constraints in the energy minimization functional.
                 The constrained functional is then minimized
                 efficiently using an iterative method. Further, we
                 apply this smoothing technique to a multiresolution
                 representation to remove arbitrary levels of detail. A
                 volume-preserving decimation algorithm is presented to
                 generate the multiresolution representation. The
                 experimental results demonstrate the combined
                 algorithm's stability and efficiency.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Curves & surfaces; geometric modeling; level of
                 detail; algorithms; mesh generation",
  volume =       "20(2)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-223,
  pages =        "125--148",
  year =         "2001",
  title =        "Recent Advances in Volume Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-223",
  author =       "Ken Brodlie and Jason Wood",
  abstract =     "In the past few years, there have been key advances in
                 the three main approaches to the visualization of
                 volumetric data: isosurfacing, slicing and volume
                 rendering, which together make up the field of volume
                 visualization. n this survey paper we set the scene by
                 describing the fundamental techniques for each of these
                 approaches, using this to motivate the range of
                 advances which have evolved over the past few years. n
                 isosurfacing, we see how the original marching cubes
                 algorithm has matured, with improvements in robustness,
                 topological consistency, accuracy and performance. In
                 the performance area, we look in detail at
                 pre-processing steps which help identify data which
                 contributes to the particular isosurface required. In
                 slicing too, there are performance gains from
                 identifying active cells quickly. In volume rendering,
                 we describe the two main approaches of ray casting and
                 projection. Both approaches have evolved technically
                 over the past decade, and the holy grail of real-time
                 volume rendering has arguably been reached. The aim of
                 this review paper is to pull these developments
                 together in a coherent review of recent advances in
                 volume visualization.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  keywords =     "Scientific visualization; data visualization; volume
                 visualization; isosurfacing; slicing; volume
                 rendering",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-224,
  pages =        "149--162",
  year =         "2001",
  title =        "Adaptive Representation of Specular Light",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-224",
  author =       "Normand Bri{\`{e}}re and Pierre Poulin",
  abstract =     "Caustics produce beautiful and intriguing illumination
                 patterns. However, their complex behavior makes them
                 difficult to simulate accurately in all but the
                 simplest configurations. To capture their appearance,
                 we present an adaptive approach based upon light beams.
                 Exploiting the coherence between the light rays forming
                 a beam greatly reduces the number of samples required
                 for precise illumination reconstruction. The beams
                 characterize the light distribution due to interactions
                 with specular surfaces in 3D space. They thus allow for
                 the treatment of illumination within single-scattering
                 participating media. A hierarchical structure enclosing
                 the light beams possesses inherent properties to detect
                 efficiently all beams reaching any 3D point, to adapt
                 itself according to illumination effects in the final
                 image, and to reduce memory consumption via caching.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  keywords =     "Caustics; global illumination; ray tracing; wavefront;
                 beam; shaft",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-225,
  pages =        "161--171",
  year =         "2001",
  title =        "A Novel Approach for Delaunay 3{D} Reconstruction with
                 a Comparative Analysis in the Light of Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-225",
  author =       "L. Nonato and R. Minghim and M. C. F. Oliveira and G.
                 Tavares",
  abstract =     "This paper presents a novel algorithm for volumetric
                 reconstruction of objects from planar sections using
                 Delaunay triangulation, which solves the main problems
                 posed to models defined by reconstruction, particularly
                 from the viewpoint of producing meshes that are
                 suitable for interaction and simulation tasks. The
                 requirements for these applications are discussed here
                 and the results of the method are presented.
                 Additionally, it is compared to another commonly used
                 reconstruction algorithm based on Delaunay
                 triangulation, showing the advantages of the
                 reconstructions obtained by our technique.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(2)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-226,
  pages =        "179--189",
  year =         "2001",
  title =        "Control of Feature-point-driven Facial Animation Using
                 a Hypothetical Face",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-226",
  author =       "Ming-Shing Su and Ming-Tat Ko and Kuo-Young Cheng",
  abstract =     "A new approach to the generation of a
                 feature-point-driven facial animation is presented. In
                 the proposed approach, a hypothetical face is used to
                 control the animation of a face model. The hypothetical
                 face is constructed by connecting some predefined
                 facial feature points to create a net so that each
                 facet of the net is represented by a Coon's surface.
                 Deformation of the face model is controlled by changing
                 the shape of the hypothetical face, which is performed
                 by changing the locations of feature points and their
                 tangents. Experimental results show that this
                 hypothetical-face-based method can generate facial
                 expressions which are visually almost identical to
                 those of a real face.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Facial Animation; Feature-point-driven; Hypothetical
                 Face; Hypothetical Surface",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-227,
  pages =        "189--200",
  year =         "2001",
  title =        "Rendering Natural Waters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-227",
  author =       "Simon Premo&zcaron;e and Michael Ashikhmin",
  abstract =     "Creating and rendering realistic water is one of the
                 most daunting tasks in computer graphics. Realistic
                 rendering of water requires that the sunlight and
                 skylight illumination are correct, the water surface is
                 modeled accurately and that the light transport within
                 water body is properly handled. This paper describes a
                 method for wave generation on a water surface using a
                 physically-based approach. The wave generation uses
                 data from the oceanographical observations and it is
                 controlled by intuitive parameters such as wind speed
                 and wind direction. The optical behavior of the water
                 surfaces is complex but is well-described in the ocean
                 science literature. We present a simple and intuitive
                 light transport approach that is easy to use for many
                 different water types such as deep ocean water, muddy
                 coastal water, and fresh water bodies. We demonstrate
                 our model for a number of water and atmospheric
                 conditions.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Rendering Realistic Water; Illumination; Optical
                 Behavior",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-228,
  pages =        "201--211",
  year =         "2001",
  title =        "Guided Exploration with Dynamic Potential Fields: the
                 Cubical Path System",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-228",
  author =       "Steffi Beckhaus and Felix Ritter and Thomas
                 Strothotte",
  abstract =     "Exploring unknown models or scenes is a highly
                 interactive and dynamic process. Systems for automatic
                 presentation of models or scenes either require
                 cinematographic rules, direct human interaction,
                 framesets, or pre-calculation of paths to a known goal.
                 In this paper we present a system which can deal with
                 rapidly changing user interest in objects of a scene or
                 model as well as with dynamic models and changes of the
                 camera position introduced interactively by the user or
                 through cuts. We describe CubicalPath, a new potential
                 field-based camera control system that helps with the
                 exploration of virtual environments.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Automatic Camera Control; Dynamic Potential Fields;
                 Guided Exploration; Animation",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-229,
  pages =        "211--224",
  year =         "2001",
  title =        "{VV}isual Perception in Realistic Image Synthesis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-229",
  author =       "Ann McNamara",
  abstract =     "Realism is often a primary goal in computer graphics
                 imagery, and we strive to create images that are
                 perceptually indistinguishable from an actual scene.
                 Rendering systems can now closely approximate the
                 physical distribution of light in an environment.
                 However, physical accuracy does not guarantee that the
                 displayed images will have authentic visual appearance.
                 In recent years the emphasis in realistic image
                 synthesis has begun to shift from the simulation of
                 light in an environment to images that look as real as
                 the physical environment they portray. In other words
                 the computer image should be not only physically
                 correct but also perceptually equivalent to the scene
                 it represents. This implies aspects of the Human Visual
                 System (HVS) must be considered if realism is required.
                 Visual perception is employed in many different guises
                 in graphics to achieve authenticity. Certain aspects of
                 the visual system must be considered to identify the
                 perceptual effects that a realistic rendering system
                 must achieve in order to reproduce effectively a
                 similar visual response to a real scene. This paper
                 outlines the manner in which knowledge about visual
                 perception is increasingly appearing in
                 state-of-the-art realistic image synthesis. After a
                 brief overview of the HVS, this paper is organized into
                 four sections, each exploring the use of perception in
                 realistic image synthesis, each with slightly different
                 emphasis and application. First, Tone Mapping
                 Operators, which attempt to map the vast range of
                 computed radiance values to the limited range of
                 display values, are discussed. Then perception based
                 image quality metrics, which aim to compare images on a
                 perceptual rather than physical basis, are presented.
                 These metrics can be used to evaluate, validate and
                 compare imagery. Thirdly, perception driven rendering
                 algorithms are described. These algorithms focus on
                 embedding models of the HVS directly into global
                 illumination computations in order to improve their
                 efficiency. Finally, techniques for comparing computer
                 graphics imagery against the real world scenes they
                 represent are discussed.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InProceedings{EVL-2001-23,
  year =         "2001",
  title =        "A Browser User Interface for Digital Television",
  author =       "Juha Vierinen and Petri Vuorimaa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-23",
  abstract =     "This paper discusses a process of designing and
                 implementing a graphical user interface (GUI) for an
                 XML browser. The process consists of four steps: a) a
                 concept of a multimedia browser for television is
                 defined; b) the GUI requirements are defined; c) a
                 prototype is designed and tested with multimedia
                 authoring tools; and d) finally, the prototype is
                 implemented, which is done in Java, and integrated with
                 an existing XML browser. The result is a browser
                 application that can be run on digital television.",
  editor =       "V. Skalavi R",
  keywords =     "Java, XML, digital television, user interface
                 design.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-230,
  pages =        "225--244",
  year =         "2001",
  title =        "Interactive Display of Global Illumination Solutions
                 for Non-diffuse Environments - {A} Survey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-230",
  author =       "Wolfgang Heidrich",
  abstract =     "In recent years there has been a lot of work on
                 interactively displaying global illumination solutions
                 for non-diffuse environments. This is an extremely
                 active field of research, in which a lot of different
                 approaches have recently been proposed. In this survey,
                 we will discuss and compare these. This will hopefully
                 prepare the ground for systematically addressing the
                 open questions in the future.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(4)",
  keywords =     "Global illumination solutions; Non-diffuse
                 environments; Environment maps",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-231,
  pages =        "245--256",
  year =         "2001",
  title =        "Haptic Device Control - Will it Fit Standardized Input
                 Models?",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-231",
  author =       "D. B. Arnoldand A. M. Day and V. Jennings and A.
                 Courtenay and D. A. Duce",
  abstract =     "Over recent years a wide variety of interaction
                 devices involving haptic feedback have been brought to
                 the market, but they vary widely in terms of input
                 measures recorded. These range from one dimensional
                 input on a haptic feedback steering wheel to a six
                 degree of freedom position and orientation device and
                 further, to assemblies of such devices. On the surface
                 most of the variations can be accommodated logically
                 with standardized input models combining existing
                 logical input devices and haptic feedback processes as
                 acknowledgement/echos. However it is very uncertain
                 whether such a model can adequately model the system
                 requirements for effective haptic feedback. In this
                 paper we review the input models that have developed
                 over the past 20 years and ask {"}Is it the end of the
                 road for the conceptual model of input incorporated
                 into the early graphics standards and elaborated over
                 the years?{"} In addition, to highlight the problems of
                 implementation with haptic interaction, we describe a
                 typical application, the simulation of a collision with
                 a virtual wall.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-232,
  pages =        "257--269",
  year =         "2001",
  title =        "{CP3}: Robust, Output-sensitive Display of Convex
                 Polyhedra in Scanline Mode",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-232",
  author =       "Ella Barkan and Dan Gordon",
  abstract =     "A new technique is developed for displaying disjoint
                 convex polyhedra. The method has the following
                 properties: It is output-sensitive, displays the
                 objects in scanline mode, and it is naturally robust.
                 There is no complex data structure uniting the
                 different polyhedra, so dynamic insertions and
                 deletions are simple. Its robustnes is based on a novel
                 method of comparing depths by representative {"}axes{"}
                 of objects instead of surfaces. The method is based on
                 two extensions of the {"}critical-points{"} method for
                 polygon scan conversion: One extension allows the
                 efficient display of planar graphs in scanline mode,
                 and another extension is into the third dimension. Test
                 runs indicate that it compares extremely favorably with
                 other methods that operate in scanline mode, as well as
                 with standard software and hardware techniques of
                 medium-level workstations.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Convex Polyhedra; Scanline Mode; Robustness;
                 Output-sensitive; Critical Points; Scan Conversion;
                 Planar Graph",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-233,
  pages =        "271--281",
  year =         "2001",
  title =        "Rendering Silhouettes with Virtual Lights",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-233",
  author =       "Domingo Mart{\'{i}}n and Juan Carlos Torres",
  abstract =     "We present a new method for obtaining
                 non-photorealistic images. These images have two main
                 visual components: silhouettes and non-realistic
                 colouring. Silhouettes are lines that define the form
                 of an object. They are used in classical animation and
                 illustration as the main expressive components. In
                 these applications, if it is necessary, colouring can
                 be added once the drawings are made. For instance,
                 generally, in illustration, colouring is flat and does
                 not transmit volume information whilst silhouettes do
                 it in an economical way. The proposed method is based
                 on the Virtual Lights model, which allows us to use
                 external components, the virtual lights, to define
                 silhouettes. In this way, the designer is free to
                 control where, when and how the silhouettes must
                 appear. The method can be used with B-rep geometric
                 models.",
  editor =       "D. Duke and R. Scopigno",
  volume =       "20(4)",
  keywords =     "Non-photorealistic rendering; silhouettes",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-234,
  pages =        "283--293",
  year =         "2001",
  title =        "Visibility Driven Rasterization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-234",
  author =       "M. Mei{\ss{}}ner and D. Bartz and R. G{\"{u}}nther and
                 W. Stra{\ss{}}er",
  abstract =     "We present a new visibility driven rasterization
                 scheme that significantly increases the rendering
                 performance of modern graphic subsystems. Instead of
                 rasterizing, texturing, lighting, and depth-testing
                 each individual pixel, we introduce a two-level
                 visibility mask within the rasterization stage which
                 facilitates the removal of groups of pixels and
                 triangles from rasterization and subsequent pipeline
                 stages. Local visibility information is stored within
                 the visibility mask that is updated several times
                 during the generation of a frame. The update can easily
                 be accomplished by extending already available (in
                 hardware) occlusion culling mechanisms (i.e. those of
                 HP and SGI), where it is possible to integrate the
                 additional functionality without any additional delay
                 cycles. In addition to these existing hardware based
                 occlusion culling approaches-which cull only geometry
                 contained in bounding volumes determined as occluded-we
                 are able to significantly accelerate the rendering of
                 the geometry determined as visible. However, our
                 approach does not specifically rely on such occlusion
                 culling hardware.The proposed new rasterization scheme
                 is well suited for hardware implementation, can easily
                 be integrated into low-cost rasterizers, and its
                 scalability can vary upon available chip real estate.
                 Only incremental modifications of modern graphics
                 subsystems are required to achieve a significant
                 improvement in rendering performance.",
  editor =       "D. Duke and R. Scopigno",
  keywords =     "Graphics hardware; rasterization; visibility and
                 occlusion culling",
  volume =       "20(4)",
  booktitle =    "Computer Graphics Forum",
  publisher =    "Blackwell Publishing",
}

@InCollection{EVL-2001-235,
  pages =        "1--9",
  year =         "2001",
  title =        "Smooth patching of refined triangulations",
  author =       "J{\"o}rg Peters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-235",
  abstract =     "This paper presents a simple algorithm for associating
                 a smooth, low-degree polynomial surface with
                 triangulations whose extraordinary mesh nodes are
                 separated by sufficiently many ordinary, 6-valent mesh
                 nodes. Output surfaces are at least tangent continuous
                 and are C2 sufficiently far away from extraordinary
                 mesh nodes; they consist of three-sided B{\'{e}}zier
                 patches of degree 4. In particular, the algorithm can
                 be used to skin a mesh generated by a few steps of
                 Loop's generalization of three-direction box-spline
                 subdivision.",
  volume =       "20(1)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-236,
  pages =        "10--38",
  year =         "2001",
  title =        "3{D} {RGB} image compression for interactive
                 applications",
  author =       "Chandrajit Bajaj and Insung Ihm and Sanghun Park",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-236",
  abstract =     "This paper presents a new 3D RGB image compression
                 scheme designed for interactive real-time applications.
                 In designing our compression method, we have
                 compromised between two important goals: high
                 compression ratio and fast random access ability, and
                 have tried to minimize the overhead caused during
                 run-time reconstruction. Our compression technique is
                 suitable for applications wherein data are accessed in
                 a somewhat unpredictable fashion, and real-time
                 performance of decompression is necessary. The
                 experimental results on three different kinds of 3D
                 images from medical imaging, image-based rendering, and
                 solid texture mapping suggest that the compression
                 method can be used effectively in developing real-time
                 applications that must handle large volume data, made
                 of color samples taken in three- or higher-dimensional
                 space.",
  volume =       "20(1)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-237,
  pages =        "39--65",
  year =         "2001",
  title =        "Spatiotemporal sensitivity and visual attention for
                 efficient rendering of dynamic environments",
  author =       "Hector Yee and Sumanita Pattanaik and Donald P.
                 Greenberg",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-237",
  abstract =     "We present a method to accelerate global illumination
                 computation in prerendered animations by taking
                 advantage of limitations of the human visual system. A
                 spatiotemporal error tolerance map, constructed from
                 psychophysical data based on velocity dependent
                 contrast sensitivity, is used to accelerate rendering.
                 The error map is augmented by a model of visual
                 attention in order to account for the tracking behavior
                 of the eye. Perceptual acceleration combined with good
                 sampling protocols provide a global illumination
                 solution feasible for use in animation. Results
                 indicate an order of magnitude improvement in
                 computational speed.",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-238,
  year =         "2001",
  title =        "Parameterization and Reconstruction from 3{D}
                 Scattered Points Based on Neural Network and {PDE}
                 Techniques",
  author =       "J. Barhak and A. Fischer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-238",
  abstract =     "Reverse engineering ordinarily uses laser scanners
                 since they can sample 3D data quickly and accurately
                 relative to other systems. These laser scanner systems,
                 however, yield an enormous amount of irregular and
                 scattered digitized point data that requires intensive
                 reconstruction processing. Reconstruction of freeform
                 objects consists of two main stages: 1)
                 parameterization and 2) surface fitting. Selection of
                 an appropriate parameterization is essential for
                 topology reconstruction as well as surface fitness.
                 Current parameterization methods have topological
                 problems that lead to undesired surface fitting
                 results, such as noisy self-intersecting surfaces. Such
                 problems are particularly common with concave shapes
                 whose parametric grid is self-intersecting, resulting
                 in a fitted surface that considerably twists and
                 changes its original shape. In such cases, other
                 parameterization approaches should be used in order to
                 guarantee non-self-intersecting behavior. The
                 parameterization method described in this paper is
                 based on two stages: 1) 2D initial parameterization and
                 2) 3D adaptive parameterization. Two methods were
                 developed for the first stage: Partial Differential
                 Equation (PDE) parameterization and neural network Self
                 Organizing Maps (SOM) parameterization. PDE
                 parameterization yields a parametric grid without
                 self-intersections. Neural network SOM parameterization
                 creates a grid where all the sampled points, not only
                 the boundary points, affect the grid, leading to a
                 uniform and smooth surface. In the second stage, a 3D
                 base surface was created and then adaptively modified.
                 To this end, the Gradient Descent Algorithm (GDA) and
                 Random Surface Error Correction (RSEC), both of which
                 are iterative surface fitting methods, were developed
                 and implemented. The feasibility of the developed
                 parameterization methods and fitting algorithms is
                 demonstrated on several examples using sculptured free
                 objects.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-239,
  pages =        "17--31",
  year =         "2001",
  title =        "On Simulated Annealing and the Construction of Linear
                 Spline Approximations for Scattered Data",
  author =       "Oliver Kreylos and Bernd Hamann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-239",
  abstract =     "We describe a method to create optimal linear spline
                 approximations to arbitrary functions of one or two
                 variables, given as scattered data without known
                 connectivity. We start with an initial approximation
                 consisting of a fixed number of vertices and improve
                 this approximation by choosing different vertices,
                 governed by a simulated annealing algorithm. In the
                 case of one variable, the approximation is defined by
                 line segments; in the case of two variables, the
                 vertices are connected to define a Delaunay
                 triangulation of the selected subset of sites in the
                 plane. In a second version of this algorithm,
                 specifically designed for the bivariate case, we choose
                 vertex sets and also change the triangulation to
                 achieve both optimal vertex placement and optimal
                 triangulation. We then create a hierarchy of linear
                 spline approximations, each one being a superset of all
                 lower-resolution ones.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-24,
  year =         "2001",
  title =        "Component-Based Architectures for Computer Vision
                 Systems",
  author =       "Aris Economopoulos and Drakoulis Martakos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-24",
  abstract =     "Research performed in the field of computer vision has
                 steadily ignored recent advances in programming tools
                 and techniques, relying on well-established traditional
                 methods, such as Unix-based C programming. While this
                 can certainly be effective, modern computer vision
                 research may benefit significantly from the new tools
                 and technologies that have recently become available.
                 This paper addresses the use of component-based
                 programming methods and proposes a model loosely based
                 on 3-tier architectures, for the creation of robust and
                 reusable computer vision systems, in order to improve
                 code modularity and reusability, and to ultimately
                 foster cooperation between researchers in the field. It
                 outlines a basic design strategy and exposes the
                 benefits and drawbacks of migrating to component-based
                 code. The model is used to build a component-driven
                 framework that is designed based on the principles of
                 3-tier applications. Its purpose is to aid in the
                 creation and maintenance of stable, dependable testing
                 and development environments. We have listed the main
                 advantages of this approach and have concluded that
                 although the learning curve for the programming skills
                 required is steep, the benefits to be reaped are worth
                 it.",
  editor =       "V. Skala",
  keywords =     "Computer vision, applications, components,
                 development, testing, cooperation, COM, MTS.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-240,
  pages =        "32--46",
  year =         "2001",
  title =        "Fast Isosurface Generation Using the Volume Thinning
                 Algorithm",
  author =       "Takayuki Itoh and Yasushi Yamaguchi and Koji
                 Koyamada",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-240",
  abstract =     "One of the most effective techniques for developing
                 efficient isosurfacing algorithms is the reduction of
                 visits to nonisosurface cells. Recent algorithms have
                 drastically reduced the unnecessary cost of visiting
                 nonisosurface cells. The experimental results show
                 almost optimal performance in their isosurfacing
                 processes. However, most of them have a bottleneck in
                 that they require more than $O(n)$ computation time for
                 their preprocessing, where $n$ denotes the total number
                 of cells. In this paper, we propose an efficient
                 isosurfacing technique, which can be applied to
                 unstructured as well as structured volumes and which
                 does not require more than $O(n)$ computation time for
                 its preprocessing. A preprocessing step generates an
                 extrema skeleton, which consists of cells and connects
                 all extremum points, by the volume thinning algorithm.
                 All disjoint parts of every isosurface intersect at
                 least one cell in the extrema skeleton. Our
                 implementation generates isosurfaces by searching for
                 isosurface cells in the extrema skeleton and then
                 recursively visiting their adjacent isosurface cells,
                 while it skips most of the nonisosurface cells. The
                 computation time of the preprocessing is estimated as
                 $O(n)$. The computation time of the isosurfacing
                 process is estimated as $O(n^{1/3} m + k)$, where $k$
                 denotes the number of isosurface cells and $m$ denotes
                 the number of extremum points since the number of cells
                 in an extrema skeleton is estimated as $O(n^{1/3}
                 m)$.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-241,
  pages =        "47--61",
  year =         "2001",
  title =        "'Meshsweeper': Dynamic Point-to-Polygonal-Mesh
                 Distance and Applications",
  author =       "Andr{\'{e}} Gu{\'{e}}ziec",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-241",
  abstract =     "We introduce a new algorithm for computing the
                 distance from a point to an arbitrary polygonal mesh.
                 Our algorithm uses a multiresolution hierarchy of
                 bounding volumes generated by geometric simplification.
                 Our algorithm is dynamic, exploiting coherence between
                 subsequent queries using a priority process and
                 achieving constant time queries in some cases. It can
                 be applied to meshes that transform rigidly or deform
                 nonrigidly. We illustrate our algorithm with a
                 simulation of particle dynamics and collisions with a
                 deformable mesh, the computation of distance maps and
                 offset surfaces, the computation of an approximation to
                 the expensive Hausdorff distance between two shapes,
                 and the detection of self-intersections. We also report
                 comparison results between our algorithm and an
                 alternative algorithm using an octree, upon which our
                 method permits an order-of-magnitude speed-up.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-242,
  pages =        "62--69",
  year =         "2001",
  title =        "Terrain Decimation through Quadtree Morphing",
  author =       "David Cline and Parris K. Egbert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-242",
  abstract =     "We present a new terrain decimation technique called a
                 Quadtree Morph, or Q-morph. The new approach eliminates
                 the usual popping artifacts associated with polygon
                 reduction, replacing them with less objectionable
                 smooth morphing. We show that Q-morphing is fast enough
                 to create a view-dependent terrain model for each frame
                 in an interactive environment. In contrast to most
                 Geomorph algorithms, Q-morphing does not use a time
                 step to interpolate between geometric configurations.
                 Instead, the geometry motion in a Q-morph is based
                 solely on the position of the viewer.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-243,
  pages =        "70--75",
  year =         "2001",
  title =        "A Graphical Representation of the State Spaces of
                 Hierarchical Level-of-Detail Scene Descriptions",
  author =       "Ashton E. W. Mason and Edwin H. Blake Ashton E. W.
                 Maso and Edwin H. Blake",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-243",
  abstract =     "We present a new graphical representation of the
                 level-of-detail state spaces generated by hierarchical
                 geometric scene descriptions with multiple levels of
                 detail. These level-of-detail graphs permit the
                 analytical investigation of the hierarchical
                 level-of-detail optimization problem that arises for
                 such descriptions. As an example of their use, we prove
                 the equivalence of two hierarchical level-of-detail
                 algorithms.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-244,
  pages =        "76--93",
  year =         "2001",
  title =        "Nonlinear Multiresolution Techniques with Applications
                 to Scientific Visualization in a Haptic Environment",
  author =       "Mohammad Waqas Asghar and Kenneth E. Barner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-244",
  abstract =     "This paper develops nonlinear multiresolution
                 techniques for scientific visualization utilizing
                 haptic methods. The visualization of data is critical
                 to many areas of scientific pursuit. Scientific
                 visualization is generally accomplished through
                 computer graphic techniques. Recent advances in haptic
                 technologies allow visual techniques to be augmented
                 with haptic methods. The kinesthetic feedback provided
                 through haptic techniques provides a second modality
                 for visualization and allows for active exploration.
                 Moreover, haptic methods can be utilized by individuals
                 with visual impairments. Haptic representations of
                 large data sets, however, can be confusing to a user,
                 especially if a visual representation is not available
                 or cannot be used. Additionally, most haptic devices
                 utilize point interactions, resulting in a low
                 information bandwidth and further complicating data
                 exploration. Multiresolution techniques can be utilized
                 to address the issues of low information bandwidth and
                 data complexity. Commonly used multiresolution
                 techniques are based on the wavelet decomposition. Such
                 linear techniques, however, tend to smooth important
                 data features, such as discontinuities or edges. In
                 contrast, nonlinear techniques can be utilized that
                 preserve edge structures while removing fine data
                 details. This paper develops a multiresolution data
                 decomposition method based on the affine median filter.
                 This results in a hybrid structure that can be tuned to
                 yield a decomposition that varies from a linear wavelet
                 decomposition to that produced by the median filter.
                 The performance of this hybrid structure is analyzed
                 utilizing deterministic signals and statistically in
                 the frequency domain. This analysis and qualitative and
                 quantitative implementation results show that the
                 affine median decomposition has advantages over
                 previously proposed methods. In addition to
                 multiresolution decomposition development, analysis,
                 and results, haptic implementation methods are
                 presented.",
  volume =       "7(1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-245,
  pages =        "97--108",
  year =         "2001",
  title =        "A Vectorial Algorithm for Tracing Discrete Straight
                 Lines in {N}-Dimensional Generalized Grids",
  author =       "Luis Ib{\'{a}}{\~{n}}ez and Chafia{\^{a}} Hamitouche
                 and Christian Roux",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-245",
  abstract =     "This paper presents an algorithm to trace discrete
                 straight lines in regular grids of any dimension. Most
                 known line tracing algorithms have been developed in
                 ${\hbox{\rlap{Z}\kern 2.0pt{\hbox{Z}}}}^{2}$ and
                 ${\hbox{\rlap{Z}\kern 2.0pt{\hbox{Z}}}}^{3}$ orthogonal
                 grids. The contribution of this paper is the definition
                 of a method to trace lines in nonorthogonal grids in
                 any dimension. This method is not restricted to being
                 used with a specific grid connectivity as other
                 widespread methods are. Good performance can be
                 achieved because only additions are used during line
                 tracing.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphicstion and Computer Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-246,
  pages =        "109--119",
  year =         "2001",
  title =        "Interactive Direct Rendering of Trivariate {B}-Spline
                 Scalar Functions",
  author =       "Alon Raviv and Gershon Elber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-246",
  abstract =     "This paper presents a direct rendering paradigm of
                 trivariate B-spline functions that is able to
                 incrementally update complex volumetric data sets in
                 the order of millions of coefficients at interactive
                 rates of several frames per second on modern
                 workstations. This incremental rendering scheme can
                 hence be employed in modeling sessions of volumetric
                 trivariate functions, offering interactive volumetric
                 sculpting capabilities. The rendering is conducted from
                 a fixed viewpoint and in two phases. The first,
                 preprocessing, stage accumulates the effect that the
                 coefficients of the trivariate function have on the
                 pixels in the image. This preprocessing stage is
                 conducted offline and only once per trivariate and
                 viewing direction. The second stage conducts the actual
                 rendering of the trivariate functions. As an example,
                 during a volumetric sculpting operation, the artist can
                 sculpt the volume and get a displayed feedback, in
                 interactive rates.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-247,
  pages =        "120--135",
  year =         "2001",
  title =        "Texture Mixing and Texture Movie Synthesis Using
                 Statistical Learning",
  author =       "Ziv Bar-Joseph and Ran El-Yaniv and Dani Lischinski
                 and Michael Werman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-247",
  abstract =     "We present an algorithm based on statistical learning
                 for synthesizing static and time-varying textures
                 matching the appearance of an input texture. Our
                 algorithm is general and automatic and it works well on
                 various types of textures, including 1D sound textures,
                 2D texture images, and 3D texture movies. The same
                 method is also used to generate 2D texture mixtures
                 that simultaneously capture the appearance of a number
                 of different input textures. In our approach, input
                 textures are treated as sample signals generated by a
                 stochastic process. We first construct a tree
                 representing a hierarchical multiscale transform of the
                 signal using wavelets. From this tree, new random trees
                 are generated by learning and sampling the conditional
                 probabilities of the paths in the original tree.
                 Transformation of these random trees back into signals
                 results in new random textures. In the case of 2D
                 texture synthesis, our algorithm produces results that
                 are generally as good as or better than those produced
                 by previously described methods in this field. For
                 texture mixtures, our results are better and more
                 general than those produced by earlier methods. For
                 texture movies, we present the first algorithm that is
                 able to automatically generate movie clips of dynamic
                 phenomena such as waterfalls, fire flames, a school of
                 jellyfish, a crowd of people, etc. Our results indicate
                 that the proposed technique is effective and robust.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-248,
  pages =        "136--151",
  year =         "2001",
  title =        "Cutting and Stitching: Converting Sets of Polygons to
                 Manifold Surfaces",
  author =       "Andr{\'{e}} Gu{\'{e}}ziec and Gabriel Taubin and
                 Francis Lazarus and Bill Horn",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-248",
  abstract =     "Many real-world polygonal surfaces contain topological
                 singularities that represent a challenge for processes
                 such as simplification, compression, and smoothing. We
                 present an algorithm that removes singularities from
                 nonmanifold sets of polygons to create manifold
                 (optionally oriented) polygonal surfaces. We identify
                 singular vertices and edges, multiply singular
                 vertices, and cut through singular edges. In an
                 optional stitching operation, we maintain the surface
                 as a manifold while joining boundary edges. We present
                 two different edge stitching strategies, called
                 pinching and snapping. Our algorithm manipulates the
                 surface topology and ignores physical coordinates.
                 Except for the optional stitching, the algorithm has a
                 linear complexity and requires no floating point
                 operations. In addition to introducing new algorithms,
                 we expose the complexity (and pitfalls) associated with
                 stitching. Finally, several real-world examples are
                 studied.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-249,
  pages =        "152--164",
  year =         "2001",
  title =        "Hierarchical Model for Real Time Simulation of Virtual
                 Human Crowds",
  author =       "Soraia Raupp Musse and Daniel Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-249",
  abstract =     "This paper describes a model for simulating crowds of
                 humans in real time. We deal with a hierarchy composed
                 of virtual crowds, groups, and individuals. The groups
                 are the most complex structure that can be controlled
                 in different degrees of autonomy. This autonomy refers
                 to the extent to which the virtual agents are
                 independent of user intervention and also the amount of
                 information needed to simulate crowds. Thus, depending
                 on the complexity of the simulation, simple behaviors
                 can be sufficient to simulate crowds. Otherwise, more
                 complicated behavioral rules can be necessary and, in
                 this case, it can be included in the simulation data in
                 order to improve the realism of the animation. We
                 present three different ways for controlling crowd
                 behaviors: 1) by using innate and scripted behaviors,
                 2) by defining behavioral rules, using events and
                 reactions, and 3) by providing an external control to
                 guide crowd behaviors in real time. The two main
                 contributions of our approach are: the possibility of
                 increasing the complexity of group/agent behaviors
                 according to the problem to be simulated and the
                 hierarchical structure based on groups to compose a
                 crowd.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-25,
  year =         "2001",
  title =        "A Non-Hierarchical Procedure for Re-Synthesis of
                 Complex Textures",
  author =       "Paul Harrison",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-25",
  abstract =     "A procedure is described for synthesizing an image
                 with the same texture as a given input image. To
                 achieve this, the output image is built up by
                 successively adding pixels selected from the input
                 image. Pixels are chosen by searching the input image
                 for patches that closely match pixels already present
                 in the output image. It is shown that the accurate
                 reproduction of features in the input texture depends
                 on the order in which pixels are added to the output
                 image. A procedure for selecting an ordering which
                 transfers large complex features of the input to the
                 output image is described. This procedure is capable of
                 reproducing large features even if only the
                 interactions of nearby pixels are considered. The
                 procedure can be altered to allow specification of the
                 placement of particular features in the output texture.
                 Several applications of this are described.",
  editor =       "V. Skala",
  keywords =     "Texture synthesis, image manipulation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-250,
  pages =        "165--172",
  year =         "2001",
  title =        "Detection and Visualization of Closed Streamlines in
                 Planar Flows",
  author =       "Thomas Wischgoll and Gerik Scheuermann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-250",
  abstract =     "The analysis and visualization of flows is a central
                 problem in visualization. Topology-based methods have
                 gained increasing interest in recent years. This
                 article describes a method for the detection of closed
                 streamlines in flows. It is based on a special
                 treatment of cases where a streamline reenters a cell
                 to prevent infinite cycling during streamline
                 calculation. The algorithm checks for possible exits of
                 a loop of crossed edges and detects structurally stable
                 closed streamlines. These global features are not
                 detected by conventional topology and feature detection
                 algorithms.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-2001-251,
  pages =        "173--192",
  year =         "2001",
  title =        "A Level-Set Approach for the Metamorphosis of Solid
                 Models",
  author =       "David E. Breen and Ross T. Whitaker",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-251",
  abstract =     "This paper presents a new approach to 3D shape
                 metamorphosis. We express the interpolation of two
                 shapes as a process where one shape deforms to maximize
                 its similarity with another shape. The process
                 incrementally optimizes an objective function while
                 deforming an implicit surface model. We represent the
                 deformable surface as a level set (iso-surface) of a
                 densely sampled scalar function of three dimensions.
                 Such level-set models have been shown to mimic
                 conventional parametric deformable surface models by
                 encoding surface movements as changes in the grayscale
                 values of a volume data set. Thus, a well-founded
                 mathematical structure leads to a set of procedures
                 that describes how voxel values can be manipulated to
                 create deformations that are represented as a sequence
                 of volumes. The result is a 3D morphing method that
                 offers several advantages over previous methods,
                 including minimal need for user input, no model
                 parameterization, flexible topology, and subvoxel
                 accuracy.",
  volume =       "7(2)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-252,
  pages =        "1--8",
  year =         "2001",
  title =        "3{D} Mesh Compression Using Fixed Spectral Bases",
  author =       "Zachi Karni and Craig Gotsman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-252",
  abstract =     "We show how to use fixed bases for efficient spectral
                 compression of 3D meshes. In contrast with compression
                 using variable bases, this permits efficient decoding
                 of the mesh. The coding procedure involves efficient
                 mesh augmentation and generation of a
                 neighborhood-preserving mapping between the vertices of
                 a 3D mesh with arbitrary connectivity and those of a
                 6-regular mesh.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Computer Graphics, Mesh Compression, Spectral
                 Decomposition",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-253,
  pages =        "9--18",
  year =         "2001",
  title =        "Watermarking 3{D} Polygonal Meshes in the Mesh
                 Spectral Domain",
  author =       "Ryutarou Ohbuchi and Shigeo Takahashi and Takahiko
                 Miyazawa and Akio Mukaiyama",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-253",
  abstract =     "Digital watermarking embeds a structure called
                 watermark into the target data, such as image and 3D
                 polygonal models. The watermark can be used, for
                 example, to enforce copyright and to detect tampering.
                 This paper presents a new robust watermarking method
                 that adds watermark into a 3D polygonal mesh in the
                 mesh's spectral domain. The algorithm computes spectra
                 of the mesh by using eigenvalue decomposition of a
                 Laplacian matrix derived only from connectivity of the
                 mesh. Mesh spectra can be obtained by projecting
                 coordinates of vertices onto the set of eigenvectors. A
                 watermark is embedded by modifying the magnitude of the
                 spectra. Watermarks embedded by using this method are
                 resistant to similarity transformation, random noise
                 added to vertex coordinates, mesh smoothing, and
                 partial resection of the meshes.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Geometric modeling, information security, information
                 hiding, graph Laplacian, mesh spectra",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-254,
  pages =        "19--26",
  year =         "2001",
  title =        "Topological Noise Removal",
  author =       "Igor Guskov and Zo{\"{e}} Wood",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-254",
  abstract =     "Meshes obtained from laser scanner data often contain
                 topological noise due to inaccuracies in the scanning
                 and merging process. This topological noise complicates
                 subsequent operations such as remeshing,
                 parameterization and smoothing. We introduce an
                 approach that removes unnecessary nontrivial topology
                 from meshes. Using a local wave front traversal, we
                 discover the local topologies of the mesh and identify
                 features such as small tunnels. We then identify
                 non-separating cuts along which we cut and seal the
                 mesh, reducing the genus and thus the topological
                 complexity of the mesh.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Meshes, irregular connectivity, topology",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-255,
  pages =        "27--36",
  year =         "2001",
  title =        "Motion Conversion based on the Musculoskeletal
                 System",
  author =       "Taku Komura and Yoshihisa Shinagawa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-255",
  abstract =     "Today, using motion capture devices is the most common
                 way to create realistic human motion data. In addition
                 to that, various methods have been proposed to edit,
                 morph and retarget such kind of motion. However, there
                 are still few methods to add physiological effects to
                 motion which are caused by fatigue, injuries, muscle
                 training and muscle shrinking. This is because the
                 innate structure of the human body, such as the
                 musculoskeletal system, has been mostly neglected when
                 handling human motion in computer graphics. In this
                 paper, we propose a method to use the musculoskeletal
                 system of the human body for editing and retargeting
                 human motion which were captured using a motion-capture
                 device. Using our method, not only physiological
                 effects such as fatigue,or injuries but also physical
                 effects caused by external force can be added to human
                 motion. By changing the muscular parameters and size of
                 the body, it is also possible to retarget the motion to
                 different bodies such as a very trained muscular body,
                 weak and narrow body, or a small childish body.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Muscle-based model, motion conversion, retargeting,
                 motion capture, human animation",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-256,
  pages =        "37--46",
  year =         "2001",
  title =        "Geometry-based Muscle Modeling for Facial Animation",
  author =       "Kolja K{\"{a}}hler and J{\"{o}}rg Haber and Hans-Peter
                 Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-256",
  abstract =     "We present a muscle model and methods for muscle
                 construction that allow to easily create animatable
                 facial models from given face geometry. Using our
                 editing tool, one can interactively specify coarse
                 outlines of the muscles, which are then automatically
                 created to fit the face geometry. Our muscle model
                 incorporates different types of muscles and the effects
                 of bulging and intertwining muscle fibers. The
                 influence of muscle contraction onto the skin is
                 simulated using a mass-spring system that connects the
                 skull, muscle, and skin layers of our model.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Physics-based facial animation, muscle/skin model,
                 muscle editor, mass-spring system",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-257,
  pages =        "47--54",
  year =         "2001",
  title =        "Novel Solver for Dynamic Surfaces",
  author =       "Sumantro Ray and Hong Qin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-257",
  abstract =     "Physics-based modeling integrates dynamics and
                 geometry. The standard methods to solve the Lagrangian
                 equations use a direct approach in the spatial domain.
                 Though extremely powerful, it requires time consuming
                 discrete-time integration. In this paper, we propose to
                 use an indirect approach using the Transformation
                 Theory. In particular, we use z-transform from the
                 digital signal processing theory, and formulate a
                 general, novel, unified solver that is applicable for
                 various models and behavior. The convergence and
                 accuracy of the solver are guaranteed if the temporal
                 sampling period is less than the critical sampling
                 period, which is a function of the physical properties
                 of the model. Our solver can seamlessly handle curves,
                 surfaces and solids, and supports a wide range of
                 dynamic behavior. The solver does not depend on the
                 topology of the model, and hence supports non-manifold
                 and arbitrary topology. Our numerical techniques are
                 simple, easy to use, stable, and efficient. We develop
                 an algorithm and a prototype software simulating
                 various models and behavior. Our solver preserves
                 physical properties such as energy, linear momentum,
                 and angular momentum. This approach will serve as a
                 foundation for many applications in many fields.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Physics-based deformable modeling, Numerical
                 techniques, Heterogeneous models, Conceptual design
                 techniques, z-Transforms",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-258,
  pages =        "55--60",
  year =         "2001",
  title =        "Simplification and Real-time Smooth Transitions of
                 Articulated Meshes",
  author =       "Jocelyn Houle and Pierre Poulin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-258",
  abstract =     "Simplification techniques have mainly been applied on
                 static models. However in movie and game industries,
                 many models are designed to be animated. We extend the
                 progressive mesh technique to handle
                 skeletally-articulated meshes in order to obtain a
                 continuous level-of-detail (CLOD) representation that
                 retains its ability to be animated. Our technique is
                 not limited to any simplification metric, nor is it
                 limited to generating models composed of a subset of
                 the original vertices. It thus preserves the full
                 simplification potential. To further improve
                 performance, we can use this CLOD representation and
                 extract a discrete set of skeletally-articulated
                 models. Each model can be independently optimized, such
                 as by using triangle strips. We can also use morphing
                 between the different models in order to create
                 smoother transitions. The result is a more accurate
                 representation of animated articulated models, suitable
                 for real-time applications.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Mesh simplification, skeletal animation, skinning,
                 LOD, real-time rendering",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-259,
  pages =        "61--70",
  year =         "2001",
  title =        "Hardware Accelerated Displacement Mapping for Image
                 Based Rendering",
  author =       "Jan Kautz and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-259",
  abstract =     "n this paper, we present a technique for rendering
                 displacement mapped geometry using current graphics
                 hardware. Our method renders a displacement by slicing
                 through the enclosing volume. The alpha-test is used to
                 render only the appropriate parts of every slice. The
                 slices need not to be aligned with the base surface,
                 e.g. it is possible to do screen-space aligned slicing.
                 We then extend the method to be able to render the
                 intersection between several displacement mapped
                 polygons. This is used to render a new kind of
                 image-based objects based on images with depth, which
                 we call image based depth objects. This technique can
                 also directly be used to accelerate the rendering of
                 objects using the image-based visual hull. Other
                 warping based IBR techniques can be accelerated in a
                 similar manner.",
  editor =       "B. Watson and J. W. Buchanan",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-26,
  year =         "2001",
  title =        "Fast Hybrid Block- and Pixel-Recursive Disparity
                 Analysis for Real-Time Applications in Immersive
                 Tele-Conference Scenarios",
  author =       "Peter Kauff and Nicole Brandenburg and Michael Karl
                 Oliver Schreerand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-26",
  abstract =     "This paper presents a fast disparity analysis approach
                 based on a hybrid block- and pixel-recursive matching
                 scheme. The key idea is to choose efficiently a small
                 number of candidate vectors in order to reduce the
                 computational effort by simultaneously achieving
                 spatial and temporal consistency in the resulting
                 disparity map. The latter aspect is very important for
                 3D videoconferencing applications, where novel views of
                 the conferee have to be synthesised in order to provide
                 motion parallax. For this application a processing of
                 video in ITU-Rec. 601 resolution is required. Our
                 algorithm is able to provide dense disparity vector
                 fields for both directions (left-to-right and
                 right-to-left) in real-time at one Pentium III, 800 MHz
                 processor in reasonable quality.",
  editor =       "V. Skala",
  keywords =     "Disparity analysis, recursive block-matching,
                 pixel-recursive matching, real-time, epipolar geometry,
                 rectification.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-260,
  pages =        "71--80",
  year =         "2001",
  title =        "The Rayset and Its Applications",
  author =       "Minglun Gong and Yee-Hong Yang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-260",
  abstract =     "In this paper, a novel concept, rayset, is proposed. A
                 new image based representation, object centered
                 concentric mosaics (OCCM), is derived based on this
                 concept. The rayset is a parametric function, which
                 consists of two mapping relations. The first mapping
                 relation maps from a parameter space to the ray space.
                 The second one maps from the parameter space to the
                 attribute space. We show that different image-based
                 scene representations can all be cast as different
                 kinds of raysets, and image-based rendering approaches
                 can be regarded as attempts to sample and reconstruct
                 the scene using different raysets. A collection of OCCM
                 is a 3-D rayset, which can be used to represent an
                 object. The storage size for OCCM is about the same as
                 an animation sequence generated with a virtual camera
                 rotated around the object. However, comparing with such
                 an animation sequence, OCCM provide a much richer
                 experience because the user can move back and forth
                 freely in the scene and observe the changes in
                 parallax.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Image-based rendering, Rayset, Plenoptic function",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-261,
  pages =        "81--90",
  year =         "2001",
  title =        "Universal Rendering Sequences for Transparent Vertex
                 Caching of Progressive Meshes",
  author =       "Alexander Bogomjakov and Craig Gotsman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-261",
  abstract =     "We present methods to generate rendering sequences for
                 triangle meshes which preserve mesh locality as much as
                 possible. This is useful for maximizing vertex reuse
                 when rendering the mesh using a FIFO vertex buffer,
                 such as those available in modern 3D graphics hardware.
                 The sequences are universal in the sense that they
                 perform well for all sizes of vertex buffers, and
                 generalize to progressive meshes. This has been
                 verified experimentally.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Rendering sequence, transparent vertex caching,
                 triangle strips, progressive meshes, space-filling
                 curves",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-262,
  pages =        "91--100",
  year =         "2001",
  title =        "Tunneling for Triangle Strips in Continuous
                 Level-of-Detail Meshes",
  author =       "A. James Stewart",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-262",
  abstract =     "This paper describes a method of building and
                 maintaining a good set of triangle strips for both
                 static and continuous level-of-detail (CLOD) meshes.
                 For static meshes, the strips are better than those
                 computed by the classic SGI and STRIPE algorithms. For
                 CLOD meshes, the strips are maintained incrementally as
                 the mesh topology changes. The incremental changes are
                 fast and the number of strips is kept very small.",
  editor =       "B. Watson and J. W. Buchanan",
  booktitle =    "Proceedings of Graphics Interface",
}

@InProceedings{EVL-2001-263,
  pages =        "101--110",
  year =         "2001",
  title =        "Truly Selective Refinement of Progressive Meshes",
  author =       "Junho Kim and Seungyong Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-263",
  abstract =     "This paper presents a novel selective refinement
                 scheme of progressive meshes. In previous schemes,
                 topology information in the neighborhood of a collapsed
                 edge is stored in the analysis phase. A vertex split or
                 edge collapse transformation is possible in the
                 synthesis phase only if the configuration of
                 neighborhood vertices in the current mesh corresponds
                 to the stored topology information. In contrast, the
                 proposed scheme makes it possible to apply a vertex
                 split or an edge collapse to any selected vertex or
                 edge in the current mesh without a precondition. Our
                 main observation is that the concept of a dual piece
                 can be used to clearly enumerate and visualize the set
                 of all possible selectively refined meshes for a given
                 mesh. Our refinement scheme is truly selective in the
                 sense that each vertex split or edge collapse can be
                 performed without incurring additional vertex split
                 and/or edge collapse transformations.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Selective refinement, progressive mesh, dual piece,
                 progressive transitive mesh space, topological detail,
                 cut vertex",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-264,
  pages =        "111--118",
  year =         "2001",
  title =        "Interacting with Image Sequences: Detail-in-Context
                 and Thumbnails",
  author =       "Oliver Kuederle and Kori Inkpen and Stella Atkins and
                 Sheelagh Carpendale",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-264",
  abstract =     "An image sequence is a series of interrelated images.
                 To enable navigation of large image sequences, many
                 current software packages display small versions of the
                 images, called thumbnails. We observed radiologists
                 during typical diagnosis sessions, where image
                 sequences are examined using photographic films and
                 sophisticated light screens. Based on these
                 observations and on previous research, we have
                 developed a new alternative to the presentation of
                 image sequences on a desktop monitor, a variation of a
                 detail-in-context technique. This paper describes a
                 controlled experiment in which we examined the way
                 users interact with detail-in-context and thumbnail
                 techniques. Our results show that our detail-in-context
                 technique accommodates many individual strategies
                 whereas the thumbnail technique strongly encourages
                 sequential examination of the images. Our findings can
                 assist in the design and development of interactive
                 systems that involve the navigation of large image
                 sequences.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Image sequences, detail-in-context, thumbnails,
                 medical imaging, information visualization",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-265,
  pages =        "119--126",
  year =         "2001",
  title =        "An Isometric Joystick as a Pointing Device for
                 Handheld Information Terminals",
  author =       "Miika Silfverberg and I. Scott MacKenzie and Tatu
                 Kauppinen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-265",
  abstract =     "Meeting the increasing demand for desktop-like
                 applications on mobile products requires powerful
                 interaction techniques. One candidate is GUI-style
                 point-and-click interaction using an integrated
                 pointing device that supports handheld use. We tested
                 an isometric joystick for this purpose. Two prototypes
                 were built. They were designed for thumb operation and
                 included a separate selection button. Twelve
                 participants performed point-and-select tasks. We
                 tested both one-handed and two-handed interaction, and
                 selection using the separate selection button and the
                 joystick?s integrated press-to-select feature. A
                 notebook configuration served as a reference. Results
                 for the handheld conditions, both one-handed and
                 two-handed, were just slightly off those for the
                 notebook condition, suggesting that an isometric
                 joystick is suitable as a pointing device for handheld
                 terminals. Inadvertent selection while moving the
                 pointer yielded high error rates for all conditions
                 using press-to-select. A separate select button is
                 therefore needed to ensure accurate selection.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Isometric joystick, pointing devices, handheld
                 devices, ISO 9241, Fitt's law",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-266,
  pages =        "127--134",
  year =         "2001",
  title =        "Aiding Manipulation of Handwritten Mathematical
                 Expressions through Style-Preserving Morphs",
  author =       "Richard Zanibbi and Kevin Novins and Jim Arvo and
                 Katherine Zanibbi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-266",
  abstract =     "We describe a technique for enhancing a user's ability
                 to manipulate hand-printed symbolic information by
                 automatically improving legibility and simultaneously
                 providing immediate feedback on the system's current
                 structural interpretation of the information. Our
                 initial application is a handwriting-based equation
                 editor. Once the user has written a formula, the
                 individual hand-drawn symbols can be gradually
                 translated and scaled to closely approximate their
                 relative positions and sizes in a corresponding typeset
                 version. These transformations preserve the
                 characteristics, or style, of the original user-drawn
                 symbols. In applying this style-preserving morph, the
                 system improves the legibility of the user-drawn
                 symbols by correcting alignment and scaling, and also
                 reveals the baseline structure of the symbols that has
                 been inferred by system. We performed a preliminary
                 user study that indicates that this new method of
                 feedback is a useful addition to a conventional
                 interpretive interface. We believe this is because the
                 style preserving morph makes it easier to understand
                 the correspondence between the original input and
                 interpreted output than methods that radically change
                 the appearance of the original input.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Formula entry, math recognition, mental map, morphing,
                 pen-based computing, user feedback",
  booktitle =    "Proceedings of Graphics Interface",
}

@InProceedings{EVL-2001-267,
  pages =        "135--142",
  year =         "2001",
  title =        "3{D} Scene Manipulation with 2{D} Devices and
                 Constraints",
  author =       "Graham Smith and Wolfgang St{\"{u}}rzlinger and Tim
                 Salzman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-267",
  abstract =     "Content creation for computer graphics applications is
                 a laborious process that requires skilled personnel.
                 One fundamental problem is that manipulation of 3D
                 objects with 2D user interfaces is very difficult for
                 non-experienced users. In this paper, we introduce a
                 new system that uses constraints to restrict object
                 motion in a 3D scene, making interaction much simpler
                 and more intuitive. We compare three different 3D scene
                 manipulation techniques based on a 2D user interface.
                 We show that the presented techniques are significantly
                 more efficient than commonly used solutions. To our
                 knowledge, this is the first evaluation of 3D
                 manipulation techniques with 2D devices and
                 constraints.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Interactive 3D environments, 3D scene construction, 3D
                 manipulation, Constraints",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-268,
  pages =        "143--150",
  year =         "2001",
  title =        "The Lit Sphere: {A} Model for Capturing {NPR} Shading
                 from Art",
  author =       "Peter-Pike Sloan and William Martin and Amy Gooch and
                 Bruce Gooch",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-268",
  abstract =     "While traditional graphics techniques provide for the
                 realistic display of three-dimensional objects, these
                 methods often lack the flexibility to emulate
                 expressive effects found in the works of artists such
                 as Michelangelo and Cezanne. We introduce a technique
                 for capturing custom artistic shading models from
                 sampled art work. Our goal is to allow users to easily
                 generate shading models which give the impression of
                 light, depth, and material properties as accomplished
                 by artists. Our system provides real-time feedback to
                 immediately illustrate aesthetic choices in shading
                 model design, and to assist the user in the exploration
                 of novel viewpoints. We describe rendering algorithms
                 which are easily incorporated into existing shaders,
                 making non-photorealistic rendering of materials such
                 as skin, metal, or even painted objects fast and
                 simple. The flexibility of these methods for generating
                 shading models enables users to portray a large range
                 of materials as well as to capture the look and feel of
                 a work of art.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Non-photorealistic rendering, interaction, shading,
                 environment maps, lighting models, painting, paint
                 programs",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-269,
  pages =        "151--158",
  year =         "2001",
  title =        "View-Dependent Particles for Interactive
                 Non-Photorealistic Rendering",
  author =       "Derek Cornish and Andrea Rowan and David Luebke",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-269",
  abstract =     "We present a novel framework for non-photorealistic
                 rendering (NPR) based on view-dependent geometric
                 simplification techniques. Following a common thread in
                 NPR research, we represent the model as a system of
                 particles, which will be rendered as strokes in the
                 final image and which may optionally overlay a
                 polygonal surface. Our primary contribution is the use
                 of a hierarchical view-dependent clustering algorithm
                 to regulate the number and placement of these
                 particles. This algorithm unifies several tasks common
                 in artistic rendering, such as placing strokes,
                 regulating the screen-space density of strokes, and
                 ensuring inter-frame coherence in animated or
                 interactive rendering. View-dependent callback
                 functions determine which particles are rendered and
                 how to render the associated strokes. The resulting
                 framework is interactive and extremely flexible,
                 letting users easily produce and experiment with many
                 different art-based rendering styles.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Non-photorealistic rendering, artistic rendering,
                 view-dependent simplification, view-dependent
                 particles",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-27,
  year =         "2001",
  title =        "Towards Continuous Image Representations",
  author =       "Frederic Labrosse and Philip Willis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-27",
  abstract =     "We propose in this paper a first step towards the
                 creation of continuous, i.e. vectorial, representations
                 that are useful for image manipulation. Such pixel-free
                 representations have many advantages and are amenable
                 to operations which are difficult or imprecise with
                 pixels. For example, they can readily be rendered at
                 different resolutions and they are a better choice for
                 ultra-high resolution applications. We explore an
                 approach in which images are decomposed into structural
                 regions that correspond to specified image
                 characteristics. This is done using relaxation
                 labelling. Information taken at different stages during
                 the relaxation is used to extract sub-pixel accurate
                 continuous contours. This accuracy is obtained by using
                 snakes as well as the blur present in images (because
                 of the acquisition process). We propose solutions,
                 parameter determination, and instability. The interior
                 of structural regions is represented to allow the
                 rendering of images as close as possible to the
                 original ones. We propose here two schemes, one using a
                 single colour for each region, the second sampling the
                 original image to allow smoothly varying colour in each
                 region.",
  editor =       "V. Skala",
  keywords =     "Continuous contour, Snake, Sub-pixel accuracy, Image
                 representation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-270,
  pages =        "159--166",
  year =         "2001",
  title =        "Realistic and Controllable Fire Simulation",
  author =       "Philippe Beaudoin and S{\'{e}}bastien Paquet and
                 Pierre Poulin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-270",
  abstract =     "We introduce a set of techniques that are used
                 together to produce realistic-looking animations of
                 burning objects. These include a new method for
                 simulating spreading on polygonal meshes. A key
                 component of our approach consists in using individual
                 flames as primitives to animate and render the fire.
                 This simplification enables rapid computation and gives
                 more intuitive control over the simulation without
                 compromising realism. It also scales well, making it
                 possible to animate phenomena ranging from simple
                 candle-like flames to complex, widespread fires.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Fire, simulation, natural phenomena, implicit
                 surfaces, volume rendering, propagation on surfaces",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-271,
  pages =        "167--174",
  year =         "2001",
  title =        "Corrosion: Simulating and Rendering",
  author =       "Stephane Merillou and Jean-Michel Dischler and
                 Djamchid Ghazanfarpour",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-271",
  abstract =     "Weathering phenomena represent a topic of growing
                 interest in computer graphics, and corrosion reactions
                 are of great importance since they affect a large
                 number of different fields. Previous investigations
                 have essentially dealt with the modeling and rendering
                 of metallic patinas. We propose an approach based on
                 simple physical characteristics to simulate and render
                 new forms of corrosion. We take into account {"}real
                 world{"} time and different atmospheric condition
                 categories with experimental data. This allows us to
                 predict the evolution of corrosion over time. The
                 reaction is simulated using a random walk technique
                 adapted to our generic model. For realistic rendering,
                 we propose a BRDF and (color- and bump-) texture
                 models, thus affecting color, reflectance and geometry.
                 We additionally propose a set of rules to automatically
                 predict the preferential starting locations of
                 corrosion.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Rendering, corrosion, natural phenomena, texturing,
                 BRDF",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-272,
  pages =        "175--182",
  year =         "2001",
  title =        "Surface Aging by Impacts",
  author =       "Eric Paquette and Pierre Poulin and George Drettakis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-272",
  abstract =     "We present a novel aging technique that simulates the
                 deformation of an object caused by repetitive impacts
                 over long periods of time. Our semi-automatic system
                 deteriorates the surface of an object by hitting it
                 with another object. An empirical simulation modifies
                 the object surface to represent the small depressions
                 caused by each impact. This is done by updating the
                 vertices of an adaptively refined object mesh. The
                 simulation is efficient, applying hundreds of impacts
                 in a few seconds. The user controls the simulation
                 through intuitive parameters. Because the simulation is
                 rapid, the user can easily adjust the parameters and
                 see the effect of impacts interactively. The models
                 processed by our system exhibit the cumulative aging
                 effects of repetitive impacts, significantly increasing
                 their realism.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Realism, simulation, aging, deterioration,
                 imperfection, impact, surface, compaction",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-273,
  pages =        "191--200",
  year =         "2001",
  title =        "Accelerated Splatting using a 3{D} Adjacency Data
                 Structure",
  author =       "Jeff Orchard and Torsten M{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-273",
  abstract =     "We introduce a new acceleration to the standard
                 splatting volume rendering algorithm. Our method
                 achieves full colour (32-bit), depth-sorted and shaded
                 volume rendering significantly faster than standard
                 splatting. The speedup is due to a 3-dimensional
                 adjacency data structure that efficiently skips
                 transparent parts of the data and stores only the
                 voxels that are potentially visible. Our algorithm is
                 robust and flexible, allowing for depth sorting of the
                 data, including correct back-to-front ordering for
                 perspective projections. This makes interactive
                 splatting possible for applications such as medical
                 visualizations that rely on structure and depth
                 information.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Volume rendering, splatting, data structure, software
                 acceleration",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-274,
  pages =        "183--190",
  year =         "2001",
  title =        "3{D}-Interaction Techniques for Planning of Oncologic
                 Soft Tissue Operations",
  author =       "Bernhard Preim and Wolf Spindler and Karl Oldhafer and
                 Heinz-Otto Peitgen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-274",
  abstract =     "We discuss interaction tasks and interaction
                 techniques for the planning of soft tissue operations
                 as for example oncologic liver and lung surgery. We
                 focus on techniques to explore the relevant structures,
                 to integrate measurements directly in 3d visualizations
                 and to specify resection volumes. The main contribution
                 of this paper is the introduction of new techniques for
                 3d measurements and for virtual resections. For both
                 interaction tasks, dedicated widgets have been
                 developed for the direct-manipulative use. In contrast
                 to surgical simulators, which are used for the
                 education of future surgeons, we concentrate on
                 surgeons in the clinical routine and attempt to provide
                 them with preoperative decision-support on the basis of
                 patient-individual data. The selection of the
                 interaction tasks to be supported is based on a
                 questionaire in which 13 surgeons described their
                 praxis of surgery planning and their requirements for
                 computer support. All visualization and interaction
                 techniques are integrated in a software, named
                 SURGERYPLANNER, which exploits the results of image
                 analysis achieved in an earlier project. With the
                 SURGERYPLANNER the anatomical and pathological
                 structures of individual patients are used for surgery
                 planning.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "3D interaction, visualization, medical graphics,
                 computer-assisted surgery",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-275,
  pages =        "201--208",
  year =         "2001",
  title =        "Assisted Visualization of {E}-Commerce Auction
                 Agents",
  author =       "Christopher Healey and Robert St. Amant and Jiae
                 Chang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-275",
  abstract =     "This paper describes the integration of perceptual
                 guidelines from human vision with an AI-based
                 mixed-initiative search technique. The result is a
                 visualization assistant, a system that identifies
                 perceptually salient visualizations for large,
                 multidimensional collections of data. Understanding how
                 the low-level human visual system &quotlsees{"} visual
                 information in an image allows us to: (1) evaluate a
                 particular visualization, and (2) direct the search
                 algorithm towards new visualizations that may be better
                 than those seen to date. In this way we can limit
                 search to locations that have the highest potential to
                 contain effective visualizations. One testbed
                 application for this work is the visualization of
                 intelligent e-commerce auction agents participating in
                 a simulated online auction environment. We describe how
                 the visualization assistant was used to choose methods
                 to effectively visualize this data.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Agents, artificial intelligence, colour, e-commerce,
                 perception, scientific visualization, texture",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-276,
  pages =        "209--216",
  year =         "2001",
  title =        "Interactive Volume Rendering based on a 'Bubble
                 Model'",
  author =       "Bal{\'{a}}zs Cs{\'{e}}bfalvi and Eduard
                 Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-276",
  abstract =     "In this paper an interactive volume rendering
                 technique is presented which is based on a novel
                 visualization model. We call the basic method {"}bubble
                 model{"} since iso-surfaces are rendered as thin
                 semi-transparent membranes similarly to blown soap
                 bubbles. The primary goal is to develop a fast
                 previewing technique for volumetric data which does not
                 require a time-consuming transfer function
                 specification to visualize internal structures. Our
                 approach uses a very simple rendering model controlled
                 by only two parameters. We also present an interactive
                 rotation technique which does not rely on any
                 specialized hardware, therefore it can be widely used
                 even on low-end machines. Due to the interactive
                 display, fine tuning is also supported since the
                 modification of the rendering parameters has an
                 immediate visual feedback.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Volume rendering, shear-warp projection, fast
                 previewing",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-277,
  pages =        "217--222",
  year =         "2001",
  title =        "Efficient View-dependent Rendering of Terrains",
  author =       "Yadong Wu and Yushu Liu and Shouyi Zhan and Chunxiao
                 Gao",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-277",
  abstract =     "Though considerable progress has been made with the
                 view-dependent techniques in terrain visualization, the
                 CPU overhead still precludes their wide application in
                 many domains. The calculation complexity of
                 view-dependent techniques mainly involves the
                 calculation of node screen space error every frame,
                 including the time-consuming screen space projection,
                 the number of nodes whose projection error remains to
                 be updated, and the evaluation of the valid life of the
                 projection error. In this paper we introduce
                 block-priority-based traversal of quadtree for reducing
                 the traversal complexity and propose view-angle-based
                 error metrics. Thus we successfully speed up the valid
                 life evaluation of projection error by means of
                 calculating the spatial relation between the viewpoint
                 and a simplified split zone. In addition, constant
                 frame rate has been achieved by scaling the split zone
                 accordingly. Corresponding experimental results have
                 shown that our methods can real-time render large scale
                 terrain on a low-cost PC so as to satisfy the demand of
                 most applications in this way.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Terrain, real-time rendering, view-dependent, error
                 metric, valid life, constant frame rate",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-278,
  pages =        "223--232",
  year =         "2001",
  title =        "Characterizing Image Fusion Techniques in Stereoscopic
                 {HTD}s",
  author =       "Zachary Wartell and Larry Hodges and William
                 Ribarsky",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-278",
  abstract =     "Stereoscopic display is fundamental to many virtual
                 reality systems. Stereoscopic systems render two
                 perspective views of a scene one for each eye of the
                 user. Ideally the user's visual system combines the
                 stereo image pairs into a single, 3D perceived image.
                 In practice, however, users can have difficulty fusing
                 the stereo image pair into a single 3D image.
                 Researchers have used a number of software methods to
                 reduce fusion problems. We are particularly concerned
                 with the effects of these techniques on stereoscopic
                 HTDs (Head-Tracked Display). In these systems the head
                 is tracked but the display is stationary, attached to a
                 desk, tabletop or wall. This paper comprehensively
                 surveys software fusion techniques. We then
                 geometrically characterize and classify the various
                 techniques and illustrate how they relate to
                 stereoscopic HTD application characteristics.",
  editor =       "B. Watson and J. W. Buchanan",
  keywords =     "Stereoscopic distortion, stereoscopic HTD",
  booktitle =    "Proceedings of Graphics Interface 2001",
}

@InProceedings{EVL-2001-279,
  pages =        "196--206",
  year =         "2001",
  title =        "Penalized-Distance Volumetric Skeleton Algorithm",
  author =       "Ingmar Bitter and Arie E. Kaufman and Mie Sato",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-279",
  abstract =     "This paper introduces a refined general definition of
                 a skeleton that is based on a penalized-distance
                 function and cannot create any of the degenerate cases
                 of the earlier Ceasar and Teasar algorithms.
                 Additionally, we provide an algorithm that finds the
                 skeleton accurately and rapidly. Our solution is fully
                 automatic, which frees the user from having to engage
                 in manual data preprocessing. We present the accurate
                 skeletons computed on a number of test datasets. The
                 algorithm is very efficient as demonstrated by the
                 running times which were all below seven minutes.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-28,
  year =         "2001",
  title =        "Physics-Enhanced {L}-Systems",
  author =       "Hansrudi Noser and Stephan Rudolph and Peter Stucki",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-28",
  abstract =     "In computer graphics and engineering many classes of
                 complex objects can be designed with L-systems. We
                 present a concept for enhancing timed and parametric
                 L-systems with physics. This simplifies considerably
                 the physically correct design of certain classes of
                 computer animations or technical objects modelled by
                 production rules. The focus is on structural extensions
                 in timed and parametric L-system theory necessary for
                 constraint propagation management for the treatment of
                 hierarchical objects and on physics enhanced
                 grammar-language extensions. The proposed concept is
                 illustrated with a design model incorporating the
                 statics of arbitrary tree structures.",
  editor =       "V. Skala",
  keywords =     "L-systems, rewriting, physics, computer graphics,
                 design, animation, engineering, conceptual design.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-280,
  pages =        "207--221",
  year =         "2001",
  title =        "Multiresolution Methods for Nonmanifold Models",
  author =       "Andreas Hubeli and Markus Gross",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-280",
  abstract =     "The concept of fairing applied to triangular meshes
                 with irregular connectivity has become more and more
                 important. Previous contributions proposed a variety of
                 fairing operators for manifolds and applied them to
                 design multiresolution representations and editing
                 tools for meshes. In this paper, we generalize these
                 powerful techniques to handle nonmanifold models. We
                 propose a method to construct fairing operators for
                 nonmanifolds which is based on standard operators for
                 the manifold setting. Furthermore, we describe novel
                 approaches to guarantee volume preservation. We
                 introduce various multiresolution techniques that allow
                 us to represent, smooth, and edit nonmanifold models
                 efficiently. Finally, we discuss a semiautomatic
                 feature preservation strategy to retain important model
                 information during the fairing process.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-281,
  pages =        "222--229",
  year =         "2001",
  title =        "Topology-Preserving Smoothing of Vector Fields",
  author =       "R{\"{u}}diger Westermann and Christopher Johnson and
                 Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-281",
  abstract =     "In this paper, we propose a technique for
                 topology-preserving smoothing of sampled vector fields.
                 The vector field data is first converted into a scalar
                 representation in which time surfaces implicitly exist
                 as level-sets. We then locally analyze the dynamic
                 behavior of level-sets by placing geometric primitives
                 in the scalar field and by subsequently distorting
                 these primitives with respect to local variations in
                 this field. From the distorted primitives, we calculate
                 the curvature normal and we use the normal magnitude
                 and its direction to separate distinct flow features.
                 Geometrical and topological considerations are then
                 combined to successively smooth dense flow fields, at
                 the same time retaining their topological structure.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-282,
  pages =        "242--252",
  year =         "2001",
  title =        "Two-Level Volume Rendering",
  author =       "Helwig Hauser and Lukas Mroz and Gian Italo Bischi and
                 M. Eduard Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-282",
  abstract =     "In this paper, we present a two-level approach for
                 volume rendering, i.e., two-level volume rendering,
                 which allows for selectively using different rendering
                 techniques for different subsets of a 3D data set.
                 Different structures within the data set are rendered
                 locally on an object-by-object basis by either DVR,
                 MIP, surface rendering, value integration (x-ray-like
                 images), or nonphotorealistic rendering. All the
                 results of subsequent object renderings are combined
                 globally in a merging step (usually compositing in our
                 case). This allows us to selectively choose the most
                 suitable technique for depicting each object within the
                 data while keeping the amount of information contained
                 in the image at a reasonable level. This is especially
                 useful when inner structures should be visualized
                 together with semitransparent outer parts, similar to
                 the focus-plus-context approach known from information
                 visualization. We also present an implementation of our
                 approach which allows us to explore volumetric data
                 using two-level rendering at interactive frame rates.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-283,
  pages =        "230--241",
  year =         "2001",
  title =        "A Phase Field Model for Continuous Clustering on
                 Vector Fields",
  author =       "Harald Garcke and Tobias Preu{\ss{}}er and Martin
                 Rumpf and Alexandru C. Telea and Ulrich Weikard and
                 Jarke J. van Wijk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-283",
  abstract =     "A new method for the simplification of flow fields is
                 presented. It is based on continuous clustering. A
                 well-known physical clustering model, the Cahn Hilliard
                 model, which describes phase separation, is modified to
                 reflect the properties of the data to be visualized.
                 Clusters are defined implicitly as connected components
                 of the positivity set of a density function. An
                 evolution equation for this function is obtained as a
                 suitable gradient flow of an underlying anisotropic
                 energy functional. Here, time serves as the scale
                 parameter. The evolution is characterized by a
                 successive coarsening of patterns-the actual
                 clustering-during which the underlying simulation data
                 specifies preferable pattern boundaries. We introduce
                 specific physical quantities in the simulation to
                 control the shape, orientation and distribution of the
                 clusters as a function of the underlying flow field. In
                 addition, the model is expanded, involving elastic
                 effects. In the early stages of the evolution shear
                 layer type representation of the flow field can thereby
                 be generated, whereas, for later stages, the
                 distribution of clusters can be influenced.
                 Furthermore, we incorporate upwind ideas to give the
                 clusters an oriented drop-shaped appearance. Here, we
                 discuss the applicability of this new type of approach
                 mainly for flow fields, where the cluster energy
                 penalizes cross streamline boundaries. However, the
                 method also carries provisions for other fields as
                 well. The clusters can be displayed directly as a flow
                 texture. Alternatively, the clusters can be visualized
                 by iconic representations, which are positioned by
                 using a skeletonization algorithm.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-284,
  pages =        "253--264",
  year =         "2001",
  title =        "Volume Illustration: Nonphotorealistic Rendering of
                 Volume Models",
  author =       "Penny Rheingans and David Ebert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-284",
  abstract =     "Accurately and automatically conveying the structure
                 of a volume model is a problem not fully solved by
                 existing volume rendering approaches. Physics-based
                 volume rendering approaches create images which may
                 match the appearance of translucent materials in
                 nature, but may not embody important structural
                 details. Transfer function approaches allow flexible
                 design of the volume appearance, but generally require
                 substantial hand tuning for each new data set in order
                 to be effective. We introduce the volume illustration
                 approach, combining the familiarity of a physics-based
                 illumination model with the ability to enhance
                 important features using nonphotorealistic rendering
                 techniques. Since features to be enhanced are defined
                 on the basis of local volume characteristics rather
                 than volume sample value, the application of volume
                 illustration techniques requires less manual tuning
                 than the design of a good transfer function. Volume
                 illustration provides a flexible unified framework for
                 enhancing structural perception of volume models
                 through the amplification of features and the addition
                 of illumination effects.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-285,
  pages =        "265--274",
  year =         "2001",
  title =        "An Attempt for Coloring Multichannel {MR} Imaging
                 Data",
  author =       "Shigeru Muraki and Toshiharu Nakai and Yasuyo Kita and
                 Koji Tsuda",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-285",
  abstract =     "This is an elementary research for assigning color
                 values to voxels of multichannel Magnetic Resonance
                 Imaging (MRI) volume data. The MRI volume data sets
                 obtained under different scanning conditions are
                 transformed to the components by independent component
                 analysis (ICA), which enhances physical characteristics
                 of the tissue. The transfer functions for generating
                 color values from independent components are obtained
                 using the radial basis function network, a kind of
                 neural net, by training the network with sample data
                 chosen from visible human female data set (VHF). The
                 resultant color volume data sets correspond well with
                 the full-color cross-sections of the visible human data
                 sets.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-286,
  pages =        "275--287",
  year =         "2001",
  title =        "Visualization Exploration and Encapsulation via a
                 Spreadsheet-Like Interface",
  author =       "T. J. Jankun-Kelly and Kwan-Liu Ma",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-286",
  abstract =     "Exploring complex, very large data sets requires
                 interfaces to present and navigate through the
                 visualization of the data. Two types of audience
                 benefit from such coherent organization and
                 representation: First, the user of the visualization
                 system can examine and evaluate their data more
                 efficiently; second, collaborators or reviewers can
                 quickly understand and extend the visualization. The
                 needs of these two groups are addressed by the
                 spreadsheet-like interface described here. The
                 interface represents a two-dimensional window into a
                 multidimensional visualization parameter space. Data is
                 explored by navigating this space via the interface.
                 The visualization space is presented to the user in a
                 manner that clearly identifies which parameters
                 correspond to which visualized result. Operations
                 defined on this space can be applied which generate new
                 parameters or results. Combined with a general purpose
                 interpreter, these functions can be utilized to quickly
                 extract desired results. Finally, by encapsulating the
                 visualization process, redundant exploration is
                 eliminated and collaboration is facilitated. The
                 efficacy of this novel interface is demonstrated
                 through examples using a variety of data sets in
                 different domains.",
  organization = "IEEE Computer Society",
  volume =       "7(3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-287,
  pages =        "289--298",
  year =         "2001",
  title =        "Preventing Self-Intersection under Free-Form
                 Deformation",
  author =       "James E. Gain and Neil A. Dodgson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-287",
  abstract =     "Free-Form Deformation (FFD) is a versatile and
                 efficient modeling technique which transforms an object
                 by warping the surrounding space. The conventional
                 user-interface is a lattice of movable control points
                 but this tends to be cumbersome and counterintuitive.
                 Directly Manipulated Free-Form Deformation (DMFFD)
                 allows the user to drag object points directly and has
                 proven useful in an interactive sculpting context. A
                 serious shortcoming of both FFD and DMFFD is that some
                 deformations cause self-intersection of the object.
                 This is unrealistic and compromises the object's
                 validity and suitability for later use. An in-built
                 self-intersection test is thus required for FFD and its
                 extensions to be truly robust. In this paper, we
                 present the following novel results: a set of
                 theoretical conditions for preventing self-intersection
                 by ensuring the injectivity (one-to-one mapping) of
                 FFD, an exact (necessary and sufficient) injectivity
                 test which is accurate but computationally costly, an
                 efficient but approximate injectivity test which is a
                 sufficient condition only, and a new form of DMFFD
                 which acts by composing many small injective
                 deformations. The latter expands the range of possible
                 deformations without sacrificing the speed of the
                 approximate test.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-288,
  pages =        "299--317",
  year =         "2001",
  title =        "Extended Specifications and Test Data Sets for Data
                 LevelComparisons of Direct Volume Rendering
                 Algorithms",
  author =       "Kwansik Kim and Craig M. Wittenbrink and Alex Pang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-288",
  abstract =     "Direct volume rendering (DVR) algorithms do not
                 generate intermediate geometry to create a
                 visualization, yet they produce countless variations in
                 the resulting images. Therefore, comparative studies
                 are essential for objective interpretation. Even though
                 image and data level comparison metrics are available,
                 it is still difficult to compare results because of the
                 numerous rendering parameters and algorithm
                 specifications involved. Most of the previous
                 comparison methods use information from the final
                 rendered images only. We overcome limitations of image
                 level comparisons with our data level approach using
                 intermediate rendering information. We provide a list
                 of rendering parameters and algorithm specifications to
                 guide comparison studies. We extend Williams and
                 Uselton's rendering parameter list with algorithm
                 specification items and provide guidance on how to
                 compare algorithms. Real data are often too complex to
                 study algorithm variations with confidence. Most of the
                 analytic test data sets reported are often useful only
                 for a limited feature of DVR algorithms. We provide
                 simple and easily reproducible test data sets, a
                 checkerboard and a ramp, that can make clear
                 differences in a wide range of algorithm variations.
                 With data level metrics, our test data sets make it
                 possible to perform detailed comparison studies. A
                 number of examples illustrate how to use these tools.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-289,
  pages =        "318--332",
  year =         "2001",
  title =        "High-Quality Texture Reconstruction from Multiple
                 Scans",
  author =       "Fausto Bernardini and Ioana M. Martin and Holly
                 Rushmeier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-289",
  abstract =     "The creation of three-dimensional digital content by
                 scanning real objects has become common practice in
                 graphics applications for which visual quality is
                 paramount, such as animation, e-commerce, and virtual
                 museums. While a lot of attention has been devoted
                 recently to the problem of accurately capturing the
                 geometry of scanned objects, the acquisition of
                 high-quality textures is equally important, but not as
                 widely studied. In this paper, we focus on methods to
                 construct accurate digital models of scanned objects by
                 integrating high-quality texture and normal maps with
                 geometric data. These methods are designed for use with
                 inexpensive, electronic camera-based systems in which
                 low-resolution range images and high-resolution
                 intensity images are acquired. The resulting models are
                 well-suited for interactive rendering on the
                 latest-generation graphics hardware with support for
                 bump mapping. Our contributions include new techniques
                 for processing range, reflectance, and surface normal
                 data, for image-based registration of scans, and for
                 reconstructing high-quality textures for the output
                 digital object.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-29,
  year =         "2001",
  title =        "A Geometric Constraint Solver with Decomposable
                 Constraint Set",
  author =       "David Podgorolec and Borut Zalik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-29",
  abstract =     "In the paper, a new constructive approach to solving
                 geometric constraints in 2D space is presented. The
                 main step of the algorithm is pre-processing, which
                 transforms both, geometric elements and geometric
                 constraints, into simple forms, and adds redundant
                 constraints of distances and angles by solving
                 triangles and determining sums and differences of
                 adjacent angles. A wide variety of well-constrained
                 problems can the be solved by a simple technique of
                 local propagation, and over-constrained scenes and
                 input data contradictory to some well-known
                 mathematical theorems can also be detected in the same
                 phase. Only with under-constrained problems and some
                 special well-constrained cases, an additional step of
                 merging clusters and/or geometrical relaxation is
                 required.",
  editor =       "V. Skala",
  keywords =     "CAD, constraint-based design, geometric constraints,
                 geometric modelling.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-290,
  pages =        "333--342",
  year =         "2001",
  title =        "Reliable Path for Virtual Endoscopy: Ensuring Complete
                 Examination of Human Organs",
  author =       "Taosong He and Lichan Hong and Dongqing Chen and
                 Zhengrong Liang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-290",
  abstract =     "Virtual endoscopy is a computerized, noninvasive
                 procedure for detecting anomalies inside human organs.
                 Several preliminary studies have demonstrated the
                 benefits and effectiveness of this modality.
                 Unfortunately, previous work cannot guarantee that an
                 existing anomaly will be detected, especially for
                 complex organs with multiple branches. In this paper,
                 we introduce the concept of reliable navigation, which
                 ensures the interior organ surface is fully examined by
                 the physician performing the virtual endoscopy
                 procedure. To achieve this, we propose computing a
                 reliable fly-through path that ensures no blind area
                 during the navigation. Theoretically, we discuss the
                 criteria of evaluating a reliable path and prove that
                 the problem of generating an optimal reliable path for
                 virtual endoscopy is NP-complete. In practice, we
                 develop an efficient method for the calculation of an
                 effective reliable path. First, a small set of center
                 observation points are automatically located inside the
                 hollow organ. For each observation point, there exists
                 at least one patch of interior surface visible to it,
                 but that cannot be seen from any of the other
                 observation points. These chosen points are then linked
                 with a path that stays in the center of the organ.
                 Finally, new points inside the organ are recursively
                 selected and connected into the path until the entire
                 organ surface is visible from the path. We present
                 encouraging results from experiments on several data
                 sets. For a medium size volumetric model with several
                 hundred thousand inner voxels, an effective reliable
                 path can be generated in several minutes.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-291,
  pages =        "343--350",
  year =         "2001",
  title =        "Minimally Immersive Flow Visualization",
  author =       "David S. Ebert and Christopher D. Shaw",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-291",
  abstract =     "This paper describes a minimally immersive interactive
                 system for flow visualization of multivariate
                 volumetric data. The system, SFA, uses perceptually
                 motivated rendering to increase the quantity and
                 clarity of information perceived. Proprioception,
                 stereopsis, perceptually motivated shape visualization,
                 and three-dimensional interaction are combined in SFA
                 to allow the three-dimensional volumetric
                 visualization, manipulation, navigation, and analysis
                 of multivariate, time-varying flow data.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-292,
  pages =        "365--379",
  year =         "2001",
  title =        "Efficient Conservative Visibility Culling Using the
                 Prioritized-Layered Projection Algorithm",
  author =       "James T. Klosowski and Cl{\'{a}}udio T. Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-292",
  abstract =     "We propose a novel conservative visibility culling
                 technique based on the Prioritized-Layered Projection
                 (PLP) algorithm. PLP is a time-critical rendering
                 technique that computes, for a given viewpoint, a
                 partially correct image by rendering only a subset of
                 the geometric primitives, those that PLP determines to
                 be most likely visible. Our new algorithm builds on PLP
                 and provides an efficient way of finding the remaining
                 visible primitives. We do this by adding a second phase
                 to PLP which uses image-space techniques for
                 determining the visibility status of the remaining
                 geometry. Another contribution of our work is to show
                 how to efficiently implement such image-space
                 visibility queries using currently available OpenGL
                 hardware and extensions. We report on the
                 implementation of our techniques on several graphics
                 architectures, analyze their complexity, and discuss a
                 possible hardware extension that has the potential to
                 further increase performance.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-2001-293,
  pages =        "351--364",
  year =         "2001",
  title =        "Extracting Objects from Range and Radiance Images",
  author =       "Yizhou Yu and Andras Ferencz and Jitendra Malik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-293",
  abstract =     "In this paper, we present a pipeline and several key
                 techniques necessary for editing a real scene captured
                 with both cameras and laser range scanners. We develop
                 automatic algorithms to segment the geometry from range
                 images into distinct surfaces, register texture from
                 radiance images with the geometry, and synthesize
                 compact high-quality texture maps. The result is an
                 object-level representation of the scene which can be
                 rendered with modifications to structure via
                 traditional rendering methods. The segmentation
                 algorithm for geometry operates directly on the point
                 cloud from multiple registered 3D range images instead
                 of a reconstructed mesh. It is a top-down algorithm
                 which recursively partitions a point set into two
                 subsets using a pairwise similarity measure. The result
                 is a binary tree with individual surfaces as leaves.
                 Our image registration technique performs a very
                 efficient search to automatically find the camera poses
                 for arbitrary position and orientation relative to the
                 geometry. Thus, we can take photographs from any
                 location without precalibration between the scanner and
                 the camera. The algorithms have been applied to
                 large-scale real data. We demonstrate our ability to
                 edit a captured scene by moving, inserting, and
                 deleting objects.",
  organization = "IEEE Computer Society",
  volume =       "7(4)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InCollection{EVL-2001-294,
  pages =        "1--13",
  year =         "2001",
  title =        "Parametric {G}^{n} blending of curves and surfaces",
  author =       "Erich Hartmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-294",
  abstract =     "We introduce a simple blending method for parametric
                 curves and surfaces that produces families of
                 parametrically defined, G^{n}-continuous blending
                 curves and surfaces. The method depends essentially on
                 the parameterizations of the curves/surfaces to be
                 blended. Hence, the flexibility of the method relies on
                 the existence of suitable parameter transformations of
                 the given curves/surfaces. The feasibility of the
                 blending method is shown by several examples. The shape
                 of the blend curve/surface can be changed in a
                 predictable way with the aid of two design parameters
                 (thumb weight and balance).",
  volume =       "17(1)",
  keywords =     "G^{n} Continuity, G^{n} blending, Parametric blending
                 curves, Parametric blending surfaces, Thumb weight",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-295,
  pages =        "14--29",
  year =         "2001",
  title =        "Experimenting with nonintrusive motion capture in a
                 virtual environment",
  author =       "Andrea Bottino and Aldo Laurentini",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-295",
  abstract =     "A growing number of promising applications requires
                 recognizing human posture and motion. Conventional
                 techniques require us to attach foreign objects to the
                 body, which in some applications is disturbing or even
                 impossible. New, nonintrusive motion capture approaches
                 are called for. The well-known shape-from-silhouette
                 technique for understanding 3D shapes could also be
                 effective for human bodies. We present a novel
                 technique for model-based motion capture that uses
                 silhouettes extracted from multiple views. A 3D
                 reconstruction of the performer can be computed from a
                 silhouette with a technique known as volume
                 intersection. We can recover the posture by fitting a
                 model of the human body to the reconstructed volume.
                 The purpose of this work is to test the effectiveness
                 of this approach in a virtual environment by
                 investigating the precision of the posture and motion
                 obtained with various numbers and arrangements of
                 stationary cameras. An average 1% position error has
                 been obtained with five cameras.",
  volume =       "17(1)",
  keywords =     "Silhouettes, Volume intersection, Model-based
                 recognition, Motion capture",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-296,
  pages =        "30--45",
  year =         "2001",
  title =        "Surface scratches: measuring, modeling and rendering",
  author =       "S. Merillou and Jean-Michel Dischler and D.
                 Ghazanfarpour",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-296",
  abstract =     "One of the most important criticisms that can be made
                 concerning synthesized images is the brand new and too
                 clean aspect of objects. Surface color modifications
                 can be used to introduce dirtiness or other
                 aging-linked characteristics. Also, techniques such as
                 bump or displacement mapping allow users to improve
                 surface aspects by introducing geometrical
                 perturbations. In parallel, the bidirectional
                 reflectance distribution function (BRDF) is a crucial
                 factor in achieving a high degree of realism. It turns
                 out that surfaces are very often covered by defects
                 such as scratches that are related to both textures and
                 BRDFs due to their size. Scratches do not always affect
                 the apparent geometry but nevertheless can remain
                 strongly visible. None of the previously mentioned
                 methods is suited for rendering these defects
                 efficiently. We propose a new method, based on
                 extensions to existing BRDFs and classical 2D texture
                 mapping techniques, to render efficiently individually
                 visible scratches. We use physical measurements on
                 {"}real objects{"} to derive an accurate geometric
                 model of scratches at small scale range (roughness
                 scale), and we introduce a new geometric level between
                 bump mapping and BRDFs. Beyond providing graphical
                 results closely matching real cases, our method opens
                 the way to a new class of considerations in computer
                 graphics based on defects that require the coupling of
                 both BRDFs and texturing techniques.",
  volume =       "17(1)",
  keywords =     "Realistic rendering, Surface scratches, Physical
                 measurements, BRDFs, Texture mapping",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-297,
  pages =        "46--54",
  year =         "2001",
  title =        "A technique for precise depth representation in
                 stereoscopic display",
  author =       "Shunsuke Yoshida and Shinya Miyazaki and Toshihito
                 Hoshino and Toru Ozeki and Junichi Hasegawa and Takami
                 Yasuda and Shigeki Yokoi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-297",
  abstract =     "In observing a virtual 3D object displayed
                 stereoscopically on a large screen, there is often a
                 difference between the calculated depth and the
                 perceived depth. This paper presents a method for
                 reducing such differences of depth. To do this, we
                 modify both the viewing position and the screen
                 position in the stereoscopic calculation. The optimal
                 amount of modification was determined from sample
                 values of depth differences. The effectiveness of the
                 proposed method is discussed on the merits of the
                 experimental results. This technique decreased the
                 average difference from 4.3 mm to 1.3 mm.",
  volume =       "17(1)",
  keywords =     "Stereoscopic display, Depth perception, Virtual
                 reality, registration",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-298,
  pages =        "55--71",
  year =         "2001",
  title =        "Visualization of time-dependent data with feature
                 tracking and event detection,",
  author =       "Freek Reinders and Frits H. Post and Hans J. W.
                 Spoelder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-298",
  abstract =     "This paper presents an innovative method to analyze
                 and visualize time-dependent evolution of features. The
                 analysis and visualization of time-dependent data are
                 complicated because of the immense number of data
                 involved. However, if the scientist's main interest is
                 the evolution of certain features, it suffices to show
                 the evolution of these features. The task of the
                 visualization method is to extract the features from
                 all frames, to determine the correspondences between
                 features in successive frames, to detect significant
                 events or stages in the evolution of the features, and,
                 finally, to visualize the results. The method described
                 here performs all these steps, and it is applied to a
                 number of applications.",
  volume =       "17(1)",
  keywords =     "Visualization, Time-dependent data, Feature tracking,
                 Event detection Copyright",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-299,
  pages =        "72--74",
  year =         "2001",
  title =        "webcorner: Computer graphics bibliographic search on
                 the Internet",
  author =       "Laurent Moccozet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-299",
  volume =       "17(1)",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InProceedings{EVL-2001-3,
  year =         "2001",
  title =        "Multimodal Medical Volume Registration Based on
                 Spherical Markers",
  author =       "M. Capeck and R. Wegenkittl and A. Koenig and W.
                 Jaschke and R. Sweeney and R. Bale",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-3",
  abstract =     "We propose volume registration procedures based on
                 spherical artificial markers presented in medical
                 multimodal data sets (MRI and CT, especially). The
                 procedures proposed are either semi-automatic or
                 fully-automatic. A semi-automatic approach requires to
                 label approximate locations of the spherical markers in
                 the data sets and then registration operates
                 autonomically. A fully-automatic approach does not
                 require any user interaction, i.e. all registration
                 subtasks - namely segmentation of spheres, finding the
                 correspondence between two sets of spheres and,
                 finally, computing geometrical transformation that maps
                 the first set of spheres onto the second one - are
                 performed automatically by the computer.",
  editor =       "V. Skala",
  keywords =     "Multimodal medical registration, markers,
                 segmentation, iterative closest point algorithm.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-30,
  year =         "2001",
  title =        "Interactive Reconstruction of 3-{D} Objects from
                 Silhouettes",
  author =       "A. Bottino and L. Cavallero and A. Laurentini",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-30",
  abstract =     "Many techniques for reconstructing 3-D shapes from 2-D
                 images use silhouette data. A problem with this
                 approach is that, if no a priori information about the
                 3-D shape is available, we do not know neither the
                 accuracy of the reconstruction, nor where it is better
                 to locate new viewpoints for improving the accuracy. We
                 present and demonstrate a new general approach to
                 interactive, object-specific shape-from-silhouette
                 algorithms. The approach holds for completely unknown
                 shapes. It is based on a necessary condition for the
                 reconstruction to have been performed with the best
                 possible accuracy. From this condition, we derive: 1) a
                 quantitative measure of reconstruction accuracy; 2)
                 rules for finding new viewpoints if the accuracy is not
                 satisfactory. The algorithm has been implemented for
                 polyhedra, and demonstrated in a virtual environment.",
  editor =       "V. Skala",
  keywords =     "Computer vision, shape from silhouette, visual hull,
                 volume intersection, interactive algorithms, geometric
                 probing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-300,
  pages =        "76--90",
  year =         "2001",
  title =        "Rendering optimal solar shadows with plural sunlight
                 depth buffers",
  author =       "Katsumi Tadamura and Xueying Qin and Guofang Jiao and
                 Eihachiro Nakamae",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-300",
  abstract =     "We propose a novel method, based on the two-pass
                 Z-buffer algorithm, to calculate shadows with
                 sufficient precision and efficiency for rendering a
                 daytime landscape with solar penumbrae. The special
                 feature of the proposed method is that the shadows can
                 be preserved with a precision superior to that of any
                 visible surface. We use the optimal number of plural
                 shadow buffers to do this; it gives a fairly satisfying
                 trade-off between computation time and quality of
                 shadows.",
  volume =       "17(2)",
  keywords =     "Shadow, Penumbra, Rendering, Two-pass Z-buffer
                 algorithm, Photo-realism",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-301,
  pages =        "91--105",
  year =         "2001",
  title =        "Human motion coordination: a juggler as an example",
  author =       "Franck Multon and St{\'{e}}phane M{\'{e}}nardais and
                 Bruno Arnaldi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-301",
  abstract =     "This paper introduces a new method for the
                 coordination of human motion based on planning and AI
                 techniques. Motions are considered as black boxes that
                 are activated according to preconditions and produce
                 postconditions in a hybrid, continuous and discrete
                 world. Each part of the body is an autonomous entity
                 that cooperates with the others as determined by global
                 criteria, such as occupation rate and distance to a
                 goal (common to all the entities). With this technique,
                 we can easily specify and solve the motion coordination
                 problem of a juggler that juggles with a dynamic number
                 of balls in real time.",
  volume =       "17(2)",
  keywords =     "Motion coordination, AI techniques, Motion control,
                 Virtual human, Motion planification",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-302,
  pages =        "147--157",
  year =         "2001",
  title =        "An efficient animation of wrinkled cloth with
                 approximate implicit integration",
  author =       "Young-Min Kang and Jeong-Hyeon Choi and Hwan-Gue Cho
                 and Do-Hoon Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-302",
  abstract =     "This paper presents an efficient method for creating
                 the animation of flexible objects. The mass-spring
                 model was used to represent flexible objects. The
                 easiest approach to creating animation with the
                 mass-spring model is the explicit Euler method, but the
                 method has a serious weakness in that it suffers from
                 an instability problem. The implicit integration method
                 is a possible solution, but a critical flaw of the
                 implicit method is that it involves a large linear
                 system. This paper presents an approximate implicit
                 method for the mass-spring model. The proposed
                 technique updates with stability the state of n mass
                 points in O(n) time when the number of total springs is
                 O(n). In order to increase the efficiency of simulation
                 or reduce the numerical errors of the proposed
                 approximate implicit method, the number of mass points
                 must be as small as possible. However, coarse
                 discretization with a small number of mass points
                 generates an unrealistic appearance for a cloth model.
                 By introducing a wrinkled cubic spline curve, we
                 propose a new technique that generates realistic
                 details of the cloth model, even though a small number
                 of mass points are used for simulation.",
  volume =       "17(3)",
  keywords =     "Cloth animation, Mass spring model, Implicit method,
                 Realistic detail, Wrinkled curve",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-303,
  pages =        "106--120",
  year =         "2001",
  title =        "Visualization of optical phenomena caused by
                 multilayer films based on wave optics",
  author =       "H. Hirayama and K. Kaneda and H. Yamashita and Y.
                 Yamaji and Y. Monden",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-303",
  abstract =     "This paper proposes a method for rendering objects
                 coated with multilayer thin films, taking into
                 consideration multiple reflection and refraction,
                 interference, and absorption of light inside the films.
                 The proposed method is based on wave optics, and it can
                 accurately visualize the optical effects of multilayer
                 films consisting of not only dielectric materials, but
                 also metallic and semiconductive materials. Optical
                 properties of a SiO_{2} film coating on a silicon base,
                 and several kinds of multilayer films coating
                 windowpanes, glasses, or teapots are visualized to
                 demonstrate the usefulness of the proposed method.",
  volume =       "17(2)",
  keywords =     "Multilayer thin film, Multiple reflection and
                 refraction, Interference, Complex refractive index,
                 Composite reflectance and transmittance",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-304,
  pages =        "121--131",
  year =         "2001",
  title =        "Simulation of postoperative 3{D} facial morphology
                 using a physics-based head model",
  author =       "Yoshimitsu Aoki and Shuji Hashimoto and Masahiko
                 Terajima and Akihiko Nakasima",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-304",
  abstract =     "We propose a prototype of a facial surgery simulation
                 system for surgical planning and the prediction of
                 facial deformation. We use a physics-based human head
                 model. Our head model has a 3D hierarchical structure
                 that consists of soft tissue and the skull, constructed
                 from the exact 3D CT patient data. Anatomic points
                 measured on X-ray images from both frontal and side
                 views are used to fire the model to the patient's head.
                 The purposes of this research is to analyze the
                 relationship between changes of mandibular position and
                 facial morphology after orthognathic surgery, and to
                 simulate the exact postoperative 3D facial shape. In
                 the experiment, we used our model to predict the facial
                 shape after surgery for patients with mandibular
                 prognathism. Comparing the simulation results and the
                 actual facial images after the surgery shows that the
                 proposed method is practical.",
  volume =       "17(2)",
  keywords =     "Head modeling, Physics-based deformation, Surgical
                 simulation, Orthodontics, X-ray image",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-305,
  pages =        "134--146",
  year =         "2001",
  title =        "Reconstructing 2{D} images with natural neighbour
                 interpolation",
  author =       "Fran{\c{c}}ois Anton and Darka Mioc and Alain
                 Fournier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-305",
  abstract =     "In this paper, we explore image reconstruction by
                 natural neighbour interpolation from irregularly spaced
                 samples. We sample the image irregularly with
                 techniques based on the Laplacian or the derivative in
                 the direction of the gradient. Local coordinates based
                 on the Voronoi diagram are used in natural neighbour
                 interpolation to quantify the {"}neighbourliness{"} of
                 data sites. Then we use natural neighbour interpolation
                 in order to reconstruct the image. The main result is
                 that the image quality is always very good in the case
                 of the sampling techniques based on the Laplacian.",
  volume =       "17(3)",
  keywords =     "Image reconstruction, Irregularly spaced samples,
                 Natural neighbour interpolation, Local coordinates",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-306,
  pages =        "158--166",
  year =         "2001",
  title =        "An improved articulated model of the human hand",
  author =       "John McDonald and Jorge Toro and Karen Alkoby and
                 Andre Berthiaume and Roymieco Carter and Pattaraporn
                 Chomwong and Juliet Christopher and Mary Jo Davidson
                 and Jacob Furst and Brian Konie and Glenn Lancaster and
                 Lopa Roychoudhuri and Eric Sedgwick and Noriko Tomuro
                 and Rosalee Wolfe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-306",
  abstract =     "We present an improved anatomically based approach to
                 modeling the human hand for use in the animation of the
                 American Sign Language. The joint rotations in the
                 model are based on the bone and muscle configurations
                 of the hand, and a forward kinematic solution is used
                 to position the hand. In particular, we investigate the
                 rotations of the base joint of the thumb. This joint is
                 a saddle joint with nontrivial rotational axes and
                 centers, and must be treated with care in such a model.
                 We take advantage of several correlations between joint
                 rotations in the hand to reduce the number of degrees
                 of freedom in the model and to provide a simple,
                 natural, and interactive interface for American Sign
                 Language handshape transcription.",
  volume =       "17(3)",
  keywords =     "Modeling, Animation, American sign language,
                 Anatomically based modeling",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-307,
  pages =        "167--178",
  year =         "2001",
  title =        "d-Dimensional parametric models for dynamic animation
                 of deformable objects",
  author =       "Yannick Remion and Jean-Michel Nourrit and Olivier
                 Nocent",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-307",
  abstract =     "This paper introduces an accurate, efficient, and
                 unified engine dedicated to dynamic animation of
                 d-dimensional deformable objects. The objects are
                 modelled as d -dimensional manifolds defined as
                 functional combinations of a mesh of 3D control points,
                 weighted by parametric blending functions. This model
                 ensures that, at each time step, the object shape
                 conforms to its manifold definitions. The object motion
                 is deduced from the control points dynamic animation.
                 In fact, control points should be viewed as the degrees
                 of freedom of the continuous object. The chosen dynamic
                 equations (Lagrangian formalism) reflect this generic
                 modelling scheme and yield an exact and computationally
                 efficient linear system.",
  volume =       "17(3)",
  keywords =     "Dynamic animation, Lagrangian equations, Parametric
                 surfaces, Parametric volumes, Deformable objects",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-308,
  pages =        "179--184",
  year =         "2001",
  title =        "Two methods for cloud visualisation from weather
                 simulation data",
  author =       "Andrzej Trembilski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-308",
  abstract =     "For the local TV presentation of weather forecast
                 data, it is important to have high-quality and fast
                 visualisation of clouds. In this paper, I present
                 methods for the visualisation of clouds from data
                 produced by a meteorological weather simulation.
                 Isosurfaces, which are originally too coarse because of
                 the data grid resolution, are refined and deformed. The
                 resulting geometry is used for cloud visualisation.",
  volume =       "17(3)",
  keywords =     "Meteorological visualisation, Cloud visualisation,
                 Surface refinement",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-309,
  pages =        "185--197",
  year =         "2001",
  title =        "Multiresolution volume visualization with a
                 texture-based octree",
  author =       "Imma Boada and Isabel Navazo and Roberto Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-309",
  abstract =     "Although 3D texture-based volume rendering guarantees
                 image quality almost interactively, it is difficult to
                 maintain an interactive rate when the technique has to
                 be exploited on large datasets. In this paper, we
                 propose a new texture memory representation and a
                 management policy that substitute the classical
                 one-texel per voxel approach for a hierarchical
                 approach. The hierarchical approach benefits nearly
                 homogeneous regions and regions of lower interest. The
                 proposed algorithm is based on a simple traversal of
                 the octree representation of the volume data. Driven by
                 a user-defined image quality, defined as a combination
                 of data homogeneity and importance, a set of octree
                 nodes (the cut) is selected to be rendered. The degree
                 of accuracy applied for the representation of each one
                 of the nodes of the cut in the texture memory is set
                 independently according to the user-defined parameters.
                 The variable resolution texture model obtained reduces
                 the texture memory size and thus texture swapping,
                 improving rendering speed.",
  volume =       "17(3)",
  keywords =     "Volume rendering, Octree, 3D Texture mapping,
                 Multiresolution representation and rendering",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InProceedings{EVL-2001-31,
  year =         "2001",
  title =        "Linear {BSP} Trees for Sets of Hyperrectangles with
                 Low Directional Density",
  author =       "Petr Tobola and K. Nechvile",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-31",
  abstract =     "We consider the problem of constructing of binary
                 space partitions (BSP) for a set S of n hyperrectangles
                 in space with constant dimension. If the set S fulfills
                 the low directional density condition defined in this
                 paper then the resultant BSP has O(n) size and it can
                 be constructed in O(n log2 n) time in R3 . The low
                 directional density condition defines a new class of
                 objects which we are able to construct a linear BSP
                 for. The method is quite simple and it should be
                 appropriate for practical implementation.",
  editor =       "V. Skala",
  keywords =     "BSP, partitioning, hyperrectangle.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-310,
  pages =        "199--208",
  year =         "2001",
  title =        "Robust and efficient surface reconstruction from
                 contours",
  author =       "G. Cong and B. Parvin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-310",
  abstract =     "We propose a new approach for surface recovery from
                 planar sectional contours. The surface is reconstructed
                 based on the so-called {"}equal importance
                 criterion,{"} which suggests that every point in the
                 region contributes equally to the reconstruction
                 process. The problem is then formulated in terms of a
                 partial differential equation, and the solution is
                 efficiently calculated from distance transformation. To
                 make the algorithm valid for different application
                 purposes, both the isosurface and the primitive
                 representations of the object surface are derived. The
                 isosurface is constructed by means of a partial
                 differential equation, which can be solved iteratively.
                 The traditional distance interpolating method, which
                 was used by several researchers for surface
                 reconstruction, is an approximate solution of the
                 equation. The primitive representations are
                 approximated by Voronoi diagram transformation of the
                 surface space. Isosurfaces have the advantage that
                 subsequent geometric analysis of the object can be
                 easily carried out while primitive representation is
                 easy to visualize. The proposed technique allows for
                 surface recovery at any desired resolution, thus
                 avoiding the inherent problems of correspondence,
                 tiling, and branching.",
  volume =       "17(4)",
  keywords =     "Infinity Laplacian equation, shape from contour, shape
                 from slices, surface reconstruction",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-311,
  pages =        "209--218",
  year =         "2001",
  title =        "The normal of a fractal surface",
  author =       "Wayne O. Cochran and Robert R. Lewis and John C.
                 Hart",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-311",
  abstract =     "We have discovered a class of fractal functions that
                 are differentiable. Fractal interpolation functions
                 have been used for over a decade to generate rough
                 functions passing through a set of given points. The
                 integral of a fractal interpolation function remains a
                 fractal interpolation function, and this new fractal
                 interpolation function is differentiable. Tensor
                 products of pairs of these fractal functions form
                 fractal surfaces with a well-defined tangent plane. We
                 use this surface normal to shade fractal surfaces, and
                 demonstrate its use with renderings of fractal
                 mirrors.",
  volume =       "17(4)",
  keywords =     "Fractal, Fractal function, Fractal terrain, Iterated
                 function system, Recurrent modeling",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-312,
  pages =        "219--235",
  year =         "2001",
  title =        "Towards cognitive evaluation of computer-drawn
                 sketche",
  author =       "Mahes Visvalingam and Kurt Dowson.",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-312",
  abstract =     "This paper seeks to raise visual comparisons beyond
                 subjective opinions into evidence-based visual
                 reasoning. It provides an informal deductive analysis
                 of the marks in sketches derived with two competing
                 line-filtering algorithms. This prompted the novel
                 speculation that the visual system might be placing
                 some types of anomalies in the foreground of mental 3D
                 space, where they can be ignored. A brief survey is
                 provided to encourage informed debate. Although the
                 proposed cognitive computation was not automated, it
                 justified the rejection of the Douglas-Peucker
                 algorithm in favour of Visvalingam's algorithm in
                 subsequent research within the Cartographic Information
                 Systems Research Group (CISRG).",
  volume =       "17(4)",
  keywords =     "Line generalisation algorithms, Terrain visualisation,
                 Gestalt perception, Cognitive analysis, Subconscious
                 computation",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-313,
  pages =        "236--242",
  year =         "2001",
  title =        "Extension of the Nicholls - Lee - Nichols Algorithm to
                 Three Dimensions",
  author =       "V{\'{a}}clav Skala and Duc Huy Bui",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-313",
  abstract =     "A new algorithm for clipping a line segment against a
                 pyramid in E3 is presented. This algorithm avoids
                 computation of intersection points that are not end
                 points of the output line segment. It also solves all
                 cases more effectively. The performance of this
                 algorithm is shown to be consistently better than that
                 of existing algorithms, including the Cohen-Sutherland,
                 Liang-Barsky, and Cyrus-Beck algorithms.",
  volume =       "17(4)",
  keywords =     "Line clipping, Computer graphics, Algorithm
                 complexity, Geometric algorithms, Algorithm complexity
                 analysis",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-314,
  pages =        "243--257",
  year =         "2001",
  title =        "Binary volume rendering using Slice-based Binary
                 Shell",
  author =       "Bo Hyoung Kim and Jinwook Seo and Yeong Gil Shin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-314",
  abstract =     "This paper presents a new data structure, Slice-based
                 Binary Shell (SBS), for efficient manipulation and
                 rendering of binary volume data. Since SBS stores only
                 surface voxels with selected attributes of the voxels
                 in a slice-based data structure that allows direct
                 access to the voxels, it shows high storage and
                 computational efficiency. This efficiency becomes more
                 prominent when representing multiple binary objects. We
                 also present an efficient rendering algorithm for SBS.
                 The algorithm, based on the shear-warp technique,
                 provides high-speed interactive rendering for binary
                 volumes of many objects on a PC with no specialized
                 hardware.",
  volume =       "17(4)",
  keywords =     "Binary volume rendering, Surface voxel, Spatial data
                 structure, Medical imaging",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-315,
  pages =        "258--271",
  year =         "2001",
  title =        "Functionally based virtual embossing",
  author =       "Alexei Sourin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-315",
  abstract =     "Embossing is the art of decorating metals in relief
                 from the reverse side. This article describes how
                 virtual embossing can be done using a functionally
                 based representation of the metal plate and the tools.
                 The program is implemented as an interactive shape
                 modeler where a functional model of the metal plate is
                 subsequently modified with offset and set-theoretic
                 operations. For visualization, interactive ray tracing
                 is used. Bounding boxes together with the spatial
                 organization of the functional model provide the
                 required fast function evaluation that is usually a
                 bottleneck for functionally based shape modeling
                 systems. The program runs on a personal computer.",
  volume =       "17(4)",
  keywords =     "Computer art, embossing, virtual reality, functionally
                 based shape modeling, F-rep.",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-316,
  pages =        "274--286",
  year =         "2001",
  title =        "Enabling cuts on multiresolution representation",
  author =       "F. Ganovelli and P. Cignoni and C. Montani and R.
                 Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-316",
  abstract =     "Multiresolution representations are widely used in
                 many visualization contexts and applications, since
                 they provide optimal management of the data
                 representation by using at each time instant a level of
                 detail most appropriate for the application
                 requirements. Unfortunately, current solutions do not
                 allow the topology of the object to be changed, and
                 this tends to prevent its adoption in applications
                 where topological changes are needed, such as virtual
                 surgery applications. By extending a known
                 multiresolution model based on simplicial complexes, we
                 develop a new approach which supports dynamic
                 topological modifications of the represented object
                 without greatly increasing the representation
                 complexity.",
  volume =       "17(5)",
  keywords =     "Multiresolution, Simplicial complexes, Deformable
                 object modelling",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-317,
  pages =        "329--336",
  year =         "2001",
  title =        "Reparameterization of piecewise rational
                 {B}{\'{e}}zier curves and its application",
  author =       "Yoshimasa Tokuyama and Kouichi Konno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-317",
  abstract =     "A piecewise rational B{\'{e}}zier curve is formed by
                 several rational B{\'{e}}zier curve segments. It can be
                 represented by a rational B-spline curve where the
                 multiplicity of each interior knot equals degree.
                 Although the curve segments are C^{1} continuous in
                 three dimensions, they may be C^{0 }continuous in four
                 dimensions. In this case, the multiplicity of each
                 interior knot cannot be reduced and the B-spline basis
                 function becomes C^{0} continuous. Using a surface
                 generation method, such as skinning these kinds of
                 rational B-spline curves to construct an interpolatory
                 surface, may generate surfaces with C^{0} continuity.
                 This paper presents a reparameterization method for
                 reducing the multiplicity of each interior knot to make
                 the curve segments C^{1} continuous in four dimensions.
                 The reparameterized rational B-spline curve has the
                 same shape and degree as before and also has a standard
                 form. Some applications in skinned surface and ruled
                 surface generation based on the reparameterized curves
                 are shown.",
  volume =       "17(6)",
  keywords =     "Piecewise rational B{\'{e}}zier curve, Rational
                 B-spline curve, Reparameterization, Skinning, Ruled",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-318,
  pages =        "287--309",
  year =         "2001",
  title =        "Crack pattern simulation based on 3{D} surface
                 cellular automata",
  author =       "St{\'{e}}phane Gobron and Norishige Chiba",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-318",
  abstract =     "This article describes a method for modeling the
                 propagation of cracks on any 3D surface. This method
                 allows almost any type of cracks on any type of
                 triangulated 3D object. Our model's main advantage is
                 that it proposes a semi-physical solution, making it
                 both user controllable and easily extensible. We first
                 introduce the general development of cracks. We then
                 present our original model of spectrum stress, followed
                 by a description of the mutual interaction between
                 cracks and stresses. Then, we describe special
                 rendering techniques including the multi-thickness
                 anti-aliasing linked-segment method and the crack
                 mirror special effect. The final section presents
                 intermediate graphical results that review the entire
                 model as well as a set of different crack patterns
                 using various types of material such as concrete,
                 ceramic, mud, and glaze.",
  volume =       "17(5)",
  keywords =     "Cellular automaton, Cracking, Hyper-texture,
                 Multi-layer modeling, Simulation",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-319,
  pages =        "310--317",
  year =         "2001",
  title =        "Visualization of eclipses and planetary conjunction
                 events. The interplay between model coherence, scaling
                 and animation",
  author =       "Walter Oberschelp and Alexander Hornung and Horst
                 Samulowitz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-319",
  abstract =     "The problem of an instructive and realistic animation
                 and visualization of the shadow- and color-conditions
                 during conjunctions of actively and passively
                 illuminated cosmic objects has found only particularly
                 satisfying solutions so far. As an example we study a
                 total solar eclipse. There are didactic shortcomings of
                 specialized astronomical software, even though
                 solutions have been given, which are very impressive
                 for experts. Using the possibilities of commercial
                 3D-animation software we give an object-oriented
                 partial solution. In order to get correct astronomical
                 representations we model - for different tasks - the
                 object space under cinematic aspects with parameters
                 for spatial and temporal scaling, for illumination and
                 coloring under couplings of varying strength. The
                 adaptation of the parameters to optimal acceptance of
                 the spectator must be done a posteriori.",
  volume =       "17(5)",
  keywords =     "Animation of celestial conjunctions, Object oriented
                 motion simulation, Illumination by bright sources,
                 Weber-Fechner-perception, Solar eclipse",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InProceedings{EVL-2001-32,
  year =         "2001",
  title =        "Examining the Generality of a Behavioural Animation
                 Framework",
  author =       "J. R. P Hanna and R. J. Millar and W. M. Johnston",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-32",
  abstract =     "This paper tests the hypothesis that it is possible to
                 build a generic Behavioural Animation system. The paper
                 outlines the four models that form the key elements of
                 a Behavioural Animation system, and a proposed generic
                 structure for the Behavioural Component. Three
                 implementations, all of which follow this structure but
                 which differ in implementation detail, are then
                 presented. Each implementation is tested with several
                 animations and the results tabulated. It is then shown
                 that the previously proposed structure is sufficiently
                 generic if goals are deleted after one cycle. It is
                 also demonstrated that, if goals are to remain in the
                 goal queue for more than one cycle, a rule-based system
                 for deciding when to delete a goal is required in order
                 to achieve the required level of genericity.",
  editor =       "V. Skala",
  keywords =     "Animation, behavioural animation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-320,
  pages =        "318--327",
  year =         "2001",
  title =        "Orientation lightmaps for photon tracing in complex
                 environments",
  author =       "Alexander Wilkie and Robert F. Tobler and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-320",
  abstract =     "We present a method that makes the use of photon
                 tracing methods feasible for complex scenes when a
                 totally accurate solution is not essential. This is
                 accomplished by using orientation lightmaps, which
                 average the illumination of complex objects depending
                 on the surface normal. Through this averaging, they
                 considerably reduce the variance of the stochastic
                 solution. In order to use these specialised lightmaps,
                 which consume comparatively small amounts of memory, no
                 changes have to be made to the basic photon-tracing
                 algorithm. Also, they can be freely mixed with normal
                 lightmaps. This gives the user good control over the
                 amount of inaccuracy he introduces by their
                 application. The area computations necessary for their
                 insertion are performed using a stochastic sampling
                 method that performs well for highly complex objects.",
  volume =       "17(5)",
  keywords =     "Photon tracing, Scene complexity, Lightmaps",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-321,
  pages =        "337--352",
  year =         "2001",
  title =        "Improved quadratic normal vector interpolation for
                 realistic shading",
  author =       "Yuan-Chung Lee and Chein-Wei Jen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-321",
  abstract =     "Interpolation for normal vectors is an important stage
                 of Phong shading. Linear interpolation cannot represent
                 the geometry of surfaces adequately in some situations.
                 Previous quadratic interpolation for normal vectors
                 generates visual artifacts for arch-type curves and
                 triangle scan conversion. To eliminate those artifacts,
                 we propose an improved quadratic interpolation for
                 normal vectors with little computational overhead.
                 Perspective-correct version for obtaining perspective
                 foreshortening is also presented.",
  volume =       "17(6)",
  keywords =     "Shading, Quadratic interpolation, Perspective
                 correction",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-322,
  pages =        "353--369",
  year =         "2001",
  title =        "Parameterization for reconstruction of 3{D} freeform
                 objects from laser-scanned data based on a {PDE}
                 method,",
  author =       "J. Barhak and A. Fischer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-322",
  abstract =     "In reverse engineering, laser scanners are commonly
                 used since 3D data is sampled fast and accurately
                 relative to other systems. However, they provide an
                 enormous amount of irregular data that requires
                 intensive reconstruction processing, based on the
                 parameterization and surface fitting stages. Selection
                 of an appropriate parameterization is essential for
                 topology reconstruction as well as surface fitness.
                 Current parameterization methods have topological
                 problems, leading to undesired noisy self-intersecting
                 surfaces. In this paper, a new adaptive
                 parameterization PDE (partial differential equation)
                 method is proposed for sculptured objects that were
                 scanned from a single direction. The method is based on
                 calculating a PDE non-self-intersecting parametric
                 grid, and then fitting the base surface using CMA
                 (control mesh approximation) and adaptive GDA (gradient
                 descent approximation) methods.",
  volume =       "17(6)",
  keywords =     "Reverse engineering, Laser scanner, Parameterization,
                 Surface reconstruction",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-323,
  pages =        "370--379",
  year =         "2001",
  title =        "Direct manipulation of {FFD}: efficient explicit
                 solutions and decomposible multiple point constraints",
  author =       "Shi-Min Hu and Hui Zhang and Chiew-Lan Tai and
                 Jia-Guang Sun",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-323",
  abstract =     "The ability to directly manipulate an embedded object
                 in the free-form deformation (FFD) method improves
                 controllability. However, the existing solution to this
                 problem involves a pseudo-inverse matrix that requires
                 complicated calculations. This paper solves the problem
                 using a constrained optimization method. We derive the
                 explicit solutions for deforming an object which is to
                 pass through a given target point. For constraints with
                 multiple target points, the proposed solution also
                 involves simple calculations, only requiring solving a
                 system of linear equations. We show that the direct
                 manipulations exhibit the commutative group property,
                 namely commutative, associative, and invertible
                 properties, which further enhance the controllability
                 of FFD. In addition, we show that multiple point
                 constraints can be decomposed into separate
                 manipulations of single point constraints, thus
                 providing the user the freedom of specifying the
                 constraints in any appropriate order.",
  volume =       "17(6)",
  keywords =     "Animation, Deformations, Geometric modeling",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-324,
  pages =        "380--395",
  year =         "2001",
  title =        "Multicriteria-optimized triangulations",
  author =       "I. Kolingerov{\'{a}} and A. Ferko",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-324",
  abstract =     "Triangulation of a given set of points in a plane is
                 one of the most commonly solved problems in computer
                 graphics and computational geometry. Because they are
                 useful in many applications, triangulations must
                 provide well-shaped triangles. Many criteria have been
                 developed to provide such meshes, namely weight and
                 angular criteria. Each criterion has its pros and cons,
                 some of them are difficult to compute, and sometimes
                 even the polynomial algorithm is not known. By any of
                 the existing deterministic methods, it is not possible
                 to compute a triangulation which satisfies more than
                 one criterion or which contains parts developed
                 according to several criteria. We explain how such a
                 mixture can be generated using genetic optimization.",
  volume =       "17(6)",
  keywords =     "Computer graphics, Computational geometry, Minimum
                 weight triangulation, Delaunay triangulation, Genetic
                 optimization",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-325,
  pages =        "397--414",
  year =         "2001",
  title =        "Volume-based three-dimensional metamorphosis using
                 sphere-guided region correspondence",
  author =       "Graham Treece and Richard Prager and Andrew Ge",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-325",
  abstract =     "The metamorphosis of one image into another has
                 frequently been used to create impressive visual
                 effects. Three-dimensional surface metamorphosis
                 extends this paradigm by interpolating between discrete
                 volume representations of the surfaces. The
                 insensitivity of the established techniques to the
                 surface topology enables morphing between completely
                 different surfaces: however it can also lead to
                 intermediate surfaces which have different topology
                 from the originals. We present a method which improves
                 on this situation by ensuring that no part of each
                 surface remains disconnected during the morph. The
                 morph is guided by region correspondence, derived
                 automatically from a sphere representation of each
                 surface: this can be combined with manual
                 correspondence to retain user control over the morph.
                 What emerges is a fast and flexible method for morphing
                 surfaces, as demonstrated on several examples.",
  volume =       "17(7)",
  keywords =     "Metamorphosis, shape interpolation, volume graphics,
                 isosurface extraction",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-326,
  pages =        "415--428",
  year =         "2001",
  title =        "Joining polyhedral objects using implicitly defined
                 surfaces",
  author =       "Karan Singh and Richard Parent",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-326",
  abstract =     "Complex polyhedral objects are often constructed from
                 simpler polyhedral objects using constructive solid
                 geometry, booleans, skinning and shrinkwrap techniques.
                 This paper presents a new technique for incrementally
                 building complex polyhedral objects from simpler
                 polyhedral parts. We provide a procedural implicit
                 function definition for a region of a polyhedral object
                 that is star-shaped with respect to a skeletal point,
                 called a blend center. We extend this definition to
                 provide a single implicit function definition for an
                 arbitrary polyhedral object, where every region is
                 star-shaped with respect to a proximal blend center,
                 chosen from an arbitrary set of blend centers. This
                 allows the application of implicit function-based
                 modeling techniques in constructing transition surfaces
                 between arbitrary polyhedral object parts. At the same
                 time the original detail and character of object parts
                 are preserved in regions where they do not blend or
                 interact with other object parts. A complete
                 implementation of the concepts presented shows
                 polyhedral implicit primitives to be an efficient and
                 general technique for building complex polyhedral
                 objects from a modular set of polyhedral object
                 parts.",
  volume =       "17(7)",
  keywords =     "Polyhedral objects, Constructive solid geometr,
                 Implicit surface, Blend surfaces, Tesselation",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-327,
  pages =        "429--444",
  year =         "2001",
  title =        "A spectrally based framework for realistic image
                 synthesis",
  author =       "Yinlong Sun and F. David Fracchia and Mark S. Drew and
                 Thomas W. Calvert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-327",
  abstract =     "We propose a new rendering framework which emphasizes
                 real spectra as input, retains full spectral
                 light-object interactions, and generates spectral
                 images (convertible to RGB images for display). We show
                 that this approach suffices to generate optical effects
                 important for realistic image synthesis. To facilitate
                 implementation, we propose a composite spectral model
                 by decomposing all spectra into a smooth background and
                 a list of spikes. This model is accurate, compact and
                 efficient, and capable of handling spike-related
                 optical effects including dispersion, diffraction, and
                 fluorescence. Finally, we propose using pixel-pixel
                 CIELab histograms to quantitatively evaluate images
                 synthesized with different methods.",
  volume =       "17(7)",
  keywords =     "Realistic image synthesis, Spectral mode, Spectral
                 rendering, Optical phenomena, Error evaluation",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-328,
  pages =        "445--456",
  year =         "2001",
  title =        "The normalform of a space curve and its application to
                 surface design",
  author =       "Erich Hartmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-328",
  abstract =     "The normalform of a space curve is introduced
                 analogously to the normalform of a plane curve and a
                 surface, i.e. an implicit representation h(x)=0 with
                 &par;&dtri;h&par;=1. The normalform function h is
                 (unlike the latter cases) not differentiable at curve
                 points. Despite of this disadvantage the normalform is
                 a suitable tool for designing surfaces which can be
                 treated as common implicit surfaces. Many examples
                 (bisector surfaces, constant distance sum/product
                 surfaces, metamorphoses, blending surfaces, smooth
                 approximation surfaces) demonstrate applications of the
                 normalform to surface design.",
  volume =       "17(7)",
  keywords =     "Normalform, bisectors, constant distance sum/product
                 surfaces, pipe surface, intersection curves,
                 intersection points, G^{n}-blending, isophotes",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@TechReport{EVL-2001-329,
  year =         "2001",
  title =        "Peripheral Vessel Investigation For Routine Clinical
                 Use",
  author =       "Armin Kanitsar and Rainer Wegenkittl and Petr Felkel
                 and Dominik Fleischmann and Dominique Sandner and
                 Eduard Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-329",
  number =       "TR-186-2-01-13",
  institution =  "Institute of Computer Graphics and Algorithms,
                 Visualization and Animation Group, Vienna University of
                 Technology",
}

@InProceedings{EVL-2001-33,
  year =         "2001",
  title =        "Towards Mechanical Level of Detail for Knitwear
                 Simulation",
  author =       "Olivier Nocent and Jean-Michel Nourrit and Yannick
                 Remion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-33",
  abstract =     "This paper introduces the foundations of a new
                 mechanical level of detail method dedicated to knitted
                 fabric simulation. This method consists as a reduction
                 of the knitted cloth parameters number, decreasing the
                 configurations space dimension of the studied cloth. We
                 stress the importance of the choice of an efficient
                 parameters reduction function in order to make the
                 method usable. The consequences of this reduction on
                 the underlying equations of motion are then detailed.
                 Finally, we present some numerical results and
                 animation snapshots that illustrate the advantages of
                 this new level of detail scheme.",
  editor =       "V. Skala",
  keywords =     "Lagrangian formalism, generalised coordinates, dynamic
                 animation, level of detail.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@TechReport{EVL-2001-330,
  pages =        "8",
  year =         "2001",
  title =        "{SMART}---Surface Models from
                 by-Axis-and-Radius-Defined Tubes",
  type =         "VRVis Research Center Technical Report",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-330",
  author =       "P.~Felkel and A.~L.~Fuhrmann and R.~Wegenkittl",
  address =      "Donau-City-Stra\ss{}e 1, A-1220 Vienna, Austria,
                 \textit{www.vrvis.at}",
  month =        feb,
  number =       "TR-VRVis-2001-026",
  institution =  "VRVis Center",
}

@TechReport{EVL-2001-331,
  pages =        "4",
  year =         "2001",
  title =        "A Fully Automatic Stitching of 2{D} Medical Data
                 Sets",
  type =         "VRVis Research Center Technical Report",
  author =       "Martin \v{C}apek and Rainer Wegenkittl and Petr
                 Felkel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-331",
  address =      "Donau-City-Stra\ss{}e 1, A-1220 Vienna, Austria,
                 \textit{www.vrvis.at}",
  month =        feb,
  number =       "TR-VRVis-2001-029",
  institution =  "VRVis Center",
}

@InProceedings{EVL-2001-332,
  pages =        "39--48",
  year =         "2001",
  title =        "Postprocessing and Visualization of peripheral {CTA}
                 data in clinical environments",
  author =       "A. Kanitsar and R. Wegenkittl and P. Felkel and D.
                 Fleischmann and D. Sandner and E. Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-332",
  abstract =     "This paper deals with computed tomography angiography
                 based vessel exploration. Large image sequences of the
                 lower extremities are investigated in a clinical
                 environment. Two different approaches for peripheral
                 vessel diagnosis dealing with stenosis and
                 calcification detection are introduced. The paper
                 presents an automated vessel-tracking tool for curved
                 planar reformation. A user interactive segmentation
                 tool for bone removal is proposed.",
  editor =       "Ivan Viola and Thomas Theu\ss{}l",
  keywords =     "computed tomography angiography, semi automated
                 segmentation, optimal path computation, vessel
                 tracking.",
  booktitle =    "Central European Seminar on Computer Graphics for
                 students, CESCG'2001",
}

@InProceedings{EVL-2001-333,
  pages =        "477--480",
  year =         "2001",
  title =        "{Computed Tomography Angiography: A Case Study of
                 Peripheral Vessel Investigation}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-333",
  author =       "A. Kanitsar and R. Wegenkittl and P. Felkel and D.
                 Fleischmann and D. Sandner and E. Gr{\"{o}}ller",
  address =      "San Diego, CA, USA",
  month =        oct,
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Visualization 2001 (Vis'2001)",
  publisher =    "IEEE Piscataway, NJ, USA",
}

@InProceedings{EVL-2001-334,
  pages =        "S236",
  year =         "2001",
  title =        "Automated vessel detection at lower extremity
                 multislice {CTA}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-334",
  author =       "A. Kanitsar and R. Wegenkittl and P. Felkel and D.
                 Sandner and E. Gr{\"o}ller and D. Fleischmann",
  abstract =     "\textbf{Purpose:} To date multislice CT angiography
                 (CTA) of the lower extremity arteries is technically
                 feasible, but substantially limited by the need for
                 time-consuming manual image editing and manipulation
                 before the datasets can be effectively interpreted. We
                 present a new computer-assisted vessel tracking and
                 segmentation method, yielding curved planar
                 reformations (CPR) through the central axes of the
                 vascular tree with a minimum of user interaction. \\n
                 \textbf{Materials and methods:} A new algorithm based
                 on a minimal cost search over the gradient cost space
                 was applied to three multislice CTA datasets (1070 -
                 1134 transverse slices from the suprarenal aorta to the
                 mid-foot) obtained from patients with occlusive
                 arterial disease. Acquisition parameters were: 4  2.5
                 mm or 4  1 mm detector configuration, 3 mm/1 mm
                 section thickness/spacing. The total time needed for
                 the generation of CPRs through the automatically traced
                 center-line of the vessels, and the imaging features of
                 occlusive disease (stenosis, hard- and soft plaques)
                 were directly compared to manually generated sagittal
                 and coronal CPRs - with digital subtraction angiography
                 (DSA) as the standard of reference. \textbf{Results:}
                 Automated CPRs through the lower extremity arteries
                 were obtained with minimal user interaction in less
                 than 15 minutes in all datasets. Manual generation of
                 CPRs required between 90 and 120 minutes per case. The
                 automated alogrithm caused less {"}pseudostenosis{"}
                 due to an eccentric course of the CPRs compared to the
                 manual CPRs. \textbf{Conclusion:} The presented
                 algorithm is fast and simple enough to become a
                 routinely applicable tool for effective interpretation
                 of lower extremity CTA.",
  note =         "Oral presentation B-0637 at European Congres of
                 Radiology '2001 (ECR 2001), March 2--6, 2001, Austria
                 Center, Vienna, Austria",
  volume =       "11(S1)",
  booktitle =    "European Radiology, 2001",
  publisher =    "Springer Verlag, Heidelberg, Germany",
}

@TechReport{EVL-2001-335,
  pages =        "10",
  year =         "2001",
  title =        "Vessel Tracking in Peripheral {CTA} Datasets -- an
                 overview",
  type =         "VRVis Center Technical report",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-335",
  author =       "P. Felkel and R. Wegenkittl",
  address =      "Lothringerstra\ss{}e. 16/4, A-1030~Vienna, Austria,
                 \textit{www.vrvis.at}",
  month =        feb,
  note =         "Preliminary version of the paper submitted to the
                 Spring conference on Computer Graphics, SCCG'2001
                 {\ci}te{Felkel:sccg01}",
  number =       "TR-VRVis-2001-009",
  institution =  "VRVis Center",
}

@TechReport{EVL-2001-336,
  pages =        "10",
  year =         "2001",
  title =        "Implementation and Complexity of the
                 Watershed-from-Markers Algorithm Computed as a Minimal
                 Cost Forest",
  type =         "VRVis Center Technical report",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-336",
  author =       "P. Felkel and M. Bruckwschwaiger and R. Wegenkittl",
  address =      "Lothringerstra\ss{}e. 16/4, A-1030~Vienna, Austria,
                 \textit{www.vrvis.at}",
  month =        feb,
  note =         "Preliminary version of the paper submitted to
                 Eurographics 2001.{\ci}te{Felkel:eg01}",
  number =       "TR-VRVis-2001-007",
  institution =  "VRVis Center",
}

@InProceedings{EVL-2001-337,
  pages =        "269--278",
  year =         "2001",
  title =        "Vessel Tracking in Peripheral {CTA} Datasets -- an
                 overview",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-337",
  author =       "P. Felkel and R. Wegenkittl",
  month =        apr,
  note =         "(Held in April 25-28, 2001, Budmerice, Slovakia)",
  editor =       "T. L. Kunii",
  booktitle =    "Spring conference on Computer Graphics SCCG'2001,
                 Conference Proceedings",
  publisher =    "Comenius University, Bratislava, Slovakia",
}

@Article{EVL-2001-338,
  year =         "2001",
  title =        "Segmentation of Vessels in Peripheral {CTA} Datasets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-338",
  author =       "Petr Felkel and Rainer Wegenkittl and Armin Kanitsar",
  annote =       "Russian Federation State Committee on Publishing Reg.
                 Number 018689 (01.04.1999)",
  month =        dec,
  note =         "SCCG'2001 Conference Post-proceedings",
  volume =       "3",
  number =       "3",
  journal =      "{"}Computer Graphics \& Geometry.{"}
                 Internet-Journal",
}

@InProceedings{EVL-2001-339,
  pages =        "232--239",
  year =         "2001",
  title =        "Vessel Tracking in Peripheral {CTA} Datasets -- An
                 Overview",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-339",
  author =       "P. Felkel and R. Wegenkittl and A. Kanitsar",
  address =      "Budmerice, Slovakia.",
  month =        apr,
  editor =       "R. \v{D}urikovi\v{c} and S. Czanner",
  booktitle =    "Spring Conference on Computer Graphics--SCCG~2001.
                 Proceedings.",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-34,
  year =         "2001",
  title =        "Moving Facial Image Transformations Based on Static
                 2{D} Prototypes",
  author =       "Bernard Tiddeman and David Perrett",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-34",
  abstract =     "This paper describes a new method for creating
                 visually realistic moving facial image sequences that
                 retain an actor's personality (individuality,
                 expression and characteristic movements) while altering
                 the facial appearance along a certain specified facial
                 dimension. We combine two existing technologies, facial
                 feature tracking and facial image transformation, to
                 create the sequences. Examples are given of
                 transforming the apparent age, race and gender of a
                 face. We also create 'virtual cartoons' by transforming
                 image sequences into the style of famous artists. The
                 results show that static 2D face models can be used to
                 create realistic transformations of sequences that
                 include changes in pose, expression and mouth shape.",
  editor =       "V. Skala",
  keywords =     "Facial image transformation, facial feature tracking,
                 image processing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-340,
  pages =        "C-26--C-35",
  year =         "2001",
  title =        "Implementation and Complexity of the
                 Watershed-from-Markers Algorithm Computed as a Minimal
                 Cost Forest",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-340",
  author =       "P. Felkel and M. Bruckwschwaiger and R. Wegenkittl",
  address =      "Manchester, United Kingdom",
  month =        sep,
  editor =       "A Chalmers and T. M. Rhyne",
  volume =       "20",
  number =       "3",
  series =       "Computer Graphics Forum, Conference Issue",
  booktitle =    "Eurographics 2001",
}

@Proceedings{EVL-2001-341,
  year =         "2001",
  title =        "17th Spring Conference on Computer Graphics",
  author =       "Anonymuous",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-341",
}

@{,
}

@InProceedings{EVL-2001-343,
  pages =        "21--28",
  year =         "2001",
  title =        "Point Set Surfaces",
  author =       "Marc Alexa and Johannes Behr and Daniel Cohen-Or and
                 Shachar Fleishman and David Levin and Claudio T.
                 Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-343",
  abstract =     "We advocate the use of point sets to represent shapes.
                 We provide a definition of a smooth manifold surface
                 from a set of points close to the original surface. The
                 definition is based on local maps from differential
                 geometry, which are approximated by the method of
                 moving least squares (MLS). We present tools to
                 increase or decrease the density of the points, thus,
                 allowing an adjustment of the spacing among the points
                 to control the fidelity of the representation. To
                 display the point set surface, we introduce a novel
                 point rendering technique. The idea is to evaluate the
                 local maps according to the image resolution. This
                 results in high quality shading effects and smooth
                 silhouettes at interactive frame rates.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Surface representation and reconstruction, moving
                 least squares, point sample rendering, 3D acquisition",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-344,
  pages =        "29--36",
  year =         "2001",
  title =        "{EWA} Volume Splatting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-344",
  author =       "Matthias Zwicker and Hanspeter Pfister and Jeroen van
                 Baar and Markus Gross",
  abstract =     "In this paper we present a novel framework for direct
                 volume rendering using a splatting approach based on
                 elliptical Gaussian kernels. To avoid aliasing
                 artifacts, we introduce the concept of a resampling
                 filter combining a reconstruction with a low-pass
                 kernel. Because of the similarity to Heckbert's EWA
                 (elliptical weighted average) filter for texture
                 mapping we call our technique EWA volume splatting. It
                 provides high image quality without aliasing artifacts
                 or excessive blurring even with non-spherical kernels.
                 Hence it is suitable for regular, rectilinear, and
                 irregular volume data sets. Moreover, our framework
                 introduces a novel approach to compute the footprint
                 function. It facilitates efficient perspective
                 projection of arbitrary elliptical kernels at very
                 little additional cost. Finally, we show that EWA
                 volume reconstruction kernels can be reduced to surface
                 reconstruction kernels. This makes our splat primitive
                 universal in reconstructing surface and volume data.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume Rendering, Splatting, Antialiasing.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-345,
  pages =        "37--44",
  year =         "2001",
  title =        "Hybrid Simplification: Combining Multi-resolution
                 Polygon and Point Rendering",
  author =       "Jonathan D. Cohen and Daniel G. Aliaga and Weiqiang
                 Zhang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-345",
  abstract =     "Multi-resolution of polygons and more recently of
                 points are familiar and useful tools for achieving
                 interactive rendering rates. We present an algorithm
                 for tightly integrating the two into a single
                 hierarchical data structure. The trade-off between
                 rendering portions of a model with points or with
                 polygons is made automatically. Our approach to this
                 problem is to apply a bottom-up simplification process
                 involving not only polygons simplification operations,
                 but point replacement and point simplification
                 operations as well. Given one or more surface meshes,
                 our algorithms produces a hybrid hierarchy comprising
                 both polygon and point primitives. This hierarchy may
                 be optimized according to the relative characteristics
                 of these primitive types on the intended rendering
                 platform. We also provide a range of aggressiveness for
                 performing point replacement operations. The most
                 conservative approach produces a hierarchy that is
                 better than a purely polygonal hierarchy in some
                 places, and roughly equal in others. A less
                 conservative approach can trade reduced complexity at
                 the far viewing ranges for some increased complexity at
                 the near viewing ranges. We demonstrate our approach on
                 a number of input models, achieving primitive counts
                 that are 1.3 to 4.7 times smaller than those of
                 triangle-only simplification.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Rendering, simplification, multi-resolution,
                 triangles, points, hybrid.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-346,
  pages =        "45--52",
  year =         "2001",
  title =        "{POP}: {A} Hybrid Point and Polygon Rendering System
                 for Large Data",
  author =       "Baoquan Chen and Xuan Nguyen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-346",
  abstract =     "We introduce a simple but effective extension to the
                 existing pure point rendering systems. Rather than
                 using only points, we use both points and polygons to
                 represent and render large mesh models. We start from
                 triangles as leaf nodes and build up a hierarchical
                 tree structure with intermediate nodes as points.
                 During the rendering, the system determines whether to
                 use a point (of a certain intermediate level node) or a
                 triangle (of a leaf node) for display depending on the
                 screen contribution of each node. While points are used
                 to speedup the rendering of distant objects, triangles
                 are used to ensure the quality of close objects. Our
                 method can accelerate the rendering of large models,
                 compromising little in image quality.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Rendering system, Spatial data structures, Level of
                 detail algorithms, hybrid rendering systems",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-347,
  pages =        "53--60",
  year =         "2001",
  title =        "Lagrangian-Eulerian Advection for Unsteady Flow
                 Visualization",
  author =       "Bruno Jobard and Gordon EYousuff Hussainirlebacher",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-347",
  abstract =     "In this paper, we propose a new technique to visualize
                 dense representations of time-dependent vector fields
                 based on a Lagrangian-Eulerian Advection (LEA) scheme.
                 The algorithm produces animations with high
                 spatio-temporal correlation at interactive rates. With
                 this technique, every still frame depicts the
                 instantaneous structure of the flow, whereas an
                 animated sequence of frames reveals the motion a dense
                 collection of particles would take when released into
                 the flow. The simplicity of both the resulting data
                 structures and the implementation suggest that LEA
                 could become a useful component of any scientific
                 visualization toolkit concerned with the display of
                 unsteady flows.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committe",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-348,
  pages =        "61--67",
  year =         "2001",
  title =        "Transport and Anisotropic Diffusion in Time-Dependent
                 Flow Visualization",
  author =       "David B&uuml;rkle and Tobias Preu&szlig;er and Martin
                 Rumpf",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-348",
  abstract =     "The visualization of time-dependent flow is an
                 important and challenging topic in scientific
                 visualization. Its aim is to represent transport
                 phenomena governed by time-dependent vector fields in
                 an intuitively understandable way, using images and
                 animations. Here we pick up the recently presented
                 anisotropic diffusion method, expand and generalize it
                 to allow a multiscale visualization of long-time,
                 complex transport problems. Instead of streamline type
                 patterns generated by the original method now
                 streakline patterns are generated and advected. This
                 process obeys a nonlinear transport diffusion equation
                 with typically dominant transport. Starting from some
                 noisy initial image, the diffusion actually generates
                 and enhances patterns which are then transported in the
                 direction of the flow field. Simultaneously the image
                 is again sharpened in the direction orthogonal to the
                 flow field. A careful adjustment of the model{\'s}
                 parameters is derived to balance diffusion and
                 transport effects in a reasonable way. Properties of
                 the method can be discussed for the continuous model,
                 which is solved by an efficient upwind finite element
                 discretization. As characteristic for the class of
                 multiscale image processing methods, we can in advance
                 select a suitable scale for representing the flow
                 field.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-349,
  pages =        "69--74",
  year =         "2001",
  title =        "Enridged Contour Maps",
  author =       "Jarke J. van Wijk and Alexandru Telea",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-349",
  abstract =     "The visualization of scalar functions of two variables
                 is a classic and ubiquitous application. We present a
                 new method to visualize such data. The method is based
                 on a non-linear mapping of the function to a height
                 field, followed by visualization as a shaded mountain
                 landscape. The method is easy to implement and
                 efficient, and leads to intriguing and insightful
                 images: The visualization is enriched by adding ridges.
                 Three types of applications are discussed:
                 visualization of iso-levels, clusters (multivariate
                 data visualization), and dense contours (flow
                 visualization).",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Contours, mapping, height fields, multivariate
                 visualization, flow visualization",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-35,
  year =         "2001",
  title =        "Interactive Evolutionary Approach to Character
                 Animation",
  author =       "M. Nordin Zakaria",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-35",
  abstract =     "We describe in this paper an approach to the animation
                 of articulated figure using a combination of forward
                 kinematics, neural net and genetic algorithms. A set of
                 kinematics motion data for an articulated model is
                 evolved to generate a set of varied motion that
                 performs according to an objective function set by the
                 user. Neural net is used to constrain the search space
                 of possible motions. User interaction with the process
                 ensures a certain style evolves from the myriad of
                 possibilities. We show how our technique makes it
                 possible to create libraries of reusable motions from a
                 small number of example motions.",
  editor =       "V. Skala",
  keywords =     "Computer Animation, Genetic Algorithm.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-350,
  pages =        "75--82",
  year =         "2001",
  title =        "Visualization of Sports using Motion Trajectories:
                 Providing Insights into Performance, Style, and
                 Strategy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-350",
  author =       "Gopal Pingali and Agata Opalach and Yves Jean and
                 Ingrid Carlbom",
  abstract =     "Remote experience of sporting events has thus far been
                 limited mostly to watching video and the scores and
                 statistics associated with the sport. However, a
                 fast-developing trend is the use of visualization
                 techniques to give new insights into performance,
                 style, and strategy of the players. Automatic
                 techniques can extract accurate information from video
                 about player performance that not even the most skilled
                 observers is able to discern. When presented as static
                 images or as three-dimensional virtual replay, this
                 information makes viewing a game an entirely new and
                 exciting experience. This paper presents one such
                 sports visualization system called LucentVision, which
                 has been developed for the sport of tennis.
                 LucentVision uses real-time video analysis to obtain
                 motion trajectories of players and the ball, and offers
                 a rich set of visualization options based on the
                 trajectory data. The system has been used extensively
                 in the broadcast of international tennis tournaments,
                 both on television and the Internet.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Sports visualization, virtual environment,
                 telepresence, real-time video analysis, multi-camera
                 tracking, multimedia indexing",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-351,
  pages =        "83--90",
  year =         "2001",
  title =        "Undersampling and Oversampling in Sample Based Shape
                 Modeling",
  author =       "Tamal K. Dey and Joachim Giesen and Samrat Goswami
                 andJames Hudson and Rephael Wenger and Wulue Zhao",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-351",
  abstract =     "Shape modeling is an integral part of many
                 visualization problems. Recent advances in scanning
                 technology and a number of surface reconstruction
                 algorithms have opened up a new paradigm for modeling
                 shapes from samples. Many of the problems currently
                 faced in this modeling paradigm can be traced back to
                 two anomalies in sampling, namely undersampling and
                 oversampling. Boundaries, non-smoothness and small
                 features create undersampling problems, whereas
                 oversampling leads to too many triangles. We use
                 Voronoi cell geometry as a unified guide to detect
                 undersampling and oversampling. We apply these
                 detections in surface reconstruction and model
                 simplification. Guarantees of the algorithms can be
                 proved. In this paper we show the success of the
                 algorithms empirically on a number of interesting data
                 sets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Computational Geometry, Surface Reconstruction,
                 Geometric Modeling, Mesh Generation, Polygonal Mesh
                 Reduction, Polygonal Modeling, Shape Recognition.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-352,
  pages =        "91--98",
  year =         "2001",
  title =        "Optimal Regular Volume Sampling",
  author =       "Thomas Theu&szlig;l and Torsten M&ouml;ller and
                 Meister Eduard Gr&ouml;ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-352",
  abstract =     "The classification of volumetric data sets as well as
                 their rendering algorithms are typically based on the
                 representation of the underlying grid. Grid structures
                 based on a Cartesian lattice are the de-facto standard
                 for regular representations of volumetric data. In this
                 paper we introduce a more general concept of regular
                 grids for the representation of volumetric data. We
                 demonstrate that a specific type of regular lattice -
                 the so-called body-centered cubic - is able to
                 represent the same data set as a Cartesian grid to the
                 same accuracy but with 29.3% fewer samples. This speeds
                 up traditional volume rendering algorithms by the same
                 ratio, which we demonstrate by adopting a splatting
                 implementation for these new lattices. We investigate
                 different filtering methods required for computing the
                 normals on this lattice. The lattice representation
                 results also in lossless compression ratios that are
                 better than previously reported. Although other regular
                 grid structures achieve the same sample efficiency, the
                 body-centered cubic is particularly easy to use. The
                 only assumption necessary is that the underlying volume
                 is isotropic and band-limited - an assumption that is
                 valid for most practical data sets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume data, Cartesian grid, close packing, hexagonal
                 sampling, body centered cubic",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-353,
  pages =        "99--106",
  year =         "2001",
  title =        "Simplicial Subdivision and Sampling Artifacts",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-353",
  author =       "Hamish Carr and Torsten M&ouml;ller and Jack
                 Snoeyink",
  abstract =     "We review several schemes for dividing cubical cells
                 into simplices (tetrahedra) in 3-D for interpolating
                 from sampled data to |R^{3} or for computing
                 isosurfaces by barycentric interpolation. We present
                 test data that reveal the geometric artifacts that
                 these subdivision schemes generate, and discuss how
                 these artifacts relate to the filter kernels that
                 correspond to the subdivision scheme.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-354,
  pages =        "107--112",
  year =         "2001",
  title =        "A Simple Algorithm for Surface Denoising",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-354",
  author =       "Jianbo Peng and Vasily Strela and Denis Zorin",
  abstract =     "We present a simple denoising technique for geometric
                 data rep-esented as a semiregular mesh, based on
                 locally adaptive Wiener filtering. The degree of
                 denoising is controlled by a single parameter (an
                 estimate of the relative noise level) and the time
                 required for denoising is independent of the magnitude
                 of the estimate. The performance of the algorihm is
                 sufficiently fast to allow interactive local
                 denoising.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-355,
  pages =        "113--120",
  year =         "2001",
  title =        "Attribute Preserving Dataset Simplification",
  author =       "Jason D. Walter and Christopher G. Healey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-355",
  abstract =     "This paper describes a novel application of feature
                 preserving mesh simplification to the problem of
                 managing large, multidimensional datasets during
                 scientific visualization. To allow this, we view a
                 scientific dataset as a triangulated mesh of data
                 elements, where the attributes embedded in each element
                 form a set of properties arrayed across the surface of
                 the mesh. Existing simplification techniques were not
                 designed to address the high dimensionality that exists
                 in these types of datasets. As well, vertex operations
                 that relocate, insert, or remove data elements may need
                 to be modified or restricted. Principal component
                 analysis provides an algorithm-independent method for
                 compressing a dataset's dimensionality during
                 simplification. Vertex locking forces certain data
                 elements maintain their spatial locations; this
                 technique is also used to guarantee a minimum density
                 in the simplified dataset. The result is a
                 visualization that significantly reduces the number of
                 data elements to display, while at the same time
                 ensuring that high-variance regions of potential
                 interest remain intact. We apply our techniques to a
                 number of well-known feature preserving algorithms, and
                 demonstrate their applicability in a real-world context
                 by simplifying a multidimensional weather dataset. Our
                 results show a significant improvement in execution
                 time with only a small reduction in accuracy; even when
                 the dataset was simplified to 10% of its original size,
                 average per attribute error was less than 1%.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Dataset management, mesh simplification, principal
                 component analysis, scientific visualization",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-356,
  pages =        "121--126",
  year =         "2001",
  title =        "A Memory Insensitive Technique for Large Model
                 Simplification",
  author =       "P. Lindstrom and C. T. Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-356",
  abstract =     "In this paper we propose three simple, but significant
                 improvements to the OoCS (Out-of-Core Simplification)
                 algorithm of Lindstrom [20] which increase the quality
                 of approximations and extend he applicability of the
                 algorithm to an even larger class of compute systems.
                 The original OoCS algorithm has memory complexity that
                 depends on the size of the output mesh, but no
                 dependency on the size of the input mesh. That is, it
                 can be used to simplify meshes of arbitrarily large
                 size, but the complexity of the output mesh is limited
                 by the amount of memory available. Our first
                 contribution is a version of OoCS that removes the
                 dependency of having enough memory to hold (even) the
                 simplified mesh. With our new algorithm, the whole
                 process is made essentially independent of the
                 available memory on the host computer. Our new
                 technique uses disk instead of main memory, but it is
                 carefully designed to avoid costly random accesses. Our
                 two other contributions improve the quality of the
                 approximations generated by OoCS. We propose a scheme
                 for preserving surface boundaries which does not use
                 connectivity information, and a scheme for constraining
                 the position of the {"}representative vertex{"} of a
                 grid cell to an optimal position inside the cell.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Polygonal surface simplification, large data,
                 out-of-core algorithms, external sorting, quadric error
                 metrics.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-357,
  pages =        "127--134",
  year =         "2001",
  title =        "Efficient Adaptive Simplification of Massive Meshes",
  author =       "Eric Shaffer and Michael Garland",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-357",
  abstract =     "The growing availability of massive polygonal models,
                 and the inability of most existing visualization tools
                 to work with such data, has created a pressing need for
                 memory efficient methods capable of simplifying very
                 large meshes. In this paper, we present a method for
                 performing adaptive simplification of polygonal
                 meshesthat are too large to fit in-core. Our algorithm
                 performs two passes over an input mesh. In the first
                 pass, the model is quantized using a uniform grid, and
                 surface information is accumulated in the form of
                 quadrics and dual quadrics. This sampling is then used
                 to construct a BSP-Tree in which the partitioning
                 planes are determined by the dual quadrics. In the
                 final pass, the original vertices are clustered using
                 the BSPTree, yielding an adaptive approximation of the
                 original mesh. TheBSP-Tree describes a natural
                 simplification hierarchy, making it possible to
                 generate a progressive transmission and construct
                 level-of-detail representations. In this way, the
                 algorithm provides some of the features associated with
                 more expensive edge contraction methods while
                 maintaining greater computational efficiency. In
                 addition to performing adaptive simplification, our
                 algorithm exhibits output-sensitive memory requirements
                 and allows fine control over the size of the simplified
                 mesh.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Surface simplification, massive meshes, quadric error
                 metric, recursive partitioning, out-of-core
                 simplification",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-358,
  pages =        "135--142",
  year =         "2001",
  title =        "Connectivity Shapes",
  author =       "Martin Isenburg and Stefan Gumhold and Craig Gotsman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-358",
  abstract =     "We describe a method to visualize the connectivity
                 graph of a mesh using a natural embedding in 3D space.
                 This uses a 3D shape representation that is based
                 solely on mesh connectivity - he connectivity shape.
                 Given a connectivity, we define its natural geometry as
                 a smooth embedding in space with uniform edge lengths
                 and describe efficient techniques to compute it. Our
                 main contribution is to demonstrate that a surprising
                 amount of geometric information is implicit in the
                 connectivity. We also show how to generate connectivity
                 shapes that approximate given 3D shapes. Potential
                 applications of connectivity shapes to modeling and
                 mesh coding are described.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committe",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Natural embedding, mesh connectivity, implicit
                 geometry, polygon meshes, shape compression",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-359,
  pages =        "143--150",
  year =         "2001",
  title =        "Quantitative Comparative Evaluation of 2{D} Vector
                 Field Visualization Methods",
  author =       "David H. Laidlaw and R. M. Kirby and J. Scott Davidson
                 and Timothy S. Miller and Marco da Silva and William H.
                 Warren and Michael Tarr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-359",
  abstract =     "We present results from a user study that compared six
                 visualization methods for 2D vector data. Two methods
                 used different distributions of short arrows, two used
                 different distributions of integral curves, one used
                 wedges located to suggest flow lines, and the final was
                 line-integral convolution (LIC). We defined three
                 simple but representative tasks for users to perform
                 using visualizations from each method: 1) locating all
                 critical points in an image, 2)identifying critical
                 point types, and 3) advecting a particle. Results show
                 different strengths and weaknesses for each method. We
                 found that users performed better with methods that: 1)
                 showed the sign of vectors within the vector field, 2)
                 visually represented integral curves, and 3) visually
                 represented the locations of critical points. These
                 results provide quantitative support for some of the
                 anecdotal evidence concerning visualization methods.
                 The tasks and testing framework also provide a basis
                 for comparing other visualization methods, for creating
                 more effective methods, and for defining additional
                 tasks to further understand tradeoffs among methods.
                 They may also be useful for evaluating 2D vector on 2D
                 surfaces embedded in 3D and for defining analogous
                 tasks for 3D visualization methods.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Scientific Visualization, User Study, Line-integral
                 Convolution, Two-dimensional Vector Fields,
                 Streamlines, Iconic Textures, Image-guided Streamlines,
                 Jittered Grid Icons, Critical Point, Advection, Fluid
                 Dynamics, Fluid Flow",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-36,
  year =         "2001",
  title =        "{DEGRADE}: {A} New Color Shading Tool",
  author =       "Vincent Boyer and Jean-Jacques Bourdin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-36",
  abstract =     "Pictures or nature rarely present uniform colored
                 regions. Most drawings include shaded and/or color
                 shaded surfaces. Therefore for any graphic designer the
                 color shading tool is of major importance. The tools
                 included in a graphics library have to respect two
                 important conditions: they have to be fast and they
                 have to be intuitive. Most color shading tools don't
                 conciliate these two constraints. This paper presents a
                 new model of color shading. Its implementation uses the
                 fastest computation techniques up-to-date. Four
                 examples with pictures, drawn with DEGRADE, the new
                 color shading tool, are then presented.",
  editor =       "V. Skala",
  keywords =     "Shading, Texture, Two-Dimensional Graphics, Animation,
                 Extrusion, Silhouette.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-360,
  pages =        "151--157",
  year =         "2001",
  title =        "A tetrahedra-based stream surface algorithm",
  author =       "Gerik Scheuermann and Tom Bobach and Hans Hagen and
                 Karim Mahrous and Bernd Hamann and Kenneth I. Joy and
                 Wolfgang Kollmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-360",
  abstract =     "This paper presents a new algorithm for the
                 calculation of stream surfaces for tetrahedral grids.
                 It propagates the surface through the tetrahedra, one
                 at a time, calculating the intersections with the
                 tetrahedral faces. The method allows us to incorporate
                 topological information from the cells, e.g., critical
                 points. The calculations are based on barycentric
                 coordinates, since this simplifies theory and
                 algorithm. The stream surfaces are ruled surfaces
                 inside each cell, and their construction is started
                 with line segments on the faces. Our method supports
                 the analysis of velocity fields resulting from
                 computational fluid dynamics (CFD) simulations.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committe",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Vector field visualization, flow visualization,
                 tetrahedral grid, unstructured grid, flow surface",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-361,
  pages =        "159--166",
  year =         "2001",
  title =        "Continuous Topology Simplification of Planar Vector
                 Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-361",
  author =       "Xavier Tricoche and Gerik Scheuermann",
  abstract =     "Vector fields can present complex structural behavior,
                 especially in turbulent computational fluid dynamics.
                 The topological analysis of these datasets reduces the
                 information but one is usually still left with too many
                 details for interpretation. In this paper, we present a
                 simplification approach that removes pairs of critical
                 points from the dataset, based on relevance measures.
                 In contrast to earlier methods, no grid changes are
                 necessary since the whole method uses small local
                 changes of the vector values defining the vector field.
                 An interpretation in terms of bifurcation underlines
                 the continuous, natural flavor of the algorithm.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Vector field topology, flow visualization,
                 unstructured grid, simplification",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-362,
  pages =        "167--174",
  year =         "2001",
  title =        "PixelFlex: {A} Reconfigurable Multi-Projector Display
                 System",
  author =       "Ruigang Yang and David Gotz and Justin Hensley and
                 Herman Towles and Michael S. Brown",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-362",
  abstract =     "This paper presents PixelFlex - a spatially
                 reconfigurable multi-projector display system. The
                 PixelFlex system is composed of ceiling-mounted
                 projectors, each with computer-controlled pan, tilt,
                 zoom and focus; and a camera for closed-loop
                 calibration. Working collectively, these controllable
                 projectors function as a single logical display capable
                 of being easily modified into a variety of spatial
                 formats of differing pixel density, size and shape. New
                 layouts are automatically calibrated within minutes to
                 generate the accurate warping and blending functions
                 needed to produce seamless imagery across planar
                 display surfaces, thus giving the user the flexibility
                 to quickly create, save and restore multiple screen
                 configurations. Overall, PixelFlex provides a new level
                 of automatic reconfigurability and usage, departing
                 from the static, one-size-fits-all design of
                 traditional large format displays. As a
                 front-projection system, PixelFlex can be installed in
                 most environments with space constraints and requires
                 little or no post-installation mechanical maintenance
                 because of the closed-loop calibration.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Large-format projection display, camera-based
                 registration and calibration",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-363,
  pages =        "175--182",
  year =         "2001",
  title =        "Dynamic Shadow Removal from Front Projection
                 Displays",
  author =       "Christopher Jaynes and Stephen Webb and R. Matt Steele
                 and Michael Brown and W. Brent Seales",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-363",
  abstract =     "Front-projection display environments suffer from a
                 fundamental problem: users and other objects in the
                 environment can easily and inadvertently block
                 projectors, creating shadows on the displayed image. We
                 introduce a technique that detects and corrects
                 transient shadows in a multi-projector display. Our
                 approach is to minimize the difference between
                 predicted (generated) and observed (camera) images by
                 continuous modification of the projected image values
                 for each display device. We are unaware of any other
                 technique that directly addresses this problem.
                 Furthermore, we speculate that the general predictive
                 monitoring framework introduced here is capable of
                 addressing more general radiometric consistency
                 problems such as display surface inter-reflections and
                 the changes in display color and intensity due to
                 projector bulb temperature variation. Using an
                 automatically-derived relative position of cameras and
                 projectors in the display environment and a
                 straightforward color correction scheme, the system
                 renders an expected image for each camera location.
                 Cameras observe the displayed image, which is compared
                 with the expected image to detect shadowed regions.
                 These regions are transformed to the appropriate
                 projector frames, where corresponding pixel values are
                 increased. In display regions where more than one
                 projector contributes to the image, shadow regions are
                 eliminated. We demonstrate an implementation of the
                 technique to remove shadows in a multi-projector front
                 projection system.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Large-scale display, shadow removal, immersive media,
                 calibration.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-364,
  pages =        "183--190",
  year =         "2001",
  title =        "The {"}Which Blair Project{"}: {A} Quick Visual Method
                 for Evaluating Perceptual Color Maps",
  author =       "Bernice E. Rogowitz and Alan D. Kalvin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-364",
  abstract =     "We have developed a fast, perceptual method for
                 selecting color scales for data visualization that
                 takes advantage of our sensitivity to luminance
                 variations in human faces. To do so, we conducted
                 experiments in which we mapped various color scales
                 onto the intensity values of a digitized photograph of
                 a face and asked observers to rate each image. We found
                 a very strong correlation between the perceived
                 naturalness of the images and the degree to which the
                 underlying color scales increased monotonically in
                 luminance. Color scales that did not include a
                 monotonically increasing luminance component produced
                 no positive rating scores. Since color scales with
                 monotonic luminance profiles are widely recommended for
                 visualizing continuous scalar data, a purely visual
                 technique for identifying such color scales could be
                 very useful, especially in situations where color
                 calibration is not integrated into i the visualization
                 environment, such as over the Internet.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Perceptual color scales, visual artifacts in
                 visualization, Internet color, human color vision.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-365,
  pages =        "191--198",
  year =         "2001",
  title =        "Circular Indident Edge Lists: a Data Structure for
                 Rendering Complex Unstructured Grids",
  author =       "Bruno L'evy and Guillaume Caumon and St'ephane
                 Conreaux and Xavier Cavin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-365",
  abstract =     "We present the Circular Incident Edge Lists (CIEL), a
                 new data structure and a high-performance algorithm for
                 generating a series of iso-surfaces in a highly
                 unstructured grid. Slicing-based volume rendering is
                 also considered. The CIEL data structure represents all
                 the combinatorial information of the grid, making it
                 possible to optimize the classical propagation from
                 local minima paradigm. The usual geometric structures
                 are replaced by a more efficient combinatorial
                 structure. An active edges list is maintained, and
                 iteratively propagated from an iso-surface to the next
                 one in a very efficient way. The intersected cells
                 incident to each active edge are retrieved, and the
                 intersection polygons are generated by circulating
                 around their facets. This latter feature enables
                 arbitrary irregular cells to be treated, such as those
                 encountered in certain computational fluid dynamics
                 (CFD) simulations. Since the CIEL data structure solely
                 depends on the connections between the cells, it is
                 possible to take into account dynamic changes in the
                 geometry of the mesh and in property values, which only
                 requires the sorted extrema list to be updated.
                 Experiments have shown that our approach is
                 significantly faster than classical methods. The major
                 drawback of our method is its memory consumption,
                 higher than most classical methods. However,
                 experimental results show that it stays within a
                 practical range.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committe",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume Rendering, Iso-Surfaces, Unstructured Grids,
                 Combinatorial Topology",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-366,
  pages =        "199--206",
  year =         "2001",
  title =        "Hardware-Software-Balanced Resampling for the
                 Interactive Visualization of Unstructured Grids",
  author =       "Manfred Weiler and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-366",
  abstract =     "In this paper we address the problem of interactively
                 resampling unstructured grids. Three algorithms are
                 presented. They all allow adaptive resampling of an
                 unstructured grid on a multiresolution hierarchy of
                 arbitrarily sized cartesian grids according to a
                 varying element size. Two of the algorithms presented
                 take advantage of hardware accelerated polygon
                 rendering and 2D texture mapping. In exploiting new
                 features of modern PC graphics adapters, the first
                 algorithm tries to significantly minimize the number of
                 polygons to be rendered. Reducing rasterization
                 requirements is the main goal of the second algorithm,
                 which distributes the computational work-load
                 differently between the main processor and the graphics
                 chip. By comparing them to a new pure software
                 approach, an optimal software-hardware balance is
                 studied. We end up with a hybrid approach which greatly
                 improves the performance of hardware assisted
                 resampling by involving the main processor to a higher
                 degree and thus enabling resampling at nearly
                 interactive rates.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-367,
  pages =        "207--213",
  year =         "2001",
  title =        "The Perspective Shear-Warp Algorithm In {A} Virtual
                 Environment",
  author =       "J{\"u}rgen P. Schulze and Roland Niemeier and Ulrich
                 Lang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-367",
  abstract =     "Since the original paper of Lacroute and Levoy [9],
                 where the shear-warp factorization was also shown for
                 perspective projections, a lot of work has been carried
                 out using the shear-warp factorization with parallel
                 projections. However, none of it has proved or improved
                 the algorithms for the perspective projection. Also in
                 Lacroute's Volpack library, the perspective shear-warp
                 volume rendering algorithm is missing. This paper
                 reports on an implementation of the perspective
                 shear-warp algorithm, which includes enhancements for
                 its application in immersive virtual environments.
                 Furthermore, a mathematical proof for the correctness
                 of the permutation of projection and warp is provided,
                 so far a basic assumption of the shear-warp perspective
                 projection.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume Rendering, Perspective Shear-Warp, Virtual
                 Environments",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-368,
  pages =        "215--222",
  year =         "2001",
  title =        "Cell-Projection of Cyclic Meshes",
  author =       "Martin Kraus and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-368",
  abstract =     "We present the first algorithm that employs
                 hardware-accelerated cell-projection for direct volume
                 rendering of cyclic meshes, i.e., meshes with
                 visibility cycles. The visibility sorting of a cyclic
                 mesh is performed by an extended topological sorting,
                 which computes and isolates visibility cycles. Measured
                 sorting times are comparable to previously published
                 algorithms, which are, however, restricted to acyclic
                 meshes. In practice, our algorithm is also useful for
                 acyclic meshes as numerical instabilities can lead to
                 false visibility cycles. Our method includes a simple,
                 hardware-assisted algorithm based on image compositing
                 that renders visibility cycles correctly. For
                 tetrahedral meshes this algorithm allows us to render
                 each tetrahedral cell (whether it is part of a cycle or
                 not) by hardware-accelerated cell-projection. In its
                 basic form our method applies only to convex cyclic
                 meshes; however, we present an exact and a simpler but
                 inexact extension of our method for nonconvex meshes.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Cell-projection, direct volume rendering, tetrahedral
                 meshes, unstructured meshes, visibility cycles,
                 visibility ordering, visibility sorting, volume
                 visualization.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-369,
  pages =        "223--230",
  year =         "2001",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-369",
  abstract =     "Automatic detection of meaningful isosurfaces is
                 important for producing informative visualizations of
                 volume data, especially when no information about the
                 data origin and imaging protocol is available. We
                 propose a computationally efficient method for the
                 automated detection of intensity transitions in volume
                 data. In this approach, the dominant transitions
                 correspond to clear maxima in cumulative
                 Laplacian-weighted gray value histograms. Only one pass
                 through the data volume is required to compute the
                 histogram. Several other features which may be useful
                 for exploration of data of unknown origin can be
                 efficiently computed in a similar manner, e.g. enclosed
                 volume, isosurface area, mean gradient. The detected
                 intensity transitions can be used for setting of
                 visualization parameters for surface rendering, as well
                 as for direct volume rendering of 3-D datasets. When
                 using surface rendering, the detected dominant
                 intensity transition values correspond to the optimal
                 surface isovalues for extraction of boundaries of the
                 objects of interest. In direct volume rendering, such
                 transitions are important for generation of the
                 transfer functions, which are used to assign
                 visualization properties to data voxels and determine
                 the appearance of the rendered image. The proposed
                 method is illustrated by examples with synthetic data
                 as well as real biomedical datasets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume data visualization, surface rendering, volume
                 rendering, isosurfaces, divergence theorem",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-37,
  year =         "2001",
  title =        "Fast Polygonization of Implicit Surfaces",
  author =       "Frederic Triquet and Philippe Meseure and
                 ChristopheChaillou",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-37",
  abstract =     "Our work is centered on the use of implicit surfaces
                 in interactive applications (at least 10 frames per
                 sec) running on high-end consumer architecture
                 (modeling, simulation, deformable body animation,
                 games). We focus on the Marching Cubes algorithm that
                 we tried to implement in an optimized way. We restrict
                 our work to blended iso-surfaces generated by
                 skeletons, since this kind of implicit surfaces is the
                 most handy to use for animations. Our implementation
                 optimizations deal with the following features:
                 simplifying the field function, accelerating its
                 evaluation for each point (voxel-based technique),
                 generating automatically the triangles for any case of
                 the Marching Cubes. Another point we have considered
                 concerns tesselation ambiguities often resulting in
                 holes appearing in the surface. We have coded a library
                 which is very easy to use and can be downloaded freely.
                 All these optimizations allow us to sample implicit
                 surfaces composed of 200 points in 45 ms on a 450 MHz
                 Pentium II Xeon.",
  editor =       "V. Skala",
  keywords =     "Implicit surfaces, marching cubes, real time,
                 library.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-370,
  pages =        "231--238",
  year =         "2001",
  title =        "Salient Iso-Surface Detection with Model-Dependent
                 Statistical Signatures",
  author =       "Shivaraj Tenginakai and Jinho Lee and Raghu
                 Machiraju",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-370",
  abstract =     "Volume graphics has not been accepted for widespread
                 use. One of the inhibiting reasons is the lack of
                 general methods for data-analysis and simple interfaces
                 for data exploration. An error-and-trial iterative
                 procedure is often used to select a desirable transfer
                 function or mine the dataset for salient iso-values.
                 New semi-automatic methods that are also data-centric
                 have shown much promise [1][7]. However, general and
                 robust methods are still needed for data-exploration
                 and analysis. In this paper, we propose general
                 model-independent statistical methods based on central
                 moments of data. Using these techniques we show how
                 salient iso-surfaces at material boundaries can be
                 determined. We provide examples from the medical and
                 computational domain to demonstrate the effectiveness
                 of our methods.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Iso-values, Transfer Functions, Surface Extraction,
                 Direct Volume Rendering",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-371,
  pages =        "239--245",
  year =         "2001",
  title =        "Distance-Field Based Skeletons for Virtual
                 Navigation",
  author =       "Ming Wan and Frank Dachille Arie Kaufman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-371",
  abstract =     "We present a generic method for rapid flight planning,
                 virtual navigation and effective camera control in a
                 volumetric environment. Directly derived from an
                 accurate distance from boundary (DFB) field, our
                 automatic path planning algorithm rapidly generates
                 centered flight paths, a skeleton, in the navigable
                 region of the virtual environment. Based on precomputed
                 flight paths and the DFB field, our dual-mode
                 physically based camera control model supports a
                 smooth, safe, and sticking-free virtual navigation with
                 six degrees of freedom. By using these techniques,
                 combined with accelerated volume rendering, we have
                 successfully developed a real-time virtual colonoscopy
                 system on low-cost PCs and confirmed the high speed,
                 high accuracy and robustness of our techniques on more
                 than 40 patient datasets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Distance fields, path planning, centerline, camera
                 control, virtual navigation, volumetric environment,
                 physically based modeling, virtual colonoscopy.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-372,
  pages =        "247--254",
  year =         "2001",
  title =        "A Complete Distance Field Representation",
  author =       "Jian Huang and Yan Li and Roger Crawfis and Shao-chiun
                 L and Shu Liou",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-372",
  abstract =     "Distance fields are an important volume
                 representation. A high quality distance field
                 facilitates accurate surface characterization and
                 gradient estimation. However, due to Nyquist Law, no
                 existing volumetric methods based on the linear
                 sampling theory can fully capture surface details, such
                 as corners and edges, in 3D space. We propose a novel
                 complete distance field representation (CDFR) that does
                 not rely on Nyquist sampling theory. To accomplish
                 this, we construct a volume where each voxel has a
                 complete description of all portions of surface that
                 affect the local distance field. For any desired
                 distance, we are able to extract a point-based contour
                 in true Euclidean distance, at any level of accuracy,
                 from the same CDFR representation. Such point-based
                 iso-distance contours have faithful per-point gradients
                 and can be interactively visualized using splatting,
                 providing per-point shaded image quality. We also
                 demonstrate applying CDFR to a cutting edge design for
                 manufacturing application involving high-complexity
                 parts at un-precedented accuracy using only commonly
                 available computational resources.",
  organization = "IEEE Computer Society Technicl Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume visualization, direct volume rendering,
                 multi-dimensional transfer function, direct
                 manipulation widgets, graphics hardware",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-373,
  pages =        "255--262",
  year =         "2001",
  title =        "Interactive Volume Rendering Using Multi-Dimensional
                 Transfer Functions and Direct Manipulation Widgets",
  author =       "Joe Kniss and Gordon Kindlmann and Charles Hansen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-373",
  abstract =     "Most direct volume renderings produced today employ
                 one-dimensional transfer functions, which assign color
                 and opacity to the volume based solely on the single
                 scalar quantity which comprises the dataset. Though
                 they have not received widespread attention,
                 multi-dimensional transfer functions are a very
                 effective way to extract specific material boundaries
                 and convey subtle surface properties. However,
                 identifying good transfer functions is difficult enough
                 in one dimension, let alone two or three dimensions.
                 This paper demonstrates an important class of
                 three-dimensional transfer functions for scalar data
                 (based on data value, gradient magnitude, and a second
                 directional derivative), and describes a set of direct
                 manipulation widgets which make specifying such
                 transfer functions intuitive and convenient. We also
                 describe how to use modern graphics hardware to
                 interactively render with multi-dimensional transfer
                 functions. The transfer functions, widgets, and
                 hardware combine to form a powerful system for
                 interactive volume exploration.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume visualization, direct volume rendering,
                 multi-dimensional transfer functions, direct
                 manipulation widgets, graphics hardware",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-374,
  pages =        "263--270",
  year =         "2001",
  title =        "Texture Hardware Assisted Rendering of Time-Varying
                 Volume Data",
  author =       "Eric B. Lum and Kwan-Liu Ma and John Clyne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-374",
  abstract =     "In this paper we present a hardware-assisted rendering
                 technique coupled with a compression scheme for the
                 interactive visual exploration of time-varying scalar
                 volume data. A palette-based decoding technique and an
                 adaptive bit allocation scheme are developed to fully
                 utilize the texturing capability of a commodity 3-D
                 graphics card. Using a single PC equipped with a modest
                 amount of memory, a texture capable graphics card, and
                 an inexpensive disk array, we are able to render
                 hundreds of time steps of regularly gridded volume data
                 (up to 45 millions voxels each time step) at
                 interactive rates, permitting the visual exploration of
                 large scientific data sets in both the temporal and
                 spatial domain.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Compression, high performance computing, out-of-core
                 processing, PC, scientific visualization, texture
                 hardware, time-varying data, transform encoding, volume
                 rendering",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-375,
  pages =        "271--278",
  year =         "2001",
  title =        "Accelerated Volume Ray-Casting using Texture Mapping",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-375",
  author =       "R{\"{u}}diger Westermann and Bernd Sevenich",
  abstract =     "Acceleration techniques for volume ray-casting are
                 primarily based on pre-computed data structures that
                 allow one to efficiently traverse empty or homogeneous
                 regions. In order to display volume data that
                 successively undergoes color lookups, however, the data
                 structures have to be re-built continuously. In this
                 paper we propose a technique that circumvents this
                 drawback using hardware accelerated texture mapping. In
                 a first rendering pass we employ graphics hardware to
                 interactively determine for each ray where the material
                 is hit. In a second pass ray-casting is performed, but
                 ray traversal starts right in front of the previously
                 determined regions. The algorithms enables interactive
                 classification and it considerably accelerates the view
                 dependent display of selected materials and surfaces
                 from volume data. In contrast to other techniques that
                 are solely based on texture mapping our approach
                 requires less memory and accurately performs the
                 composition of of material contributions along the
                 ray.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume rendering, Ray-Casting, Texture Mapping,
                 Visualization, Graphics Hardware",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-376,
  pages =        "279--286",
  year =         "2001",
  title =        "{RTVR} - a flexible Java library for interactive
                 volume rendering",
  author =       "Lukas Mroz and Helwig Hauser",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-376",
  abstract =     "This paper presents several distinguishing design
                 features of RTVR - a Java-based library for interactive
                 volume rendering. We describe, how the careful design
                 of data structures, which in our case are based on
                 voxel enumeration, and an intelligent use of lookup
                 tables enable interactive volume rendering even on
                 low-end PC hardware. By assigning voxels to distinct
                 objects within the volume and by using an individual
                 setup and combination of lookup tables for each object,
                 object-aware rendering can be performed: different
                 transfer functions, shading models and compositing
                 modes (MIP, DVR) can be mixed within a single scene,
                 while still providing rendering results in real-time.
                 While providing frame rates similar to volume
                 visualization using 3D consumer hardware, RTVR provides
                 much more flexibility and extensibility due to it's
                 pure software nature. Furthermore, due to an efficient
                 intermediate data representation, RTVR can be used to
                 provide volume viewing facilities over low-bandwidth
                 networks, with almost full control over rendering and
                 visualization mapping parameters (clipping, shading,
                 transfer function) for the user. This paper also
                 addresses specific problems which arise by the use of
                 Java for interactive Visualization.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Interactive volume visualization, Internet-based
                 visualization, Java",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-377,
  pages =        "287--294",
  year =         "2001",
  title =        "Multiresolution Feature Extraction from Unstructured
                 Meshes",
  author =       "Andreas Hubeli and Markus Gross",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-377",
  abstract =     "We present a framework to extract mesh features from
                 unstructured two-manifold surfaces. Our method computes
                 a collection of piecewise linear curves describing the
                 salient features of surfaces, such as edges and ridge
                 lines. We extend these basic techniques to a
                 multiresolution setting which improves the quality of
                 the results and accelerates the extraction process. The
                 framework is semi-automatic, that is,the user is
                 required to input a few control parameters and to
                 select the operators to be applied to the input
                 surface. Our mesh feature extraction algorithm can be
                 used as a preprocessor for a variety of applications in
                 geometric modeling including mesh fairing and
                 subdivision.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Surface Representations, Geometric Modeling, Triangle
                 Decimation, Multiresolution Models, Feature
                 Extraction.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-378,
  pages =        "295--302",
  year =         "2001",
  title =        "Fast Extraction of Adaptive Multiresolution Meshes
                 with Guaranteed Properties from Volumetric Data",
  author =       "Marcel Gavrilu and Joel Carranza and David E. Breen
                 and Alan H. Barr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-378",
  abstract =     "We present a new algorithm for extracting adaptive
                 multiresolution triangle meshes from volume datasets.
                 The algorithm guarantees that the topological genus of
                 the generated mesh is the same as the genus of the
                 surface embedded in the volume dataset at all levels of
                 detail. In addition to this {"}hard constraint{"} on
                 the genus of the mesh, the user can choose to specify
                 some number of soft geometric constraints, such as
                 triangle aspect ratio, minimum or maximum total number
                 of vertices, minimum and/or maximum triangle edge
                 lengths, maximum magnitude of various error metrics per
                 triangle or vertex, including maximum curvature (area)
                 error, maximum distance to the surface, and others. The
                 mesh extraction process is fully automatic and does not
                 require manual adjusting of parameters to produce the
                 desired results as long as the user does not specify
                 incompatible constraints. The algorithm robustly
                 handles special topological cases, such as trimmed
                 surfaces (intersections of the surface with the volume
                 boundary), and manifolds with multiple disconnected
                 components (several closed surfaces embedded in the
                 same volume dataset). The meshes may self-intersect at
                 coarse resolutions. However, the self-intersections are
                 corrected automatically as the resolution of the meshes
                 increase. We show several examples of meshes extracted
                 from complex volume datasets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-379,
  pages =        "303--310",
  year =         "2001",
  title =        "Wavelet Representation of Contour Sets",
  author =       "Martin Bertram and Daniel E. Laney and Mark A.
                 Duchaineau and Charles D. Hansen and Bernd Hamann and
                 Kenneth I. Joy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-379",
  abstract =     "We present a new wavelet compression and
                 multiresolution modeling approach for sets of contours
                 (level sets). In contrast to previous wavelet schemes,
                 our algorithm creates a parametrization of a scalar
                 field induced by its contours and compactly stores this
                 parametrization rather than function values sampled on
                 a regular grid. Our representation is based on
                 hierarchical polygon meshes with subdivision
                 connectivity whose vertices are transformed into
                 wavelet coefficients. From this sparse set of
                 coefficients, every set of contours can be efficiently
                 reconstructed at multiple levels of resolution. When
                 applying lossy compression, introducing high
                 quantization errors, our method preserves contour
                 topology, in contrast to compression methods applied to
                 the corresponding field function. We provide numerical
                 results for scalar fields defined on planar domains.
                 Our approach generalizes to volumetric domains,
                 time-varying contours, and level sets of
                 vectorfields.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Contours, Geometry Compression, Iso-surfaces, Level
                 Sets, Multiresolution Methods, Wavelets.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-38,
  year =         "2001",
  title =        "Geometric Simplification for Efficient Occlusion
                 Culling in Urban Scenes",
  author =       "Rick Germs and Frederik W. Jansen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-38",
  abstract =     "Most occlusion culling algorithms select a subset of
                 suitable occluder planes or geometries to exclude
                 invisible objects from further visualization
                 processing. Preferably these occluders are large and
                 simple. For complex scenes it is worthwhile to generate
                 virtual occluders to replace complex occluder
                 geometries by simple polygonal structures. In urban
                 scenes, the facade of buildings comprises most of the
                 occlusion potential. In this paper we present an
                 algorithm to extract simplified facades from complex
                 building models, exploiting the fact that, in most
                 cases, buildings have a 2.5D structure.",
  editor =       "V. Skala",
  keywords =     "Walkthrough visualization, occlusion culling, virtual
                 occluders, geometric simplification.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-380,
  pages =        "311--318",
  year =         "2001",
  title =        "User-Centric Viewpoint Computation for Haptic
                 Exploration and Manipulation",
  author =       "Miguel A. Otaduy and Ming C. Lin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-380",
  abstract =     "We present several techniques for user-centric viewing
                 of the virtual objects or datasets under haptic
                 exploration and manipulation. Depending on the type of
                 tasks performed by the user, our algorithms compute
                 automatic placement of the user viewpoint to navigate
                 through the scene, to display the near-optimal views,
                 and to reposition the viewpoint for haptic
                 visualization. This is accomplished by conjecturing the
                 user's intent based on the user's actions, the object
                 geometry, and intra- and inter-object occlusion
                 relationships. These algorithms have been implemented
                 and interfaced with both a 3-DOF and a 6-DOF PHANToM
                 arms. We demonstrate their application on haptic
                 exploration and visualization of a complex structure,
                 as well as multiresolution modeling and 3D painting
                 with a haptic interface.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-381,
  pages =        "319--324",
  year =         "2001",
  title =        "Fitting Subdivision Surfaces",
  author =       "Nathan Litke and Adi Levin and Peter Schr{\"o}der",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-381",
  abstract =     "We introduce a new algorithm for fitting a
                 Catmull-Clark subdivision surface to a given shape
                 within a prescribed tolerance, based on the method of
                 quasi-interpolation. The fitting algorithm is fast,
                 local and scales well since it does not require the
                 solution of linear systems. Its convergence rate is
                 optimal for regular meshes and our experiments show
                 that it behaves very well for irregular meshes. We
                 demonstrate the power and versatility of our method
                 with examples from interactive modeling, surface
                 fitting, and scientific visualization.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Animation, CAD, Curves & Surfaces, Geometric Modeling,
                 Digital Geometry Processing, Subdivision Schemes,
                 Approximation, Quasi-Interpolation, Catmull-Clark",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-382,
  pages =        "325--331",
  year =         "2001",
  title =        "Nonmanifold Subdivision",
  author =       "Lexing Ying and Denis Zorin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-382",
  abstract =     "Commonly-used subdivision schemes require manifold
                 control meshes and produce manifold surfaces. However,
                 it is often necessary to model nonmanifold surfaces,
                 such as several surface patches meeting at a common
                 boundary. In this paper, we describe a subdivision
                 algorithm that makes it possible to model nonmanifold
                 surfaces. Any triangle mesh, subject only to the
                 restriction that no two vertices of any triangle
                 coincide, can serve as an input to the algorithm.
                 Resulting surfaces consist of collections of manifold
                 patches joined along nonmanifold curves and vertices.
                 If desired, constraints may be imposed on the tangent
                 planes of manifold patches sharing a curve or a vertex.
                 he algorithm is an extension of a well-known Loop
                 sub-division scheme, and uses techniques developed for
                 piece-wise smooth surfaces.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Subdivision surfaces, Nonmanifold surfaces, Geometric
                 modeling.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-383,
  pages =        "333--340",
  year =         "2001",
  title =        "Normal Bounds for Subdivison-Surface Interference
                 Detection",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-383",
  author =       "Eitan Grinspun and Peter Schr{\"o}der",
  abstract =     "Subdivision surfaces are an attractive representation
                 when modeling arbitrary topology free-from surfaces and
                 show great promise for applications in engineering
                 design [5, 6] and computer animation [10]. Interference
                 detection is a critical tool in many of these
                 applications. In this paper we derive normal bounds for
                 subdivision surfaces and use these to develop an
                 efficient algorithm for (self-) interference
                 detection.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Subdivision Surfaces, Multiresolution Surfaces,
                 Selfinterference, Gauss map, Loop's scheme",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-384,
  pages =        "341--347",
  year =         "2001",
  title =        "Smooth Approximation and Rendering of Large Scattered
                 Data Sets",
  author =       "J{\"o}rg Haber and Frank Zeilfelder and Oleg Davydov
                 and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-384",
  abstract =     "We present an efficient method to automatically
                 compute a smooth approximation of large functional
                 scattered data sets given over arbitrarily shaped
                 planar domains. Our approach is based on the
                 construction of a $C^1$-continuous bivariate cubic
                 spline and our method offers optimal approximation
                 order. Both local variation and non-uniform
                 distribution of the data are taken into account by
                 using local polynomial least squares approximations of
                 varying degree. Since we only need to solve small
                 linear systems and no triangulation of the scattered
                 data points is required, the overall complexity of the
                 algorithm is linear in the total number of points.
                 Numerical examples dealing with several real world
                 scattered data sets with up to millions of points
                 demonstrate the efficiency of our method. The resulting
                 spline surface is of high visual quality and can be
                 efficiently evaluated for rendering and modeling. In
                 our implementation we achieve real-time frame rates for
                 typical fly-through sequences and interactive frame
                 rates for recomputing and rendering a locally modified
                 spline surface.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Scattered data approximation, least squares
                 approximation, terrain visualization, data
                 compression",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-385,
  pages =        "349--356",
  year =         "2001",
  title =        "Real-time Decompression And Visualization Of Animated
                 Volume Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-385",
  author =       "Stefan Guthe and Wolfgang Stra{\ss}er",
  abstract =     "Interactive exploration of animated volume data is
                 required by many applications, but the huge amount of
                 computational time and storage space needed for
                 rendering does not allow the visualization of animated
                 volumes by now. In this paper we introduce an algorithm
                 running at interactive frame rates using 3d wavelet
                 transforms that allows for any wavelet, motion
                 compensation techniques and various encoding schemes of
                 the resulting wavelet coefficients to be used. We
                 analyze different families and orders of wavelets
                 compression ratio and the introduced error. We use a
                 quantization that has been optimized for the visual
                 impression of the reconstructed volume independent of
                 the viewing. This enables us to achieve very high
                 compression ratios while still being able to
                 reconstruct the volume with as few visual artifacts as
                 possible. A further improvement of the compression has
                 been achieved by applying a a motion compensation
                 scheme to exploit temporal coherency. Using this scheme
                 we are capable of decompressing each volume of our
                 animation at inter active frame rates, while
                 visualizing these decompressed volumes on a single PC.
                 We also present a number of improved visualization
                 algorithms for high quality display using OpenGL
                 hardware running at interactive frame rates on a
                 standard PC.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Time critical Visualization, Compression for
                 Visualization, Volume Rendering",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-386,
  pages =        "357--362",
  year =         "2001",
  title =        "Compressing Large Polygonal Models",
  author =       "Jeffrey Ho and Kuang-Chih Lee and David Kriegman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-386",
  abstract =     "We present an algorithm that uses partitioning and
                 gluing to compress large triangular meshes which are
                 too complex to fit in main memory. The algorithm is
                 based largely on the existing mesh compression
                 algorithms, most of which require an 'in-core'
                 representation of the input mesh. Our solution is to
                 partition the mesh into smaller submeshes and compress
                 these submeshes separately using existing mesh
                 compression techniques. Since a direct partition of the
                 input mesh is out of question, instead, we partition a
                 simplified mesh and use the partition on the simplified
                 model to obtain a partition on the original model. In
                 order to recover the full connectivity, we present a
                 simple scheme for encoding/decoding the resulting
                 boundary structure from the mesh partition. When
                 compressing large models with few singular vertices, a
                 negligible portion of the compressed output is devoted
                 to gluing information. On desktop computers, we have
                 run experiments on models with millions of vertices,
                 which could not be compressed using standard
                 compression software packages, and have observed
                 compression ratios as high as 17 to 1 using our
                 technique.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Compression algorithms",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-387,
  pages =        "363--370",
  year =         "2001",
  title =        "Visualization of Large Terrains Made Easy",
  author =       "Peter Lindstrom and Valerio Pascucci",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-387",
  abstract =     "We present an elegant and simple to implement
                 framework for performing out-of-core visualization and
                 view-dependent refinement of large terrain surfaces.
                 Contrary to the recent trend of increasingly elaborate
                 algorithms for large-scale terrain visualization, our
                 algorithms and data structures have been designed with
                 the primary goal of simplicity and efficiency of
                 implementation. Our approach to managing large terrain
                 data also departs from more conventional strategies
                 based on data tiling. Rather than emphasizing how to
                 segment and efficiently bring data in and out of
                 memory, we focus on the manner in which the data is
                 laid out to achieve good memory coherency for data
                 accesses made in a top-down (coarse-to-fine) refinement
                 of the terrain. We present and compare the results of
                 using several different data indexing schemes, and
                 propose a simple to compute index that yields
                 substantial improvements in locality and speed over
                 more commonly used data layouts. Our second
                 contribution is a new and simple, yet easy to
                 generalize method for view-dependent refinement.
                 Similar to several published methods in this area, we
                 use longest edge bisection in a top-down traversal of
                 the mesh hierarchy to produce a continuous surface with
                 subdivision connectivity. In tandem with the
                 refinement, we perform view frustum culling and
                 triangle stripping. These three components are done
                 together in a single pass over the mesh. We show how
                 this framework supports virtually any error metric,
                 while still being highly memory and compute
                 efficient.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-388,
  pages =        "371--378",
  year =         "2001",
  title =        "Integrating Occlusion Culling with View-Dependent
                 Rendering",
  author =       "Jihad El-Sana and Neta Sokolovsky and Cl'audio T.
                 Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-388",
  abstract =     "We present a novel approach that integrates occlusion
                 culling within the view-dependent rendering framework.
                 View-dependent rendering provides the ability to change
                 level of detail over the surface seamlessly and
                 smoothly in real-time. The exclusive use of view
                 parameters to perform level-of-detail selection causes
                 even occluded regions to be rendered in high level of
                 detail. To overcome this serious drawback we have
                 integrated occlusion culling into the level selection
                 mechanism. Because computing exact visibility is
                 expensive and it is currently not possible to perform
                 this computation in real time, we use a visibility
                 estimation technique instead. Our approach reduces
                 dramatically the resolution at occluded regions.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-389,
  pages =        "379--386",
  year =         "2001",
  title =        "Approximate Shading for the Re-Illumination of
                 Synthetic Images",
  author =       "Randy Scoggins and Raghu Machiraju and Robert J.
                 Moorhead",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-389",
  abstract =     "This paper presents a novel method to estimate
                 illumination-dependent properties in image synthesis
                 prior to rendering. A preprocessing step is described
                 in which a linear image basis is developed and a
                 lighting-independent formulation defined. A reflection
                 function, similar to hemispherical reflectance,
                 approximates normal Lambertian shading. Intensity
                 errors resulting from this approximation are reduced by
                 use of a polynomial gamma correction function and
                 scaling to a normalized display range. This produces
                 images that are similar to normal Lambertian shading
                 without employing the maximum ( max) function. For a
                 single object view, images can then be expressed in a
                 linear form so that lighting direction can be factored
                 out. During normal rendering, image quantities for
                 arbitrary light directions can be found without
                 rendering. This method is demonstrated for estimating
                 image intensity and level-of-detail (LOD) error prior
                 to rendering an object.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Rendering, level-of-detail, image metrics,
                 perception.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-39,
  year =         "2001",
  title =        "Variable Resolution Level-of-Detail of Multiresolution
                 Ordered Meshes",
  author =       "J. Ribelles and A. Lopez and O. Belmonte and J.
                 Remolar and M. Chover",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-39",
  abstract =     "Variable resolution level-of-detail is an important
                 application of multiresolution modeling, since the use
                 of different resolutions across the surface allows
                 interactive visualization of highly detailed objects.
                 Multiresolution Ordered Meshes (MOM) was first
                 introduced as a model that achieves the efficient
                 management of an ample range of uniform resolution
                 levels-of-detail and presents reduced storage space
                 requirements. In this paper, we introduce an algorithm
                 capable of retrieving variable resolution
                 levels-of-detail from a multiresolution MOM
                 representation without having to reorganize data in new
                 structures or store new information. The proposed
                 algorithm starts from the level of highest detail and
                 simplifies it adaptively to reach the required
                 resolution in each area of the surface. Experiments
                 with data sets of varying complexity demonstrate that
                 the new algorithm obtains variable resolution levels of
                 detail while retaining the advantages of MOM.",
  editor =       "V. Skala",
  keywords =     "Multiresolution modeling, level of detail, dynamic
                 simplification, interactive visualization.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-390,
  pages =        "387--394",
  year =         "2001",
  title =        "Volume Rendering of Fine Details Within Medical Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-390",
  author =       "Feng Dongg and Gordon J. Clapworthy and Mel Krokos",
  abstract =     "This work presents a method concerning the volume
                 rendering of fine details, such as blood vessels and
                 nerves, from medical data. The realistic and efficient
                 visualization of such structures is often of great
                 medical interest, and conventional rendering techniques
                 do not always deal with them adequately. Our method
                 uses pre-processing to reconstruct fine details that
                 are difficult to segment and label. It detects the
                 presence of fine geometrical structures, such as cracks
                 or cylinders that suggest the existence of, for
                 example, blood vessels or nerves; the subsequent volume
                 rendering then displays fine geometrical objects that
                 lie on a surface. The method can also show structures
                 within the volume, using a special {"}integration
                 sampling{"} scheme to portray reconstructed volume
                 texture, such as that exhibited by muscle fibers. By
                 combining the surface structure and volume texture in
                 the rendering, realistic results can be produced;
                 examples are provided.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume Rendering, Fine Details, Medical Visualization,
                 Image Processing, Volume Textures",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-391,
  pages =        "395--402",
  year =         "2001",
  title =        "Visualization and Interaction Techniques for the
                 Exploration of Vascular Structures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-391",
  author =       "Horst K. Hahn and Bernhard Preim and Dirk Selle and
                 Heinz-Otto Peitgen",
  abstract =     "We describe a pipeline of image processing steps for
                 deriving symbolic models of vascular structures from
                 radiological data which reflect the branching pattern
                 and diameter of vessels. For the visualization of these
                 symbolic models , concatenated truncated cones are
                 smoothly blended at branching points. We put emphasis
                 on the quality of the visualizations which is achieved
                 by anti-aliasing operations in different stages of the
                 visualization. The methods presented here are referred
                 to as HQVV (High Quality Vessel Visualization).
                 Scalable techniques are provided to explore vascular
                 structures of different orders of magnitude. The
                 hierarchy as well as the diameter of the branches of
                 vascular systems are used to restrict visualizations to
                 relevant subtrees and to emphasize parts of vascular
                 systems. Our research is inspired by clear
                 visualizations in textbooks and is targeted towards
                 medical education and therapy planning. We describe the
                 application of vessel visualization techniques for
                 liver surgery planning. For this application it is
                 crucial to recognize the morphology and branching
                 pattern of vascular systems as well as the basic
                 spatial relations between vessels and other anatomic
                 structures.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Vessel visualization, medical visualization,
                 computer-assisted surgery",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-392,
  pages =        "403--410",
  year =         "2001",
  title =        "Variational Classification for Visualization of 3{D}
                 Ultrasound Data",
  author =       "Raanan Fattal and Dani Lischinskiy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-392",
  abstract =     "We present a new technique for visualizing surfaces
                 from 3D ultrasound data. 3D ultrasound datasets are
                 typically fuzzy, contain a substantial amount of noise
                 and speckle, and suffer from several other problems
                 that make extraction of continuous and smooth surfaces
                 extremely difficult. We propose a novel opacity
                 classification algorithm for 3D ultrasound datasets,
                 based on the variational principle. More specifically,
                 we compute a volumetric opacity function that optimally
                 satisfies a set of simultaneous requirements. One
                 requirement makes the function attain nonzero values
                 only in the vicinity of a user-specified value,
                 resulting in soft shells of finite, approximately
                 constant thickness around isosurfaces in the volume.
                 Other requirements are designed to make the function
                 smoother and less sensitive to noise and speckle. The
                 computed opacity function lends itself well to explicit
                 geometric surface extraction, as well as to direct
                 volume rendering at interactive rates. We also describe
                 a new splatting algorithm that is particularly well
                 suited for displaying soft opacity shells. Several
                 examples and comparisons are included to illustrate our
                 approach and demonstrate its effectiveness on real 3D
                 ultrasound datasets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "3D ultrasound, classification, isosurface extraction,
                 opacity function, splatting, the variational principle,
                 volume rendering",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-393,
  pages =        "411--418",
  year =         "2001",
  title =        "Nonlinear Virtual Colon Unfolding",
  author =       "Anna Vilanova Bartroli and Rainer Wegenkittl and
                 Andreas K{\"{o}}nig and Eduard Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-393",
  abstract =     "The majority of virtual endoscopy techniques tries to
                 simulate a real endoscopy. A real endoscopy does not
                 always give the optimal information due to the physical
                 limitations it is subject to. In this paper, we deal
                 with the unfolding of the surface of the colon as a
                 possible visualization technique for diagnosis and
                 polyp detection. A new two-step technique is presented
                 which deals with the problems of double appearance of
                 polyps and nonuniform sampling that other colon
                 unfolding techniques suffer from. In the first step, a
                 distance map from a central path induces nonlinear rays
                 for unambiguous parameterization of the surface. The
                 second step compensates for locally varying distortions
                 of the unfolded surface. A technique similar to
                 magnification fields in information visualization is
                 hereby applied. The technique produces a single view of
                 a complete virtually dissected colon.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Volume Rendering, Virtual Endoscopy",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-394,
  pages =        "421--424",
  year =         "2001",
  title =        "Ping{TV}: {A} Case Study in Visual Network
                 Monitoring",
  author =       "Alexander Gubin and William Yurcik and Larry
                 Brumbaugh",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-394",
  abstract =     "PingTV generates a logical map of a network that is
                 used as an overlay on a physical geographical image of
                 the location from the user perspective (buildings,
                 floors within buildings, etc.). PingTV is used at
                 Illinois State University as a visualization tool to
                 communicate real-time network conditions to the
                 university community via a dedicated channel on the
                 campus cable TV system. Colored symbols allow students
                 and staff to discern high congestion {"}rush hours{"}
                 and understand why their specific Internet connectivity
                 is {"}broken{"} from the wide range of potential
                 causes. Lessons learned include the use of color to
                 visually convey confidence intervals using color
                 shading and the visualization of cyclical network
                 traffic patterns. Our implementation is general and
                 flexible with potential for application for other
                 domains.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Network visualization , active network measurement,
                 real-time television monitoring system",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-395,
  pages =        "425--428",
  year =         "2001",
  title =        "Case Study: Medical Web Service for the Automatic 3{D}
                 Documentation for Neuroradiological Diagnosis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-395",
  author =       "S. Iserhardt-Bauer and P. Hastreiter and T. Ertl and
                 K. Eberhardt and B. Tomandl",
  abstract =     "This case study presents a medical web service for the
                 automatic analysis of CTA (Computer Tomography
                 Angiography) datasets. It aims at the detection and
                 evaluation of intracranial aneurysms which are
                 malformations of cerebral blood vessels. To obtain a
                 standardized 3D visualization digital videos are
                 automatically generated. The time-consuming video
                 production caused by the manual delineation of
                 structures, software based volume rendering, and the
                 interactive definition of an optimized camera path is
                 considerably improved with a fully automatic strategy.
                 Therefore, a previously suggested approach [11] is
                 applied which uses an optimized transfer function as a
                 template and automatically adapts it to an individual
                 dataset. Furthermore, we introduce hardware-accelerated
                 morphologic filtering in oder to detect the location of
                 mid-size and giant aneurysms. The actual generation of
                 the video is finally integrated into a hardware
                 off-screen rendering process based on 3D texture
                 mapping ensuring fast visualization of high quality.
                 Overall, clinical routine can be considerably assisted
                 by providing a web service combining automatic
                 detection and standardized visualization.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Medical visualization, segmentation, automatic web
                 service, video generation",
  booktitle =    "Proceedings Visualization 200",
}

@InProceedings{EVL-2001-396,
  pages =        "429--432",
  year =         "2001",
  title =        "Case Study: Visual Debugging of Cluster Hardware",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-396",
  author =       "Patricia Grosso and Rena Haynes",
  abstract =     "This paper presents a novel use of visualization
                 applied to debugging the Cplant^{TM} cluster hardware
                 at Sandia National Laboratories. As commodity clusters
                 grow in popularity and grow in size, tracking component
                 failures within the hardware will become more and more
                 difficult. We have developed a tool that facilitates
                 visual debugging of errors within the switches and
                 cables connecting the processors. Combining an abstract
                 system model with color-coding for both error and job
                 information enables failing components to be
                 identified.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Visual debugging, hardware modeling, design analysis,
                 and performance optimization",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-397,
  pages =        "433--436",
  year =         "2001",
  title =        "Case Study on Real-Time Visualization of Virtual
                 {T}{\"u}bingen on Commodity {PC} Hardware",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-397",
  author =       "Michael Mei{\ss}ner and Jasmina Orman and Stephan J.
                 Braun",
  abstract =     "For psychophysical studies in spatial cognition a
                 virtual model of the picturesque old town of
                 T{\"u}bingen has been constructed. On order to perform
                 psychophysical experiments in highly realistic virtual
                 environments the model is based on high quality texture
                 maps adding up to several hundreds of MByres. To
                 accomplish the required real-time frame updates, view
                 frustum and occlusion culling without visibility
                 pre-processing, levels of detail, and texture
                 compression are applied in an interleaved manner.
                 Shared memory communication and a standard PC with two
                 commodity graphic cards is used to enable the powerful
                 combination of those techniques because this
                 combination is not yet available on a single graphics
                 card.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Virtual Environments, Virtual Reality, Spatial
                 Cognition, PC graphics card, Culling, levels-of-detail,
                 Texture Compression",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-398,
  pages =        "437--440",
  year =         "2001",
  title =        "An Immersive Virtual Environment for {DT}-{MRI} Volume
                 Visualization Applications: a Case Study",
  author =       "S. Zhang and C and . Demiralp and D. F. Keefe and M.
                 DaSilva andD. H. Laidlaw and B. D. Greenberg and P. J.
                 Basser andC. Pierpaoli and E. A. Chiocca andT. S.
                 Deisboeck",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-398",
  abstract =     "We describe a virtual reality environment for
                 visualizing tensor-valued volumetric datasets acquired
                 with diffusion tensor magnetic resonance imaging
                 (DT-MRI). We have prototyped a virtual environment that
                 displays geometric representations of the volumetric
                 second-order diffusion tensor data and are developing
                 interaction and visualization techniques for two
                 application areas: studying changes in white-matter
                 structures after gamma-knife capsulotomy and
                 pre-operative planning for brain tumor surgery. Our
                 feedback shows that compared to desktop displays, our
                 system helps the user better interpret the large and
                 complex geometric models, and facilitates communication
                 among a group of users.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Scientific Visualization, DT-MRI, Diffusion, Medical
                 Imaging, Virtual Reality.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-399,
  pages =        "441--444",
  year =         "2001",
  title =        "Chromatin Decondensation: a Case Study of Tracking
                 Features in Confocal Data",
  author =       "Wim de Leeuw and Robert van Liere",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-399",
  abstract =     "In this case study we discuss an interactive feature
                 tracking system and its use for the analysis of
                 chromatin decondensation. Features are described as
                 points in a multidimensional attribute space. Distances
                 between points are used as a measure for feature
                 correspondence. Users can interactively experiment with
                 the correspondence measure in order to gain insight in
                 chromatin movement. In addition, by defining time as an
                 attribute, tracking problems related to noisy confocal
                 data can be circumvented.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Feature tracking, multidimensional visualization,
                 biomedical imaging.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-4,
  year =         "2001",
  title =        "Pre-processing of Car Geometry Data for Crash
                 Simulation and Visualization",
  author =       "N. Frisch and D. Rose and O. Sommer and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-4",
  abstract =     "In this paper we focus on a visualization tool for car
                 crash simulations. By means of some examples we show
                 how various data pre-processing features can facilitate
                 the engineer's work. For example, parts can be
                 assembled, replaced, welded, bonded, or deformed. Data
                 pre-processing within the visualization tool means that
                 some modifications can be done directly on the finite
                 element mesh which is the input for the crash
                 simulation. Some of the features are new within a crash
                 visualization environment and some operations needed
                 new algorithms to be developed: We present the
                 generation of curved spotweld lines and adhesive
                 bondings along flanges. Further we show how modern
                 hardware features like textures and alpha blending can
                 be employed for efficient visualization in the context
                 of data pre-processing. The new features allow crash
                 simulations in an early development phase, they also
                 allow to test the impact of a potential improvement or
                 to remove some shortcomings due to mesh generation out
                 of CAD data. Thanks to these new features, the crash
                 simulation engineer needs no longer to return the model
                 data back to the CAD department for minor modification
                 and re-meshing.",
  editor =       "V. Skala",
  keywords =     "Visualization of car crash simulation, finite
                 elements, pre-processing, CAD.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-40,
  year =         "2001",
  title =        "Geometric processing of Volumetric Objects",
  author =       "Romildo Silva and Jonas Gomes and Cicero Mota",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-40",
  abstract =     "This paper introduces techniques of Riemannian
                 geometry for processing and visualizing volumetric
                 graphical objects. A family of non-linear high-pass
                 filters, based on the curvature tensor, is introduced
                 and used to study the local redundancy on objects. It
                 is shown how to reconstruct an objects from geometric
                 non-redundant regions and applications are presented
                 and discussed.",
  editor =       "V. Skala",
  keywords =     "Volumetric objects, processing, coding.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-400,
  pages =        "445--448",
  year =         "2001",
  title =        "Case Study: An Environment for Understanding Protein
                 Simulations Using Game Graphics",
  author =       "Donna Gresh and Frank Suits and Yuk Yin Sham",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-400",
  abstract =     "We describe a visualization system designed for
                 interactive study of proteins in the field of
                 computational biology. Our system incorporates
                 multiple, custom, three-dimensional and two-dimensional
                 linked views of the proteins. We take advantage of
                 modern commodity graphics cards, which are typically
                 designed for games rather than scientific visualization
                 applications, to provide instantaneous linking between
                 views and three-dimensional interactivity on standard
                 personal computers. Furthermore, we anticipate the
                 usefulness of game techniques such as bump maps and
                 skinning for scientific applications.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Visualization, proteins, computational biology,
                 molecular modeling, molecular dynamics, game graphics,
                 DirectX",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-401,
  pages =        "449--452",
  year =         "2001",
  title =        "Surgical Simulator for Hysteroscopy: {A} Case Study of
                 Visualization in Surgical Training",
  author =       "Kevin Montgomery and LeRoy Heinrichs and Cynthia
                 Bruyns andSimon Wildermuth and hristopher Hasser and
                 Stephanie Ozenne andDavid Bailey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-401",
  abstract =     "Computer-based surgical simulation promises to provide
                 a broader scope of clinical training through the
                 introduction of an atomic variation, simulation of
                 untoward events, and collection of performance data. We
                 present a haptically-enabled surgical simulator for the
                 most common techniques in diagnostic and operative
                 hysteroscopy- cervical dilation, endometrial resection
                 and ablation, and lesion excision. Engineering
                 tradeoffs in developing a real-time, haptic-rate
                 simulator are discussed.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Surgical simulation, hysteroscopy, haptics",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-402,
  pages =        "453--456",
  year =         "2001",
  title =        "Case Study: Reconstruction, Visualization And
                 Quantification Of Neural Fiber Pathways",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-402",
  author =       "Zhaohua Ding and John C. Gore and Adam W. Anderson",
  abstract =     "It is of significant interest for neurological studies
                 to determine and visualize neuronal fiber pathways in
                 the human brain. By exploiting the capability of
                 diffusion tensor magnetic resonance imaging to detect
                 local orientations of neuronal fibers, we have
                 developed a system of algorithms to reconstruct,
                 visualize and quantify neuronal pathways in vivo.
                 Illustrative results show that the system is a
                 promising tool for visual analysis of fiber
                 connectivity and quantitative studies of neuronal
                 fibers.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Neuronal fiber pathway, diffusion tensor imaging",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-403,
  pages =        "457--460",
  year =         "2001",
  title =        "Visualizing 2{D} Probability Distributions from {EOS}
                 Satellite Image-Derived Data Sets: {A} Case Study",
  author =       "David Kao and Jennifer L. Dungan and Alex Pang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-403",
  abstract =     "Maps of biophysical and geophysical variables using
                 Earth Observing System (EOS) satellite image data are
                 an important component of Earth science. These maps
                 have a single value derived at every grid cell and
                 standard techniques are used to visualize them. Current
                 tools fall short, however, when it is necessary to
                 describe a \emph{distribution} of values at each grid
                 cell. Distributions may represent a frequency of
                 occurrence over time, frequency of occurrence from
                 multiple runs of an ensemble forecast or possible
                 values from an uncertainty model. We identify these
                 ``distribution data sets{"} and present a case study to
                 visualize such 2D distributions. Distribution data sets
                 are different from multivariate data sets in the sense
                 that the values are for a single variable instead of
                 multiple variables. Data for this case study consists
                 of multiple realizations of percent forest cover,
                 generated using a geostatistical technique that
                 combines ground measurements and satellite imagery to
                 model uncertainty about forest cover. We present two
                 general approaches for analyzing and visualizing such
                 data sets. The first is a pixel-wise analysis of the
                 probability density functions for the 2D image while
                 the second is an analysis of features identified within
                 the image. Such pixel-wise and feature-wise views will
                 give Earth scientists a more complete understanding of
                 distribution data sets. See
                 www.cse.ucsc.edu/research/avis/nasa_is for additional
                 information.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Uncertainty, probability density function,
                 geostatistics, conditional simulation, data
                 assimilation",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-404,
  pages =        "461--464",
  year =         "2001",
  title =        "Case Study: Application of Feature Tracking to
                 Analysis of Autoignition Simulation Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-404",
  author =       "Wendy S. Koegler",
  abstract =     "The focus of this paper is to evaluate the usefulness
                 of some basic feature tracking algorithms as analysis
                 tools for combustion datasets by application to a
                 dataset modeling autoignition. Features defined as
                 areas of high intermediate concentrations were examined
                 to explore the initial phases in the autoignition
                 process.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Feature detection, feature tracking, combustion,
                 autoignition",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-405,
  pages =        "465--468",
  year =         "2001",
  title =        "Case Study: Visualization of Particle Track Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-405",
  author =       "Xiaoming Wei and Arie E. Kaufman and Timothy J.
                 Hallman",
  abstract =     "The Relativistic Heavy Ion Collider (RHIC) experiment
                 at the Brookhaven National Lab is designed to study how
                 the universe came into being. It is believed that after
                 the Big Bang , the universe expanded and cooled,
                 consisting of a soup of quarks, gluons, electrons and
                 neutrinos. As the temperature lowered, electrons
                 combined with protons and formed neutral atoms. Later,
                 clouds of atoms contracted into stars. In this paper we
                 describe how techniques of volume rendering and
                 information visualization are used to visualize the
                 large particle track data set generated from this high
                 energy physics experiment. The system, called TrackVis,
                 is based on our earlier work of VolVis - Volume
                 Visualization software. Example images of particle
                 collision data are shown, which are helpful to
                 physicists in investigating the behavior of strongly
                 interacting matter at high energy density.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-406,
  pages =        "469--472",
  year =         "2001",
  title =        "Case Study: Interacting with Cortical Flat Maps of the
                 Human Brain",
  author =       "Monica K. Hurdal and Kevin W. Kurtz and David C.
                 Banks",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-406",
  abstract =     "The complex geometry of the human brain contains many
                 folds and fissures, making it impossible to view the
                 entire surface at once. Since most of the cortical
                 activity occurs on these folds, it is desirable to be
                 able to view the entire surface of the brain in a
                 single view. This can be achieved using quasi-conformal
                 flat maps of the cortical surface. Computational and
                 visualization tools are now needed to be able to
                 interact with these flat maps of the brain to gain
                 information about spatial and functional relationships
                 that might not otherwise be apparent. Such information
                 can contribute to earlier diagnostic tools for diseases
                 and improved treatment. Our group is developing
                 visualization and analysis tools that will help
                 elucidate new information about the human brain through
                 the interaction between a cortical surface and its
                 corresponding quasi-conformal flat map.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Conformal, Cortical Features, Human Brain, Flat Map,
                 Interaction, MRI, Neuroscience, Surface",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-407,
  pages =        "473--476",
  year =         "2001",
  title =        "4{D} Space-Time Techniques: {A} Medical Imaging Case
                 Study",
  author =       "Melanie Tory and Niklas R{\"o}ber and Torsten
                 M{\"o}ller and Anna Celler and M. Stella Atkins",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-407",
  abstract =     "We present the problem of visualizing time-varying
                 medical data. Two medical imaging modalities are
                 compared - MRI and dynamic SPECT. For each modality, we
                 examine several derived scalar and vector quantities
                 such as the change in intensity over time, the spatial
                 gradient, and the change of the gradient over time. We
                 compare several methods for presenting the data,
                 including isosurfaces, direct volume rendering, and
                 vector visualization using glyphs. These techniques may
                 provide more information and context than methods
                 currently used in practice; thus it is easier to
                 discover temporal changes and abnormalities in a data
                 set.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Health, display algorithms, animations, 4D
                 visualization, MRI, dynamic SPECT, direct volume
                 rendering, isosurface, glyph",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-408,
  pages =        "477--480",
  year =         "2001",
  title =        "Computed Tomography Angiography: {A} Case Study of
                 Peripheral Vessel Investigation",
  author =       "A. Kanitsar and R. Wegenkittl and P. Felkel and D.
                 Fleischmann and D. Sandner and E. Gr{\"{o}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-408",
  abstract =     "This paper deals with vessel exploration based on
                 computed tomography angiography. Large image sequences
                 of the lower extremities are investigated in a clinical
                 environment. Two different approaches for peripheral
                 vessel diagnosis dealing with stenosis and
                 calcification detection are introduced. The paper
                 presents an automated vessel-tracking tool for curved
                 planar reformation. An interactive segmentation tool
                 for bone removal is proposed.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Computed Tomography Angiography (CTA), semi automatic
                 segmentation, optimal path computation",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-409,
  pages =        "481--484",
  year =         "2001",
  title =        "Graphical Strategies to Convey Functional
                 Relationships in the Human Brain: {A} Case Study",
  author =       "Tomihisa Welsh and Klaus Mueller and Wei Zhu and Nora
                 Volkow and Jeffrey Meade",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-409",
  abstract =     "Brain imaging methods used in experimental brain
                 research such as Positron Emission Tomography (PET) and
                 Functional Magnetic Resonance (fMRI) require the
                 analysis of large amounts of data. Exploratory
                 statistical methods can be used to generate new
                 hypotheses and to provide a reliable measure of a given
                 effect. Typically, researchers report their findings by
                 listing those regions which show significant
                 statistical activity in a group of subjects under some
                 experimental condition or task. A number of methods
                 create statistical parametric maps (SPMs) of the brain
                 on a voxel-basis. In our approach statistics are
                 computed not on individual voxels but on predefined
                 anatomical regions-of-interest (ROIs). A correlation
                 coefficient is used to quantify similarity in response
                 for various regions during an experimental setting.
                 Since the functional inter-relationships can become
                 rather complex and spatially widespread, they are best
                 understood in the context of the underly-ing 3-D brain
                 anatomy. However, despite the power of the 3-D model,
                 the relative location of ROIs in 3-D can be obscured
                 due the inherent problem of presenting 3-D spatial
                 information on a 2-D screen. In order to address this
                 problem, we have explored a number of visualization
                 techniques to aid the brain researcher in exploring the
                 spatial relationships of brain activity. In this paper,
                 we present a novel 3-D interface that allows the
                 interactive exploration of correlation datasets.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-410,
  pages =        "485--488",
  year =         "2001",
  title =        "A Case Study on Interactive Exploration and Guidance
                 Aids for Visualizing Historical Data",
  author =       "Stanislav L. Stoev and Wolfgang Stra{\ss}er",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-410",
  abstract =     "In this paper, we address the problem of historical
                 data visualization. We describe the data acquisition,
                 preparation, and visualization. Since the data contain
                 four dimensions, the standard 3D exploration techniques
                 have to be extended or appropriately adapted in order
                 to enable interactive exploration. We discuss in detail
                 two interaction concepts: (1) navigation with one fixed
                 dimension, and (2) quasi 4D navigation allowing to
                 simultaneously explore the four-dimensional space. In
                 addition, we also present a picture-in-picture display
                 mode, enabling the user to interactively view the data,
                 while {"}flying with{"} a particular event, tracking
                 its motion in time and space. Finally, we present a
                 technique for guided exploration and animation
                 generation, allowing for a vivid gain of insight into
                 the historical data.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Visualization, Historical Data, Interaction,
                 Time-dependent Data, Visualization Techniques.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-411,
  pages =        "489--492",
  year =         "2001",
  title =        "The Met{VR} Case Study: Meteorological Visualization
                 in an Immersive Virtual Environment",
  author =       "Sean Ziegeler and Robert J. Moorhead and Paul J. Croft
                 and Duanjun Lu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-411",
  abstract =     "Traditional methods for displaying weather products
                 are generally two-dimensional (2D) plots or just text
                 format. It is hard for forecasters to get the entire
                 picture of the atmosphere using these methods. The
                 problems apparent in 2D with comparing and correlating
                 multiple layers are overcome simply by adding a
                 dimension. This is important because pertinent features
                 in the data sets may lie in multiple layers and span
                 several time steps. However, simply using a
                 three-dimensional (3D) approach is not enough. The
                 capacity for analysis of small-scale, but important,
                 features in 2D is lost when transitioning to 3D. We
                 propose that 3D's advantages can be incorporated with
                 2D's small-scale analysis by using an immersive virtual
                 environment. In this case study, we evaluate our
                 current standing with the project: have we met our
                 goals, and how should we proceed from this point? To
                 evaluate our application, we invited meteorologists to
                 use the application to explore a data set. Then we
                 presented our goals and asked which ones had we met,
                 from a meteorologist's perspective. The results
                 qualitatively reflected that our application was
                 effective and further research would be worthwhile.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Virtual environments, virtual reality, immersion,
                 visualization, meteorology.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-412,
  pages =        "493--496",
  year =         "2001",
  title =        "Archaeological Data Visualization in {VR}: Analysis of
                 Lamp Finds at the Great Temple of Petra, a Case Study",
  author =       "Daniel Acevedo and Eileen Vote and David H. Laidlaw
                 and Martha S. Joukowsky",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-412",
  abstract =     "We present the results of an evaluation of the ARCHAVE
                 system, an immersive virtual reality environment for
                 archaeological research. ARCHAVE is implemented in a
                 Cave. The evaluation studied researchers analyzing lamp
                 and coin finds throughout the excavation trenches at
                 the Petra Great Temple site in Jordan. Experienced
                 archaeologists used our system to study excavation
                 data, confirming existing hypotheses and postulating
                 new theories they had not been able to discover without
                 the system. ARCHAVE provided access to the excavation
                 database, and researchers were able to examine the data
                 in the context of a life-size representation of the
                 present day architectural ruins of the temple. They
                 also had access to a miniature model for site-wide
                 analysis. Because users quickly became comfortable with
                 the interface, they concentrated their efforts on
                 examining the data being retrieved and displayed. The
                 immersive VR visualization of the recovered information
                 gave them the opportunity to explore it in a new and
                 dynamic way and, in several cases, enabled them to make
                 discoveries that opened new lines of investigation
                 about the excavation.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Scientific Visualization, Archaeological Data
                 Analy-sis, Immersive Virtual Reality Interfaces",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-413,
  pages =        "497--500",
  year =         "2001",
  title =        "Virtual Temporal Bone Dissection: {A} Case Study",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-413",
  author =       "Jason Bryan and Don Stredney and Dr. Greg Wiet and
                 Dennis Sessanna",
  abstract =     "The Temporal Bone Dissection Simulator is an ongoing
                 research project for the construction of a synthetic
                 environment suitable for virtual dissection of human
                 temporal bone and related anatomy. Funded by the
                 National Institute on Deafness and Other Communication
                 Disorders (NIDCD), the primary goal of this project is
                 to provide a safe, robust, and cost-effective virtual
                 environment for learning the anatomy and surgical
                 procedures associated with the temporal bone. Direct
                 volume visualization has been indispensable for the
                 necessary level of realism and interactivity that is
                 vital to the success of this project. The work is been
                 conducted by the Ohio Supercomputer Center in
                 conjunction with the Department of Otolaryngology at
                 the Ohio Sate University, and NIDCD.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Temporal Bone Dissection",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-414,
  pages =        "501--504",
  year =         "2001",
  title =        "Semi-Immersive Space Mission Design and Visualization:
                 Case Study of the {"}Terrestrial Planet Finder{"}
                 Mission.",
  author =       "Ken Museth and Alan Barr and Martin W. Lo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-414",
  abstract =     "This paper addresses visualization issues of the
                 Terrestrial Planet Finder Mission[2]. The goal of this
                 mission is to search for chemical signatures of life in
                 distant solar systems using five satellites flying in
                 formation to simulate a large telescope. To design and
                 visually verify such a delicate mission one has to
                 analyze and interact with many different 3D spacecraft
                 trajectories, which is often difficult in 2D. We employ
                 a novel trajectory design approach using invariant
                 manifold theory, which is best understood and utilized
                 in an immersive setting. The visualization also
                 addresses multi-scale issues related to the vast
                 differences in distance, velocity, and time at
                 different phases of the mission. Additionally, the
                 parameterization and coordinate frames used for
                 numerical simulations may not be suitable for direct
                 visualization. Relative motion presents a more serious
                 problem where the patterns of the trajectories can only
                 be viewed in particular rotating frames. Some of these
                 problems are greatly relieved by using interactive,
                 animated stereo 3D visualization in a semi-immersive
                 environment such as a Responsive Workbench. Others were
                 solved using standard techniques such as a stratify
                 approach with multiple windows to address the
                 multi-scale issues, reparameterizations of trajectories
                 and associated 2D manifolds and relative motion of the
                 camera to {"}evoke{"} the desired patterns.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-415,
  pages =        "505--508",
  year =         "2001",
  title =        "Wind Tunnel Data Fusion and Immersive Visualization:
                 {A} Case Study",
  author =       "Kurt Severance and Paul Brewster and Barry Lazos and
                 Daniel Keefe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-415",
  abstract =     "This case study describes the process of fusing the
                 data from several wind tunnel experiments into a single
                 coherent visualization. Each experiment was conducted
                 independently and was designed to explore different
                 flow features around airplane landing gear. In the
                 past, it would have been very difficult to correlate
                 results from the different experiments. However, with a
                 single 3-D visualization representing the fusion of the
                 three experiments, significant insight into the
                 composite flowfield was observed that would have been
                 extremely difficult to obtain by studying its component
                 parts. The results are even more compelling when viewed
                 in an immersive environment.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  keywords =     "Data fusion, Photogrammetry, Line Integral
                 Convolution, Reconstruction, Oil flow, Particle Image
                 Velocimetry, Wind tunnel, Landing gear, Texture
                 mapping, Image-based rendering, VRML.",
  booktitle =    "Proceedings Visualization 2001",
}

@InProceedings{EVL-2001-416,
  pages =        "509--512",
  year =         "2001",
  title =        "A Virtual Environment for Simulated Rat Dissection:
                 {A} Case Study of Visualization for Astronaut
                 Training",
  author =       "Kevin Montgomery and Cynthia Bruyns and Simon
                 Wildermuth",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-416",
  abstract =     "Animal dissection for the scientific examination of
                 organ subsystems is a delicate procedure. Performing
                 this procedure under the complex environment of
                 microgravity presents additional challenges because of
                 the limited training opportunities available that can
                 recreate the altered gravity environment. Traditional
                 astronaut crew training often occurs several months in
                 advance of experimentation, provides limited realism,
                 and involves complicated logistics. We have developed
                 an interactive virtual environment that can simulate
                 several common tasks performed during animal
                 dissection. In this paper, we describe the imaging
                 modality used to reconstruct the rat, provide an
                 overview of the simulation environment and briefly
                 discuss some of the techniques used to manipulate the
                 virtual rat.",
  organization = "IEEE Computer Society Technical Committee on
                 Visualization and Graphics Executive Committee",
  editor =       "Thomas Ertl and Ken Joy and Amitabh Varshney",
  booktitle =    "Proceedings Visualization 2001",
}

@InCollection{EVL-2001-417,
  pages =        "457--474",
  year =         "2001",
  title =        "Layered animation of captured data",
  author =       "W. Sun and A. Hilton and R. Smith and J. Illingworth",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-417",
  abstract =     "This paper proposes a novel technique for building
                 layered animation models of real articulated objects
                 from 3D surface measurement data. Objects are scanned
                 using a hand-held 3D sensor to acquire 3D surface
                 measurements. A novel geometric fusion algorithm is
                 presented which enables reconstruction of a single
                 surface model from the captured data. This algorithm
                 overcomes the limitations of previous approaches which
                 cannot be used for hand-held sensor data as they assume
                 that measurements are on a structured planar grid. The
                 geometric fusion introduces the normal volume of a
                 triangle to convert individual triangles to a
                 volumetric representation. A layered model is
                 constructed to animate the reconstructed
                 high-resolution surface. The model consists of 3
                 layers: a skeleton for animation from key-frame or
                 motion capture; a low-resolution control model for
                 real-time mesh deformation; and a high-resolution model
                 to represent the captured surface detail. Initially the
                 skeleton model is manually placed inside the
                 low-resolution control model and high-resolution
                 scanned data. Automatic techniques are introduced to
                 map both the control model and captured data into a
                 single layered model. The high-resolution captured data
                 is mapped onto the low-resolution control model using
                 the normal volume. The resulting model enables
                 efficient, seamless animation by manipulation of the
                 skeleton while maintaining the captured high-resolution
                 surface detail. The animation of high-resolution
                 captured data based on a low-resolution generic model
                 of the object opens up the possibility of rapid capture
                 and animation of new objects based on libraries of
                 generic models.",
  volume =       "17(8)",
  keywords =     "Layered animation, 3D scanning, 3D reconstruction",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-418,
  pages =        "475--490",
  year =         "2001",
  title =        "A method for generating pavement textures using the
                 square packing technique",
  author =       "K. Miyata and T. Itoh and K. Shimada",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-418",
  abstract =     "This paper presents a method for generating pavement
                 textures using the square packing technique. A pattern
                 of packed square cells for a given area is generated by
                 performing particle simulation with proximity-based
                 forces. The pavement texture is then obtained by
                 generating a stone shape for each cell with the
                 subdivision surface method and then applying fractal
                 noise to create a detailed surface geometry. In this
                 method, the boundary shape of the pavement and the
                 average size of the pavement stones are specified as
                 input geometric data, along with attribute data such as
                 the roughness, color, and optical attributes of the
                 stones. The proposed method automatically generates a
                 visually realistic pavement texture for an arbitrarily
                 shaped pavement with much lower computational cost than
                 previous methods.",
  volume =       "17(8)",
  keywords =     "Texture, Pavement, Square packing, Stone, Subdivision
                 surface",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-419,
  pages =        "491--505",
  year =         "2001",
  title =        "Efficiently simulating scattering of light by leaves",
  author =       "Gladimir V. G. Baranoski and Jon G. Rokne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-419",
  abstract =     "Rendering techniques currently used in computer
                 graphics enable the generation of very realistic images
                 for a wide range of materials. Despite the latest
                 achievements in this field, however, there are still
                 areas, such as biological imaging, open for further
                 investigation. In this paper an efficient approach for
                 simulating light scattering by leaves is presented. It
                 consists of pre-computing reflectance and transmittance
                 values and applying a simplified scattering model for
                 foliar tissues. This model accounts for the main
                 biological characteristics of these organic materials,
                 while avoiding undue complexity in order to increase
                 the efficiency of the light scattering simulations. Its
                 design and formulation are based on standard Monte
                 Carlo methods, and it can be incorporated into global
                 illumination systems without a significant
                 computational overhead. The accuracy and performance of
                 the proposed scattering model are examined through
                 comparisons with a more detailed biologically based
                 model whose spectral readings have been verified
                 against experimental data for real specimens.",
  volume =       "17(8)",
  keywords =     "Biologically based rendering, Scattering model,
                 Natural phenomena. Biological imaging",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InProceedings{EVL-2001-42,
  year =         "2001",
  title =        "A Separate Least Squares Algorithm for Efficient
                 Arithmetic Coding in Lossless Image Compression",
  author =       "G. A. Triantafyllidis and M. G. Strintzis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-42",
  abstract =     "The overall performance of discrete wavelet transforms
                 for lossless image compression may be further improved
                 by properly designing efficient entropy coders. In this
                 paper a novel technique is proposed for the
                 implementation of context-based adaptive arithmetic
                 entropy coding. It is based on the prediction of the
                 value of the current transform coefficient. The
                 proposed algorithm employs a weighted least squares
                 method applied separately for the HH, HL and LH bands
                 of each level of the multiresolution structure, in
                 order to achieve appropriate context selection for
                 arithmetic coding. Experimental results illustrate and
                 evaluate the performance of the proposed technique for
                 lossless image compression.",
  editor =       "V. Skala",
  keywords =     "Least Squares, Arithmetic Coding, Context Selection,
                 Lossless Image Processing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InCollection{EVL-2001-420,
  pages =        "506--518",
  year =         "2001",
  title =        "Virtual spectrophotometric measurements for
                 biologically and physically based rendering",
  author =       "Gladimir V. G. Baranoski and Jon G. Rokne and Guangwu
                 Xu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-420",
  abstract =     "Virtual spectrophotometric measurements have important
                 applications in biologically and physically based
                 rendering. These measurements are used to evaluate
                 reflectance and transmittance models through
                 comparisons with actual spectrophotometric
                 measurements. Moreover, they are also used to generate
                 spectrophotometric data, which are dependent either on
                 the wavelength or on the illuminating geometry of the
                 incident radiation, from previously validated models.
                 In this paper the ray casting based formulation for
                 virtual spectrophotometers is discussed, and an
                 original ray density analysis is presented, which
                 increases the efficiency of these virtual devices.
                 Specifically, a mathematical bound based on probability
                 theory is proposed to determine the number of rays
                 needed to obtain asymptotically convergent readings in
                 the shortest possible computation time. Practical
                 experiments are provided which illustrate the validity
                 and usefulness of the proposed approach.",
  volume =       "17(8)",
  keywords =     "Spectral measurements, Ray density, Rendering",
  booktitle =    "The Visual Computer",
  publisher =    "Springer",
}

@InCollection{EVL-2001-421,
  pages =        "67--94",
  year =         "2001",
  title =        "Computer puppetry: An importance-based approach",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-421",
  author =       "Hyun Joon Shin and Jehee Le and Sung Yong Shin and
                 Michael Gleicher",
  abstract =     "Computer puppetry maps the movements of a performer to
                 an animated character in real-time. In this article, we
                 provide a comprehensive solution to the problem of
                 transferring the observations of the motion capture
                 sensors to an animated character whose size and
                 proportion may be different from the performer's. Our
                 goal is to map as many of the important aspects of the
                 motion to the target character as possible, while
                 meeting the online, real-time demands of computer
                 puppetry. We adopt a Kalman filter scheme that
                 addresses motion capture noise issues in this setting.
                 We provide the notion of dynamic importance of an
                 end-effector that allows us to determine what aspects
                 of the performance must be kept in the resulting
                 motion. We introduce a novel inverse kinematics solver
                 that realizes these important aspects within tight
                 real-time constraints. Our approach is demonstrated by
                 its application to broadcast television performances.",
  editor =       "Jessica Hodgins",
  keywords =     "Human-figure animation, motion retargetting,
                 performance-based animation, real-time animation",
  volume =       "20(2)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-422,
  pages =        "95--126",
  year =         "2001",
  title =        "Spherical averages and applications to spherical
                 splines and interpolation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-422",
  author =       "Samuel R. Buss and Jay P. Fillmore",
  abstract =     "This article introduces a method for computing
                 weighted averages on spheres based on least squares
                 minimization that respects spherical distance. We prove
                 existence and uniqueness properties of the weighted
                 averages, and give fast iterative algorithms with
                 linear and quadratic convergence rates. Our methods are
                 appropriate to problems involving averages of spherical
                 data in meteorological, geophysical, and astronomical
                 applications. One simple application is a method for
                 smooth averaging of quaternions, which generalizes
                 Shoemake's spherical linear interpolation.The weighted
                 averages methods allow a novel method of defining
                 B{\'{e}}zier and spline curves on spheres, which
                 provides direct generalization of B{\'{e}}zier and
                 B-spline curves to spherical spline curves. We present
                 a fast algorithm for spline interpolation on spheres.
                 Our spherical splines allow the use of arbitrary knot
                 positions; potential applications of spherical splines
                 include smooth quaternion curves for applications in
                 graphics, animation, robotics, and motion planning.",
  editor =       "Jessica Hodgins",
  keywords =     "B{\'{e}}zier curve, B-spline, barycentric coordinates,
                 least squares minimization, quaternion interpolation,
                 quaternions, spherical average, spherical
                 interpolation, spherical mean, spline curve, spline
                 interpolation",
  volume =       "20(2)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-423,
  pages =        "127--150",
  year =         "2001",
  title =        "Real-time texture synthesis by patch-based sampling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-423",
  author =       "Lin Liang and Ce Liu and Ying-Qing Xu and Baining Gu
                 and Heung-Yeung Shum",
  abstract =     "We present an algorithm for synthesizing textures from
                 an input sample. This patch-based sampling algorithm is
                 fast and it makes high-quality texture synthesis a
                 real-time process. For generating textures of the same
                 size and comparable quality, patch-based sampling is
                 orders of magnitude faster than existing algorithms.
                 The patch-based sampling algorithm works well for a
                 wide variety of textures ranging from regular to
                 stochastic. By sampling patches according to a
                 nonparametric estimation of the local conditional MRF
                 density function, we avoid mismatching features across
                 patch boundaries. We also experimented with documented
                 cases for which pixel-based nonparametric sampling
                 algorithms cease to be effective but our algorithm
                 continues to work well.",
  editor =       "Jessica Hodgins",
  keywords =     "Texture synthesis, patch-pasted nonparametric
                 sampling",
  volume =       "20(3)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-424,
  pages =        "151--168",
  year =         "2001",
  title =        "Collisions and perception",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-424",
  author =       "Carol O'Sullivan and John Dingliana",
  abstract =     "Level of Detail (LOD) techniques for real-time
                 rendering and related perceptual issues have received a
                 lot of attention in recent years. Researchers have also
                 begun to look at the issue of perceptually adaptive
                 techniques for plausible physical simulations. In this
                 article, we are particularly interested in the problem
                 of realistic collision simulation in scenes where large
                 numbers of objects are colliding and processing must
                 occur in real-time. An interruptible and therefore
                 degradable collision-handling mechanism is used and the
                 perceptual impact of this degradation is explored. We
                 look for ways in which we can optimize the realism of
                 such simulations and describe a series of
                 psychophysical experiments that investigate different
                 factors affecting collision perception, including
                 eccentricity, separation, distractors, causality, and
                 accuracy of physical response. Finally, strategies for
                 incorporating these factors into a perceptually
                 adaptive real-time simulation of large numbers of
                 visually similar objects are presented.",
  editor =       "Jessica Hodgins",
  keywords =     "Animation, collision handling, graphics and
                 perception, simulation levels of detail, time-critical
                 computing",
  volume =       "20(3)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-425,
  pages =        "169--201",
  year =         "2001",
  title =        "The virtual mesh: a geometric abstraction for
                 efficiently computing radiosity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-425",
  author =       "L. Alonso and F. Cuny and S. Petit Jean and J. C. Paul
                 and S. Lazard and E. Wies",
  abstract =     "In this article, we introduce a general-purpose method
                 for computing radiosity on scenes made of parametric
                 surfaces with arbitrary trimming curves. In contrast
                 with past approaches that require a tessellation of the
                 input surfaces (be it made up of triangles or patches
                 with simple trimming curves) or some form of geometric
                 approximation, our method takes full advantage of the
                 rich and compact mathematical representation of
                 objects. At its core lies the virtual mesh, an
                 abstraction of the input geometry that allows complex
                 shapes to be illuminated as if they were simple
                 primitives. The virtual mesh is a collection of
                 normalized square domains to which the input surfaces
                 are mapped while preserving their energy properties.
                 Radiosity values are then computed on these supports
                 before being lifted back to the original surfaces. To
                 demonstrate the power of our method, we describe a
                 high-order wavelet radiosity implementation that uses
                 the virtual mesh. Examples of objects and environments,
                 designed for interactive applications or virtual
                 reality, are presented. They prove that, by exactly
                 integrating curved surfaces in the resolution process,
                 the virtual mesh allows complex scenes to be rendered
                 more quickly, more accurately, and much more naturally
                 than with previously known methods.",
  editor =       "Jessica Hodgins",
  keywords =     "Curves and surfaces, illumination, radiosity, virtual
                 mesh, wavelets",
  volume =       "20(3)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-426,
  pages =        "203--231",
  year =         "2001",
  title =        "Controllable morphing of compatible planar
                 triangulations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-426",
  author =       "Vitaly Surazhsky and Craig Gotsman",
  abstract =     "Two planar triangulations with a correspondence
                 between the pair of vertex sets are compatible
                 (isomorphic) if they are topologically equivalent. This
                 work describes methods for morphing compatible planar
                 triangulations with identical convex boundaries in a
                 manner that guarantees compatibility throughout the
                 morph. These methods are based on a fundamental
                 representation of a planar triangulation as a matrix
                 that unambiguously describes the triangulation.
                 Morphing the triangulations corresponds to
                 interpolations between these matrices.We show that this
                 basic approach can be extended to obtain better control
                 over the morph, resulting in valid morphs with various
                 natural properties. Two schemes, which generate the
                 linear trajectory morph if it is valid, or a morph with
                 trajectories close to linear otherwise, are presented.
                 An efficient method for verification of validity of the
                 linear trajectory morph between two triangulations is
                 proposed. We also demonstrate how to obtain a morph
                 with a natural evolution of triangle areas and how to
                 find a smooth morph through a given intermediate
                 triangulation.",
  editor =       "Jessica Hodgins",
  keywords =     "Compatible triangulations, controllable Morphing,
                 isomorphic triangulations, linear Morph, local Control,
                 morphing, self-intersection elemination",
  volume =       "20(4)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InCollection{EVL-2001-427,
  pages =        "232--279",
  year =         "2001",
  title =        "On numerical solutions to one-dimensional integration
                 problems with applications to linear light sources",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-427",
  author =       "Marc J. Ouellette and Eugene Fium",
  abstract =     "Many key problems in computer graphics require the
                 computation of integrals. Due to the nature of the
                 integrand and of the domain of integration, these
                 integrals seldom can be computed analytically. As a
                 result, numerical techniques are used to find
                 approximate solutions to these problems. While the
                 numerical analysis literature offers many integration
                 techniques, the choice of which method to use for
                 specific computer graphic problems is a difficult one.
                 This choice must be driven by the numerical efficiency
                 of the method, and ultimately, by its visual impact on
                 the computed image. In this paper, we begin to address
                 these issues by methodically analyzing deterministic
                 and stochastic numerical techniques and their
                 application to the type of one-dimensional problems
                 that occur in computer graphics, especially in the
                 context of linear light source integration. In addition
                 to traditional methods such as Gauss-Legendre
                 quadratures, we also examine Voronoi diagram-based
                 sampling, jittered quadratures, random offset
                 quadratures, weighted Monte Carlo, and a newly
                 introduced method of compounding known as a difficulty
                 driven compound quadrature.We compare the effectiveness
                 of these methods using a three-pronged approach. First,
                 we compare the frequency domain characteristics of all
                 the methods using periodograms. Next, applying ideas
                 found in the numerical analysis literature, we examine
                 the numerical and visual performance profiles of these
                 methods for seven different one-parameter problem
                 families. We then present results from the application
                 of the methods for the example of linear light sources.
                 Finally, we summarize the relative effectiveness of the
                 methods surveyed, showing the potential power of
                 difficulty-driven compound quadratures.",
  editor =       "Jessica Hodgins",
  keywords =     "Difficulty-driven compound quadrature, Voronoi
                 sampling, integration, linear light sources, numerical
                 quadrature, performance profile, periodogram,
                 rendering, stochastic sampling",
  volume =       "20(4)",
  booktitle =    "ACM Transactions on Graphics",
  publisher =    "ACM Press",
}

@InProceedings{EVL-2001-428,
  pages =        "1--12",
  year =         "2001",
  title =        "Interactive Animation of Cloth-like Objects in Virtual
                 Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-428",
  author =       "Mark Meyer and Gilles Debunne and Mathieu Desbrun and
                 Alan H. Barr",
  abstract =     "Modeling and animation of cloth have experienced
                 important developments in recent years. As a
                 consequence, complex textile models can be used to
                 realistically drape objects or human characters in a
                 fairly efficient way. However, real-time realistic
                 simulation remains a major challenge, even if
                 applications are numerous, from rapid prototyping to
                 e-commerce. In this paper, we present a stable,
                 real-time algorithm for animating cloth-like materials.
                 Using a hybrid explicit/implicit algorithm, we perform
                 fast and stable time integration of a physically based
                 model with rapid collision detection and response, as
                 well as wind or liquid drag effects to enhance realism.
                 We demonstrate our approach through a series of
                 examples in virtual reality environments, proving that
                 real-time animation of cloth, even on low-end
                 computers, is now achievable.",
  editor =       "Nadia Magnenat-Thalmann and Bernd Eberhardt",
  keywords =     "Interactive animation, cloth modeling, deformable
                 bodies, virtual reality",
  volume =       "12(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-429,
  pages =        "13--22",
  year =         "2001",
  title =        "Realistic and Efficient Rendering of Free-form
                 Knitwear",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-429",
  author =       "Hua Zhong and Ying-Qing Xu and Baining Guo and
                 Heung-Yeung Shum",
  abstract =     "We present a method for rendering knitwear on
                 free-form surfaces. This method has three main
                 advantages. First, it renders yarn microstructure
                 realistically and efficiently. Second, the rendering
                 efficiency of yarn microstructure does not come at the
                 price of ignoring the interactions between the
                 neighboring yarn loops. Such interactions are modeled
                 in our system to further enhance realism. Finally, our
                 approach gives the user intuitive control on a few key
                 aspects of knitwear appearance: the fluffiness of the
                 yarn and the irregularity in the positioning of the
                 yarn loops. The result is a system that efficiently
                 produces highly realistic rendering of free-form
                 knitwear with user control on key aspects of visual
                 appearance.",
  editor =       "Nadia Magnenat-Thalmann and Bernd Eberhardt",
  keywords =     "Knitwear, textile modeling and rendering, parametric
                 surfaces, microstructure rendering, Recursive
                 Perturbation",
  volume =       "12(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-43,
  year =         "2001",
  title =        "Image Edge Detection in a Mimic Spiral Architecture",
  author =       "Qiang Wu and Tom Hintz and Xiangjian He",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-43",
  abstract =     "Detection of edge points of 3-dimensional physical
                 objects in a 2-dimensional image is one of the main
                 research areas of computer vision. Object contour
                 detection and object recognition rely heavily on edge
                 detection. In this paper, we present an edge detection
                 scheme using Gaussian Multi-resolution Theory based on
                 a mimic Spiral Architecture. The Spiral Architecture
                 has been described in many papers. Although it has many
                 advantages such as powerful computational features in
                 image processing especially in image edge detection,
                 there is no available image capture device yet to
                 support this structure. Hence, we mimic the Spiral
                 Architecture from the existing image structure. This
                 mimic structure inherits all computational features of
                 the Spiral Architecture. The Gaussian Multi-resolution
                 Theory is used to reduce noise and unnecessary details
                 of the image.",
  editor =       "V. Skala",
  keywords =     "Edge Detection, Computer Vision, Spiral Architecture,
                 Image Processing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-430,
  pages =        "23--29",
  year =         "2001",
  title =        "A Match Moving Technique for Merging {CG} Cloth and
                 Human Movie Sequences",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-430",
  author =       "Jun'ichi Hoshino and Masanobu Yamamoto and Hirofumi
                 Saito",
  abstract =     "Merging computer-generated objects onto a human video
                 sequence is an important technique for many
                 applications such as augmented reality and special
                 effects in movies. In this paper, we propose a new
                 method for merging virtual objects onto the human video
                 sequence. First, we track the current 3D pose of the
                 human body by using spatio-temporal analysis. Then we
                 generate CG objects and merge them with the human body
                 in video. In this paper, we demonstrate examples of
                 merging virtual cloth with the video-captured images.",
  editor =       "Nadia Magnenat-Thalmann and Bernd Eberhardt",
  keywords =     "Cloth simulation, human body tracking, match moving,
                 augmented reality",
  volume =       "12(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-431,
  pages =        "31--44",
  year =         "2001",
  title =        "A Computer Vision Approach for Textile Quality
                 Control",
  author =       "C. Anagnostopoulos and D. Vergados and E. Kayafas and
                 V. Loumos and G. Stassinopoulos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-431",
  abstract =     "Txtile manufacturers have to monitor the quality of
                 their products in order to maintain the high-quality
                 standards established for the clothing industry. Thus,
                 textile quality control is a key factor for the
                 increase of competitiveness of their companies. Textile
                 faults have traditionally been detected by human visual
                 inspection. However, human inspection is time consuming
                 and does not achieve a high level of accuracy.
                 Therefore, industrial vision units are of strategic
                 interest for the textile industry as they could form
                 the basis of a system achieving a high degree of
                 accuracy on textile inspection. This work describes the
                 software core of a system designed for fabric
                 inspection on the basis of simple image-processing
                 operations as well as its efficiency on detection of
                 usual textile defects. The prerequisites of the overall
                 system are then discussed analytically, as well as the
                 limitations and the restrictions imposed due to the
                 nature of the problem. The software algorithm and the
                 evaluation of the first results are also presented in
                 details.",
  editor =       "Nadia Magnenat-Thalmann and Bernd Eberhardt",
  volume =       "12(1)",
  keywords =     "Textile, quality control, texture segmentation,
                 computer vision, image processing, fabric inspection
                 system",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-432,
  pages =        "45--53",
  year =         "2001",
  title =        "Integrating Deformations between Bodies and Clothes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-432",
  author =       "Fr{\'{e}}d{\'{e}}ric Cordie and Pascal Volino and
                 Nadia Magnenat-Thalmann",
  abstract =     "In this paper, we present a framework for both skin
                 and cloth deformation. Traditionally, skin and cloth
                 deformations are managed by a non-unified strategy
                 although the skin interacts on clothes and vice versa.
                 In general, the skin is considered as a rigid surface
                 on which the clothes have no effect. We propose in this
                 paper a unified approach for cloth and body
                 deformation. Our system is able to handle both skin and
                 cloth deformation with collision response. These
                 deformations are managed with a particle system. The
                 skin deformation model is a hybrid model, where
                 deformation due to the skeleton motion is controlled
                 geometrically, and where the collision response is
                 handled by a physical-based model. In case of collision
                 among skin and clothes, a reaction and friction forces
                 are applied on both skin and cloth to avoid
                 penetration.",
  editor =       "Nadia Magnenat-Thalmann and Bernd Eberhardt",
  keywords =     "Body deformation, cloth deformation, collision
                 response, collision detection, interaction, virtual
                 actor",
  volume =       "12(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-433,
  pages =        "55--66",
  year =         "2001",
  title =        "Motion Visualization of Human Left Ventricle with a
                 Time-varying Deformable Model for Cardiac Diagnosis",
  author =       "Soo-Mi Choi and Myoung-Hee Kim",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-433",
  abstract =     "We present a time-varying deformable model to
                 visualize and analyze the motion of the left ventricle
                 from a time series of 3-D images. The model is composed
                 of a non-rigid body that deforms around a reference
                 shape obtained from the previous time step. At each
                 time step, the position and orientation of the left
                 ventricle are extracted from the feature points of
                 images. This information gives the position and
                 orientation of the coordinate system attached to the
                 non-rigid body. To compute a dense non-rigid motion
                 field over the entire endocardial wall of the left
                 ventricle, we introduce a 3-D blob finite element and
                 Galerkin interpolants based on 3-D Gaussian, and use a
                 physically based finite element method and a modal
                 analysis. Then, cinematic attributes are visualized in
                 pseudo colors on the reconstructed surface in order to
                 help medical doctors in their interpretation of the
                 data. Using the presented model, we estimate clinically
                 useful quantitative parameters such as regional wall
                 motion and ejection fraction. Experimental results are
                 shown in a time series of X-ray angiographic images.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  volume =       "12(2)",
  keywords =     "Motion visualization, finite element method,
                 time-varying deformable model, left ventricle",
  booktitle =    "The Journal of Visualization and Computer Animation",
}

@InProceedings{EVL-2001-434,
  pages =        "67--79",
  year =         "2001",
  title =        "Performance-driven Muscle-based Facial Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-434",
  author =       "Byoungwon Choe and Hanook Lee and Hyeong-Seok Ko",
  abstract =     "We describe a system to synthesize facial expressions
                 by editing captured performances. For this purpose, we
                 use the actuation of expression muscles to control
                 facial expressions. We note that there have been
                 numerous algorithms already developed for editing gross
                 body motion. While the joint angle has direct effect on
                 the configuration of the gross body, the muscle
                 actuation has to go through a complicated mechanism to
                 produce facial expressions. Therefore,we devote a
                 significant part of this paper to establishing the
                 relationship between muscle actuation and facial
                 surface deformation. We model the skin surface using
                 the finite element method to simulate the deformation
                 caused by expression muscles. Then, we implement the
                 inverse relationship, muscle actuation parameter
                 estimation, to find the muscle actuation values from
                 the trajectories of the markers on the performer's
                 face. Once the forward and inverse relationships are
                 established, retargeting or editing a performance
                 becomes an easy job. We apply the original performance
                 data to different facial models with equivalent muscle
                 structures, to produce similar expressions. We also
                 produce novel expressions by deforming the original
                 data curves of muscle actuation to satisfy the
                 key-frame constraints imposed by animators.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  keywords =     "Performance-driven facial animation, facial expression
                 capture, physically based facial modeling, muscle-based
                 facial model, facial expression editing",
  volume =       "12(2)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-435,
  pages =        "81--91",
  year =         "2001",
  title =        "Reproducing Works of Calder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-435",
  author =       "Dongkyoo Lee and Hee-Jung Bae and Chang Tae Kim and
                 Dong-Chun Lee and Dae-Hyun Jung and Nam-Kyung Lee and
                 Kyoo-Ho Lee and Nakhoon Baek and J. Won Lee and Kwan
                 Woo Ryu and James K. Hahn",
  abstract =     "Many fine art pieces have been reproduced in digital
                 form. The digital reproductions have been used to store
                 and transmit the original work. In contrast, mobiles,
                 or moving sculptures, such as those designed by
                 Alexander Calder cannot be reproduced realistically by
                 photographs and/or static images. The real
                 characteristics of mobiles come from the motions
                 generated by interactive external forces applied to
                 their structures. Hence people could not fully enjoy
                 them through static images or even static
                 three-dimensional models. We present a virtual mobile
                 system where users can easily control the mobile and
                 can feel the impressions that the artist originally
                 intended to provide. Virtual winds are generated by
                 blowing on a microphone which then exert external
                 forces to the mobile. This microphone interface lets
                 users control the mobile while they are watching it
                 through a monitor. We introduce a linear time solution
                 for the constraint dynamics and an improved impulse
                 dynamics to speed up the simulation. Using these
                 techniques, we achieve a real-time simulation of the
                 mobile on personal computers. The techniques presented
                 can easily be extended to simulate other interactive
                 dynamics systems.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  keywords =     "Virtual mobile, constraint dynamics, virtual wind,
                 impulse dynamics",
  volume =       "12(2)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-436,
  pages =        "93--106",
  year =         "2001",
  title =        "Animating Radiosity Environments through the
                 Multi-Frame Lighting Method",
  author =       "Gonzalo Besuievsky and Xavier Pueyo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-436",
  abstract =     "Realistic rendering animation is known to be an
                 expensive processing task when physically based global
                 illumination methods are used in order to improve
                 illumination details. This paper presents the
                 Multi-Frame Lighting Method, an efficient algorithm to
                 compute animations in radiosity environments. The
                 method, based on global Monte Carlo techniques,
                 performs the lighting simulation of groups of
                 consecutive frames in a single process. All frames
                 computed have the same accuracy as if they were
                 computed independently while a significant high
                 speed-up is achieved. Results show that the method it
                 is an interesting alternative for computing
                 non-interactive radiosity animations for moderately
                 complex scenarios.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  keywords =     "Realistic Animation, Monte Carlo, Radiosity",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-437,
  pages =        "107--115",
  year =         "2001",
  title =        "A 3{D} Parametric Tongue Model for Animated Speech",
  author =       "Scott A. King and Richard E. Parent",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-437",
  abstract =     "We present a tongue model for use in computer-animated
                 speech. The model is capable of representing tongue
                 shapes during the production of English vowels and
                 consonants as well as general motion of the tongue.
                 Geometrically, the model is composed of a B-spline
                 surface with 60 control points and an 813 grid of
                 bi-cubic patches. We also present a parameterization of
                 the model that requires only six parameters for use
                 during speech production. This parameterization can
                 also be used to define the deformation of other tongue
                 models.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  volume =       "12(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-438,
  pages =        "117--129",
  year =         "2001",
  title =        "A Grasp-based Motion Planning Algorithm for Character
                 Animation",
  author =       "Maciej Kalisiak and Michiel van de Panne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-438",
  abstract =     "The design of autonomous characters capable of
                 planning their own motions continues to be a challenge
                 for computer animation. We present a novel kinematic
                 motion-planning algorithm for character animation which
                 addresses some of the outstanding problems. The problem
                 domain for our algorithm is as follows: given a
                 constrained environment with designated handholds and
                 footholds, plan a motion through this space towards
                 some desired goal. Our algorithm is based on a
                 stochastic search procedure which is guided by a
                 combination of geometric constraints, posture
                 heuristics, and distance-to-goal metrics. The method
                 provides a single framework for the use of multiple
                 modes of locomotion in planning motions through these
                 constrained, unstructured environments. We illustrate
                 our results with demonstrations of a human character
                 using walking, swinging, climbing, and crawling in
                 order to navigate through various obstacle courses.",
  volume =       "12(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-439,
  pages =        "131--144",
  year =         "2001",
  title =        "Using Cartesian Product for Animation",
  author =       "X. Skapi and P. Lienhardt",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-439",
  abstract =     "In the field of geometric modelling for animation, 4D
                 modelling (time being the fourth dimension) seems to be
                 a natural extension of 3D modelling. But time dimension
                 is not easy to apprehend and 4D objects are difficult
                 to interpret and to control in general. We study the
                 application of space-time Cartesian product to
                 construct 4D space-time objects; Cartesian product is
                 applied to space-time objects for which topological
                 dimension is lesser than 4, and which are easy to
                 interpret as animations. We propose here an
                 interpretation of such objects, and we show how 4D
                 space-time objects, resulting from Cartesian product,
                 can be interpreted (and therefore controlled) according
                 to the operands of Cartesian product.",
  editor =       "Editorial Nadia Magnenat-Thalmann and Daniel
                 Thalmann",
  volume =       "12(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-44,
  year =         "2001",
  title =        "Automatic and Continuous Projector Display Surface
                 Calibration Using Every-Day Imagery",
  author =       "Ruigang Yang and Greg Welch",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-44",
  abstract =     "Projector-based display systems have been used in
                 computer graphics for about as long as the field has
                 existed. While projector-based systems have many
                 advantages, a significant disadvantage is the need to
                 obtain and then adhere to an accurate analytical model
                 of the mechanical setup, including the external
                 parameters of the projectors, and an estimate of the
                 display surface geometry. We introduce a new method for
                 the latter - for continuous display surface
                 autocalibration. Using a camera that observes the
                 display surface, we match image features in whatever
                 imagery is being projected, with the corresponding
                 features that appear on the display surface, to
                 continually refine an estimate for the display surface
                 geometry. In effect we enjoy the high signal-to-noise
                 ratio of {"}structured{"} light (without getting to
                 choose the structure) and the unobtrusive nature of
                 passive correlation-based methods. The approach is
                 robust and accurate, and can be realized with
                 commercial off-the-shelf components. The method can be
                 used with a variety of projector-based displays, for
                 scientific visualization, trade shows, entertainment,
                 tele-immersion, or the Office of the Future. And
                 although we do not demonstrate it in this paper, we
                 have also been working on extending the method to
                 include continual estimation of other system parameters
                 that vary over time.",
  editor =       "V. Skala",
  keywords =     "Computer Vision, Image Processing, Shape
                 Recognition.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-440,
  pages =        "159--166",
  year =         "2001",
  title =        "Mobility Culling: An Efficient Rendering Algorithm
                 Using Temporal Coherence",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-440",
  author =       "Kyoung-Su Oh and Byeong-Seok Shin and Yeong Gil Shin",
  abstract =     "Interactive display of complex scenes is a challenging
                 problem in computer graphics. Such current approaches
                 as z-buffer, level of detail and visibility culling
                 have not fully used the temporal coherence between
                 consecutive frames. When the viewing condition is
                 fixed, the color and depth values of static polygons
                 can be obtained from the result of the previous frame
                 and only the remaining dynamic polygons require
                 rendering. We present a method that enhances the speed
                 of the conventional z-buffer algorithm by exploiting
                 the above temporal coherence. This algorithm is simple
                 to combine with existing graphics hardware that
                 supports the conventional z-buffer algorithm. It can
                 manipulate any scene suitable for the z-buffer
                 algorithm without preprocessing or human intervention.
                 The rendering time is proportional to the number of
                 dynamic polygons in each frame. Experimental results
                 show that our method is faster than the conventional
                 z-buffer algorithm and the performance enhancement
                 becomes higher as the fraction of static polygons
                 increases.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  keywords =     "Z-buffer, real-time rendering, culling, temporal
                 coherence",
  volume =       "12(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-442,
  pages =        "167--180",
  year =         "2001",
  title =        "Visualization Techniques for 3{D} Vector Fields: An
                 Application to Electrostatic Fields of Molecules",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-442",
  author =       "Susumu Handa and Hiroshi Kashiwagi and Toshikazu
                 Takada",
  abstract =     "In chemistry, computer graphics is now well
                 established as a tool to interpret simulation results,
                 since molecules are complicated in their structures and
                 mutual interactions. As a probe to study such molecular
                 interactions, electrostatic fields are considered to be
                 useful. However, since they are given as 3D vector
                 fields having cusps in the fields, conventional drawing
                 techniques are not applicable. In this article, two new
                 approaches are presented to visualize the electrostatic
                 fields of molecules. One is an extension of topological
                 skeletons, by which interactions between atoms having
                 opposite charges are expressed, which is not shown with
                 the conventional methods. The other is to define new
                 functions called selective functions to select regions
                 of interest only from the geometrical features of the
                 fields. Furthermore, from the definition of the new
                 functions, mathematical relations between the
                 topological skeletons and selective functions are
                 discussed. An example is presented in applications to
                 chemical reactions to show how the scheme is used.",
  editor =       "Nadia Magnenat-Thalmann and Daniel Thalmann",
  keywords =     "3D vector field, selective visualization, topological
                 skeleton, critical point, molecular graphics, stream
                 function",
  volume =       "12(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-443,
  pages =        "181--189",
  year =         "2001",
  title =        "Function-based Flow Modeling and Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-443",
  author =       "Ergun Akleman and Zeki Melek and Jeff S. Haberl",
  abstract =     "This paper summarizes a function-based approach to
                 model and animate 2D and 3D flows. We use periodic
                 functions to create cyclical animations that represent
                 2D and 3D flows. These periodic functions are
                 constructed with an extremely simple algorithm from a
                 set of oriented lines. The speed and orientation of the
                 flow are described directly by the orientation and the
                 lengths of these oriented lines. The resulting cyclical
                 animations are then obtained by sampling the
                 constructed periodic functions. Our approach is
                 independent of dimension, i.e. for 2D and 3D flow the
                 same types of periodic functions are used. Rendering
                 images for 2D and 3D flows is slightly different. In 2D
                 function values directly are mapped to color values. On
                 the other hand, in 3D function values are first mapped
                 to color and opacity and then the volume is rendered by
                 our volume renderer. Modeled and animated flows are
                 used to improve the visualization of operations of
                 rolling piston and rotary vane compressors.",
  editor =       "Isaac Rudomin",
  keywords =     "Visualization, image synthesis, computer animation,
                 flow Modeling and flow animation",
  volume =       "12(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-444,
  pages =        "191--201",
  year =         "2001",
  title =        "Using Particles for 3{D} Texture Sculpting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-444",
  author =       "Bedrich Benes and Enrique Espinosa",
  abstract =     "Particle systems have been used in computer graphics
                 for many different purposes, including visual
                 simulation of fur, grass, hair, and similar fuzzy
                 textures and shapes. The underlying theories used in
                 these algorithms are usually quite complex and are
                 mostly based on simulation of diffuse-limited
                 aggregation, cellular development, reaction-diffusion
                 models, etc. This leads to high time complexity of
                 these algorithms. The purpose of this paper is to show
                 that collision detection and distance keeping among
                 moving particles can generate similar realistic
                 textures efficiently. This approach is easy to
                 implement, sufficiently fast allowing for interactive
                 modeling, and inherits the major features from the
                 previously published techniques. We first construct a
                 scene consisting of generators of particles,
                 attractors, and cutters. The generators generate
                 oriented particles, and the attractors attract or
                 repulse them. When collision with the cutter is
                 detected, the particle performs an action according to
                 its state and position in the 3D space. Every particle
                 has assigned a table of possible actions that is used
                 for solving these critical states. Trajectories of the
                 particles are then used as a resulting shape of the
                 texture.",
  editor =       "Isaac Rudomin",
  keywords =     "Computer graphics, realistic image synthesis, texture,
                 particle systems, procedural modeling",
  volume =       "12(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-445,
  pages =        "203--214",
  year =         "2001",
  title =        "The Caracol Time Travel Project",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-445",
  author =       "Charles E. Hughes and J. Michael Moshell and Dean Reed
                 and Diane Z. Chase and Arlen F. Chase",
  abstract =     "Virtual drama is based on the use of a shareable
                 virtual world as a stage setting, with avatars
                 controlled by actors and audience members. The Caracol
                 Time Travel Project was an experiment in the use of
                 virtual drama for learning about archaeology. Eighteen
                 undergraduate students at the University of Central
                 Florida used a locally developed Java-based system for
                 sharing VRML worlds. They designed and constructed a
                 virtual drama to teach basic concepts of Mesoamerican
                 archaeology and the cultural history of the ancient
                 Maya for middle schools. This paper presents their
                 story design and details of the system we developed to
                 support interaction in this shared virtual world. We
                 then discuss performance issues, lessons learned and
                 newer features that we did not have available at the
                 time.",
  editor =       "Isaac Rudomin",
  keywords =     "Virtual environments, virtual drama, VRML, situated
                 cognition, level-of-detail, streaming media",
  volume =       "12(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-446,
  pages =        "215--226",
  year =         "2001",
  title =        "Multilayer Garments Using Isosurfaces and Physics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-446",
  author =       "Isaac Rudom{\'{i}}n and Roberto P{\'{e}}rez-Urbiola
                 and Maria Elena Mel{\'{o}}n and Jose Luis Castillo",
  abstract =     "This article extends our previously published
                 technique that allows several layers of clothing to be
                 placed and animated over an articulated character by
                 using implicit ellipsoidal muscles to generate a scalar
                 field and placing each layer of clothing on a separate
                 isosurface, thereby avoiding the use of any collision
                 detection calculations between the different layers.
                 The method is first extended to a class of characters
                 that include a surface skin generated by a commercial
                 animation package. A second extension combines
                 isosurface placement with physically based methods.
                 These extensions afford increased realism while
                 retaining the performance obtained by using
                 isosurfaces.",
  editor =       "Isaac Rudomin",
  keywords =     "Cloth modelling, implicit modelling",
  volume =       "12(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-447,
  pages =        "227--240",
  year =         "2001",
  title =        "Rapid Modeling of Animated Faces from Video",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-447",
  author =       "Zicheng Liu and Zhengyou Zhang and Chuck Jacobs and
                 Michael Cohen",
  abstract =     "Generating realistic 3D human face models and facial
                 animations has been a persistent challenge in computer
                 graphics. We have developed a system that constructs
                 textured 3Dface models from videos with minimal user
                 interaction. Our system takes images andvideo sequences
                 of a face with an ordinary video camera. After five
                 manual clicks ontwo images to tell the system where the
                 eye corners, nose top and mouth corners are, thesystem
                 automatically generates a realistic looking 3D human
                 head model and the constructed model can be animated
                 immediately. A user with a PC and an ordinary camera
                 can use our system to generate his/her face model in a
                 few minutes.",
  editor =       "Isaac Rudomin",
  keywords =     "Geometric modelling, image based modelling and
                 rendering, face modeling, face animation",
  volume =       "12(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-448,
  pages =        "241--252",
  year =         "2001",
  title =        "Advanced Techniques for Interactive Visualization of
                 Multi-resolution Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-448",
  author =       "Markus Grabner",
  abstract =     "This paper addresses the problem of interactive
                 visualization of multi-resolution triangle meshes.
                 Visible switching between different levels of detail is
                 avoided by smoothly interpolating mesh geometry between
                 different levels. The interpolation parameter is
                 derived from the screen-space geometric error of the
                 affected mesh region instead of assigning a fixed time
                 to the transition. The shortcomings of straightforward
                 frame rate control mechanisms (i.e., overshooting and
                 oscillation) are avoided by a semi-predictive
                 algorithm. The average rendering time per triangle is
                 measured and used to determine the desired number of
                 faces. A set of experiments demonstrates the advantages
                 of both methods.",
  editor =       "ERoman Durikovic and Andrej Ferko",
  keywords =     "Interactive visualization, geomorphing, frame rate
                 control, view-dependent simplification",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-449,
  pages =        "253--262",
  year =         "2001",
  title =        "Projected Slabs: Approximation of Perspective
                 Projection and Error Analysis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-449",
  author =       "A. Vilanova Bartrol{\'{i}} and R. Wegenkittl and E.
                 Gr{\"{o}}ller",
  abstract =     "Virtual endoscopy is a promising medical application
                 for volume-rendering techniques where perspective
                 projection is mandatory. Most of the acceleration
                 techniques for direct volume rendering use parallel
                 projection. This paper presents an algorithm to
                 approximate perspective volume rendering using parallel
                 projected slabs. The introduced error due to the
                 approximation is investigated. An analytical study of
                 the maximum and average error is made. This method is
                 applied to VolumePro 500. Based on the error analysis,
                 the basic algorithm is improved. This improvement
                 increases the frame rate, keeping the global maximum
                 error bounded. The usability of the algorithm is shown
                 through the virtual endoscopic investigation of various
                 types of medical data sets.",
  editor =       "Roman Durikovic and Andrej Ferko",
  keywords =     "Direct volume rendering, virtual endoscopy.
                 perspective projection, volumePro technology",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-45,
  year =         "2001",
  title =        "Anti-Aliased Hemicubes for Performance Improvement in
                 Radiosity Solutions",
  author =       "Naga Kiran and S. P. Mudur and Sharat Chandran and
                 Nilesh Dalvi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-45",
  abstract =     "Several important and fascinating aspects of realistic
                 images are captured by the radiosity method. In this
                 paper we use an alternate form of the classical
                 hemicube that reduces aliasing problems inherent in the
                 original method without giving up the computational
                 advantages of the hemicube. Unlike other methods, we
                 explicitly consider the effect of the relative order of
                 partial visibility in a hemicube cell when recording
                 form factors. This enables us to compute form factors
                 accurately (even) in progressive refinement radiosity
                 with adaptive substructuring. Our empirical results
                 with progressive refinement radiosity show superior
                 mesh density where fine details are required, (such as
                 in soft shadows), as well as in areas that produce
                 singularities (such as when inter element distances
                 tend to zero). Our method is contrasted with
                 hierarchical radiosity which uses raycasting for form
                 factor and visibility computations. On a low end Intel
                 Linux platform, and for a comparable image quality, our
                 method takes substantially less time.",
  editor =       "V. Skala",
  keywords =     "Anti-aliasing, visibility order, adaptive
                 substructuring.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-450,
  pages =        "263--276",
  year =         "2001",
  title =        "High-quality Texture Reconstruction from Multiple
                 Views",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-450",
  author =       "Alexander Bornik and Konrad Karner and Joachim Bauer
                 and Franz Leberl and Heinz Mayer",
  abstract =     "This paper presents a method to automatically
                 calculate texture maps for a given three-dimensional
                 object out of a sequence of images. It is used in our
                 image-based modeling approach after the registration of
                 the images and the geometric modeling is done. We show
                 that the presented method uses the information from all
                 images by implicitly applying a weighting function.
                 Using this approach the consideration of modeled
                 occlusions as well as the detection and removal of
                 non-modeled occlusions is accomplished. The final
                 resolution of the texture maps can be adjusted on a
                 pixel/cm basis.",
  editor =       "Roman Durikovic and Andrej Ferko",
  keywords =     "Texture reconstruction, multiresolution, texture
                 mapping, occlusion detection, photorealistic
                 rendering",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-451,
  pages =        "277--286",
  year =         "2001",
  title =        "Exploiting Coherence in Hierarchical Visibility
                 Algorithms",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-451",
  author =       "Jir{\'{i}} Bittner and Vlastimil Havran",
  abstract =     "We present a series of simple improvements that make
                 use of temporal and spatial coherence in the scope of
                 hierarchical visibility algorithms. The hierarchy
                 updating avoids visibility tests of certain interior
                 nodes of the hierarchy. The visibility propagation
                 algorithm reuses information about visibility of
                 neighbouring spatial regions. Finally, the conservative
                 hierarchy updating avoids visibility tests of the
                 hierarchy nodes that are expected to remain visible. We
                 evaluate the presented methods in the context of
                 hierarchical visibility culling using occlusion
                 trees.",
  editor =       "Roman Durikovic and Andrej Ferko",
  keywords =     "Visibility culling, coherence, 4D-tree",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-452,
  pages =        "287--295",
  year =         "2001",
  title =        "Growth Animation of Human Organs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-452",
  author =       "Roman Durikovic and Silvester Czanner and Hirofumi
                 Inoue",
  abstract =     "The growth of the organs of the human embryo changes
                 significantly over a short period of time in the
                 mother's body. The shape of the human organs is organic
                 and has many folds that are difficult to model or
                 animate using conventional techniques. Convolution
                 surface and function representation are a good choice
                 in modelling such organs as human embryo stomach and
                 brain. Two approaches are proposed for animating organ
                 growth: the first uses a simple line segment skeleton
                 demonstrated on a stomach model and the other method
                 uses a tubular skeleton calculated automatically from a
                 2D object outline. Growth speed varies with the
                 position within the organ and thus the model is divided
                 into multiple geometric primitives that are later glued
                 by a blending operation. Animation of both the embryo
                 stomach and brain is shown.",
  editor =       "Roman Durikovic and Andrej Ferko",
  keywords =     "Geometric modelling, skeleton, blending operation,
                 convolution surface",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-453,
  pages =        "297--310",
  year =         "2001",
  title =        "Constructive Texturing Based on Hypervolume Modeling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-453",
  author =       "B. Schmitt and A. Pasko and V. Adzhiev and C.
                 Schlick",
  abstract =     "The concept of solid texturing is extended in two
                 directions: constructive modeling of space partitions
                 for texturing and modeling of multidimensional textured
                 objects called hypervolumes. A hypervolume is
                 considered as a point set with attributes of both
                 physical (density, temperature, etc.) and photometric
                 (color, transparency, diffuse and specular reflections,
                 etc.) nature. The point set geometry and attributes are
                 modeled independently using real-valued scalar
                 functions of several variables. Each real-valued
                 function defining geometry or an attribute is evaluated
                 at the given point by a procedure traversing a
                 constructive tree structure with primitives in the
                 leaves and operations in the nodes of the tree. This
                 approach provides a framework for modeling, texturing
                 and visualization of 3D solids, time-dependent and
                 multidimensional objects in a completely uniform
                 manner. We introduced a special modeling language and
                 implemented software tools supporting the proposed
                 approach. The concept of constructive hypervolume
                 textures is independent of the geometry representation.
                 We provide examples of textured Frep and BRep objects
                 as illustrations.",
  editor =       "Roman Durikovic and Andrej Ferko",
  keywords =     "Multidimensional point sets, Function Representation
                 (FRep), volume modeling, hypervolume model,
                 constructive hypervolume texture, attributes and solid
                 texturing",
  volume =       "12(5)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-2001-454,
  pages =        "5--12",
  year =         "2001",
  title =        "Multiple Conceptions of Character-Based Interactive
                 Installations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-454",
  author =       "Bruce Blumberg and Bill Tomlinson and Marc Downie",
  abstract =     "This paper presents ways of approaching the design of
                 successful character-based interactive installations.
                 We rationalize our arguments within the context of both
                 Disney's &ldquo;illusion of life&rdquo; and Daniel
                 Dennett's &ldquo;intentional stance&rdquo;. We present
                 six perspectives from which intentional characters can
                 be viewed: as interactors on a variety of time scales;
                 as reciprocal interactors with each other, as entities
                 exhibiting a dynamic expressive range, as creatures
                 with life cycles; as a combination of allusions to
                 existing media, and as a collection of well-balanced
                 components. By conceptualizing characters in these
                 ways, creators can generate installations that enable
                 participants to read the desires, beliefs, and actions
                 of the characters. This approach forms the basis of a
                 successful character-based interactive installation.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-455,
  pages =        "15--22",
  year =         "2001",
  title =        "An Inverse Kinematics Method Based on Muscle
                 Dynamics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-455",
  author =       "Taku Komura and Yoshihisa Shinagawa and Tosiyasu L.
                 Kunii",
  abstract =     "Inverse kinematics is one of the most popular method
                 in computer graphics to control 3D multi-joint
                 characters. In this paper, we propose an inverse
                 kinematics algorithm that takes the characteristics of
                 human bodies into account. The mausculoskeletal model
                 is used to solve the redundancy of the human body.
                 Using our method, feasible human body motion can be
                 obtained simply by specifying the motion of several end
                 effectors or body segments. Since muscle dynamics is
                 taken into account, the configuration space of the
                 human body is automatically calculated, and unrealistic
                 postures can be avoided. It is also possible to tune
                 the motion by changing the external load applied to the
                 muscles. Using our method, the amount of work by the
                 animators is reduced to create natural human
                 animation.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-456,
  pages =        "23--30",
  year =         "2001",
  title =        "3{D} Animated Movie Actor Training Using Fuzzy Logic",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-456",
  author =       "Savant Karunaratne and Hong Yan",
  abstract =     "Computer animation has come a long way during the last
                 decade and is now capable of producing near-realistic
                 rendered 3D computer graphics models of expressive,
                 talking, acting humanoids and other characters
                 inhabiting virtual worlds. However, the component of
                 work that needs to be done by animators and artists in
                 producing these synthetic character performances is
                 quite significant. In this paper, we present an expert
                 system based on fuzzy knowledge bases that helps in
                 moving towards automating the task of animating virtual
                 human heads and faces. Our Virtual Actor (Vactor)
                 framework is based on several subsystems that use
                 mainly fuzzy and some non-fuzzy rules to teach virtual
                 actors to know the emotions and gestures to use in
                 different situations. Theories of emotion, personality,
                 dialogue, and acting, as well as empirical evidence are
                 incorporated into our framework and knowledge bases to
                 produce convincing results.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-457,
  pages =        "31--37",
  year =         "2001",
  title =        "Towards Real Time Virtual Human Life Simulation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-457",
  author =       "Etienne de Sevin and Marcelo Kallmann and Daniel
                 Thalmann",
  abstract =     "We describe an approach to construct interactive
                 virtual environments, which are suitable for the
                 development of artificial virtual human life
                 simulations. Our main goal is to have virtual human
                 actors living and working autonomously in virtual
                 environments. In our approach, virtual actors have
                 their own motivations and needs, and by sensing and
                 exploring their environment, an action selection
                 mechanism is able to determine at anytime the suitable
                 actions to take. We adapt basic actor motivations and
                 needs to urban situations, where most actions involve
                 interactions with the environment. Thus, a specific
                 technique to define actor-object interactions is used,
                 where pre-defined interaction plans are put inside
                 interactive objects, and just selected during the
                 simulation. We explain the steps taken in order to
                 construct and animate such environments, and we also
                 present a test simulation example.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-458,
  pages =        "38--44",
  year =         "2001",
  title =        "Principal Components of Expressive Speech Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-458",
  author =       "Sumedha Kshirsagar and Tom Molet and Nadia
                 Magnenat-Thalmann",
  abstract =     "We describe a new technique for expressive and
                 realistic speech animation. We use an optical tracking
                 system that extracts the 3D positions of markers
                 attached at the feature point locations to capture the
                 movements of the face of a talking person. We use the
                 feature points as defined by the MPEG-4 standard. We
                 then form a vector space representation by using the
                 principal component analysis of this data. We call this
                 space &ldquo;expression and viseme space&rdquo;. Such a
                 representation not only offers insight into improving
                 realism of animated faces, but also gives a new way of
                 generating convincing speech animation and blending
                 between several expressions. As the rigid body
                 movements and deformation constraints on the facial
                 movements have been considered through this analysis,
                 the resulting facial animation is very realistic.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-459,
  pages =        "47--54",
  year =         "2001",
  title =        "Paint By Relaxation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-459",
  author =       "Aaron Hertzmann",
  abstract =     "We use relaxation to produce painted imagery from
                 images and video. An energy function is first
                 specified; we then search for a painting with minimal
                 energy. The appeal of this strategy is that, ideally,
                 we need only specify what we want, not how to directly
                 compute it. Because the energy function is very
                 difficult to optimize, we use a relaxation algorithm
                 combined with search heuristics. This formulation
                 allows us to specify painting style by varying the
                 relative weights of energy terms. The basic energy
                 function yields an economical style that conveys an
                 image with few strokes. This style produces greater
                 temporal coherence for video than previous techniques.
                 The system allows as fine user control as desired: the
                 user may interactively change the painting style,
                 specify variations of style over an image, and/or add
                 specific strokes to the painting.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-46,
  year =         "2001",
  title =        "Parallel Implementation of Stochastic Iteration
                 Algorithms",
  author =       "Roel Martinez and Laselo Szirmay-Kalos and Mateu Sbert
                 and Ali Mohamed Abbas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-46",
  abstract =     "The paper examines the parallel implementation of
                 iteration type global illumination algorithms. The
                 steps of iteration depend on each other, thus their
                 parallel implementation is not as straight forward as
                 for random walks. This paper solves the interdependency
                 problem by applying stochastic iteration. In this
                 framework two two fundamental questions are
                 investigated: how many processors can be efficiently
                 used in an algorithms and how often the processors
                 should exchange their information. These questions are
                 answered by a theoretical model and also by
                 simulations.",
  editor =       "V. Skala",
  keywords =     "Global illumination, parallel computing, Monte-Carlo
                 method, radiosity.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-460,
  pages =        "55--61",
  year =         "2001",
  title =        "Affine-Invariant Sketch-Based Retrieval of Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-460",
  author =       "Horace Ho-Shing Ip and Angus K. Y. Cheng and William
                 Y. F. Wong and Jun Feng",
  abstract =     "The advent of the digital library and multimedia
                 database require robust techniques for multimedia
                 content searches. Content-based retrieval techniques
                 have been developed to overcome some of the limitations
                 associated with conventional keyword-based browsing or
                 searching of visual data. We present an affine
                 invariant shape-based retrieval technique for image
                 retrieval which is efficient and robust. More
                 importantly, the technique supports a query presented
                 in the form of hand-drawn sketches and has the
                 potential of supporting affine invariant partial shape
                 retrieval.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-461,
  pages =        "62--69",
  year =         "2001",
  title =        "Web-Based Image Retrieval: {A} Hybrid Approach",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-461",
  author =       "Yueting Zhuang and Qing Li and Rynson W. H. Lau",
  abstract =     "In recent years, image retrieval has received
                 tremendous attention and some progress has been made.
                 However, most existing work on image retrieval focuses
                 on specific issues and techniques local to image
                 computing or access. Little work has been done to
                 combine various aspects into a single framework. We
                 describe our approach of developing a general-purpose
                 image retrieval system over the Web. A main feature of
                 our system is its hybrid approach by integrating both
                 semantic and visual feature retrieval methods. By
                 combining keyword-based query selection with
                 content-based retrieval techniques, the system is able
                 to provide effective image searching and retrieval. The
                 results are further improved through a relevance
                 feedback process. A research prototype system has been
                 constructed on the Web environment, and experimental
                 results demonstrate the effectiveness of the new
                 approach.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-462,
  pages =        "73--80",
  year =         "2001",
  title =        "Water Animation with Disturbance Model",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-462",
  author =       "Qianhua Chen and Jiansong Deng and Falai Chen",
  abstract =     "This paper provides a physically based model to
                 animate water. A disturbance model is proposed to
                 simulate various kinds of waves. We use a powerful
                 solver called the finite volume method to solve the
                 water fluid equation and give various kinds of
                 disturbance to the solutions according to different
                 disturbance sources such as wind and rain droplets. In
                 this way, we can nicely simulate the movement of waves
                 such as superposition and reflection, and thus easily
                 simulate scenes of a raining pool, windy lake, etc.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-463,
  pages =        "81--88",
  year =         "2001",
  title =        "Animating the Escape Response of the Sea Anemone,
                 Stomphia Coccinea from the Starfish, Dermasterias
                 imbricata Modeled Using Implicit Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-463",
  author =       "Mai Ali Nur and Xikun Liang and Brian Wyvill and
                 George B. Bourne",
  abstract =     "Many of the most interesting simulations in the
                 biological world have to do with interactions between
                 species. The predator-prey interactions among aquatic
                 organisms are an interesting part of the natural world
                 which has not been seen much in computer animation.
                 This paper explores the interaction between various sea
                 anemones and the starfish Dermasterias imbricata.
                 Although a simulation between a specific sea anemone,
                 Stomphia coccinea to Dermasterias imbricata was
                 created, an approach was taken such that different
                 anemones with minor parameter changes can be used to
                 replace Stomphia coccinea. The animation was created
                 using a parametric key-frame approach using tracks and
                 a hierarchical procedural implicit modeling method. The
                 anemone and starfish were modeled using the Blob-Tree.
                 Using a hierarchical construction, the model is refined
                 locally and deformed globally at arbitrary layers while
                 maintaining the consistency and integrity of surface
                 details. A hierarchy of tracks was used to control the
                 object's motion.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-464,
  pages =        "89--95",
  year =         "2001",
  title =        "Visual Simulation of Lightning Taking into Account
                 Cloud Growth",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-464",
  author =       "Batjargal Sosorbaram and Tadahiro Fujimoto and
                 Kazunobu Muraoka and Norishige Chiba",
  abstract =     "Simulation of weather scenery in computer graphics
                 (CG) is an important issue that has a wide range of
                 applications related to image content, such as in
                 movies and in various landscape simulations. In
                 general, research to date into CG lightning has focused
                 on generating a lightning pattern by applying
                 probability and statistics techniques, which do not
                 consider the generating mechanism of lightning. In
                 natural lightning, an electrical-discharge path
                 (stepped leader) is generated under the influence of an
                 electric field formed by electrification of a cloud. We
                 present a method that generates a lightning pattern by
                 simulating a stepped leader in an electric field
                 defined by placement of electric charges. We also give
                 rendering methods for lightning in a cloud that are
                 based on the volume rendering method using point light
                 sources.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-465,
  pages =        "99--106",
  year =         "2001",
  title =        "Volumes of Expression: Artistic Modelling and
                 Rendering of Volume Datasets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-465",
  author =       "Steve M. F. Treavett and Min Chen and Richard
                 Satherley and Mark W. Jones",
  abstract =     "This paper presents the design and implementation of
                 artistic effects in modelling and rendering of volume
                 datasets. Following different stages of a volume-based
                 graphics pipeline, we examine various properties of
                 volume data, and illustrate how expressive and
                 non-photorealistic effects can be implemented. We
                 demonstrate that the true 3D nature of volume data
                 makes it particularly applicable for this use, allowing
                 the addition of complex effects at the modelling stage
                 as well as during rendering.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-466,
  pages =        "107--112",
  year =         "2001",
  title =        "Animating Chinese Landscape Paintings and Panorama
                 Using Multi-Perspective Modeling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-466",
  author =       "Nelson Siu-Hang Chu and Chiew-Lan Tai",
  abstract =     "This paper describes a multi-perspective modeling
                 technique for making fly-through animations from a
                 single large landscape painting or panorama. These
                 images have sub-scenes that are taken from different
                 perspective views. The technique constructs a simple
                 global model for the entire input image and builds a
                 local model for each subscene using the TIP spidery
                 mesh interface. Animation is generated by switching
                 smoothly between a local model and the global model
                 while moving a virtual camera along an animation path.
                 Novel views are rendered by mapping the texture images,
                 extracted from the original image, onto the active
                 model. The usefulness of the technique is demonstrated
                 with two animation examples from a Chinese landscape
                 painting and a spherical panoramic image.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-467,
  pages =        "113--120",
  year =         "2001",
  title =        "Creating pen-and-ink illustration using stroke
                 morphing method",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-467",
  author =       "Hye-Sun Kim; Hee-Jeong Jin; Young-Jung Yu; Hwan-Gue
                 Cho",
  abstract =     "Most illustration systems need a lot of user strokes
                 to generate natural-looking pen-and-ink illustrations.
                 In order to reduce the number of user strokes
                 necessary, we propose a new method for pen-and-ink
                 illustrations using a stroke morphing concept. For
                 this, we introduce a general stroke morphing procedure,
                 which consists of both flow-oriented morphing and
                 shape-oriented morphing. Using this morphing technique,
                 we can make more natural-looking pen-and-ink
                 illustration with fewer user strokes. This work can be
                 applied to generate simplified pictures for dictionary
                 typesetting. The main purpose of this paper is to
                 describe this method, which requires fewer user strokes
                 than other previous methods. Experimental results are
                 given in the final section.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-468,
  pages =        "123--130",
  year =         "2001",
  title =        "Efficient 3{D} Image Warping for Composing Novel
                 Views",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-468",
  author =       "Xin Zheng and Enhua Wu",
  abstract =     "This paper presents an efficient inverse warping
                 algorithm for generating novel views by combining
                 multiple reference images taken from different
                 viewpoints. The method proceeds in three steps.
                 Firstly, the reference images are preprocessed for
                 extracting edge pixels. Secondly, an inverse warping is
                 performed to render the desired image from one primary
                 reference image. By taking advantages of epipolar line
                 features and depth discontinuities in reference images,
                 the inverse warping can be efficiently applied by
                 segments, to accelerate the rendering substantially.
                 Finally, holes in the desired image are filled up
                 through searching the corresponding points in other
                 reference images. At this stage, two accelerating
                 techniques are presented. By using the proposed
                 algorithm we can navigate a virtual environment at
                 interactive rate.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-469,
  pages =        "131--138",
  year =         "2001",
  title =        "A Hybrid Approach to the Recovery of Deformable
                 Superquadric Models from 3{D} Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-469",
  author =       "James Sinnott and Toby Howard",
  abstract =     "The problem of recovering the shape of objects from
                 three-dimensional data is important to many areas of
                 computer graphics and vision. We present here a method
                 for the recovery of single-part objects from
                 unstructured 3D points sets, based on the fitting of
                 deformable superquadric models. The limitations of
                 least-squares minimisation as a technique for fitting
                 superquadric models are discussed. After investigating
                 the possibility of using a genetic algorithm as an
                 alternative, we propose a hybrid approach to the
                 recovery of deformable superquadrics based on a
                 two-stage fitting process that combines a genetic
                 algorithm and nonlinear least-squares minimization.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-47,
  year =         "2001",
  title =        "Visibility Complexity of Animation in Flatland",
  author =       "Jaume Rigau and Miquel Feixas and Mateu Sbert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-47",
  abstract =     "In this paper we address the problem of measuring the
                 visibility complexity of scene animation in flatland.
                 In our previous work we proposed a complexity measure
                 which quantifies the information transfer in a static
                 scene. Here we introduce two measures to capture the
                 complexity of movement. The first approach measures the
                 dissimilarity between successive frames of an animation
                 using the variation of information exchange between
                 each pair of patches. The second one is the euclidean
                 distance between form factor distributions. We present
                 preliminary results which show that both approaches
                 attain a satisfactory and very similar animation
                 classification.",
  editor =       "V. Skala",
  keywords =     "Animation, complexity, euclidean distance, information
                 theory, visibility.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-470,
  pages =        "139--146",
  year =         "2001",
  title =        "A Motion Prediction Method for Mouse-Based
                 Navigation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-470",
  author =       "Addison Chan and Rynson W. H. Lau and Antonio Si",
  abstract =     "A distributed virtual reality system allows remote
                 users to share and to view a common virtual environment
                 via connected networks. However, network latency and
                 bandwidth are often the most crucial performance
                 bottlenecks. We have recently developed a distributed
                 virtual walkthrough environment that supports on-demand
                 model transmission over the Internet through the use
                 of, in addition to other techniques, a simple
                 prefetching technique called EWMA. Although the
                 prefetching technique has been shown to be effective in
                 predicting 3D motion during our simulation experiments,
                 it is less effective in our prototype experiments. The
                 main reason is that most input devices used for
                 navigation are 2D in nature (mostly 2D mice) and EWMA
                 is not too effective in predicting user motion in
                 moving a 2D mouse. To overcome this limitation, we
                 propose in this paper a method for predicting the user
                 motion in moving the mouse during a 3D navigation. To
                 improve the accuracy of the prediction, we also propose
                 a constrained navigation method. We will demonstrate
                 the effectiveness of the new method through
                 experimental results.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-471,
  pages =        "149--156",
  year =         "2001",
  title =        "Modeling and Rendering of Various Natural Phenomena
                 Consisting of Particles",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-471",
  author =       "Tomoyuki Nishita and Yoshinori Dobashi",
  abstract =     "The simulation of various natural phenomena is one of
                 the important research fields in computer graphics. In
                 particular, aspects such as sky, clouds, water, fire,
                 trees, smoke, terrains, desert scenes, snow and fog are
                 indispensable for creating realistic images of natural
                 scenes, flight simulators and so on. Therefore, a lot
                 of researchers have been trying to develop methods for
                 simulating and rendering these. In this paper, we focus
                 on sky, clouds, smoke, desert scenes and atmospheric
                 effects, such as shafts of light. These phenomena have
                 the common feature that they consist of the effects of
                 small particles. To create realistic images, physical
                 based simulation and rendering are required. In
                 particular, the color greatly depends on the properties
                 of light scattering due to particles. In general,
                 however, the simulation and rendering of these images
                 is assumed to be very time-consuming. This paper
                 describes efficient methods for creating realistic
                 images of such natural phenomena.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-472,
  pages =        "159--166",
  year =         "2001",
  title =        "Implementation of Object Attachments by Cellular
                 Modeling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-472",
  author =       "Masayuki Hisada and Tosiyasu L. Kunii",
  abstract =     "We research the defects of geometric modeling in
                 representing object attachments. It is difficult to
                 represent different types of object attachments such as
                 glueing or fusing in current computer graphics. We
                 consider two types of different attachments such that
                 an object is &ldquo;put&rdquo; on the top of another
                 object, and an object is &ldquo;fused&rdquo; to the top
                 of another object. To represent the relationships of
                 object attachments, we assume a hypothesis such that we
                 can represent the information of object attachments in
                 computer graphics based on the cellular models, and
                 consider the real implementation in computer graphics
                 for proving that the cellular model of object
                 attachments meet the hypothesis. The results of our
                 research are expected to influence major applications
                 including computer integrated manufacturing (CIM).",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-473,
  pages =        "167--174",
  year =         "2001",
  title =        "The Hash Function and the Principle of Duality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-473",
  author =       "V{\'{a}}clav Skala and Martin Kuchar",
  abstract =     "An algorithm complexity is a very crucial issue in the
                 algorithm design, especially if large data sets are to
                 be processed. Data search is very often used in many
                 algorithms and hash function use gives us a possibility
                 to speed up the process significantly. Nevertheless, it
                 is very difficult to design a good hash function
                 especially for geometric applications. This paper
                 describes a new hash function, its behaviour and use
                 for non-trivial problems. Some problems can be solved
                 effectively using the principle of duality and the hash
                 data structure. Also some problems that cannot be
                 solved in Euclidean space can be solved if dual
                 representation is used and some examples are presented
                 too.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-474,
  pages =        "175--182",
  year =         "2001",
  title =        "Polygonizing Non-Uniformly Distributed 3{D} Points by
                 Advancing Mesh Frontiers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-474",
  author =       "Indriyati Atmosukarto and Luping Zhou and Wee Kheng
                 Leow and Zhiyong Huang",
  abstract =     "3D digitization devices produce very large sets of 3D
                 points sampled from the surfaces of the objects being
                 scanned. A mesh construction procedure needs to be
                 applied to derive polygon mesh from the 3D point sets.
                 As the 3D points derived from digitization devices
                 based on digital imaging technologies are inherently
                 non-uniformly distributed over regions that may contain
                 surface discontinuities, existing methods are not
                 suitable for polygonizing them. This paper describes a
                 novel polygonization algorithm for constructing
                 triangle mesh from unorganized 3D points. In contrast
                 to existing methods, this algorithm begins the mesh
                 construction process from 3D points lying on smooth
                 surfaces, and advances the mesh frontier towards 3D
                 points lying near surface discontinuities. If 3D points
                 along the edges and at the corners are sampled, then
                 the algorithm will form an edge where two advancing
                 frontiers meet, and a corner where three or more
                 frontiers meet. Otherwise, the algorithm constructs
                 approximations of the edges and corners. It can be
                 shown that this frontier advancing algorithm performs
                 2D Delaunay triangulation of 3D points lying on a plane
                 in 3D space.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-475,
  pages =        "183--190",
  year =         "2001",
  title =        "Blendeforming: Ray Traceable Localized Foldover-Free
                 Space Deformation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-475",
  author =       "David Mason and Geoff Wyvill",
  abstract =     "Since the introduction of Free Form Deformation (FFD)
                 by Sederberg and Parry (1986), a great deal of research
                 has been performed in the area of space deformation for
                 3D shape modeling. However, many techniques do not
                 provide local control, and almost all are non-unique
                 mappings that are capable of producing foldover, making
                 them unsuitable for use with implicit surfaces, or any
                 object that should not self-intersect, and causing some
                 problems for ray tracing. In this paper a new approach
                 to deformation is presented. Blendeforming (Blended
                 Deforming) provides foldover-free deformation with
                 local control, which preserves implicit functions.
                 Blendeformers are reversible deformations that are
                 suitable for use in interactive modeling and ray
                 tracing.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-476,
  pages =        "93--200",
  year =         "2001",
  title =        "Using {A}-Buffer in Radiosity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-476",
  author =       "Q. Wang and J. H. Tang and C. H. Lim and H. C. Teh and
                 Z. Y. Huang",
  abstract =     "Anti-aliasing in form factor computation and precise
                 determination of shadow boundaries are important issues
                 for radiosity method. A-buffer is a technique
                 introduced by L. Carpenter (1984) for anti-aliasing in
                 hidden surface removal. In this paper, we propose a
                 method based on the A-buffer to replace the Z-buffer in
                 the computation of form factors using the Hemicube for
                 anti-aliasing. The method directly computes shadow
                 boundaries in contrast to the adaptive re-meshing
                 approach. Though the processing of shadow boundaries is
                 not fast, only one discontinuity meshing (no iteration)
                 is required. We implement the method in an
                 infrastructure building design system.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-477,
  pages =        "201--208",
  year =         "2001",
  title =        "A Monte Carlo Method for Accelerating the Computation
                 of Animated Radiosity Sequences",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-477",
  author =       "Gonzalo Besuievsky and Xavier Pueyo",
  abstract =     "Realistic rendering animation is known to be an
                 expensive processing task when physically-based global
                 illumination methods are used in order to improve
                 illumination details. This paper presents an
                 acceleration technique to compute animations in
                 radiosity environments. The technique is based on an
                 interpolated approach that exploits temporal coherence
                 in radiosity. A fast global Monte Carlo pre-processing
                 step is introduced to the whole computation of the
                 animated sequence to select important frames. These are
                 fully computed and used as a base for the interpolation
                 of all the sequence. The approach is completely
                 view-independent. Once the illumination is computed, it
                 can be visualized by any animated camera. Results
                 present significant high speed-ups showing that the
                 technique could be an interesting alternative to
                 deterministic methods for computing non-interactive
                 radiosity animations for moderately complex
                 scenarios.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-478,
  pages =        "209--214",
  year =         "2001",
  title =        "Hardware-Accelerated Rendering of Antialiased Shadows
                 with Shadow Maps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-478",
  author =       "Stefan Brabec and Hans-Peter Seidel",
  abstract =     "We present a hardware-accelerated method for rendering
                 high quality, antialiased shadows using the shadow map
                 approach. Instead of relying on dedicated hardware
                 support for shadow map filtering, we propose a general
                 rendering algorithm that can be used on most graphics
                 workstations. The filtering method softens shadow
                 boundaries by using a technique called percentage
                 closer filtering which is commonly used in software
                 renderers, e.g. ray tracing. We describe how the
                 software algorithm can be efficiently mapped to
                 hardware. In order to achieve real-time or at least
                 interactive frame rates we also propose a slightly
                 modified shadow filtering method that saves valuable
                 hardware resources while still achieving good image
                 quality.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-479,
  pages =        "215--222",
  year =         "2001",
  title =        "Ray Tracing Surfaces of Revolution: An Old Problem
                 with {A} New Perspective",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-479",
  author =       "George Baciu and Jinyuan Jia and Gibson Lam",
  abstract =     "We present a new subdivision scheme that is shown to
                 improve the performance of ray tracing surfaces of
                 revolution over Kajiya's (1983) classical work. This is
                 based on a monotonic interval partitioning of a
                 generatrix of a surface of revolution. The algorithm
                 has a search complexity upper bound of O(log(m n)) for
                 m monotonic intervals and n subdivisions for each
                 interval and runs up to three times faster on large
                 scenes. This method also suggests a novel hybrid
                 bounding volume scheme that reduces this number of
                 intersection tests between a ray and the actual object
                 surface.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-48,
  year =         "2001",
  title =        "Towards Interactivity on Texturing Implicit Surfaces:
                 {A} Distributed Approach",
  author =       "R. Zonenschein and J. Gomes and L. Velho and N.
                 Rodriguez",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-48",
  abstract =     "We describe a distributed system for texture mapping
                 implicit surfaces. The method uses a particle system
                 associated with the gradient vector field of the
                 function that defines an implicit surface to acquire
                 texture coordinates at a support surface. As each
                 particle trajectory is independent of each other, this
                 method has a special suitability to run in parallel.
                 Our results show good improvement as more processors
                 are added to the system, with speedups of up to 13 with
                 32 processors, performance such as required by an
                 interactive system.",
  editor =       "V. Skala",
  keywords =     "Implicit surfaces, texture mapping, parallelism.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-480,
  pages =        "225--231",
  year =         "2001",
  title =        "Cartoon Image Vectorization Based on Shape
                 Subdivision",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-480",
  author =       "Ju Jia Zou and Hong Yan",
  abstract =     "This paper presents a non-pixel-based skeletonization
                 method for vectorizing cartoon images. The constrained
                 Delaunay triangulation technique is applied to
                 subdivide a shape into a set of non-overlapping
                 triangles. Then, certain triangles in the triangulation
                 are merged to remove artifacts. The skeleton of a shape
                 is obtained from the skeletons of its constituent
                 parts. Experimental results show that the proposed
                 method is more accurate and efficient than a typical
                 thinning method.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-481,
  pages =        "232--239",
  year =         "2001",
  title =        "Region Matching and Optimal Matching Pair Theorem",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-481",
  author =       "Zhanggui Zeng and Hong Yan",
  abstract =     "This paper presents an approach to region feature
                 extraction and region matching for cartoon image
                 processing. By introducing the inertia coordinate
                 system, the invariant features of regions have been
                 extracted. Fuzzy dissimilarity is proposed as a
                 matching variable. To simplify the optimal region
                 matching, an optimal matching pair theorem is proposed
                 and proved. Experiments are conducted for verifying the
                 feature extraction and the optimal matching pair
                 theorem.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-482,
  pages =        "240--246",
  year =         "2001",
  title =        "An Intelligent System for Integrating Semantic and
                 Iconic Features for Image Retrieval",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-482",
  author =       "Lilian H. Y. Tang and Rudolf Hanka and Horace Ho-Shing
                 Ip and Kent K. T. Cheung and Ringo W. K. Lam",
  abstract =     "The I-Browse project aimed to develop a prototype
                 system to provide facilities for supporting intelligent
                 retrieval of medical images through a combination of
                 iconic and semantic content. The resulting prototype
                 system, I-Browse, is able to extract and represent
                 relevant iconic and semantic information from input
                 images and to automatically generate textual
                 annotations for images. Techniques were also developed
                 for retrieving relevant images from the database given
                 either an example image query or a textual query. The
                 facilities provided by I-Browse were evaluated by
                 medical colleagues and judged to have the potential of
                 alleviating some of the time-consuming tasks that
                 doctors now have to perform daily and providing a means
                 of identifying previously unknown relationships between
                 visual appearance and histological events.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-483,
  pages =        "249--256",
  year =         "2001",
  title =        "A Multi-Layered Reflection Model of Natural Human
                 Skin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-483",
  author =       "Carmen So-Ling Ng and Ling Li",
  abstract =     "The reflection model of human skin depends on the
                 optical properties of its structural components,
                 including a layer of naturally secreted sebum. Sebum is
                 found over most parts of the body, causing skin to look
                 more specular, depending on the viewing conditions. To
                 capture the differences in appearance caused by the
                 presence of sebum on skin surface, we have developed a
                 three-layer simulation model. The resultant reflection
                 model consists of the specular reflection due to the
                 Fresnel effect, as well as the diffuse reflection from
                 subsurface scattering. Optical and geometric properties
                 are used as control parameters to influence the surface
                 reflection and subsurface scattering of light within
                 the three layers. The bi-directional reflectance
                 distribution function obtained from the simulation is
                 verified experimentally and used to render the natural
                 appearance of human skin.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-484,
  pages =        "257--264",
  year =         "2001",
  title =        "Physically-Based Real-Time Animation of Draped Cloth",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-484",
  author =       "Chiyi Cheng and Jiaoying Shi and Ying-Qing Xu and
                 Heung-Yeung Shum",
  abstract =     "We propose a new physically-based model for real-time
                 animation of draped cloth which not only speeds up the
                 rendering greatly, but also maintains visually
                 appealing results. Our simplified model is represented
                 as a grid object composed of mass points connected by
                 semi-rigid rods, whose behavior is governed by
                 non-rigid dynamics. The longitudinal (vertical) and
                 latitudinal (horizontal) directions of our model are
                 decoupled and processed separately, and later combined
                 to generate the final cloth. Moreover, we provide a
                 uniform treatment of internal and external forces.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-485,
  pages =        "265--272",
  year =         "2001",
  title =        "Comparing Efficiency of Integration Methods for Cloth
                 Simulation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-485",
  author =       "Pascal Volino and Nadia Magnenat-Thalmann",
  abstract =     "Any cloth simulation system needs efficient numerical
                 methods for integrating the equations that describe the
                 mechanical behavior of the discrete representation of
                 the cloth. Choosing the adequate method should be done
                 with full knowledge of the advantages and weaknesses of
                 the main techniques. This paper presents a quantitative
                 comparison of the efficiency of the most common
                 integration techniques used for cloth simulation, and
                 raises the key considerations for optimal
                 implementations depending on the practical kind of
                 simulation problematic.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-486,
  pages =        "275--282",
  year =         "2001",
  title =        "A New Approach of Point-Based Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-486",
  author =       "Qunsheng Peng and Wei Hua and Xuehui Yang",
  abstract =     "A new approach of point-based rendering is presented
                 for objects with complex shape. By this approach, an
                 object is represented by a set of discrete sample
                 points, each point stands for a sampled surface area
                 whose boundary is assumed to be a circle when viewed
                 along its normal. An algorithm is proposed to build a
                 point-based model with level of details. Efficiency of
                 the rendering process is ensured by performing adaptive
                 sampling of the model without calculating the sample
                 points on the fly. To get rid of the aliasing effects
                 due to point sampling, we have developed a
                 delta-z-buffer and a flag buffer to smooth the boundary
                 of two adjacent sampled areas and to fill the holes
                 that may appear inside the image of an object.
                 Experimental results show that our new approach is
                 capable of rendering complex objects consisting of only
                 50,000 points with acceptable image quality, and high
                 rendering efficiency.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-487,
  pages =        "285--290",
  year =         "2001",
  title =        "Dynamic Refinement of Deformable Triangle Meshes for
                 Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-487",
  author =       "Kolja K{\"{a}}hler and J{\"{o}}rg Haber and Hans-Peter
                 Seidel",
  abstract =     "We present a method to adaptively refine an irregular
                 triangle mesh as it deforms in real-time. The method
                 increases surface smoothness in regions of high
                 deformation by splitting triangles in a fashion similar
                 to one or two steps of loop subdivision. The refinement
                 is computed for an arbitrary triangle mesh and the
                 subdivided triangles are simply passed to the rendering
                 engine, leaving the mesh itself unchanged. The
                 algorithm can thus be easily plugged into existing
                 systems to enhance the visual appearance of animated
                 meshes. The refinement step has a very low
                 computational overhead and is easy to implement. We
                 demonstrate the use of the algorithm in our
                 physics-based facial animation system.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-488,
  pages =        "291--298",
  year =         "2001",
  title =        "Hierarchical Implicit Surface Refinement",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-488",
  author =       "Xikun Liang and Brian Wyvill",
  abstract =     "A novel method of hierarchical implicit modeling is
                 presented in which an implicit object is modeled using
                 a hierarchy of implicit surfaces. The hierarchy
                 provides both layered local refinement and global
                 deformation. Local refinement allows the introduction
                 of higher-level detailed surfaces. Global deformation
                 changes the overall shape of the surface while
                 maintaining the integrity of surface details. The model
                 is gradually refined by introducing appropriate new
                 primitives in specified surface areas. Refinement
                 constraints, such as a local area constraint and a
                 level constraint, are designed to be applied to the
                 implicit object so as to achieve finer control of the
                 local surface. The method provides a dynamic
                 representation of implicit surfaces and can be used in
                 modeling complex implicit objects, animating the
                 surfaces, and simulating the deformations of various
                 objects.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-489,
  pages =        "299--305",
  year =         "2001",
  title =        "Providing Local Interpolation, Tension and Normal
                 Control in the Manipulation of Loop Subdivision
                 Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-489",
  author =       "Johan Claes and Marc Ramaekers and Frank Van Reeth",
  abstract =     "Recursive subdivision surfaces allow a lot of freedom
                 in designing surfaces of arbitrary topology. However,
                 tools to manipulate them are still not as powerful as
                 existing tools for established modeling paradigms such
                 as B-spline surfaces. Furthermore, the most well
                 behaved and most widely used schemes are approximating
                 schemes, not interpolating their initial control
                 points. We describe a new modeling paradigm providing
                 local interpolation together with interactive normal
                 and tension control to the approximating loop scheme,
                 without interfering with its uniform subdivision rules.
                 These tools are investigated for vertices both in the
                 interior and at the border.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-49,
  year =         "2001",
  title =        "Interaction Approach for Digital Video Based
                 Storytelling",
  author =       "Norbert Braun",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-49",
  abstract =     "This paper shows an approach for a storytelling
                 oriented interaction on digital video. All the
                 interaction capabilities of the system are driven by
                 the video context and therefore media centric.
                 Interaction possibilities are given to the audience by
                 conversation (on topics of the video content) or by
                 classical Direct Manipulation (of video objects).
                 Conversations can be done by the user with a
                 personalized assistance or directly with the video. The
                 implementation of the approach is shown by a discussion
                 about an application architecture on the basis of Real
                 Media Server, SMIL and Java 3D. The application runs as
                 a Video On Demand system, accessible through the world
                 wide web.",
  editor =       "V. Skala",
  keywords =     "Interaction, Conversation, Media Centred, Digital
                 Video, Digital Storytelling.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-490,
  pages =        "306--313",
  year =         "2001",
  title =        "A Marching Voxels Method for Surface Rendering of
                 Volume Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-490",
  author =       "Chin-Feng Lin and Don-Lin Yang and Yeh-Ching Chung",
  abstract =     "The marching cubes method is a well-known surface
                 extraction method by using the surface configurations
                 of cubes for surface rendering of volume data. The
                 marching cubes method has three main disadvantages: it
                 is time consuming, ambiguous, and holes are generated.
                 All these disadvantages come from the use of the
                 surface configurations of cubes. We propose an
                 efficient surface extraction method, the marching
                 voxels method, for surface rendering of volume data.
                 Instead of using the surface configurations of cubes,
                 the marching voxels method first generates triangles
                 for inner voxels. Then it combines the triangles of
                 inner voxels to produce the surface of an object.
                 Finally, the surface of an object is projected to a
                 plane to form the final image. Since the marching
                 voxels method considers the combination of triangles of
                 voxels not cubes and the combination of triangles is
                 performed in a deterministic way, there is neither
                 ambiguous case of a combination nor holes for the
                 generated surface. The experimental results show that
                 the marching voxels method saves about 30% of the
                 surface rendering time compared to the marching cubes
                 method for test samples.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-491,
  pages =        "317--320",
  year =         "2001",
  title =        "Visualization of Geostress Tensor along Oil Well
                 Trajectory",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-491",
  author =       "Yuan Zhou and Quan-Lin Li and Song De Ma",
  abstract =     "A simple and efficient method is proposed to visualize
                 a symmetric second order tensor along 3D curves. It is
                 also applied to an oil well model in which, along the
                 well trajectory, the spatial distribution of geostress
                 is visualized to predict the stability of the oil well
                 bore. To give visual representations several forms are
                 used: (1) we visually compare the magnitude of stresses
                 to determine the mud weight; (2) based on a set of
                 ellipse cross-sections along the well trajectory and
                 the ellipticity of the ellipse we predict the shear
                 failure direction and the stability of the oil well
                 bore; (3) from a hinged curved surface, we emphasize
                 the direction of principal stresses; (4) to give useful
                 information we apply other skills such as magnified
                 details, animation and an interactive interface. Our
                 method avoids two defects in the references, that is,
                 the visual cluster caused by discrete point tensor
                 glyphs, and the integration problem of the
                 hyperstreamline.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-492,
  pages =        "321--324",
  year =         "2001",
  title =        "3{D} Discrete Clothoid Splines",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-492",
  author =       "Guiqing Li and Xianmin Li and Hua Li",
  abstract =     "A clothoid spline is a planar G2 curve with piecewise
                 linear curvature. Its discrete analogon, planar
                 discrete clothoid spline (PDCS for short) generated by
                 non-linear subdivision, is also of high quality owing
                 to piecewise linear curvature distribution. We extend
                 the PDCS to 3D by introducing discrete curvature
                 binormal vectors and a discrete Frenet frame. An
                 algorithm similar to the planar case is developed for
                 creation of 3D discrete clothoid splines, Experiments
                 show that 3D discrete clothoid spline still retains
                 high quality. This result makes it possible to
                 construct discrete clothoid spline surfaces on open
                 triangle meshes or curve nets of arbitrary topology.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-493,
  pages =        "325--328",
  year =         "2001",
  title =        "What Do You Think You're Doing? Measuring Perception
                 in Fish Tank Virtual Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-493",
  author =       "Michael Treadgold and Kevin Novins and Geoff Wyvill
                 and Brian Niven",
  abstract =     "Fish tank virtual reality (FTVR) systems use eye
                 tracking to present perspective projections to a user
                 that are tailored to his or her view position. These
                 perspective views can create the illusion of a world
                 behind the display screen-a virtual fish tank. However,
                 the illusion is not perfect &ldquo;users do not feel
                 that the virtual objects are real. Despite this, users
                 clearly are getting some indication of depth. What
                 exactly are they perceiving? We review previous work in
                 FTVR and identify key differences between the virtual
                 world and the real world. We show that we still have an
                 incomplete understanding of perception in fish tank
                 virtual reality and that some important questions may
                 be impossible to answer.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-494,
  pages =        "329--332",
  year =         "2001",
  title =        "Applying Functional Networks to Fit Data Points from
                 {B}-Spline Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-494",
  author =       "Andr{\'{e}}s Iglesias and Akemi G{\'{a}}lvez",
  abstract =     "A powerful extension of neural networks, the so-called
                 functional network, was recently introduced. This kind
                 of network is more versatile than neural networks and
                 so can be successfully applied to several problems in
                 computer-aided geometric design (CAGD). As an
                 illustration, the simplest functional network
                 representing tensor product surfaces is obtained. Then,
                 functional network formalism is advantageously used to
                 fit given sets of data from B-spline surfaces through a
                 Bezier surface. The proposed method also determines the
                 degree and coefficients (control points) of the
                 approximating surface that fits the given data better.
                 This new approach is very general and can also be
                 applied to any other interesting family of
                 approximating basis functions in CAGD.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-495,
  pages =        "333--336",
  year =         "2001",
  title =        "Javra: {A} Simple, Extensible Java Package for
                 {VRML}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-495",
  author =       "Huub van de Wetering",
  abstract =     "Javra is a Java package for handling VRML scene
                 graphs; it operates either stand-alone or in
                 combination with a VRML browser. The combination of
                 Javra and a VRML browser forms an effective start for
                 generating 3D interactive applications. With Javra a
                 VRML scene graph can be handled: both classes for VRML
                 nodes and methods for setting and getting the fields of
                 these nodes are supplied. Furthermore, VRML events
                 generated, for instance, after a user action, can be
                 caught and handled in Javra. The Javra node classes
                 have an inheritance structure which allows strict
                 compile time type checking of the construction of the
                 scene graph. The programmer interface is intended to be
                 simple enough to be used by students of an introductory
                 programming course. The node classes are generated
                 completely automatically, resulting in a robust
                 package. The automatic code generation can also be used
                 to create custom Java packages for programmer-defined
                 VRML prototypes, effectively resulting in the extension
                 of the set of Javra nodes.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-496,
  pages =        "337--340",
  year =         "2001",
  title =        "{NURBS} Streams",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-496",
  author =       "Frederick W. B. Li and Rynson W. H. Lau",
  abstract =     "Deformable objects are important in modeling clothing,
                 facial expressions, animal characters and other soft
                 objects. However, they are rarely used in distributed
                 virtual environments. This is because both the
                 transmission and the tessellation of deformable objects
                 are too time-consuming to run in real time. Part of the
                 problem can be solved by our earlier (1997, 1999)
                 method on the real-time rendering of deformable NURBS
                 (non-uniform rational B-spline) surfaces. In this
                 paper, we present a technique to organize the data
                 structures that are used to store the pre-computed
                 polygon models and the deformation coefficients in
                 supporting the rendering of deformable NURBS surfaces
                 from a hierarchical form into a linear form, called
                 NURBS streams. The NURBS streams not only support the
                 progressive transmission and rendering of deformable
                 NURBS surfaces, but also simplify the software and
                 hardware implementation of the rendering method. To
                 further speed up the rendering of these NURBS streams,
                 we also introduce the idea of a virtual rendering list
                 to keep track of incremental node changes between
                 successive frames.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-497,
  pages =        "343--346",
  year =         "2001",
  title =        "Technical Illustration Based on Human-Like Approach",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-497",
  author =       "Weidong Geng and Monika Fleischmann and Hongfeng Yu
                 and Yunhe Pan",
  abstract =     "Presents a human-like, non-photorealistic rendering
                 approach. A typical process of how human engineers
                 learn to paint a technical illustration is as follows.
                 First, they are trained in how to paint separate
                 primitives, such as cubes and spheres, and accordingly
                 accumulate empirical drawing principles and skills
                 during their continuous practice, and finally they can
                 freely express complicated shapes by composing the
                 related primitives' drawings together. We manage to
                 mimic this human-like approach by embedding established
                 illustration rules into primitives' lighting models and
                 drawing algorithms, and implement it in an illustration
                 system called RETOUCH.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-498,
  pages =        "347--350",
  year =         "2001",
  title =        "Real-Time Physically-Based Facial Expression Animation
                 Using Mass-Spring System",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-498",
  author =       "Yu Zhang and Edmond C. Prakash and Eric Sung",
  abstract =     "We propose a physically-based approach based on
                 anatomical knowledge for real-time facial expression
                 animation. The facial model incorporates a
                 physically-based approximation to facial skin and a set
                 of anatomically-motivated facial muscles. The skin is
                 modeled by a mass-spring system with nonlinear springs
                 which have a biphasic stress-strain relationship to
                 simulate the elastic dynamics of real facial skin.
                 Facial muscles are modeled as forces deforming the
                 spring mesh. Based on the action units (AUs) of the
                 facial action coding system (FACS), various expressions
                 can be generated by a combination of contractions of a
                 set of facial muscles. Lagrangian mechanics governs the
                 dynamics, dictating the deformation of the facial
                 surface in response to muscle forces. Simulation
                 results show real-time facial deformation as well as
                 realistic expression animation.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-499,
  pages =        "351--354",
  year =         "2001",
  title =        "Layout Adjustment and Boundary Detection for a
                 Diagram",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-499",
  author =       "Wei Lai",
  abstract =     "Diagrams (i.e. practical graphs where the nodes vary
                 in shape and size) have been widely used in software
                 engineering and information systems. A typical example
                 is the UML diagram. Applying classical graph-drawing
                 algorithms to practical graphs may result in
                 overlapping nodes and/or node-edge intersections. This
                 is because such algorithms were often originally
                 designed for abstract graphs, where the nodes take up
                 little or no space. This paper presents an approach for
                 practical graph layout, layout adjustment for removing
                 overlapping node images and node-edge intersections,
                 and boundary detection for ensuring that a diagram fits
                 in a viewing area.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-5,
  year =         "2001",
  title =        "Object-Centered Navigation in Virtual Construction
                 Applications",
  author =       "Colette Elcacho and Thomas Dingel and Reinhard Klein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-5",
  abstract =     "In this paper we present a novel concept for
                 navigation in virtual environments. While a variety of
                 navigation metaphors have been proposed for immersive
                 VR, desktop scenarios are typically based on mouse
                 navigation, using a flying, walking or driving
                 metaphor. Steering all six degrees of freedom in this
                 way is a complicated task even for a trained user. We
                 propose an innovative object-centered concept for
                 navigation, which allows exploring an unknown
                 environment by directly going to the objects one
                 desires looking at closely. The user specifies the
                 object by its name, e.g. {"}table{"}, an aspect, e.g.
                 {"}front{"} and a distance level, e.g. {"}near{"} in an
                 intuitive fashion and is immediately transferred to the
                 desired look-at point. This is done by pointing to an
                 object with the mouse or using a simple speech
                 recognition approach. The object-centered viewpoints
                 are computed dynamically and need not be predefined.
                 Our new navigation concept has been approved by
                 different users and has been tested in an interactive
                 construction environment.",
  editor =       "V. Skala",
  keywords =     "Navigation, object-centered, DOF, interaction, human
                 computer interaction, construction kits, virtual
                 reality (VR), desktop VR.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-50,
  year =         "2001",
  title =        "Toward the Effective Animation of American Sign
                 Language",
  author =       "Eric Sedgwick and Karen Alkoby and Mary Jo Davidson
                 and Roymieco Carter and Juliet Christopher and Brock
                 Craft and Jacob Furst and Damien Hinkle and Brian Konie
                 andGlenn Lancaster and Steve Luecking and Ashley Morris
                 and John McDonald and Noriko Tomuro and Jorge Toro and
                 Rosalee Wolfe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-50",
  abstract =     "American Sign Language (ASL) is the primary language
                 used by the North American Deaf Community. We present
                 our method for producing natural animations of
                 fingerspelling, a functionally important subset of ASL.
                 User testing demonstrates that our animations are
                 readily identified by members of the deaf community.",
  editor =       "V. Skala",
  keywords =     "Animation, American Sign Language, Fingerspelling.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-500,
  pages =        "355--358",
  year =         "2001",
  title =        "A Realtime Rough Surface Renderer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-500",
  author =       "Tsuneo Ikedo and Eisaku Obuch",
  abstract =     "A real-time hardware renderer for light-reflected
                 images with rough surface properties has been developed
                 as one of the graphics functions embedded within a
                 single chip. The renderer comprises Bui-Tuong Phong's
                 (1975) and R.L. Cook & K.E. Torrance's (1982) models on
                 the basis of empirical, physical or wave theory. To
                 simplify the hardware architecture and to avoid vector
                 normalization, every polygon interpolation, surface
                 normal, light-source direction, view direction and bump
                 normal are represented by an angular form. The renderer
                 produces reflective images of rough surfaces within 0.8
                 ns (1.25 billion pixels). This paper describes the new
                 hardware-based algorithms and architectures.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-501,
  pages =        "359--362",
  year =         "2001",
  title =        "Robust Invisible Watermarking of Volume Data Using the
                 3{D} {DCT}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-501",
  author =       "Yinghui Wu and Xin Guan and Mohan S. Kankanhalli and
                 Zhiyong Huang",
  abstract =     "Proposes a novel watermarking algorithm for 3D volume
                 data based on the spread-spectrum communication
                 technique, which is invisible and robust.
                 &ldquo;Invisible&rdquo; means that the 2D rendered
                 image of this watermarked volume is perceptually
                 indistinguishable from that of the original volume.
                 &ldquo;Robust&rdquo; watermarking implies that the
                 watermark is resistant to most intentional or
                 unintentional attacks. We have implemented the
                 algorithm in a software package and have conducted
                 experiments showing that the watermark is invisible in
                 the volume-rendered 2D images. This is further
                 confirmed by computing the signal-to-noise ratio (SNR)
                 and the peak signal-to-noise ratio (PSNR) of the 3D
                 watermarked volume data. We addressed different
                 attacks, putting more emphasis on those attacks that
                 are most commonly performed by attackers. The
                 experiments show that the watermarking scheme is
                 robust.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-502,
  pages =        "363--365",
  year =         "2001",
  title =        "Mosaic and Warping for Forward Moving Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-502",
  author =       "Su Zhang and Hanfeng Chen and Pengfei Shi",
  abstract =     "Discusses mosaicking and warping for forward-motion
                 (or forward-moving) images (FMI). In FMI, the field of
                 view of one frame is wider than that of a later one;
                 however, the resolution of the later frame is higher
                 than that of previous one. Warp-function algorithms
                 based respectively on a mesh warp and a triangle grid
                 are presented, and a multi-resolution mosaic based on
                 the wavelet transform is given. In-between images could
                 be warped from the single mosaic image. An experiment
                 with a real scene shows that the mesh warp algorithm is
                 more time-consuming than that of triangle grid, but the
                 image quality of the latter is better than that of the
                 former. It also shows that the multi-resolution mosaic
                 image is smooth and natural, eliminating the impact of
                 the seams.",
  month =        jul,
  editor =       "Horace Ho-Shing Ip and Nadia Magnenat-Thalmann and
                 Rynson W. H. Lau and Tat-Seng Chua",
  booktitle =    "Computer Graphics International 2001 Proceedings",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-2001-51,
  year =         "2001",
  title =        "The Magic of the {Z}-Buffer: {A} Survey",
  author =       "Theoharis Theoharis and Georgios Papaioannou and
                 Evaggelia-Aggeliki Karabassi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-51",
  abstract =     "The wide availability of hardwired Z-buffer has
                 sparked an explosion in the number of applications of
                 the algorithm whose origins lie in hidden surface
                 elimination. This paper presents a much-needed survey
                 of key applications of the Z-buffer from the fields of
                 rendering, modelling and vision in a common notation in
                 order to help users make better use of this resource.",
  editor =       "V. Skala",
  keywords =     "Z-buffer, depth-buffer, rendering.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-52,
  year =         "2001",
  title =        "Computer-Generated Chinese Painting for Landscapes and
                 Portraits",
  author =       "Der-Lor Way and Chih-Wei Hsu and Hsin-Yi Chiu and
                 Zen-Chung Shih",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-52",
  abstract =     "Practiced for more than thousand years, Chinese
                 painting stresses the notion of {"}implicit meaning{"}
                 in which painters use a minimum of strokes to express
                 their deepest feelings. Chinese landscape and figure
                 paintings are the two major themes of Chinese painting.
                 Of relevant interest is more thoroughly understand
                 Chinese art by analyzing basic rules of Chinese
                 painting. This paper proposes two novel methods capable
                 of synthesizing rock textures in Chinese landscape
                 painting and synthesizing portraits in Chinese
                 painting, respectively. In addition to saving time
                 during trial and error, these two methods attempt to
                 synthesize painting styles. With the requirement of
                 familiarity with painting skills, individuals can paint
                 various styles of Chinese painting.",
  editor =       "V. Skala",
  keywords =     "Non-Photorealistic Rendering (NPR), Chinese landscape
                 painting, TS'UN, Hemp-Fiber Texture Strokes.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-53,
  year =         "2001",
  title =        "Synthetic Images of Underwater Scenes: {A} First
                 Approximation",
  author =       "E. Cerezo and F. J. Seron",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-53",
  abstract =     "The creation and rendering of realistic water scenes
                 is one of the challenging tasks in Computer Graphics.
                 To reproduce the illumination and colour inside water
                 bodies an algorithm capable of dealing with media with
                 anisotropic and multiple scattering has to be used. We
                 have chosen the discrete ordinates method to solve the
                 problem of light transport. Both the theoretical basis
                 and the algorithm that has been implemented are
                 described in the paper. A couple of simple images
                 calculated in different waters are presented. Results
                 indicate the relevant role played by the spectral
                 behaviour of the absorption and scattering coefficients
                 in the process of image generation.",
  editor =       "V. Skala",
  keywords =     "Participating Media, Discrete Ordinates, Ocean Optics,
                 Global Illumination",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-54,
  year =         "2001",
  title =        "Enhancements to Directional Coherence Maps",
  author =       "Annette Scheel and Marc Stamminger and J{\"o}rg
                 P{\"u}tz and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-54",
  abstract =     "Directional Coherence Maps as proposed by Guo in '98
                 are a very efficient acceleration technique for ray
                 tracing based global illumination renderers. It vastly
                 reduces the number of pixels which have to be computed
                 exactly by identifying regions which are suitable for
                 interpolation. By using oriented finite elements for
                 interpolation, the sampling density can be kept low for
                 large regions of the target image. In this paper we
                 describe extensions of the method. An improved object
                 test is presented which prevents that small objects are
                 missed. Additionally, it is shown how to handle
                 textures efficiently, which was not possible with the
                 original approach.",
  editor =       "V. Skala",
  keywords =     "Directional Coherence Maps, Rendering,
                 Interpolation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-55,
  year =         "2001",
  title =        "Progressive Light Path Building",
  author =       "Laslo Szirmay-Kalos and Gy{\"o}rgy Antal and Mateu
                 Abert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-55",
  abstract =     "The paper proposes a global illumination method that
                 builds up light paths progressively taking into account
                 all relevant previous information. The basis of the
                 method is a self-correcting stochastic iteration
                 scheme, which works with a population of photon hits.
                 In each iteration step a ray is generated randomly
                 either from a light source or by reflecting an earlier
                 hit, the the ray is traced to obtain a new hit. In
                 order to limit the size of the hit population, hits are
                 decimated randomly after certain iteration steps.
                 Comparing the new approach to random walk techniques,
                 this method can reuse the illumination and visibility
                 information gathered with previous rays. By defining
                 the decimation strategy properly, the view-importance
                 can be built into the algorithm.",
  editor =       "V. Skala",
  keywords =     "Global illumination, stochastic iteration,
                 light-tracing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-56,
  year =         "2001",
  title =        "Testing Monte-Carlo Global Illumination Methods with
                 Analytically Computable Scenes",
  author =       "Laszlo Szirmay-Kalos and Laszlo Kovacs and Ali Mohamed
                 Abbas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-56",
  abstract =     "This paper presents analytically computable scenes for
                 testing global illumination algorithms with arbitrary
                 BRDFs. The task of these scenes is to enable us to
                 compare global illumination algorithms and check the
                 correctness of the implementation. In our first
                 approach a criterion is given that makes the radiance
                 constant for an arbitrary closed scene allowing either
                 arbitrary BRDFs or arbitrary light source models. In
                 the second approach the geometry is assumed to be an
                 internal surface of a sphere. Here homogeneous diffuse
                 and mirror like reflections can be tested with
                 arbitrary light source models.",
  editor =       "V. Skala",
  keywords =     "Global illumination, BRDF sampling, albedo, test
                 scenes, Monte-Carlo integration, rendering equation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-57,
  year =         "2001",
  title =        "{PL}-Geodesics on {PL}-Continuous Partial Meshes",
  author =       "O. E. Ruiz and A. Carlos and M. Cadavid",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-57",
  abstract =     "Geometric characteristics of 2-manifolds embedded in
                 53 space have been analyzed from the point of view of
                 differential geometry and topology. In the past,
                 results relevant to these areas have been found for &e^
                 curves and surfaces. However, current scientific,
                 industrial, entertainment and medical applications, and
                 availability of more powerful point sampling systems,
                 press for characterization of discrete counterparts for
                 the continuous properties and characteristics evaluated
                 previously in &e^ curves and surfaces. Recent works
                 have presented estimation methods for properties such
                 as the principal and rotated quadrics of point sampled
                 surfaces. The present article uses the findings of
                 previous investigations to propose and implement a
                 method for evaluation of planarity of surfaces. It is
                 based on: (i) Estimation of a &0 partial mesh fitting
                 sets of planar or grid sample points. (ii) Evaluation
                 of the piecewise-linear (PL) version of families of
                 geodesic curves on the mesh. (iii) Diagnostic of the
                 property of planarity based on the behavior of the
                 families of geodesic curves. The present work can be
                 applied in the area of design and manufacturing of
                 products based on sheet materials, such as apparel,
                 metal stamping, thin structures, etc.",
  editor =       "V. Skala",
  keywords =     "Discrete differential geometry, PL geodesics, partial
                 meshing, geometric algorithms, shape reconstruction.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-58,
  year =         "2001",
  title =        "Improved Shading Performance by Avoiding Vector
                 Normalization",
  author =       "Anders Hast and Tony Barrera",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-58",
  abstract =     "Phongs illumination model requires unit length
                 vectors. The surface normal has to be normalized due to
                 the linear interpolation, and if we use single point
                 light sources or a fixed view point, we have to
                 normalize the vectors pointing to the light source and
                 to the viewer. Unfortunately, normalization is a
                 relatively costly operation. One of the main reasons
                 for this is the square root involved. But when we
                 calculate the reflection vector, we actually do not
                 need a normalized normal. This fact can be used in
                 order to get an approximation for the vector we want
                 when we interpolate between normals. The result is
                 faster Phong shading and faster lighting calculations
                 when we are using a single point light source or having
                 a viewer which is not placed at infinity.",
  editor =       "V. Skala",
  keywords =     "Shading, Illumination, Normalization",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-59,
  year =         "2001",
  title =        "{N}-Dimensional Greogory-Bezier For {N}-Dimensional
                 Cellular Complexes",
  author =       "S. Thery and D. Bechmann and Y. Bertrand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-59",
  abstract =     "With industrial modelling tools, topological
                 structures and free form surfaces are often managed
                 separately, and patches used for embedding are limited
                 to dimension 2. A new approach is to combine topology
                 structures of any dimension with embedding of same
                 dimension, and use topological operations to modify the
                 shape and the properties of surfaces and volumes. The
                 use of Chains of map as topological structure and
                 Gregory-Be'zier as embedding allows the conception of
                 very general objects, made of various dimensional
                 rectangular and triangular patches.",
  editor =       "V. Skala",
  keywords =     "Modelling tool, topological structure, Be'zier-Gregory
                 patch, association.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-6,
  year =         "2001",
  title =        "Tour into the Picture Revisited",
  author =       "N. Li and Z. Huang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-6",
  abstract =     "TIP (Tour Into the Picture) was introduced by Horry et
                 al. [HAA97]. Given only one picture, a viewer can tour
                 into the scene as painted on the picture. Based on our
                 implementation of TIP, we noticed a problem: the visual
                 quality drops drastically when the viewpoint tours into
                 the scene. It contradicted the real world experience.
                 We addressed this problem by introducing the use of
                 multiresolution representation of the picture. We have
                 achieved the goal that the visual quality keeps nearly
                 unchanged in the touring. Moreover, we have integrated
                 the 3D models into TIP. By estimating the light
                 sources, we generated shadows into the scene.",
  editor =       "V. Skala",
  keywords =     "Image based rendering/modeling, tour into picture, VR
                 walk through, multiresolution image, shadowing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-60,
  year =         "2001",
  title =        "Fixed-Point Ellipse Drawing Algorithm",
  author =       "Ramon Molla and Roberto Vivo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-60",
  abstract =     "This algorithm draws ellipses with integer centres and
                 decimal radii on discrete devices using fixed-point
                 arithmetic. These ellipses have both X and Y axis
                 parallel to the coordinate axes. It uses forward
                 differences to diminish its cost. It has a low
                 computational complexity while the error is lower than
                 traditional algorithms. This algorithm works in the
                 squared R 2 space (fixed -point) and translates
                 directly the decimal points to the Z2 natural screen
                 space.",
  editor =       "V. Skala",
  keywords =     "Fixed-point arithmetic, ellipse-drawing, scan
                 conversion.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-61,
  year =         "2001",
  title =        "A New Parallel Volume Rendering Algorithm",
  author =       "Hyun Chin and R. S. Ramakrishna",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-61",
  abstract =     "On screen division of objects for parallel volume
                 rendering is considered in this paper. The suggested
                 algorithm runs on private-memory based parallel
                 computers. The notable characteristic of the algorithm
                 is that it effects data transfers only when it is
                 absolutely necessary.",
  editor =       "V. Skala",
  keywords =     "Parallel volume rendering, computer graphics, parallel
                 algorithm, screen-divided volume rendering,
                 object-divided volume rendering.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-62,
  year =         "2001",
  title =        "Zero Variance Importance Sampling Driven Potential
                 Tracing Algorithms for Global Illumination",
  author =       "Qing Xu and Jizhou Sun and Zunce Wei and Yantai Shu
                 and Stefano Messelodo and Jing Cai",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-62",
  abstract =     "This paper presents a novel Monte Carlo strategy for
                 solving the global illumination problem. The usual
                 Monte Carlo approach is different from the one, which
                 is utilized widely in prevalent global illumination
                 algorithms at present, and breaks a new Monte Carlo
                 path for settling the problem. In this way, plenty of
                 unbiased estimators can be employed to enrich the
                 solutions so as to lead to simple error control, and
                 also various variance reduction techniques can be
                 applied conveniently to speed up the estimation at a
                 little additional cost. Especially, an implementation
                 of a theoretically zero variance importance sampling
                 driven potential tracing algorithm on premise of the
                 new scheme has been proposed and carried out. Results
                 having been obtained and comparisons with traditional
                 algorithms show that this new framework and new
                 algorithm is very promising.",
  editor =       "V. Skala",
  keywords =     "Global illumination, Monte Carlo, zero variance
                 importance sampling, random walk.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-63,
  year =         "2001",
  title =        "Parallel Ray Tracing with 5{D} Adaptive Subdivision",
  author =       "G. Simiakakis and Th. Theoharis and A. M. Day",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-63",
  abstract =     "We present strategies for parallelising ray tracing
                 based on 5D adaptive subdivision. Our goals are to
                 obtain good speed-up and to efficiently balance the
                 load between the processors while minimising the
                 required memory per processor inherently large in 5D
                 subdivision . First, loosely coupled strategies are
                 presented, which are ideal for implementation on
                 clusters of workstations, the most commonly used form
                 of parallel processing nowadays . Then we consider a
                 tightly coupled algorithm ideal for multiprocessors
                 with fast interconnection network or shared memory .
                 Finally, results on a cluster of workstations are
                 presented and discussed.",
  editor =       "V. Skala",
  keywords =     "Ray tracing, directional subdivision, ray
                 classification, adaptive subdivision, parallel
                 processing, distributed processing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-64,
  year =         "2001",
  title =        "Occlusion Evaluation in Hierarchical Radiosity",
  author =       "Yann Dupuy and Mathias Paulin and Rene' Caubet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-64",
  abstract =     "In any hierarchical radiosity method, the most
                 expensive part is the evaluation of the visibility.
                 Many methods use sampling and ray casting to determine
                 this term. Space partitioning considerably speeds up
                 the computation process. Partitioning with shafts,
                 leads to a quite precise subdivision of 3D space, as
                 far as interactions between pair of objects are
                 concerned. The use of bounding boxes allows to speed-up
                 many computations, such as collision or intersection
                 detection. Those intersections can profitably be used
                 to determine visibility between objects. Axis aligned
                 bounding boxes allow very fast evaluation of
                 intersection, but are not that precise, whereas
                 oriented bounding boxes, much closer to the 3D object
                 achieve more accurate visibility evaluation. We present
                 here a method that allow to quickly and accurately
                 determine the relative position of an object and a
                 shaft (inside, outside, occluding), and how to
                 implement it in a hierarchical radiosity algorithm, in
                 order to limit the hierarchy construction where not
                 necessary.",
  editor =       "V. Skala",
  keywords =     "Radiosity, shaft, visibility, occlusion, bounding
                 boxes.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-65,
  year =         "2001",
  title =        "A Graph Based Process for Easy Design of Refinement
                 Oracles in Hierarchical Radiosity",
  author =       "Jeremie Turbet and Francois X. Sillion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-65",
  abstract =     "Refinement is the part of the hierarchical radiosity
                 algorithm that decides the best subdivision of the
                 scene geometry to meet user goals using minimum
                 resources. The refinement oracle is a central component
                 of the radiosity algorithm, because it affects the
                 computation time and the radiosity computation error.
                 Hierarchical radiosity refinement remains a research
                 topic today because of the variety of the geometric and
                 radiometric configurations encountered: currently there
                 does not exist a universal oracle that works well in
                 all the different scene geometries and lighting
                 configurations. It is therefore highly desirable to
                 develop flexible tools for the generation of
                 appropriate oracles suited to different tasks. In this
                 paper we propose a graph structure for the refinement
                 process and a classification of the elementary problems
                 the oracle can handle during the refinement. This
                 representation clarifies the complex refinement process
                 by reducing it to the composition of simple tools. New
                 refiners can easily be created or modified with a
                 marginal increase of the computation time, and many
                 advantages in terms of automatic checking and
                 performance analysis.",
  editor =       "V. Skala",
  keywords =     "Radiosity, hierarchical refinement, lighting, global
                 illumination, refinement oracle.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-66,
  year =         "2001",
  title =        "Searching Triangle Strips Guided by Simplification
                 Criterion",
  author =       "O. Belmonte and J. Ribelles and I. Remolar and M.
                 Chover",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-66",
  abstract =     "Triangle strips are widely used as a method to
                 accelerate the visualisation process of polygon models
                 in interactive graphics applications. Another widely
                 used method to improve drawing speed is the utilisation
                 of multiresolution models. These models are constructed
                 based on simplification algorithms. None of the current
                 algorithms for searching strips contemplates the
                 posterior simplification of the initial model. In this
                 paper an algorithm for searching strips is presented.
                 The triangles forming a strip are selected based on a
                 simplification criterion according to the average
                 quadratic error associated with the contraction of
                 edges so that the model is simplified. In this manner
                 the strips encountered are conserved as the model is
                 being simplified. The strips generated in this way may
                 be used to draw the polygon model in an incremental
                 form or to transmit it progressively within a computer
                 network.",
  editor =       "V. Skala",
  keywords =     "Triangle strip searching, interactive visualisation,
                 simplification algorithms, multiresolution models.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-67,
  year =         "2001",
  title =        "Algorithms to Test Ray-Triangle Intersection.
                 Comparative Study",
  author =       "Rafael J. Segura1 and Francisco R. Feito",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-67",
  abstract =     "In this article we present an algorithm to determine
                 the intersection between rays and triangles based on
                 the idea of the study of signs with respect to
                 triangles. One of the advantages of this approach is
                 its robustness due to its lack of trigonometric
                 operations or complex divisions which might alter the
                 result of the calculations. The algorithm is similar
                 (or even better) in time to other existing algorithms,
                 but it is based exclusively on the study of signs, so
                 that the results obtained are more precise. A
                 comparative study of times between the algorithm and
                 other similar algorithms is presented.",
  editor =       "V. Skala",
  keywords =     "Algorithm complexity, computer graphics,
                 triangle-meshes, optimal algorithm, geometric
                 algorithms.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-68,
  year =         "2001",
  title =        "On Flexible Body Approximations of Rigid Body
                 Dynamics",
  author =       "John McDonald",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-68",
  abstract =     "This paper demonstrates that techniques in flexible
                 body dynamics can yield surprising results when applied
                 to rigid bodies. The discussion presents a technique
                 for constructing rigid bodies from collections of
                 masses and springs, and demonstrates that the
                 simulation calculates many features of rigid body
                 dynamics such as the behavior of the center of mass and
                 the moments of inertia, free of charge. Moreover,
                 complex rotational features such as the precession of a
                 spinning top and the behavior of a gyroscope arise,
                 again without the need of extending the model in any
                 way.",
  editor =       "V. Skala",
  keywords =     "Physically based modeling, animation, rigid body
                 dynamics, particle systems, mass-spring systems.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-69,
  year =         "2001",
  title =        "Visualization Issues in Virtual Environments: From
                 Computer Graphics Techniques to Intentional
                 Visualization",
  author =       "A. Angelidis and G. Fouquier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-69",
  abstract =     "Rendering efficiently large virtual environment scenes
                 composed of many elements, dynamic objects, and a
                 highly moving viewpoint is a major issue. This paper
                 focuses on the first of the two viewing stage
                 operation: required elements determination, the second
                 being shading/filtering. We propose a classification,
                 extending the existing computer graphics techniques
                 toward display scalability requirements, that
                 distinguishes two key points: keeping only required
                 elements (culling), and keeping only required details
                 (which includes traditional LODs). The mechanisms
                 needed for display scalability are presented.",
  editor =       "V. Skala",
  keywords =     "Virtual Environment, LOD, intentional display,
                 semantics.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-7,
  year =         "2001",
  title =        "On Synchronized Simulation in a Distributed Virtual
                 Environment",
  author =       "Marko Meister and Charles A. W{\"u}thrich",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-7",
  abstract =     "This paper addresses communication problems in a
                 distributed virtual reality system. The paper presents
                 VOODIE, a system that provides a framework for
                 distributed virtual environments and overcomes the
                 communication load problems by computing as much as
                 possible at the user end. It then concentrates on the
                 communication load generated by a shared virtual world
                 in a general purpose network and proves that the use of
                 intelligent objects can reduce communication in a
                 distributed system to a minimum. It also measures the
                 effect of end user interaction on the network load.",
  editor =       "V. Skala",
  keywords =     "Distributed virtual environments, distributed systems,
                 interaction based simulation, active objects, object
                 behavior.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-70,
  year =         "2001",
  title =        "Using {CORBA} Middleware to Support the Development of
                 Distributed Virtual Environment Applications",
  author =       "S. Wilson and H. Sayers and M. D. J. McNeill",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-70",
  abstract =     "In this paper we report on using Common Object Request
                 Broker Architecture (CORBA) middleware as a means of
                 supporting the rapid development of Distributed Virtual
                 Environment (DVE) applications. We show how CORBA
                 services can be exploited to provide many of the
                 typical functional requirements that developers of DVE
                 applications require, thus reducing the programming
                 effort necessary to develop DVE applications rapidly.
                 We also present the design, implementation and
                 experimental results of NOMAD, our CORBA-based
                 framework for developing DVE applications.",
  editor =       "V. Skala",
  keywords =     "Virtual Reality, Distributed Virtual Environments,
                 CORBA, Real-time, Component-Based Design, Computer
                 Graphics.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-71,
  year =         "2001",
  title =        "Photo-Realistic Simulation and Rendering of Halos",
  author =       "Jean-Christophe Gonzato and Sylvain Marchand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-71",
  abstract =     "We present a technique for efficiently generating
                 photo-realistic pictures of halos and inserting them
                 into existing photographs. First, we describe an
                 algorithm for producing images of halos from physical
                 parameters either coming from existing photographs or
                 supplied by the user. The problem with the resulting
                 images is that they are sampled in a non-uniform way.
                 Then, we propose a specific algorithm for
                 reconstructing the uniform version of these images from
                 their non-uniform sampling. Finally, we explain the
                 complete algorithm for effectively including
                 computer-generated halos into real photographs, thus
                 leading to new pictures with halos looking as if they
                 had been part of the natural scenes captured by the
                 camera.",
  editor =       "V. Skala",
  keywords =     "Halo, natural phenomena, irregular sampling &
                 reconstruction, augmented reality.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-72,
  year =         "2001",
  title =        "Visualising the Execution of Concurrent
                 Object-Oriented Programs Dynamically Using {UML}",
  author =       "Hugo Leroux and Chris Exton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-72",
  abstract =     "Understanding the intricacies behind concurrency
                 within object-oriented programming languages has always
                 been a challenge for undergraduate students. This is
                 particularly true since both are complex issues in
                 their own rights. Visualisation, when used adequately,
                 can be of tremendous assistance in expediting
                 comprehension of such complex issues. The aim of this
                 paper is to discuss the potential of UML, as a medium
                 within visualisation, to assist the comprehension of
                 the execution of a concurrent object-oriented program.
                 We thus investigate the qualities of UML as a language;
                 discuss some of the issues associated with concurrency
                 and Java and finally discuss the design of our
                 visualisation tool.",
  editor =       "V. Skala",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-73,
  year =         "2001",
  title =        "A Digital Teletext Service",
  author =       "Chengyuan Peng and Petri Vuorimaa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-73",
  abstract =     "Digital Teletext is one of the most important
                 interactive services of emerging digital television. As
                 an enhanced version of existing analogue Teletext
                 service, it's a resident application of the set-top
                 box. Its information will be stored in a data carousel
                 and transmitted via broadcasting network. The new
                 service can display smooth graphics and images, have
                 page links and a menu driven navigation method.
                 However, there is no standard specification available
                 for the content format. This paper presents a content
                 format based on Extensible Markup Language (XML). A
                 prototype of digital Teletext system is implemented
                 using the Java programming language.",
  editor =       "V. Skala",
  keywords =     "Digital Teletext, XML, content format, Java, user
                 interface.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-74,
  year =         "2001",
  title =        "The Multimedia {CAI} Software for Engineering
                 Drawing",
  author =       "Zongyi Zuo and Bing Chen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-74",
  abstract =     "An integrated Multimedia CAI system of Engineering
                 Drawing is working in Guangdong University of
                 Technology. It includes a set of multimedia software of
                 a teaching system, exercise system, test system,
                 management system of Engineering Drawing and a Computer
                 Graphics instruction system that was developed by the
                 research group of the Department of Engineering and
                 Computer Graphics. More than ten multimedia-classes are
                 equipped with large screen projectors connected to PIII
                 computers in the network. Also a big PC computer
                 laboratory is set for students' self-study and
                 exercise. The entire multimedia CAI for Engineering
                 Drawing is carried out to all engineering students.
                 This paper introduces the configuration of the computer
                 network, the developing work of the multimedia
                 software, the methods and the effects of using this
                 system.",
  editor =       "V. Skala",
  keywords =     "Multimedia CAI, Engineering Drawing",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-75,
  year =         "2001",
  title =        "Co-operative and Concurrent Blending Motion
                 Generators",
  author =       "Vincent Bonnafous and Eric Menou and Jean-Pierre
                 Jessel and Rene' Caubet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-75",
  abstract =     "In this paper we will be describing a new animation
                 architecture and its implementation in our system LIVE.
                 This model introduces a new blending layer approach
                 which uses several motion generators on the same
                 character at the same time. Despite the numerous
                 studies done, animating characters or complex objects
                 in a virtual world remains a difficult problem. In
                 fact, the complexity of the data used to represent the
                 characters (often articulated rigid bodies) makes their
                 control by an animator difficult and using only one
                 technique to generate motion tends to limit the quality
                 of the animation. The solution will probably be
                 provided by the co-operative or concurrency use of
                 several motion control methods.",
  editor =       "V. Skala",
  keywords =     "Character animation, articulated rigid bodies, motion
                 generators, kinematics, dynamic, motion blending.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-76,
  year =         "2001",
  title =        "Supporting the Search for the Optimal Location of
                 Facilities",
  author =       "A. Biancardi and R. De Lotto and E. Ferrari",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-76",
  abstract =     "Solving the problem of locating services in a given
                 context requires a mathematical abstraction so that its
                 complexity can be managed by means of an iterative
                 search through context simulations. Keeping the
                 interface between planners and the model within
                 planners' knowledge domain is achieved by an
                 interactive tool that encourages exploration and
                 comparison among different possible solutions.
                 Additionally workspaces, for managing conveniently
                 multiple parameter-sets, are introduced together with
                 other tools to improve the ability of getting a proper
                 insight of each change to the plan.",
  editor =       "V. Skala",
  keywords =     "Urban planning, interactive analysis, visual
                 data-managment.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-77,
  year =         "2001",
  title =        "An Interactive Facial Animation System",
  author =       "Fatih Erol and Ugur G{\"u}d{\"u}bay",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-77",
  abstract =     "In this paper, an interactive facial animation system
                 is described. The system is built on top of the facial
                 animation system developed by Keith Waters, which uses
                 a muscle-based face model. We built an interactive
                 facial animation system on top of it to produce
                 keyframe animations of a synthetic face model. The user
                 creates keyframes interactively by either giving
                 predefined universal expressions to the face (or a
                 meaningful combination of these expressions blended
                 naturally on the face model), or giving expressions to
                 the face by moving the muscle vectors defined on the
                 face model. The user might also change the orientation
                 of the face for a keyframe by rotating it in x- and
                 y-directions. To create intermediate frames, we use
                 cosine interpolation.",
  editor =       "V. Skala",
  keywords =     "Interpolation, keyframing, facial animation,
                 expression blending.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-78,
  year =         "2001",
  title =        "Virtual Environment for Cooperative Assistance in
                 Teleoperation",
  author =       "Olivier Heguy and Nancy Rodriguez and Herve' Luga",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-78",
  abstract =     "In order to help the user to accomplish a task,
                 teleoperation systems have to integrate different tools
                 such as visualization, divers interaction devices,
                 planning tools, etc.... The interface must be able to
                 give complete information of the real world and the
                 user can, using a distributed platform, be helped by
                 others users as well as by autonomous robots. The
                 objective of our project is to combine teleoperation,
                 virtual reality and adaptive systems to improve the
                 control of teleoperation missions.",
  editor =       "V. Skala",
  keywords =     "Teleoperation, adaptive systems, virtual reality,
                 cooperative work.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-79,
  year =         "2001",
  title =        "University Students' Activities in the Development of
                 Multimedia Software",
  author =       "Bing Chen and Zhicong Fu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-79",
  abstract =     "Our university has developed a completed Multimedia
                 CAI system of Engineering Drawing. It is available to
                 all students of Engineering Specialties. This education
                 software has been developed by the Department of
                 Engineering and Computer Graphics led by Prof. Zongyi
                 ZUO. During the process of developing the system
                 especially the software, students played a great part
                 in it. This paper introduces the multimedia software
                 Engineering Drawing and how we developed it.",
  editor =       "V. Skala",
  keywords =     "Multimedia CAI Software, Animation, Image
                 Processing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-8,
  year =         "2001",
  title =        "Rolling Rigid Objects",
  author =       "Thoams Warken and Elmar Sch{\"o}mer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-8",
  abstract =     "Simulating the dynamics of rigid bodies plays an
                 important role in virtual reality applications such as
                 assembly planning and ergonomy studies but also in the
                 field of computer animation. In order to decrease the
                 complexity of the object representations and to
                 increase the accuracy of the simulation algorithms one
                 goal is to deal with objects with curved surfaces
                 directly instead of approximating them by polyhedra.
                 One important aspect of the dynamic behaviour of
                 objects with curved surfaces is the rolling process. In
                 this paper we develop the dynamics equations that
                 describe the rolling motion of arbitrarily shaped rigid
                 objects that are in a one- or two-point contact with an
                 arbitrary surface. As a method to keep track of the
                 pairs of closest points we use techniques from
                 differential geometry.",
  editor =       "V. Skala",
  keywords =     "Rolling motion, constraint based simulation.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-80,
  year =         "2001",
  title =        "Optimizing Ray-Tracing for Complex Solids",
  author =       "J. J. Jime'nez and R. J. Segura and F. R. Feito",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-80",
  abstract =     "Ray-tracing algorithm is very used to produce
                 realistic synthetic images in Computer Graphics. Yet,
                 the time necessary to obtain images of high quality is
                 very high. The bottleneck of this method of
                 visualization appears when the intersection between the
                 rays and the differents objects appearing in the scene,
                 is computed. In this paper, an implementation of the
                 ray-tracing method is presented, using a robust and
                 efficient algorithm to determine the intersection
                 between a ray and a polygon in 3D. We think that the
                 use of this algorithm could decrease the time necessary
                 to produce realistic synthetic images.",
  editor =       "V. Skala",
  keywords =     "Visualization, ray-tracing, intersection, realistic
                 images.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-81,
  year =         "2001",
  title =        "Interactive Data Exploration with Customized Glyphs",
  author =       "Martin Kraus and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-81",
  abstract =     "We present a visualization system allowing
                 non-programmers to visualize, explore, and analyze
                 unknown multivariate data by designing an appropriate
                 glyph representation with minimal user interaction.
                 Nonetheless, our system is powerful enough to allow the
                 user to generate a great variety of glyphs. Our tool is
                 implemented as a set of modules with graphical user
                 interfaces for the IRIS Explorer using standard data
                 types. Therefore, it is one of the few visualization
                 systems enabeling non-programmers to define almost
                 arbitrarily complex glyphs, to generate many views of
                 the same data set with minimal effort, and to integrate
                 these views into existing applications based on the
                 IRIS Explorer. Our experiences with an application of
                 this tool in the automotive industry are briefly
                 reported.",
  editor =       "V. Skala",
  keywords =     "Data mining, glyphs, information visualization,
                 multivariate data.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-82,
  year =         "2001",
  title =        "Exploring 3{D}-Models in a Stereoscopic Way: {A} Tool
                 for Knowledge, Documentation and Measurement of
                 Cultural Heritage",
  author =       "L. Menci and F. Ceccaroni and P. Salonia",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-82",
  abstract =     "The paper presents some applications realised with the
                 digital stereoscopic navigation system on two
                 architectural subjects and particularly on documentary
                 photograms of mural paintings. The system StereoSpace
                 is a three-dimensional stereoscopic navigator,
                 specifically thought to be used in digital
                 photogrammetry.",
  editor =       "V. Skala",
  keywords =     "Stereo-space navigator, 3D models, vector/raster,
                 computer graphics.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-83,
  year =         "2001",
  title =        "Modeling Solids and Surfaces with Sketches: an
                 Empirical Evaluation",
  author =       "Manuel Oliveira and Vladimiro Colaco and Joaquim Jorge
                 and Manuel Fonseca",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-83",
  abstract =     "This paper presents and evaluates a simple editor for
                 modeling solids and surfaces. The editor uses sketches
                 and gestures as the main interaction paradigm. We want
                 to show that sketch-based interaction for creating 3D
                 scenes is more natural and intuitive than conventional
                 approaches.",
  editor =       "V. Skala",
  keywords =     "3D Scene Modeling, Sketch-based interaction, Usability
                 testing.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-84,
  year =         "2001",
  title =        "A {DSP} Implementation of an {AOM} and its Application
                 to Defects Detection in Textile Material",
  author =       "F. Ibarra-Pico and F. Garcia-Crespi and S. A.
                 Cuenca-Asensi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-84",
  abstract =     "This paper explains a method of defects detection in
                 textile material using a DSP. This Supervised Learning
                 method will allow the detection of defects in anyone of
                 the phases of production. An algorithm of pattern
                 classification based on minimum distance is used to
                 carry out this method. Scalar distance in an
                 Associative Orthogonal Memory (AOM network) is used to
                 provide a measure of the angle which form the 2
                 compared vectors too. In our system, we can appreciate
                 that the method doesn't require an excessive processing
                 time, so we can implement it for real time processing.
                 Other advantage of the system is that it is applied to
                 different types of clothes and defects (In general,
                 other approaches are centred in only one type of
                 defect). In the other hand, our algorithms produce
                 rates of success around 94%. These results are quite
                 encouraging if we keep in mind that it has been
                 analysed some complex cloth types (such as lined
                 cloth). To finish, and since the results obtained both
                 in error rate and in execution times have been quite
                 good, the application of this method can be very
                 advantageous, moreover knowing that the development
                 environment used is relatively simple.",
  editor =       "V. Skala",
  keywords =     "Defects detection, automatic inspection, quality
                 control, visual inspection.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-85,
  year =         "2001",
  title =        "Multiresolution Flow Visualization",
  author =       "Bruno Jobard and Wilfrid Lefer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-85",
  abstract =     "Flow visualization has been an active research field
                 for several years and various techniques have been
                 proposed to visualize vector fields, streamlines and
                 textures being the most effective and popular ones.
                 While streamlines are suitable to get rough information
                 on the behavior of the flow, textures depict the flow
                 properties at the pixel level. Depending on the
                 situation the suitable representation could be
                 streamlines or texture. This paper presents a method to
                 compute a sequence of streamline-based images of a
                 vector field with different densities, ranging from
                 sparse to texture like representations. It is based on
                 an effective streamline placement algorithm and a
                 production scheme that recalls those used in the
                 multiresolution theory. Indeed a streamline defined at
                 level J of the hierarchy is defined for all levels J' >
                 J. A viewer allows us to interactively select the
                 desired density while zooming in and out in a vector
                 field. The density of streamlines in the image can also
                 be automatically computed as a function of a derived
                 quantity, such as velocity or vorticity.",
  editor =       "V. Skala",
  keywords =     "Flow Visualization, Streamlines, Multiresolution
                 Representation, Interactive Visualization, Large Vector
                 Fields.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-86,
  year =         "2001",
  title =        "Multiresolution and Shape Optimization of Implicit
                 Skeletal Model",
  author =       "S. Prevost and L. Lucas and E. Bittar",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-86",
  abstract =     "Display of large volumes, progressive rendering and
                 selective refinements are some of the operations
                 supported by multiresolution technology. In this paper,
                 a general framework relying on the use of such
                 techniques applied to volume data rendering is
                 presented. Based on a pyramidal representation of data,
                 two aspects of our work are considered. First, the
                 decimation algorithm itself is described. The general
                 principle consists in gradually removing nodes of a
                 structural graph previously established while
                 respecting constraints. A second part introduces the
                 refinement of preliminary obtained Levels of Details
                 (LOD). The problem is to preserve as well as possible
                 the initial volume of studied objects. The goal is to
                 build an interactive system of visualization for the
                 analysis of volumetric data. The speed of treatments
                 associated with a good visualization should enable to
                 achieve a 3D survey of a natural object in a
                 quasi-interactive manner. The method has been
                 successfully applied to both synthetic and real data
                 (medical imaging).",
  editor =       "V. Skala",
  keywords =     "Volume data visualization, skeleton shape description,
                 implicit surface, multiresolution, graph
                 representation, shape simplification, optimization,
                 genetic algorithms.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-87,
  title =        "Multimodale Recherchezug{\"{a}}nge: Neue Wege bei der
                 Konzeption der integrierten Informationssysteme
                 {ELVIRA} und {GESINE}",
  editor =       "Oberquelle and Horst; Oppermann and Rainer; Krause and
                 J{\"{u}}rgen",
  number =       "55",
  series =       "Berichte des German Chapter of the ACM",
  booktitle =    "Mensch & Computer, 1. Fach{\"{u}}bergreifende
                 Konferenz vom 5.-8.M{\"{a}}rz 2001 in Bad Honnef
                 (Bonn).",
  publisher =    "B.G.Teubner",
  pages =        "326--335",
  year =         "2001",
  author =       "Maximilian Eibl and Maximilian Stempfhuber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-87",
  abstract =     "In diesem Artikel wird die Konzeption der
                 Retrievalsysteme ELVIRA und GESINE beschrieben. Hierbei
                 soll neben der Beschreibung der verschiedenen
                 Recherchezug{\"{a}}nge der Fokus auf der Konzeption
                 nicht-traditioneller Zug{\"{a}}nge liegen. Ein solcher
                 Zugang besteht in einer Visualisierung, die im Rahmen
                 einer Kooperation von Softwareergonomie und Graphik
                 Design am Informationszentrum Sozialwissenschaften,
                 Bonn, geschaffen wurde und ergonomischen wie
                 {\"{a}}sthetischen Anspr{\"{u}}chen gen{\"{u}}gen soll.
                 Ein anderer Alternativzugang ist ein tabellenbasierter
                 visueller Formalismus, der speziell f{\"{u}}r die
                 Recherche {\"{u}}ber mehrere heterogene
                 Datenbest{\"{a}}nde entworfen wurde.",
  organization = "German Chapter of the ACM",
  keywords =     "Visualisierung, Information retrieval, Dokument
                 Retrieval, HCI, Usability, Graphik Design, Design,
                 {\"{A}}sthetik",
}

@InProceedings{EVL-2001-88,
  pages =        "15--22",
  year =         "2001",
  title =        "Visual Simulation of Smoke",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-88",
  author =       "Ronald Fedkiw and Jos Stam and Henrik Wann Jensen",
  abstract =     "In this paper, we propose a new approach to numerical
                 smoke simulation for computer graphics applications.
                 The method proposed here exploits physics unique to
                 smoke in order to design a numerical method that is
                 both fast and efficient on the relatively coarse grids
                 traditionally used in computer graphics applications
                 (as compared to the much finer grids used in the
                 computational fluid dynamics literature). We use the
                 inviscid Euler equations in our model, since they are
                 usually more appropriate for gas modeling and less
                 computationally intensive than the viscous
                 Navier-Stokes equations used by others. In addition, we
                 introduce a physically consistent vorticity confinement
                 term to model the small scale rolling features
                 characteristic of smoke that are absent on most coarse
                 grid simulations. Our model also correctly handles the
                 interaction of smoke with moving objects.",
  editor =       "Eugene Fiume",
  keywords =     "Smoke, computational fluid dynamics, Navier-Stokes
                 equations, Euler equations, Semi-Lagrangian methods,
                 stable fluids, vorticity confinement, participating
                 media",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-89,
  pages =        "23--30",
  year =         "2001",
  title =        "Practical Animations of Liquids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-89",
  author =       "Nick Foster and Ronald Fedkiw",
  abstract =     "We present a general method for modeling and animating
                 liquids. The system is specifically designed for
                 computer animation and handles viscous liquids as they
                 move in a 3D environment and interact with graphics
                 primitives such as parametric curves and moving
                 polygons. We combine an appropriately modified
                 semi-Lagrangian method with a new approach to
                 calculating fluid flow around objects. This allows us
                 to efficiently solve the equations of motion for a
                 liquid while retaining enough detail to obtain
                 realistic looking behavior. The object interaction
                 mechanism is extended to provide control over the
                 liquid's 3D motion. A high quality surface is obtained
                 from the resulting velocity field using a novel
                 adaptive technique for evolving an implicit surface",
  editor =       "Eugene Fiume",
  keywords =     "Animation, computational fluid dynamics, implicit
                 surface, level set, liquids, natural phenomena,
                 Navier-Stokes, particles, semi-Lagrangian",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-9,
  year =         "2001",
  title =        "Quantitively Comparing Virtual and Real Draping of
                 Clothes",
  author =       "A. Bottino and A. Laurentini and S. Scalabrin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-9",
  abstract =     "Several cloth modeling techniques have been proposed.
                 Their main purpose is the realistic simulation of the
                 garments of virtual actors for entertainment and
                 advertising. In these areas, the main concern is to
                 construct fast algorithms, able to produce impressive
                 and qualitatively satisfactory draping of clothes.
                 Other applications, as designing, manufacturing and
                 selling real garments, call for a virtual draping which
                 not only satisfies the human eye, but also closely
                 mimics the real physical draping of clothes. From this
                 viewpoint we will discuss some modeling techniques, and
                 present some result concerning the quantitative
                 comparison between the draping of real clothes and
                 their virtual counterpart.",
  editor =       "V. Skala",
  keywords =     "Computer graphics, virtual draping algorithms,
                 Kawabata tests, quantitative comparison.",
  booktitle =    "WSCG 2001 Conference Proceedings",
}

@InProceedings{EVL-2001-90,
  pages =        "31--36",
  year =         "2001",
  title =        "Dynamic Real-Time Deformations Using Space & Time
                 Adaptive Sampling",
  author =       "Gilles Debunne and Mathieu Desbrun and Marie-Paule
                 Cani and Alan H. Barr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-90",
  abstract =     "This paper presents a robust, adaptive method for
                 animating dynamic visco-elastic deformable objects that
                 provides a guaranteed frame rate. Our approach uses a
                 novel automatic space and time adaptive level of detail
                 technique, in combination with a large-displacement
                 (Green) strain tensor formulation. The body is
                 partitioned in a non-nested multiresolution hierarchy
                 of tetrahedral meshes. The local resolution is
                 determined by a quality condition that indicates where
                 and when the resolution is too coarse. As the object
                 moves and deforms, the sampling is refined to
                 concentrate the computational load into the regions
                 that deform the most. Our model consists of a
                 continuous differential equation that is solved using a
                 local explicit finite element method. We demonstrate
                 that our adaptive Green strain tensor formulation
                 suppresses unwanted artifacts in the dynamic behavior,
                 compared to adaptive mass-spring and other adaptive
                 approaches. In particular, damped elastic vibration
                 modes are shown to be nearly unchanged for several
                 levels of refinement. Results are presented in the
                 context of a virtual reality system. The user interacts
                 in real-time with the dynamic object through the
                 control of a rigid tool, attached to a haptic device
                 driven with forces derived from the method.",
  editor =       "Eugene Fiume",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-91,
  pages =        "37--46",
  year =         "2001",
  title =        "Optimization-Based Animation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-91",
  author =       "Victor J. Milenkovic and Harald Schmidl",
  abstract =     "Current techniques for rigid body simulation run
                 slowly on scenes with many bodies in close proximity.
                 Each time two bodies collide or make or break a static
                 contact, the simulator must interrupt the numerical
                 integration of velocities and accelerations. Even for
                 simple scenes, the number of discontinuities per frame
                 time can rise to the millions. An efficient
                 optimization-based animation (OBA) algorithm is
                 presented which can simulate scenes with many convex
                 three-dimensional bodies settling into stacks and other
                 {"}crowded{"} arrangements. This algorithm simulates
                 Newtonian (second order) physics and Coulomb friction,
                 and it uses quadratic programming (QP) to calculate new
                 positions, momenta and accelerations strictly at frame
                 times. Contact points are synchronized at the end of
                 each frame. The extremely small integration steps
                 inherent to traditional simulation techniques are
                 avoided. Non-convex bodies are simulated as unions of
                 convex bodies. Links and joints are simulated
                 successfully with bi-directional constraints. A hybrid
                 of OBA and retroactive detection (RD) has been
                 implemented as well. A review of existing work finds no
                 other packages that can simulate similarly complex
                 scenes in a practical amount of time.",
  editor =       "Eugene Fiume",
  keywords =     "Animation, Animation w/Constraints, Physically Based
                 Animation, Physically Based Modeling, Scientific
                 Visual-ization, Solid Modeling",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-92,
  pages =        "47--56",
  year =         "2001",
  title =        "Kizamu: {A} System for Sculpting Digital Characters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-92",
  author =       "Ronald N. Perry and Sarah F. Frisken",
  abstract =     "This paper presents Kizamu, a computer-based sculpting
                 system for creating digital characters for the
                 entertainment industry. Kizamu incorporates a blend of
                 new algorithms, significant technical advances, and
                 novel user interaction paradigms into a system that is
                 both powerful and unique. To meet the demands of
                 high-end digital character design, Kizamu addresses
                 three requirements posed to us by a major production
                 studio. First, animators and artists want digital clay
                 - a medium with the characteristics of real clay and
                 the advantages of being digital. Second, the system
                 should run on standard hardware at interactive rates.
                 Finally, the system must accept and generate standard
                 3D representations thereby enabling integration into an
                 existing animation production pipeline At the heart of
                 the Kizamu system are Adaptively Sampled Distance
                 Fields (ADFs), a volumetric shape representation with
                 the characteristics required for digital clay. In this
                 paper, we describe the system and present the major
                 research advances in ADFs that were required to make
                 Kizamu a reality.",
  editor =       "Eugene Fiume",
  keywords =     "Digital sculpting, graphics systems, character design,
                 rendering, ADFs, distance fields, volume modeling,
                 triangulation",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-93,
  pages =        "57--66",
  year =         "2001",
  title =        "Feature-Sensitive Surface Extraction From Volume
                 Data",
  author =       "Leif P. Kobbelt and Mario Botsch and Ulrich Schwanecke
                 and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-93",
  abstract =     "The representation of geometric objects based on
                 volumetric data structures has advantages in many
                 geometry processing applications that require, e.g.,
                 fast surface interrogation or boolean operations such
                 as intersection and union. However, surface based
                 algorithms like shape optimization (fairing) or
                 freeform modeling often need a topological manifold
                 representation where neighborhood information within
                 the surface is explicitly available. Consequently, it
                 is necessary to find effective conversion algorithms to
                 generate explicit surface descriptions for the geometry
                 which is implicitly defined by a volumetric data set.
                 Since volume data is usually sampled on a regular grid
                 with a given step width, we often observe severe alias
                 artifacts at sharp features on the extracted surfaces.
                 In this paper we present a new technique for surface
                 extraction that performs feature sensitive sampling and
                 thus reduces these alias effects while keeping the
                 simple algorithmic structure of the standard Marching
                 Cubes algorithm. We demonstrate the effectiveness of
                 the new technique with a number of application examples
                 ranging from CSG modeling and simulation to surface
                 reconstruction and remeshing of polygonal models.",
  editor =       "Eugene Fiume",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-94,
  pages =        "67--76",
  year =         "2001",
  title =        "Reconstruction and Representation of 3{D} Objects With
                 Radial Basis Functions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-94",
  author =       "Jonathan C. Carr and Richard K. Beatson and Jon B.
                 Cherrie and Tim J. Mitchell and W. Richard Fright and
                 Bruce C. McCallum and Tim R. Evans",
  abstract =     "We use polyharmonic Radial Basis Functions (RBFs) to
                 reconstruct smooth, manifold surfaces from point-cloud
                 data and to repair incomplete meshes. An object's
                 surface is defined implicitly as the zero set of an RBF
                 fitted to the given surface data. Fast methods for
                 fitting and evaluating RBFs allow us to model large
                 data sets, consisting of millions of surface points, by
                 a single RBF - previously an impossible task. A greedy
                 algorithm in the fitting process reduces the number of
                 RBF centers required to represent a surface and results
                 in significant compression and further computational
                 advantages. The energy-minismation characterisation of
                 polyharmonic splines result in a {"}smoothest{"}
                 interpolant. This scale-independent characterisation is
                 well-suited to reconstructing surfaces from
                 non-uniformly sampled data. Holes are smoothly filled
                 and surfaces smoothly extrapolated. We use a
                 non-interpolating approximation when the data is noisy.
                 The functional representation is in effect a solid
                 model, which means that gradients and surface normals
                 can be determined analytically. This helps generate
                 uniform meshes and we show that the RBF representation
                 has advantages for mesh simplification and remeshing
                 applications. Results are presented for real-world
                 rangefinder data.",
  editor =       "Eugene Fiume",
  keywords =     "Variational implicit surfaces, Radial Basis Function,
                 RBF, mesh repair, point-cloud surfacing, surface
                 reconstruction, geometry compression, solid modeling",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-95,
  pages =        "77--86",
  year =         "2001",
  title =        "Reliable Two-Dimensional Graphing Methods for
                 MathematicalFormulae with Two Free Variables",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-95",
  author =       "Jeff Tupper",
  abstract =     "This paper presents a series of new algorithms for
                 reliably graphing two-dimensional implicit equations
                 and inequalities. A clear standard for interpreting the
                 graphs generated by two-dimensional graphing soft are
                 is introduced and used to evaluate the presented
                 algorithms. The first approach presented uses a
                 standard interval arithmetic library. This approach is
                 shown to be faulty; an analysis of the failure reveals
                 a limitation of standard interval arithmetic.
                 Subsequent algorithms are developed in parallel with
                 improvements and extensions to the interval arithmetic
                 used by the graphing algorithms. Graphs exhibiting a
                 variety of mathematical and artistic phenomena are
                 shown to be graphed correctly by the presented
                 algorithms. A brief comparison of the final algorithm
                 presented to other graphing algorithms is included.",
  editor =       "Eugene Fiume",
  keywords =     "Interval arithmetic, Tupper interval arithmetic,
                 interval analysis, implicit curves, algebraic curves,
                 graphing, relation graphing, formula graphing, GrafEq",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-96,
  year =         "2001",
  title =        "Scanning Physical Interaction Behavior of 3{D}
                 Objects",
  author =       "Dinesh K. Pai and Kees van den Doel and Doug L. James
                 and Jochen Lang and John E. Lloyd and Joshua L.
                 Richmond and Som H. Yau",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-96",
  abstract =     "We describe a system for constructing computer models
                 of several aspects of physical interaction behavior, by
                 scanning the response of real objects. The behaviors we
                 can successfully scan and model include deformation
                 response, contact textures for interaction with
                 force-feedback, and contact sounds. The system we
                 describe uses a highly automated robotic facility that
                 can scan behavior models of whole objects. We provide a
                 comprehensive view of the modeling process, including
                 selection of model structure, measurement, estimation,
                 and rendering at interactive rates. The results are
                 demonstrated with two examples: a soft stuffed toy
                 which has significant deformation behavior, and a hard
                 clay pot which has significant contact textures and
                 sounds. The results described here make it possible to
                 quickly construct physical interaction models of
                 objects for applications in games, animation, and
                 e-commerce.",
  editor =       "Eugene Fiume",
  keywords =     "Behavioral Animation, Deformations, Haptics,
                 Multimedia, Physically Based Modeling, Robotics, Sound
                 Visualization",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-97,
  pages =        "97--106",
  year =         "2001",
  title =        "Synthesizing Bidirectional Texture Functions for
                 Real-World Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-97",
  author =       "Xinguo Liu and Yizhou Yu and Heung-Yeung Shum",
  abstract =     "In this paper, we present a novel approach to
                 synthetically generating bidirectional texture
                 functions (BTFs) of real-world surfaces. Unlike a
                 conventional two-dimensional texture, a BTF is a
                 six-dimensional function that describes the appearance
                 of texture as a function of illumination and viewing
                 directions. The BTF captures the appearance change
                 caused by visible small-scale geometric details on
                 surfaces. From a sparse set of images under different
                 viewing/lighting settings, our approach generates BTFs
                 in three steps. First, it recovers approximate 3D
                 geometry of surface details using a shape-from-shading
                 method. Then, it generates a novel version of the
                 geometric details that has the same statistical
                 properties as the sample surface with a non-parametric
                 sampling method. Finally, it employs an appearance
                 preserving procedure to synthesize novel images for the
                 recovered or generated geometric details under various
                 viewing/lighting settings, which then define a BTF. Our
                 experimental results demonstrate the effectiveness of
                 our approach.",
  editor =       "Eugene Fiume",
  keywords =     "Bidirectional Texture Functions, Reflectance and
                 Shading Models, Texture Synthesis, Shape-from-Shading,
                 Photo-metric Stereo, Image-Based Rendering",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-98,
  pages =        "107--116",
  year =         "2001",
  title =        "Image-Based Rendering of Diffuse, Specular and Glossy
                 Surfaces From a Single Image",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-98",
  author =       "Samuel Boivin and Andr{\'{e}} Gagalowicz",
  abstract =     "In this paper, we present a new method to recover an
                 approximation of the bidirectional reflectance
                 distribution function (BRDF) of the surfaces present in
                 a real scene. This is done from a single photograph and
                 a 3D geometric model of the scene. The result is a full
                 model of the reflectance properties of all surfaces,
                 which can be rendered under novel illumination
                 conditions with, for example, viewpoint modification
                 and the addition of new synthetic objects. Our
                 technique produces a reflectance model using a small
                 number of parameters. These parameters nevertheless
                 approximate the BRDF and allow the recovery of the
                 photometric properties of diffuse, specular, isotropic
                 or anisotropic textured objects. The input data are a
                 geometric model of the scene including the light source
                 positions and the camera properties, and a single image
                 captured using this camera. Our algorithm generates a
                 new synthetic image using classic rendering techniques,
                 and a lambertian hypothesis about the reflectance model
                 of the surfaces. Then, it iteratively compares the
                 original image to the new one, and chooses a more
                 complex reflectance model if the difference between the
                 two images is greater than a user-defined threshold. We
                 present several synthetic images that are compared to
                 the original ones, and some possible applications in
                 augmented reality.",
  editor =       "Eugene Fiume",
  keywords =     "Image-Based Rendering, Reflectance Recovery, BRDF
                 Models, Radiance, Radiosity, Rendering, Inverse
                 Rendering, Rerendering, Global Illumination",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}

@InProceedings{EVL-2001-99,
  pages =        "117--128",
  year =         "2001",
  title =        "A Signal-Processing Framework for Inverse Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-2001-99",
  author =       "Ravi Ramamoorthi and Pat Hanrahan",
  abstract =     "Realism in computer-generated images requires accurate
                 input models for lighting, textures and BRDFs. One of
                 the best ways of obtaining high-quality data is through
                 measurements of scene attributes from real photographs
                 by inverse rendering. However, inverse rendering
                 methods have been largely limited to settings with
                 highly controlled lighting. One of the reasons for this
                 is the lack of a coherent mathematical framework for
                 inverse rendering under general illumination
                 conditions. Our main contribution is the introduction
                 of a signal-processing framework which describes the
                 reflected light field as a convolution of the lighting
                 and BRDF, and expresses it mathematically as a product
                 of spherical harmonic co-efficients of the BRDF and the
                 lighting. Inverse rendering can then be viewed as
                 deconvolution. We apply this theory to a variety of
                 problems in inverse rendering, explaining a number of
                 previous empirical results. We will show why certain
                 problems are ill-posed or numerically ill-conditioned,
                 and why other problems are more amenable to solution.
                 The theory developed here also leads to new practical
                 representations and algorithms. For instance, we
                 present a method to factor the lighting and BRDF from a
                 small number of views, i.e. to estimate both
                 simultaneously when neither is known.",
  editor =       "Eugene Fiume",
  keywords =     "Signal Processing, Spherical Harmonics, Inverse
                 Rendering, Radiance, Light Field, Irradiance,
                 Illumination, BRDF",
  series =       "Annual Conference Series",
  booktitle =    "SIGGRAPH 2001, Computer Graphics Proceedings",
  publisher =    "ACM Press / ACM SIGGRAPH",
}
