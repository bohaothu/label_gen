%
% (Compiled by) 1993
%
%   Wolfgang Schreiner <Wolfgang.Schreiner at risc.uni-linz.ac.at>
%   Research Institute for Symbolic Computation (RISC-Linz)
%   Johannes Kepler University, A-4040 Linz, Austria
%
% PLEASE DO NOT REMOVE THIS NOTICE.
%

@inproceedings{Rabh93,
author = {Rabhi, Fethi A.},
title = {{Functional Languages and Parallelism}},
booktitle = {3rd Workshop on Parallel Computing},
address = {Polytechnique de Mons, Belgium, September 23--24},
year = {1993},
abstract = {
This document explains the relation between functional languages and
parallelism and presents some of the techniques used to execute functional
programs on multi-processors without intervention from the programmer. It is
intended to be an introduction to non-functional programmers and contains 100
references for readers interested in a particular aspect. The document is
divided as follows: BACKGROUND (introduces functional languages and their
theoretical foundations), PARALLEL GRAPH REDUCTION (describes some of the
issues related to evaluating functional programs in parallel), THE
``PARADIGM-ORIENTED'' SOLUTION (describes the relationship between parallel
programming paradigms and functional languages and how it could be used to
improve the performance of implementations).
},
keywords = {Survey, Parallel Graph Reduction, Skeletons},
}

@incollection{Rabh93a,
author = {Rabhi, F. A.},
title = {{Exploiting Parallelism in Functional Languages: A
``Paradigm-Oriented'' Approach}},
booktitle = {Abstract Machine Models for Highly Parallel Computers},
editor = {Lake, T. and Dew, P.},
publisher = {Oxford University Press},
year = {1993},
abstract = {
Deriving parallelism automatically from functional programs is simple in
theory but very few practical implementations have been realised. Programs may
contain too little or too much parallelism causing a degradation in
perrformance. Such parallelism could be more efficiently controlled if {\em
parallel algorithmic structures\/} (or skeletons) are used in the design of
algorithms. A structure captures the behaviour of a {\em parallel programming
paradigm\/} and acts as a template in the design of an algorithm. This paper
presents some important parallel programming paradigms and defines a structure
for each of these paradigms. The iterative transformation paradigm (or
geometric parallelism) is discussed in detail and a framework under which
programs can be developed and transformed into efficient and portable
implementations is presented.
},
keynote = {Algorithmic Skeletons},
}

@incollection{RaSc93,
author = {Rabhi, F. W. and Schwarz, J.},
title = {{``Paradigm-Oriented'' Design of Parallel
Iterative Programs Using Functional Languages}},
booktitle = {Applications of Supercomputers in Engineering III}, 
editor = {Brebbia, C. A. and Power, H.},
series = {Computational Mechanics Publications},
publisher = {Elsevier Applied Science}, 
year = {1993},
abstract = {
Functional languages offer a high degree of abstraction to the progammer while
containing a great deal of implicit parallelism. This parallelism could be
efficiently exploited if {\em parallel algorithmic structures\/} were used in
the design of algorithms. A structure captures the behaviour of a parallel
programming paradigm and acts as a template in the design of an algorithm.
This paper addresses the issue of defining a structure for static iterative
transformation (SIT) algorithms which are coarse-grained data parallel
algorithms with an iterative control structure. The parameters required by the
structure are supplied by the programmer using a functional language, forming
the problem specification. This specification can then be successively turned
into a sequential functional program, then into a parallel program for a graph
reduction machine, and finally into a program that maps on a specific parallel architecture.
},
keywords = {Algorithmic Skeletons, Parallel Graph Reduction},
}

@inproceedings{Rabh93c,
author = {Rabhi, F. A.},
title = {{Run-time Control of the Granularity in Functional Languages}},
booktitle = {European Workshop on Parallel Computing},
address = {Barcelona, Spain, March},
year = {1992},
editor = {Joosen, W. and Milgrom, E.},
publisher = {IOS Press},
pages = {356--359},
abstract = {
This reference examines the issue of controlling the granularity of programs
according to the sequential complexity of tasks.
},
keywords = {Granularity Control},
}

@article{RaMa91,
author = {Rabhi, F. A. and Manson, G. A.},
title = {{Divide-and-Conquer and Parallel Graph Reduction}},
journal = {Parallel Computing},
volume = {17},
year = {1991},
publisher = {North Holland},
address = {Amsterdam},
pages = {189--205},
abstract = {
This reference describes a mechanism of controlling the execution of
functional programs with a Divide-and-Conquer type of parallelism.
},
keywords = {Parallel Graph Reduction, Program Control},
}

@article{Rabh91,
author = {Rabhi, Fethi A.},
title = {{Experiments with a Transputer-Based Parallel Graph Reduction
Machine}}, 
journal = {Concurrency: Practice and Experience},
volume = {3},
number = {4},
pages = {413--422},
month = {August},
year = {1991},
abstract = {This paper is concerned with the implementation of functional
languages on a parallel architecture, using graph reduction as a model of
computation. Parallelism in such systems is automatically derived by the
compiler but a major problem is the fine granularity, illustrated in
Divide-and-Conquer problems at the leaves of the computational tree. The paper
addresses this issue and proposes a method based on static analysis combined
with run-time tests to remove the excess in parallelism. We report experiments
on a prototype machine, simulated on several connected INMOS transputers.
Performance figures show the benefits in adopting the method and the
difficulty of automatically deriving the optimum partitioning due to
differences among the problems.},
keywords = {Parallel Graph Reduction, Granularity Control},
}

@techreport{Blel93,
author = {Blelloch, Guy E.},
title = {{Nesl: A Nested Data-Parallel Language (Version 2.6)}},
type = {Technical Report},
institution = {School of Computer Science},
address = {Carnegie Mellon University},
number = {CMU-CS-93-129},
month = {April},
year = {1993},
abstract = {This report describes NESL, a strongly-typed, applicative,
data-parallel language. NESL is intended to be used as a portable interrface
for programming a variety of parallel and vector supercomputers, and as a
basis for teaching parallel algorith.s Parallelism is supplied through a
simple set of data-parallel constructs based on sequences (ordered sets),
including a mechanism for applying any function over the elements of a
sequence in parallel and a rich set of parallel functions that manipulate
sequences. NESL fully supports nested sequences and nested parallelism --- the
ability to take a parallel function and apply it over multiple instances in
parallel. Nested parallelism is important for implementing algorithms with
complex and dynamically changing data structures, such as required in many
graph and sparse matrix algorithms. NASL also provides a mechanism for
calculating the asymptotic running time for a program on various parallel
machine models, including the parallel random access machine (PRAM). This is
useful for estimating running times of algorithms on actual machines and, when
teaching algorithms, for supplying a close correspondence beteen the code and
the theoretical complexity. This report defines NESL and describes several
examples of algorithms coded in the language. The examples include algorithms
for median finding, sorting, string searching, finding prime numbers, and
finding a planar convex hull. NESL currently compiles to an intermediate
language called VCODE, which runs on the CRAY Y-MP, Connection Machine CM-2,
and Encore Multimax. For many algorithms, the current implementation gives
performance close to optimized machine-specific code for these machines.
},
keywords = {Nested Data Parallelism},
}

@inproceedings{BCH*93,
author = {Blelloch, Guy E. and Chatterjee, Siddharta and Jonathan, C.
Hardwick and Sipelstein, Jay and Zagha, Marco},
title = {{Implementation of a Portable Nested Data-Parallel Language}},
booktitle = {Fourth ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming PPoPP},
address = {San Diego, California, May 19--22},
year = {1993},
pages = {102--112},
publisher = {ACM Press},
abstract = {This paper gives an overview of the implementation of NESL, a
portable nested data-parallel language. This language and its implementation
are the first to fully support nested data structures as well as nested
data-parallel function calls. These features allow the concise description of
parallel algorithms on irregular data, such as sparse matrices and graphs. In
addition, they maintain the advantages of data-parallel languages: a simple
programming model and portability. The current NESL implementation is based on
an intermediate language called VCODE and a library of vectore routines called
CVL. It runs on the Connection Machine CM-2, the Cray Y-MP C90, and serial
machines. We compare initial benchmark results of NESL with those of
machine-specific code on these machines for three algorithms: least-squares
line-fitting, median finding, and a sparse-matrix vector product. These
results show that NESL's performance is competitive with that of
machine-specific codes for regular dense data, and is often superior for
irregular data.},
keywords = {Nested Data Parallelism},
}

@techreport{WSYR86,
author = {Welcome, M. L. and Skedzielewski, S. K. and Yates, R. K.  and
Ranelletti, J. E.},
title = {{IF2: An Applicative Language Intermediate Form with
Explicit Memory Management}},
number = {{M}-195},
type = {Manual},
institution = {University of California Lawrence Livermore National
Laboratory},
month = {November},
year = {1986},
abstract = {This defines an intermediate form that includes IF1 and has
the capability for describing operations on storage.},
keywords = {Intermediate Language},
}

@phdthesis{Cann89,
author = {Cann, D. C.},
title = {{Compilation Techniques for High Performance Applicative
Computation}},
school = {Colorado State University},
address = {Computer Science Department, Fort Collins, CO},
year = {1989},
abstract = {},
keywords = {SISAL, Compilation},
}

@article{BoSa89,
author = {B\"{o}hm, A. P. W.  and Sargeant, J.},
title = {{Code Optimization for Tagged-Token Dataflow Machines}},
journal = {IEEE Transactions on Computers},
month = {January},
year = {1989},
pages = {4-14},
abstract = {},
keywords = {Dataflow, Compilation, SISAL},
}

@article{FCO90,
author = {Feo, J. T. and Cann, D. C. and Oldehoeft, R. R.},
title = {{A Report on the {SISAL} Language Project}},
journal = {Journal of Parallel and Distributed Computing},
volume = {10},
number = {4},
month = {December}, 
year = {1990},
pages = {349-366},
abstract = {},
keywords = {SISAL},
}

@article{OlMc90,
author = {Oldehoeft, R. R. and McGraw, J. R.},
title = {{Mixed Applicative and Imperative Programs}},
year = {1990},
journal = {Parallel Computing},
volume = {13},
number = {2},
pages = {175-191},
abstract = {A modular organization for SISAL allows interface with
conventional libraries.},
keywords = {SISAL},
}

@conference{SaCa90,
author = {Sarkar, V. and Cann, D.},
title = {{POSC --- A Partitioning and Optimizing SISAL Compiler}},
booktitle = {Proceedings of the ACM International Conference on
Supercomputing},
year = {1990},
month = {June},
abstract = {},
keywords = {Compilation, SISAL},
}

@conference{Cann91a,
author = {Cann, D. C.},
title = {{Vectorization of an Applicative Language: Current Results and Future
Directions}},
booktitle = {Compcon 91},
year = {1991},
month = {February},
pages = {396-402},
abstract = {},
keywords = {Compilation, SISAL},
}

@article{Cann92,
author = {Cann, D. C.},
title = {{Retire {F}ortran?  {A} Debate Rekindled}},
journal = {Communications of the ACM},
year = {1992},
volume = {35},
number = {8},
pages = {81-89},
month = {August},
abstract = {We compare the execution performance of Sisal, a functional
language for large-scale scientific computing, and Fortran on a Cray Y-MP/864.
We examine the functional programming paradigm, discuss its attribute and
advantages, and highlight the alient features of Sisal. In the remaining
sections we illustrate the potential inefficiencies of functional computing,
present our most recent performance data, and give some closing remarks
regarding Fortran and the future of high-speed computing.
},
keywords = {SISAL, Single Assignment, Performance Measurements},
}

@techreport{BCFO91,
author = {B\"{o}hm, A. P. W. and Cann, D. C. and Feo, J. T. and Oldehoeft, 
R. R.},
title = {{SISAL 2.0 Reference Manual}},
institution = {Computer Science Department, Colorado State University},
address = {Fort Collins, CO},
number = {CS-91-118},
month = {November},
year = {1991},
abstract = {},
keywords = {SISAL, Single Assignment},
}

@inproceedings{RNW93,
author = {Roh, Lucas and Najjar, Walid A. and Wim B{\"o}hm, A. P.},
title = {{Generation and Quantitative Evaluation of Dataflow Clusters}},
booktitle = {FPCA '93 Conference on Functional Programming Languages and
Computer Architecture},
address = {Copenhagen, Denmark, June 9--11},
year = {1993},
publisher = {ACM Press},
pages = {159--168},
abstract = {Mulithreaded or hybrid von Neumann/dataflow execution models have
an advantage over the fine-grain dataflow model in that they significantly
reduce the run time overhead incurred by matching. In this paperr, we look at
two issues related to the evaluation of a coarse-grain dataflow model of
execution. The first issue concerns the compilation into a coarse-grain code
from a fine-grain one. In this study, the concept of coarse-grain code is
captured by {\em clusters\/} which can be thought of as mini-dataflow graphs
which execute strictly, deterministically and without blocking. We look at two
bottom-up algorithms: the {\em basic block\/} and the {\em dependence sets\/}
methods, to partition dataflow graphs into clusters. The second issue is the
actual performance of the cluster-based execution as several architecture
parameters are varied (e.g.\ number of processors, matching costs, network
latency, etc.). From the extensive simulation data we evaluate (1) the
potential speedup over the fine-grain execution and (2) the effects of the
various architecture parameterrs on the coarse-grain execution time, allowing
us to draw conclusions on their effectiveness. The results indicate that even
with a simple bottom-up algorithm for generating clusters, cluster execution
offers a good speedup over the fine-grain execution over a wide range of
architectures. They also indicate that coarse-grain execution is scalable,
tolerrates network latency and high matching cost well; it can benefit from a
higher output bandwidth of a processor and finally, a simple superscalar
processor with the issue rate of two is sufficient to exploit the internal
parallelism of a cluster.},
keywords = {Dataflow, Compilation, Partitioning},
}

@inproceedings{Ang93,
author = {Ang, Boon Seong},
title = {{Efficient Implementation of Sequential Loops in Dataflow
Computation}},
booktitle = {FPCA '93 Conference on Functional Programming Languages and
Computer Architecture},
address = {Copenhagen, Denmark, June 9--11},
year = {1993},
publisher = {ACM Press},
pages = {169--178},
abstract = {The implementation of sequential loops in dataflow computation had
traditionally not received very much attention as it was assumed that most
loops would be executed in parallel. This assumption was valid for earlier
dataflow machines such as the MIT Tagged Token Dataflow Architecture (TTDA),
Sigma-1 but not for the newest generation of dataflow machines including
Monsoon, EM-4 and Epsilon-2. On the latter machines, sequential loops use less
memory, and can execute in fewer instructions, albeit with lower parallelism
than the parallel versions. This characterisation of sequential and parallel
loops suggests that programs should have parallel outer loops and sequential
inner loops. The run time of sequential loops therefore becomes significant in
the overall run time. We also found that previous implementations of
sequential loops can incur fairly high overheads. In this paper, we present
two new ways of implementing sequential loops that have lower overhead than
previous methods. We studied this problem in the context of compiling Id for
Mosoon.}, 
keywords = {Dataflow, Compilation, Granularity},
}

@inproceedings{HaLi93,
author = {Hammarlund, Per and Lisper, Bj{\"o}rn},
title = {{On the Relation between Functional and Data Parallel Programming
Languages}}, 
booktitle = {FPCA '93 Conference on Functional Programming Languages and
Computer Architecture},
address = {Copenhagen, Denmark, June 9--11},
year = {1993},
publisher = {ACM Press},
pages = {210--219},
abstract = {Data parallel programming is becoming an increasingly important
tool for exploiting parallelism in data-intensive applications, especially on
SIMD and vectore computers. Many algorithms appearing in such applications are
very succinctly expressed in data parallel languages: this indicates that data
parallel programming can be a powerful abstract programming paradigm than just
a syntax for explict programming of SIMD computers. The data parallel
languages in practical use today are, however, exponents of exactly the later
point of view: even though they incorporate some elements of abstraction,
their semantics are all to some extent based on a SIMD execution model.
Therefore it is hard to use these languages to express algorithms in the
problem domain in an abstract, machine-independent way. This is likely to make
programming in these languages more error-prone and programs less portable
than if they had been designed with a more clean-cut abstract semantics. Here,
we present some mathematical defininitions of data parallel primitives that
can be used to guide the design of data parallel languages with a higher level
of abstraction. The key idea is to view data parallel entities as tabulated
functions where the tables are stored in a distributed fashion. Operations on
data parallel entities are then simply operations on functions, just as
operations in pure functional languages. An interesting obervation is that
also traditional data structures, like lists and arrays, are covered by our
view. This illustrates the level of abstraction achieved. An especially
interesting possibility is to integrate data parallel and higher order
functional languages. Data parallel entities are then just a particular class
of functions that can be represented in a particular way. We believe that such
languages are very suitable as specification languages for data parallel
algorithms.}, 
keywords = {Data Parallelism},
}

@inproceedings{Nock93,
author = {N{\"o}cker, Eric},
title = {{Strictness Analysis using Abstract Reduction}},
booktitle = {FPCA '93 Conference on Functional Programming Languages and
Computer Architecture},
address = {Copenhagen, Denmark, June 9--11},
year = {1993},
publisher = {ACM Press},
pages = {210--219},
abstract = {In this paper we present a new and general strictness analysis
technique for lazy functional languages. In contrast to many other methods,
this method is practically usable. This is shown with results of a real
implementation. The key idea is the use of an abstract domain of which the
elements represent various kinds of sets of concrete values. Reduction as well
as pattern matching in the abstract domain mimics reduction reduction and
pattern matching in the concrete domain in a very natural way. With this
abstract domain various kinds of strictness analysis are possible. In
particular, higher order functions and general data types (tuples and lists,
but also user defined data types) fit perfectly well in this mechanism. In
contrast to methods based on abstract interpretation, the abstract domain is
infinite. Recursive functions are handled by a technique called reduction path
analysis. Complicated and expensive fixed point derivation techniques are not
needed. The implementation in the Concurrent Clean system shows that the
analyser is very fust and that it finds much strictness information.},
keywords = {Strictness Analysis, Concurrent Clean},
}

@inproceedings{DFH*93,
author = {Darlington, J. and Field, A. J. and Harrison, P. G. and Kelly, P. H.
J. and Sharp, D. W. N. and Wu, Q.},
title = {{Parallel Programming Using Skeleton Functions}},
booktitle = {PARLE 93: Parallel Languages and Architectures Europe},
address = {Munich, Germany, June 14--18},
year = {1993},
abstract = {},
keywords = {Algorithmic Skeletons},
}

@inproceedings{DGT93,
author = {Darlington, J. and Ghanen, M. and To, H. W.},
title = {{Structured Parallel Programming}},
booktitle = {Working Conference on Massively Parallel Programming Models:
Suitability, Realization, and Performance},
address = {Berlin, Germany, September 20--23},
year = {1993},
abstract = {We propose the use of a range of algorithmic forms, known as
skeletons, which abtract useful parallel computational structures from
applications. Applications are then constructed as instatiations of these
algorithmic forms. Our skeletons are expressed in a functional language as
higher order functions that can be defined in the language.}, 
keywords = {Algorithmic Skeletons},
}

@inproceedings{Brat93,
author = {Bratvold, Tore A.},
title = {{A Skeleton-Based Parallelising Compiler for ML}},
booktitle = {5th International Workshop on the Implementation of Functional
Languages}, 
address = {Numegen, The Netherlands, September},
year = {1993},
pages = {23--33},
abstract = {Languages with explicitly parallel constructs typically give the
programmer detailed control of low level machine resources. Such control
allows programs to be finely tuned to deliver high performances, but
developing massively parallel programs from scratch in this way is a tedious
and delicate task. Skeletons provide powerful abstractions over common
patterns of parallel computation, and can be expressed as higher order
functions (HOFs) in functional languages. In this paper we present SkelML, a
skeleton-based compiler for ML, that exploits parallelism in certain HOFs and
in function composition. Proviling information and performance models for
skeletons are used to decide whether and how parallelism should be exploited.
Issues of design and implementation are discussed, and an example is presented
illustrating the compilation process and the performance of a prototype of the
system.},
keywords = {Algorithmic Skeletons, ML},
}

@inproceedings{MBBW93,
author = {Michaelson, Greg and Bratvold, Tore and Busvine, David and Waugh,
Kevin}, 
title = {{Parallel Implementations from Functional Prototype
Instrumentation}},
booktitle = {PARCO' 93},
address = {Grenoble, France, September},
year = {1993},
publisher = {Elsevier North-Holland},
abstract = {We use the dynamic instrumentation of prototypes built from
higher-order functions (HOFs) to decide where parallelism is best exploited.
Performance models relate HOFs behaviour in prototypes to parallel harness
behaviour in implementations. Where useful parallelism is identified, generic
imperative parallel harnesses are used to implement HOFs. Otherwise, standard
functional to sequential imperative translation is employed. Our prototypes
are built in a pure functional subset of Standard ML. Final implementations
are in Occam2 on a transputer based Meiko Computing Surface.},
keywords = {Algorithmic Skeletons, Granularity Control, ML},
}

@phdthesis{Busv93,
author = {Busvine, David},
title = {{Detecing Parallel Structures in Functional Programs}},
school = {Department of Computing and Electrical Engineering},
address = {Heriot-Watt University},
month = {October},
year = {1993},
abstract = {},
keywords = {Algorithmic Skeletons},
}

@incollection{Szym91a,
author = {Szymanski, Boleslaw K.},
title = {{EPL --- Parallel Programming with Recurrent Equations}},
booktitle = {Parallel Functional Languages and Compilers},
editor = {Szymanski, Boleslaw K.},
publisher = {ACM Press},
address = {New York},
year = {1991},
series = {Frontier Series},
chapter = {3},
pages = {51--104},
abstract = {This chapter presents recent work on creating and implementing a
simple language for programming parallel scientific and engineering
computations that is based on a few basic principles and yet satisfies the
postulates discussed above. In addition, the chapter illustrates how a
parallelizing compiler for this language works. The language, called {\em
Equational Programming Language\/} (EPL), has been developed at Rensselaer
Polytechnic Institute by the author and a group of his graduate students.},
keywords = {Dataflow, EPL, Annotations},
}

@inproceedings{SpSz90,
author = {Spier, Kevin L. and Szymanski, B. K.},
title = {{Interprocess Analysis and Optimization in the Equational Language
Compiler}}, 
booktitle = {CONPAR 90 --- VAPP IV, Joint International Conference on Vector
and Parallel Processing},
address = {Zurich, Switzerland, September 10--13},
year = {1990},
editor = {Burkhart, H.},
volume = {457},
series = {Lecture Notes in Computer Science},
publisher = {Springer, Berlin},
pages = {324--335},
abstract = {Our approach to interprocess data dependence analysis is to extend
a dependence graph beyond the scope of each process. The extended graph is
then analyzed using the attribute propagation technique developed in the EPL
compiler for individual process consistency checking. The derived dependences,
called external, are specific to each use of a process in a particular
computation. As such, they are kept separate from the process dependence
graph. A process needs to be recompiled only if the changes in other process'
definitions change in external dependences. The described algorithm, although
cast in terms of the EPL data dependence graph can be adapted to other
languages. The method of analyzing the implications of interprocess
interactions in parallel and distributed computations may be of value to other
system.}, 
keywords = {Data Dependence Analysis},
}

@techreport{Szym93,
author = {Szymanki, Boleaslaw K.},
title = {{Parallel Functional Language --- EPL and Its Compiler}},
institution = {Department of Computer Science},
address = {Rensselaer Polytechnic Institute, Troy, New York},
month = {October},
year = {1993},
abstract = {This is a review paper that discusses the research and development
centered around the parallel functional language EPL --- Equational
Programming Language --- and its compiler. The emphasis is on opportunities
and challenges arising from the ue of functional paradigm for parallel
processing. Our approach is based on a two-level computation description:
individual process description in EPL and the macro data flow description
called configuration. The configuration represents an overall computation as a
set of interacting processes. The EPL compiler performs an extensive program
analysis and customizes the generated parallel code to specific architectures.
The parallel computation decomposition and synchronization is based on the
source program annotations provided by the user and on the user's defined
configurations. The paper concentrates on motivation and general description
of the features of the language, the detailed technical results were discussed
elsewhere. There is also a comparison of EPL to other parallel functional
languages, in particular SISAL and Crystal. The paper provides a brief
description of future research agenda for EPL. It ends with a discussion of
the general properties required of parallel functional languages and an
assessment of EPL from that point of view.},
keywords = {EPL},
}

@article{Szym87,
author = {Szymanski, B.},
title = {{Parallel Programming with Recurrent Equations}},
journal = {International Journal of Supercomputing Applications},
volume = {1},
number = {2},
pages = {74--74},
year = {1987},
abstract = {},
keywords = {EPL},
}

@inproceedings{ArVi90,
author = {Armstrong, J. L. and Virding, R.},
title = {{Erlang --- An Experimental Telephony Switching Language}},
booktitle = {XIII International Switching Symposium},
address = {Stockholm, Sweden, May 27 -- June 1},
year = {1991},
abstract = {
We present an experimental programming language called Erlang which is
suitable for programming telephony applications. We discuss some of the
requirements of such a language and introduce it by a series of small examples
which show how both of sequential and concurrent activities can be programmed.
We discuss the error recovery mechanism used in Erlang and the performance
characteristics of the current implementation.
}, 
keywords = {Erlang, Message Passing}, 
}

@techreport{AVW91,
author = {Armstrong, J. L. and Virding, R. H. and Williams, M. C.},
title = {{Erlang User's Guide and Reference Manual Version 3.2}},
institution = {Ellemtel Utvecklings AB},
address = {Sweden},
year = {1991},
abstract = {},
keywords = {Erlang, Message Passing},
}

@inproceedings{ADVW92,
author = {Armstrong, J. L. and Daecker, B. O. and Virding, S. R. and
Williams, M. C.},
title = {{Implementing a functional language for highly parallel real-time
applications}}, 
booktitle = {Software Engineering for Telecommunication Switching Systems},
address = {Florence, Italy, March 30 -- April 1},
year = {1992},
abstract = {This paper describes a fast, highly portable implementation of the
real time, symbolic, declarative programming language Erlang.},
keywords = {Erlang, Message Passing, Implementation},
}


@book{AWV93,
author = {Armstrong, J. and Williams, D. and Virding, R.},
title = {{Concurrent Programming in Erlang}},
publisher = {Prentice Hall},
year = {1993},
abstract = {
Erlang is  a  concurrent functional programming  language designed
for large industrial real-time systems.  Erlang  is  dynamically typed
and has  a  pattern  matching  syntax.   Functions are  defined  using
recursion  equations.   Erlang  provides   explicit  concurrency,  has
asynchronous message passing and is relatively free from side effects.
},
keywords = {Erlang, Message Passing},
}

@inproceedings{Berk75,
author = {Berkling, K. J.},
title = {{Reduction Languages for Reduction Machines}},
booktitle = {2nd International Symposium on Computer Architecture},
year = {1975},
publisher = {ACM/IEEE 75CH0916-7C}, 
pages = {133-140},
abstract = {
},
keywords = {Graph Reduction},
}

@inproceedings{Buel*93,
author = {Buelck, T. and others},
title = {{Preliminary Experience with a $\pi$-RED Implementation
on an  nCUBE/2 System}},
booktitle = {5th International Workshop on the Implementation of Functional
Languages},
address = {Nijmegen, The Netherlands},
year = {1993},
pages = {101--113},
abstract = {
},
keywords = {Graph Reduction, Distributed Memory},
}

@phdthesis{Zimm91a,
author = {Zimmer, R. M.},
title = {{Zur Pragmatik eines Operationalisierten $\lambda$-Kalkuels
als Basis fuer Interaktive Reduktionssysteme (On the Pragmatics of an
Operationalized $\lambda$-Calculus as a Basis of Interactive Reduction
Systems) (In German)}},
school = {Gesellschaft f{\"u}r Mathematik und Datenverarbeitung},
address = {Oldenbourg, Germany},
note ={GMD Bericht 192},
year = {1991},
abstract = {
},
keywords = {Graph Reduction},
}

@book{Kluge92,
author = {Kluge, Werner},
title = {{The Organization of Reduction, Data Flow, and Control Flow
Systems}},
publisher = {MIT Press},
year = {1992},
abstract = {
},
keywords = {Graph Reduction, Data Flow},
}

@inproceedings{SGH*86,
author = {Schmittgen, C. and Gerdts, A. and Haumann, J. and Kluge, W. E. and
Woitass, M.},
title = {A System-Supported Workload Balancing Scheme for Cooperating
Reduction Machines}, 
booktitle = {19th Annual Hawaii International Conference on System Sciences},
volume = {1}, 
year = {1986},
pages = {67--77},
abstract = {
},
keywords = {Load Balancing},
}

@article{Klug83,
author = {Kluge, W. E.},
title = {{Cooperating Reduction Machines}},
journal = {Transactions on Computers},
volume = {C 32}, 
number = {11},
year = {1983}, 
pages = {1002--1012},
abstract = {
},
keywords = {Graph Reduction},
}
