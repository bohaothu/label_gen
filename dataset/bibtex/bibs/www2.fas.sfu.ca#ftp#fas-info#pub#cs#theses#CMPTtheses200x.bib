String{SFU_CS_School = "School of Computing Science, Simon Fraser
                 University"}

@PhdThesis{U-SFraser-CMPT-PhD:2000@ONeill_Melissa,
  author =       "Melissa Elizabeth O'Neill",
  title =        "For Version Stamps for Functional Arrays and
                 Determinacy Checking: Two Applications of Ordered Lists
                 for Advanced Programming Languages",
  supervisor =   "F. Warren Burton",
  month =        "",
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "30-11-2000",
  pages =        "xxxii,283",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/MelissaEONeillPhD.pdf",
  abstract =     "This thesis describes the fat-elements method for
                 providing functional arrays and the LR-tags method for
                 determinacy checking. Although these topics may seem
                 very different, they are actually closely linked: Both
                 methods provide reproducibility in advanced programming
                 languages and share many implementation details, such
                 as tagging data using version stamps taken from an
                 ordered list.
                 
                 The fat-elements method provides arrays as a true
                 functional analogue of imperative arrays with the
                 properties that functional programmers expect from data
                 structures. It avoids many of the drawbacks of previous
                 approaches to the problem, which typically sacrifice
                 usability for performance or vice versa.
                 
                 The fat-elements method efficiently supports array
                 algorithms from the imperative world by providing
                 constant-time operations for single-threaded array use.
                 Fully persistent array accesses may also be performed
                 in constant amortized time if the algorithm satisfies a
                 simple requirement for uniformity of access. For
                 algorithms that do not access the array uniformly or
                 single-threadedly, array reads or updates take at most
                 O(log n) amortized time for an array of size n. The
                 method is also space efficient -- creating a new array
                 version by updating a single array element requires
                 constant amortized space.
                 
                 The LR-tags method is a technique for detecting
                 indeterminacy in asynchronous parallel programs -- such
                 as those using nested parallelism on shared-memory MIMD
                 machines -- by checking Bernstein's conditions, which
                 prevent determinacy races by avoiding write/write and
                 read/write contention between parallel tasks. Enforcing
                 such conditions at compile time is difficult for
                 general parallel programs. Previous attempts to enforce
                 the conditions at runtime have had non--constant-factor
                 overheads, sometimes coupled with serial-execution
                 requirements.
                 
                 The LR-tags method can check Bernstein's (or Steele's
                 generalized) conditions at runtime while avoiding some
                 of the drawbacks present in previous approaches to this
                 problem. The method has constant-factor space and time
                 overheads when run on a uniprocessor machine and runs
                 in parallel on multiprocessor machines, whereas the
                 best previous solution, CILK's Nondeterminator (a
                 constant-space and nearly constant-time approach),
                 requires serial execution.
                 
                 Both parts of the thesis include theoretical and
                 practical material. Tests from implementations of both
                 techniques indicate that the methods should be quite
                 usable in practice.",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2000@MacArthur_JeffreyDouglas,
  author =       "Jeffrey Douglas MacArthur",
  title =        "Visualization of {MIDI} Encoded Music",
  supervisor =   "David Fracchia",
  month =        apr,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-04-14",
  pages =        "60",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/JeffreyDouglasMacArthurMSc.ps.gz",
  abstract =     "In our day to day lives we normally consider the human
                 sensory facilities of sight and hearing as independent.
                 Although the mechanisms involved in seeing and hearing
                 can be very different, they do have aspects in common.
                 When we hear music we can identify the individual
                 instruments and when we see we recognize distinct
                 objects. The objects we see vary in colour and the
                 notes of each instrument we hear vary in tone.
                 
                 Previous research has visualized music using a variety
                 of different approaches such as animation, digital
                 audio filtering, and MIDI encoding. When using sound
                 waves it is diffcult to extract all of the different
                 sources of sound and their individual activities.
                 However, this does not accurately convey many aspects
                 of how people hear music nor draw upon the similarities
                 between sight and sound. MIDI stores the events that
                 would create the sound instead of the sound itself.
                 This allows for easy access to pitch, velocity,
                 instrument, and timing information involved in the
                 playing of a piece of music.
                 
                 It is the goal of this thesis to investigate methods
                 for mapping sound, in the form of music, to spatial
                 symbols and positions. The implications of succeeding
                 with this goal are important both artistically and
                 educationally. By considering how we might interpret
                 aural input using the sense of sight we can increase
                 the potential for understanding the input itself and
                 perhaps even aid the hearing impaired in experiencing
                 music in another manner. We map each of the parameters
                 contained in a MIDI event to the screen in a way that
                 reflects some of the natural mappings between seeing
                 and hearing. Through these mappings we translate
                 musical data into spatial and textural information and
                 assemble the results into a QuickTime(TM) movie. The
                 result is a visualization synchronized with the music.
                 
                 We have conducted a user study as a preliminary
                 investigation into our mapping theories. This study
                 provided a group of test subjects with several
                 different visualizations, in the form of completed
                 QuickTime TM movies, of the same musical piece. The
                 subjects then rated which mappings most directly
                 conveyed the musical activity and which were most
                 aesthetically pleasing. The results of this have given
                 us useful and encouraging feedback on the effectiveness
                 of our initial mappings in conveying channel, pitch,
                 and velocity information as well as further directions
                 to explore." ,
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2000@Sun_Yinlong,
  author =       "Yinlong Sun",
  title =        "A {SPECTRUM}-{BASED} {FRAMEWORK} {FOR} {REALISTIC}
                 {IMAGE} {SYNTHESIS}",
  supervisor =   "F. David Fracchia and Mark Drew",
  month =        jul,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-07-18",
  pages =        "191",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/YinlongSunPhD.pdf",
  abstract =     "Realistic image synthesis provides principles and
                 techniques for creating realistic imagery based on
                 models of real-world objects and behaviors. It has
                 widespread applications in 3D design, computer
                 animation, and scientific visualization. While it is
                 common to describe light and objects in terms of
                 colors, this approach is not sufficiently accurate and
                 cannot render many spectral phenomena such as
                 interference and diffraction. Many researchers have
                 explored spectral rendering and proposed several
                 methods for spectral representation, but none satisfy
                 all representation criteria such as accuracy,
                 compactness and efficiency. Furthermore, previous
                 studies have focused on distinct behaviors of natural
                 phenomena but few on their commonality and generality,
                 and it is difficult to combine existing algorithms to
                 simulate complex processes.
                 
                 This thesis proposes solutions to these problems within
                 a spectrum-based rendering framework. The pipeline
                 begins by loading spectra from a database to specify
                 light sources and objects, then generates a spectral
                 image based on loca l and global illumination models,
                 projects the spectral image into a CIE image, an d
                 finally converts the CIE image into an RGB image for
                 display or a CIELab image for evaluation. In spite of
                 omitting the light phase information, it is shown that
                 this approach suffices to generate all optical effects
                 important for realistic image synthesis. As components
                 of the new framework, a heuristic metho d is proposed
                 for deriving spectra from colors and an error metric is
                 provided for evaluating synthesized images.
                 
                 The new spectral representation proposed in this thesis
                 is called the composite model. Its key point is to
                 decompose any spectrum into a smooth background and a
                 collection of spikes. The smooth part can be
                 represented by Fourier coefficients and a spike by its
                 location and height. A re-sampling technique is
                 proposed to improve performance. Based upon the
                 characteristics of human perception, the sufficiency of
                 a low-dimensional representation is shown analytically.
                 This mode l improves upon existing methods with aspect
                 to accuracy, compactness and efficiency, and offers an
                 effective vehicle for rendering optical effects
                 involving spiky spectra.
                 
                 Using the proposed framework and composite spectral
                 model, this thesis develops new illumination models for
                 rendering a number of optical effects including
                 dispersion, interference, diffraction, fluorescence,
                 and volume absorption. The rendered images closely
                 correspond to their real-world counterparts. Overall,
                 this thesis improves realistic image synthesis by
                 expanding its rendering capabilities. It may serve as a
                 basis for a more sophisticated rendering environment
                 for high-quality computer image generation.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2000@Tatu,
  author =       "Serban G. Tatu",
  title =        "{BIBP}: {A} Bibliographic Protocol and System for
                 Distributed Reference Linking to Document Metaservices
                 on the Web",
  supervisor =   "Dr. Robert D. Cameron",
  month =        jul,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-07-25",
  pages =        "83",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/SerbanTatuMSc.ps.gz",
  abstract =     "Bibliographic Protocol (BIBP) is a proposed method for
                 the creation and resolution of abstract reference links
                 on the World-Wide Web. In this context, an abstract
                 reference link is a bibliographic citation denoting the
                 referenced document as an abstract entity, independent
                 of any particular manifestation or service with respect
                 to it. Resolution of such a link logically provides
                 access to a document metaservice, which in turn may
                 provide access to a selection of alternative sources,
                 formats or further services with respect to the
                 document.
                 
                 A distributed system based on BIBP is proposed, which
                 links documents identified by Universal Serial Item
                 Names (USINs) to document metaservers -- Web services
                 providing metadata about the documents. USINs are
                 abstract identifiers that can be derived from standard
                 bibliographic information; they can be used in
                 conjunction with the BIBP scheme to form Uniform
                 Resource Identifiers, which in turn can be embedded as
                 hyperlinks in HTML documents. Client software resolves
                 USINs to metadata retrieved from document metaservers
                 chosen by the user. The document metaservers form a
                 network that builds upon a large database of
                 bibliographic information which is acquired in a
                 piecemeal fashion by each metaserver. A prototype BIBP
                 system has been implemented to illustrate the concepts
                 presented in the thesis and to serve as a tool for the
                 evaluation of technical issues.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Bart,
  author =       "Bradley Bart",
  title =        "Representations of and Strategies for Static
                 Information, Nonc ooperative Games with Imperfect
                 Information",
  supervisor =   "Dr.~James P. Delgrande",
  month =        aug,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-08",
  pages =        "57",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/BradleyBartMSc.ps.gz",
  abstract =     "In this thesis, we will examine a card game called
                 MYST, a var iant of the Parker Brothers' classic board
                 game Clue. In MYST, a set of cards is divided
                 uniformly among a set of players, and the remaining
                 cards form a hidde n pile. The goal of each player is
                 to be the first to determine the contents of the hidden
                 pile. On their turn, a player asks a question about the
                 holdings of the other players, and, through a process
                 of elimination, a player can determine the contents of
                 the hidden pile.
                 
                 MYST is one of few static information games, wherein
                 the position does not ch ange during the course of the
                 game. To do well, players need to reason about their
                 opponents' holdings over the course of multiple turns,
                 and therefore a sound representation of knowledge is
                 required. MYST is an interesting game for AI be cause
                 it ties elements of knowledge representation to game
                 theory and game strategy.
                 
                 After informally introducing the essential elements of
                 the game, we will offer a formal specification of the
                 game in terms of first-order logic and the situation
                 calculus developed by Levesque et al.. Strategies will
                 be discussed including: existence of a winning
                 strategy, randomized strategies, and bluffing.
                 Implementation of some strategies will be discussed.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Kuederle,
  author =       "Oliver Kuederle",
  title =        "Presenting Image Sequences: {A} Detail-in-Context
                 Approach",
  supervisor =   "Kori Inkpen and Stella Atkins and Sheelagh
                 Carpendale",
  month =        aug,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-10",
  pages =        "1--88",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/OliverKuederleMSc.ps.gz",
  pdf =          "ftp://fas.sfu.ca/pub/cs/TH/2000/OliverKuederleMSc.pdf",
  pdfnote =      "characters are missing from figures in the PDF version",
  abstract =     "Due to the continuous drop in computer hardware
                 prices, the use of high-end computer systems has become
                 attractive to many hospitals. Radiology departments are
                 now facing the transition from the use of traditional
                 light screens and photographic films to online medical
                 imaging systems. These new systems offer several
                 advantages over traditional methods: films are less
                 likely to be lost, automatic anomaly detection can
                 improve diagnosis, and high-speed networks allow for
                 cross-site consultations (telemedicine). However, the
                 use of desktop monitors severely limits the space in
                 which medical images can be viewed. This applies
                 particularly to Magnetic Resonance Imaging (MRI) where,
                 frequently, up to eight films, each containing up to 20
                 images, are viewed simultaneously on a light screen. As
                 a result, the screen real-estate problem inherent in
                 desktop monitors becomes a serious restriction for the
                 radiologists.
                 
                 In a previous requirements analysis, researchers
                 suggested displaying selected images in full resolution
                 with the surrounding images remaining on the screen
                 although reduced in size (called detail-in-context).
                 Based on feedback obtained from this previous research,
                 and from our additional observations of radiologists in
                 their workplace, we have extended the algorithm of this
                 detail-in-context technique. We have implemented the
                 extended technique in a software system that allows
                 users to navigate sequences of images such as those
                 found in MRI and in other types of image sequences such
                 as in Meteorology, Video Editing, and Animation.
                 
                 To evaluate our system, we conducted a user study with
                 university students. The detail-in-context technique
                 was compared to an implementation of the thumbnail
                 technique which is utilized in many commercially
                 available medical imaging systems. Results show that
                 performance as well as user preference was similar for
                 both display techniques. Our analysis of the computer
                 logs recorded during the study, however, suggests that
                 the detail-in-context technique can accomodate a
                 variety of individual strategies and offers strong
                 comparison capabilities. The thumbnail technique, on
                 the other hand, strongly encourages a sequential
                 examination of the images but allows for higher
                 magnification factors. This research has implications
                 on the selection of appropriate display techniques in
                 many areas dealing with image sequences, including
                 radiology.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2000@Rochefort,
  author =       "Stephen Rochefort",
  title =        "Logic Programming Applications in Educational
                 Environments",
  supervisor =   "Veronica Dahl and Paul Tarau",
  month =        aug,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-10",
  pages =        "182",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2000/StephenRochefortPhD.ps.gz",
  abstract =     "A dramatic increase in the number of people wishing to
                 obtain higher education via distance education has
                 resulted in the need for advancements in asynchronous
                 learner support tools. Students that use distance
                 delivery systems typically work asynchronously, meaning
                 that they have little or no interaction with the
                 instructional team and tend to progress through the
                 course material at their own pace. During the learning
                 process students face many challenges which in a
                 typical classroom they could approach a fellow student
                 or the instructor to resolve. In distance delivery
                 systems, there must be support mechanisms that assist
                 the student through the challenging areas.
                 
                 This thesis presents The Logic Programming in Education
                 for Asynchronous Learning Environment (LPed ALE) which
                 is an integration of logic programming technologies,
                 providing reasoning capabilities, into a Java-based
                 delivery system. Through the use of multi-agent
                 technologies, software agents have been implemented to
                 provide automated, asynchronous learner-based tools.
                 The tools that are provided include a natural language
                 interface, communication mechanisms, question-answer
                 interactions, resource indexing and searching, tutor
                 assistance and quizzing mechanisms.
                 
                 The research for this thesis has resulted in three
                 significant contributions. The first is the development
                 of asynchronous learning support in distance education
                 delivery systems, via the construction of LPed ALE. A
                 second contribution has been the realization of a
                 multi-agent system architecture for educational
                 environments. This architecture examines the approach
                 needed for integrating agent technologies within a
                 standard application development. This has also led to
                 the third contribution which is the creation of a
                 methodology for developing multi-agent systems for
                 educational environments. This methodology identifies
                 modifications to the traditional software development
                 approaches required to incorporate software agent
                 specification, design, implementation and testing.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Baker,
  author =       "Gregory G. Baker",
  title =        "Caches in a Theoretical Model of Multicasting",
  supervisor =   "Arthur L. Liestman",
  month =        aug,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-10",
  pages =        "63",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2000/GregoryGBakerMSc.pdf",
  ps =           "ftp://fas.sfu.ca/pub/cs/theses/2000/GregoryGBakerMSc.ps.gz",
  abstract =     "Multicasting is the process by which a single node,
                 using a sequence of calls, sends a message to a set of
                 nodes in a communication network. The message is passed
                 from the source, through intermediate nodes to the
                 destinations. These intermediate nodes do not remember
                 the message once it is passed on. There is a
                 possibility of transmission failure with each call. A
                 failure forces retransmission from the source.
                 
                 Some intermediate nodes can be designated as
                 \emph{caches}. Once they have received the message,
                 caches will remember it. Thus, these nodes can also be
                 used to retransmit the message if there is a failure.
                 If the retransmission can be done from a cache node
                 that is closer than the source, the total number of
                 calls necessary will be decreased.
                 
                 The expected number of calls necessary to complete a
                 multicast is examined. Two methods of counting the
                 number of calls are presented. Upper and lower bounds
                 on the expected number of calls are given.
                 
                 Placement of a given number of caches is examined in
                 order to determine the locations which minimize the
                 expected traffic. Optimal placements are given for
                 small graphs and a heuristic is given which can be used
                 to place caches in larger graphs.
                 
                 It is shown that determining if the caches can be
                 placed so that the expected traffic does not exceed a
                 given threshold is NP-complete in directed acyclic
                 graphs.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2000@dakic,
  author =       "Tamara Dakic",
  title =        "On the Turnpike Problem",
  supervisor =   "Arvind Gupta and Ramesh Krishnamurti",
  month =        "08",
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-11",
  pages =        "124",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/TamaraDakicPhD.ps.gz",
  abstract =     "The turnpike problem, also known as the {\em partial
                 digest problem}, is:
                 
                 \begin{equation*}
                 \begin{minipage}{4in}{ Given a multiset of $n \choose 2
                 $ - positive numbers $\Delta X$, does there exist a set
                 $X$ such that $\Delta X$ is exactly the multiset of all
                 positive pairwise differences of the elements of $X$.
                 }
                 \end{minipage}
                 \end{equation*}
                 
                 The complexity of the problem is not known.
                 
                 We write the turnpike problem as a $0-1$ quadratic
                 program. In order to solve a quadratic program, we
                 relax it to a semidefinite program, which can be solved
                 in polynomial time. We give three different
                 formulations of the turnpike problem as a $0-1$
                 quadratic program.
                 
                 For the first $0-1$ quadratic program we introduce a
                 sequence of semidefinite relaxations, similar to the
                 sequence of semidefinite relaxations proposed by Lov\aa
                 sz and Schrijver in their seminal paper ``Cones of
                 matrices and set-functions and $0-1$ optimization''
                 ({\it SIAM Journal on Optimization} 1, pp 166-190,
                 1990). Although a powerful tool, this method has not
                 been used except in their original paper to develop a
                 polynomial time algorithm for finding stable sets in
                 perfect graphs. We give some theoretical results on
                 these relaxations and show how they can be used to
                 solve the turnpike problem in polynomial time for some
                 classes of instances. These classes include the class
                 of instances constructed by Zhang in his paper ``An
                 exponential example for partial digest mapping
                 algorithm'' ({\it Tech Report, Computer Science Dept.,
                 Penn State University} 1993) and the class of instances
                 that have a unique solution and all the numbers in
                 $\Delta X$ are different and on which Skiena, Smith and
                 Lemke's backtracking procedure, from their paper
                 ``Reconstructing sets from interpoint distances'' ({\it
                 Proc. Sixth ACM Symp. Computational Geometry}, pp 332 -
                 339, 1990) backtracks only a constant number of steps.
                 Previously it was not known how to solve the former in
                 polynomial time.
                 
                 We use our theoretical formulations to develop a
                 polynomial time heuristic to solve general instances of
                 the problem.
                 
                 We perform extensive numerical testing of our methods.
                 To date we do not have an instance of the turnpike
                 problem for which our methods do not yield a solution.
                 
                 The second $0-1$ quadratic program formulation of the
                 turnpike problem will be too large for practical
                 purposes. We use association schemes and some other
                 methods, to reduce its size and obtain the third $0-1$
                 quadratic program. We establish a connection between
                 this relaxation and the first relaxation and show its
                 limitations.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Chee,
  author =       "Sonny Han Seng Chee",
  title =        "RecTree: {A} Linear Collaborative Filtering
                 Algorithm",
  supervisor =   "Jiawei Han",
  month =        sep,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-09-28",
  pages =        "",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/SonnyHanSengChee.pdf",
  abstract =     "With the ever-increasing amount of information
                 available for our consumption, the problem of
                 information overload is becoming increasingly acute.
                 Automated techniques such as information retrieval (IR)
                 and information filtering (IF), though useful, have
                 proven to be inadequate. This is clearly evident to the
                 casual user of Internet search engines (IR) and news
                 clipping services (IF); a simple query and profile can
                 result in the retrieval of hundreds of items or the
                 delivery of dozens of news clippings into his mailbox.
                 The user is still left to the tedious and
                 time-consuming task of sorting through the mass of
                 information and evaluating each item for its relevancy
                 and quality. Collaborative filtering (CF) is a
                 complimentary technique to IR/IF that alleviates this
                 problem by automating the sharing of human judgements
                 of relevancy and quality. Collaborative filtering has
                 recently enjoyed considerable commercial success and is
                 the subject of active research. However, previous works
                 have dealt with improving the accuracy of the
                 algorithms and have largely ignored the problem of
                 scalability. This thesis introduces a new algorithm,
                 RecTree that to the best of our knowledge is the first
                 collaborative filtering algorithm that scales linearly
                 with the size of the data set. RecTree is compared
                 against the leading nearest-neighbour collaborative
                 filter, CorrCF [RIS+94], and found to outperform CorrCF
                 in execution time, accuracy and coverage. RecTree has
                 good accuracy even when the item-rating density is low
                 a region of difficulty for all previously published
                 nearest-neighbour collaborative filters and commonly
                 referred to as the sparsity problem. Our experimental
                 and performance studies have demonstrated the
                 effectiveness and efficiency of this new algorithm.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Zhang,
  author =       "Zhen Zhang",
  title =        "An integrated prefetching and caching algorithm for
                 web proxies using a correlation-based prediction
                 model",
  supervisor =   "Qiang Yang",
  month =        dec,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-12-5",
  pages =        "77",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/ZhenZhangMSc.ps.gz",
  abstract =     "Reducing Web latency is one of the primary concerns of
                 the Internet research. Web caching and Web prefetching
                 are two effective techniques to achieve this. However,
                 most of previous researches address only one of these
                 two techniques. We propose a novel model to integrate
                 Web caching and Web prefetching. In this model,
                 prefetching aggressiveness, replacement policy and
                 increased network traffic caused by prefetching are all
                 taken into account instead of being modeled separately.
                 The core of our integrated solution is an effective
                 prediction model based on statistical correlation
                 between Web documents. By utilizing the prediction
                 power of the model, we develop an integrated
                 prefetching and caching algorithm, Pre-GDSF. We conduct
                 simulations to examine the effectiveness of our
                 algorithm by using realistic Web logs. We show the
                 tradeoff between the latency reduction and the
                 increased network traffic achieved by Pre-GDSF. We also
                 show why prefetching is more effective for smaller
                 caches than larger ones.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Lu,
  author =       "Xuebin Lu",
  title =        "Fast Computation of Sparse Data Cubes and Its
                 Applications",
  supervisor =   "Dr. Jiawei Han",
  month =        dec,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-12-05",
  pages =        "100",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/XuebinLuMSc.ps.gz",
  abstract =     "Data cube queries compute aggregates over database
                 relations at a variety of granularities, and they
                 constitute an important class of decision support
                 queries. For large and sparse data cubes, many
                 applications only compute aggregates functions over one
                 or a set of attributes to find aggregate values above
                 some threshold. Those kinds of queries are called
                 iceberg queries. Iceberg queries over all the
                 combinations of the grouping attributes are called
                 Iceberg-CUBEs. Iceberg-CUBEs have wide applications in
                 data warehousing and data mining.
                 
                 An algorithm called BUC is proposed recently to compute
                 Iceberg-CUBEs efficiently. On large and sparse data
                 cubes, this algorithm outperforms all the other known
                 CUBE computation algorithms by a big margin. However,
                 the original algorithm and implementation suffer from
                 two problems which prevent it from achieving optimal
                 performance.
                 
                 In this thesis, we introduce two approaches to solve
                 those two issues in the original BUC algorithm. Our
                 study shows that the improved BUC algorithm reduces the
                 memory requirement by as much as 50% while improving
                 the performance by as much as 120%.
                 
                 To demonstrate the wide applications of the BUC
                 algorithm, we present an approach to adapt the BUC
                 algorithm to mine multi-dimensional association rules.
                 Our performance study shows that on the large and
                 sparse data cubes, the adapted BUC algorithm is
                 significantly faster than its closest competitors.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2000@Scurtescu,
  author =       "Marius Scurtescu",
  title =        "Java Program Representation and Manipulation in
                 Prolog",
  supervisor =   "Dr. Veronica Dahl",
  month =        nov,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-11-24",
  pages =        "69",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/MariusScurtescuMSc.pdf",
  sourcecode =   "ftp://fas.sfu.ca/pub/cs/TH/2000/MariusScurtescuMSc.Java2Prolog.zip",
  abstract =     "Traditionally, program source code is stored as plain
                 text. In most cases one would use a simple text editor
                 to write programs and then would run them through a
                 compiler to generate machine code. The compiler parses
                 the text file based on the grammar of the programming
                 language and then, using the parse tree, generates the
                 machine code based on the semantics of the language. If
                 the compiler encounters syntactic or semantic errors
                 then those errors will be reported, the original source
                 file has to be edited and fixed, and compilation
                 started again.
                 
                 In order to catch most of the syntactic errors while
                 editing, a new class of editors was created. These
                 editors will support you with features specific to the
                 grammar of the language you are writing in. Examples of
                 such support are syntax highlighting, identifier
                 cross-reference, source code navigation, code
                 pretty-printing etc. To be able to do this, the editor
                 must have a parser for the grammar itself and knowledge
                 about the language semantics. This leads to a situation
                 where the source code, even though is stored as plain
                 text, is continuously parsed by the editor. So you have
                 plain text on the outside and parse tree in the inside.
                 
                 This project explores an alternative way of storing
                 source code in order to facilitate better editor
                 support and faster compilation. Java2Prolog is a Java
                 language parser, written in Java, that generates a
                 representation of a Java program as Prolog predicates.
                 These Prolog predicates can be queried later using
                 Prolog queries in order to detect possible semantic
                 errors or restructure/edit the program.",
  gen =          "0->",
}
@MastersThesis{U-SFraser-CMPT-MSc:2000@Shoemaker,
  author =       "Garth B. D. Shoemaker",
  title =        "Single Display Privacyware: Augmenting Public Displays
                 with Private Information",
  supervisor =   "Kori M. Inkpen",
  month =        nov,
  year =         "2000",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-11-28",
  pages =        "83",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2000/GarthBDShoemakerMSc.pdf",
  abstract =     "The traditional human-computer interaction model
                 limits collaboration by restricting physical computer
                 access to one user at a time. Single Display Groupware
                 (SDG) research confronts the standard interaction model
                 by examining how to best support groups of users
                 interacting with a shared display. One problem that
                 arises with the use of SDG systems is that they often
                 do not adequately support the display of private
                 information. Having the ability to keep information
                 private can be useful for addressing issues such as the
                 ``screen real-estate'' problem, and problems associated
                 with awareness overload. With normal SDG systems, any
                 information that is to be kept private by a user cannot
                 be displayed on the shared display, as that display is
                 visible to all users. Some researchers have addressed
                 the privacy issue by developing techniques that allow
                 small private displays to be networked with a large
                 shared display. This technique is useful for many
                 applications, but has limitations. For example, private
                 information cannot be shown within the context of
                 related public information, and users are required to
                 constantly shift attention from the private display to
                 the shared display. This dissertation introduces Single
                 Display Privacyware (SDP), a new interaction model that
                 extends the SDG model to include the display of private
                 information on a shared display. Not only is public
                 information shown on the shared display, but private
                 information is also shown on the display, and is kept
                 private by ltering the output of the display. This
                 interaction model can be used to address the screen
                 real-estate problem and the awareness overload problem,
                 and also, unlike other solutions, allows private
                 information to be shown within the public context of
                 the shared display. A description of a prototype
                 implementation of an SDP system is given, along with
                 results of a user study performed to investigate users
                 interacting with the system. The signicance of SDP and
                 conclusions regarding future research in the area are
                 discussed.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2001@Lee,
  author =       "Tim Kam Lee",
  title =        "Measuring Border Irregularity and Shape of Cutaneous
                 Melanocytic Lesions",
  supervisor =   "Dr. Stella Atkins",
  month =        jan,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-01-08",
  pages =        "130",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/TimKamLeePhD.pdf",
  abstract =     "Cutaneous melanocytic lesions, commonly known as
                 moles, are mostly benign; however, some of them are
                 malignant melanomas, the most fatal form of skin
                 cancer. Because the survival rate of melanoma is
                 inversely proportional to the thickness of the tumor,
                 early detection is vital to the treatment process. Many
                 dermatologists have advocated the development of
                 computer-aided diagnosis systems for early detection of
                 melanoma.
                 
                 One of the important clinical features differentiating
                 benign nevi from malignant melanomas is the lesion
                 border irregularity. There are two types of border
                 irregularity: texture and structure irregularities.
                 Texture irregularities are the small variations along
                 the border, while structure irregularities are the
                 global indentations and protrusions that may suggest
                 either the unstable growth in a lesion or regression of
                 a melanoma. An accurate measurement of structure
                 irregularities is essential to detect the malignancy of
                 melanoma.
                 
                 This thesis extends the classic curvature scale-space
                 filtering technique to locate all structure irregular
                 segments along a melanocytic lesion border. An
                 area-based index, called irregularity index, is then
                 computed for each segment. From the individual
                 irregularity index, two important new measures, the
                 most significant irregularity index and the overall
                 irregularity index, are derived. These two indices
                 describe the degree of irregularity along the lesion
                 border .
                 
                 A double-blind user study is performed to compare the
                 new measures with twenty experienced dermatologists'
                 evaluations. Forty melanocytic lesion images were
                 selected and their borders were extracted automatically
                 after dark thick hairs were removed by a preprocessor
                 called DullRazor. The overall irregularity index and
                 the most significant irregularity index were calculated
                 together with three other common shape descriptors. All
                 computed measures and the dermatologists' evaluations
                 were analysed statistically. The results showed that
                 the overall irregularity index was the best predictor
                 for the clinical evaluation, and both the overall
                 irregularity index and the most significant
                 irregularity index outperformed the other shape
                 descriptors. The new method has great potential for
                 computer-aided diagnosis systems.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Zhang,
  author =       "Haining Zhang",
  title =        "Improving Performance on {WWW} Using Path-based
                 Predictive Caching and Prefetching",
  supervisor =   "Dr. Qiang Yang",
  month =        mar,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-03-01",
  pages =        "82",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/HainingZhangMSc.ps.gz",
  abstract =     "Caching and prefetching are well known strategies for
                 improving the performance of Internet systems. The
                 heart of a caching system is its page replacement
                 policy, which selects the pages to be replaced in a
                 proxy cache when a request arrives. By the same token,
                 the essence of a prefetching algorithm lies in its
                 ability to accurately predict future requests. In this
                 paper, we present a method for caching variable-sized
                 web objects using an n-gram based prediction of future
                 web requests. Our method aims at mining a Markov model
                 from past web document access patterns and using it to
                 extend the well-known GDSF caching policies. In
                 addition, we present a new method to integrate this
                 caching algorithm with our n-gram based prefetching
                 algorithm using the mined Markov model. We empirically
                 show that the system performance is greatly improved
                 using the integrated approach.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2001@Wang_Zhaoxia,
  author =       "Zhaoxia Wang",
  title =        "Collaborative Filtering Using Error-Tolerant
                 Fascicles",
  supervisor =   "Dr. Jiawei Han",
  month =        apr,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-April-11",
  pages =        "67",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/ZhaoxiaWangMSc.ps.gz",
  pdf =          "ftp://fas.sfu.ca/pub/cs/TH/2001/ZhaoxiaWangMSc.pdf",
  abstract =     "With the rapid growth of the Internet, information
                 overload is becoming increasingly acute. Recently,
                 collaborative filtering has enjoyed considerable
                 commercial success in addressing the information
                 overload problem and thus it becomes an active research
                 area. In this thesis, we propose a new approach for
                 collaborative filtering, which makes use of the concept
                 of error-tolerant fascicle. An error-tolerant fascicle
                 is defined as a set of records that share similar
                 values among the majority but not all of a set of
                 attributes. In this thesis, we define the new concept
                 of error-tolerant fascicle, investigate the properties
                 that the error-tolerant fascicles have, and propose an
                 algorithm for mining the complete set of frequent and
                 strong error-tolerant fascicles in the data.
                 Furthermore, we also propose the method for applying
                 error-tolerant fascicles to collaborative filtering.
                 Our performance studies show that our ET-Projection
                 algorithm is efficient and scalable, and using
                 error-tolerant fascicles is a novel and effective way
                 for collaborative filtering.",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2001@Pinto,
  author =       "Helen Pinto",
  title =        "Multi-dimensional Sequential Pattern Mining",
  supervisor =   "Dr. Jiawei Han",
  month =        apr,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-04-10",
  pages =        "70",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/HelenPintoMSc.ps.gz",
  abstract =     "With our recently developed sequential pattern mining
                 algorithms, such as PrefixSpan, it is possible to mine
                 sequential user-access patterns from Web-logs. While
                 this information is very useful when redesigning
                 web-sites for easier perusal and fewer network traffic
                 bottlenecks, it would be so much richer if we could
                 incorporate multiple dimensions of information. For
                 example, if you knew the referral site that users
                 frequently come from, you might be able to determine
                 what information on your own web-site is of interest to
                 them --- and enhance or separate this information as
                 needed. Similarly, if you knew what weekday and time
                 certain access patterns frequently occur at, you could
                 ensure updated information is ready and available for
                 these users.
                 
                 This thesis proposes and explores two different
                 techniques, HYBRID and PSFP, to incorporate additional
                 dimensions of information into the process of mining
                 sequential patterns. It investigates the strengths and
                 limitations of each approach. The HYBRID method first
                 finds frequent dimension value combinations, and then
                 mines sequential patterns from the set of sequences
                 that satisfy each of these combinations. PSFP
                 approaches the problem from the opposite direction. It
                 mines the sequential patterns for the whole dataset
                 only once (using PrefixSpan), and mines the
                 corresponding frequent dimension patterns alongside
                 each sequential pattern (using existing association
                 algorithm FP-growth). Experiments show that HYBRID is
                 most effective at low support in datasets that are
                 sparse with respect to dimension value combinations but
                 dense with respect to the sequential patterns present.
                 PSFP is the better alternative in every other case,
                 including datasets that are dense with respect to both
                 dimension values combinations and sequential items at
                 low support.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Mao,
  author =       "Runying Mao",
  title =        "Adaptive-{FP}: An Efficient and Effective Method For
                 Multi-Level Multi-Dimensional Frequent Pattern Mining",
  supervisor =   "Jiawei Han",
  month =        apr,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-April-12",
  pages =        "89",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/RunyingMaoMSc.ps.gz",
  abstract =     "Real life transaction databases usually contain both
                 item information and dimension information. Moreover,
                 taxonomies about items likely exist. Knowlege about
                 multi-level multi-dimensional frequent patterns is
                 interesting and useful.
                 
                 The classic frequent pattern mining algorithms based on
                 a uniform minimum support, such as Apriori and
                 FP-growth, either miss interesting patterns of low
                 support or suffer from the bottleneck of itemset
                 generation. Other frequent pattern mining algorithms,
                 such as Adaptive Apriori, though taking various
                 supports, focus mining at a single abstraction level.
                 Furthermore, as an Apriori-based algorithm, the
                 efficiency of Adaptive Apriori suffers from the
                 multiple database scans.
                 
                 In this thesis, we extend FP-growth to attack the
                 problem of multi-level multi-dimensional frequent
                 pattern mining. We call our algorithm Ada-FP, which
                 stands for Adaptive FP-growth. The efficiency of our
                 Ada-FP is guaranteed by the high scalability of
                 FP-growth. To increase the effectiveness, our Ada-FP
                 pushes various support constraints into the mining
                 process. First, item taxonomy has been explored. Our
                 Ada-FP can discover both inter-level frequent patterns
                 and intra-level frequent patterns. Second, in our
                 Ada-FP, dimension information has been taken into
                 account. We show that our Ada-FP is more flexible at
                 capturing desired knowledge than previous studies.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Mortazavi-Asl,
  author =       "Behzad Mortazavi-Asl",
  title =        "Discovering And Mining User Web-page Traversal
                 Patterns",
  supervisor =   "Dr. Jiawei Han",
  month =        apr,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-04-09",
  pages =        "93",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/BehzadMortazav-AslMSc.ps.gz",
  abstract =     "As the popularity of WWW explodes, a massive amount of
                 data is gathered by Web servers in the form of Web
                 access logs. This is a rich source of information for
                 understanding Web user surfing behavior. Web Usage
                 Mining, also known as Web Log Mining, is an application
                 of data mining algorithms to Web access logs to find
                 trends and regularities in Web users' traversal
                 patterns. The results of Web Usage Mining have been
                 used in improving Web site design, business and
                 marketing decision support, user profiling, and Web
                 server system performance. In this thesis we study the
                 application of assisted exploration of OLAP data cubes
                 and scalable sequential pattern mining algorithms to
                 Web log analysis. In multidimensional OLAP analysis,
                 standard statistical measures are applied to assist the
                 user at each step to explore the interesting parts of
                 the cube. In addition, a scalable sequential pattern
                 mining algorithm is developed to discover commonly
                 traversed paths in large data sets. Our experimental
                 and performance studies have demonstrated the
                 effectiveness and efficiency of the algorithm in
                 comparison to previously developed sequential pattern
                 mining algorithms. In conclusion, some further research
                 avenues in web usage mining are identified as well.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Au,
  author =       "James Ka Sin Au",
  title =        "Object Segmentation and Tracking Using Video Locales",
  supervisor =   "Dr. Ze-Nian Li and Dr. Mark S. Drew",
  month =        jul,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-7-13",
  pages =        "94",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/JamesKaSinAuMSc.ps.gz",
  abstract =     "The ability to automatically locate and track objects
                 from videos has always been very important in
                 traditional applications such as surveillance,
                 robotics, and object recognition. With the
                 proliferation of digital videos and online multimedia
                 data and the need of content-based multimedia encoding
                 and retrieval, locating and tracking objects in digital
                 videos become ever more important.
                 
                 This thesis presents a new technique based on feature
                 localization for segmenting and tracking objects in
                 videos. A video locale is a sequence of image feature
                 locales that share similar features (color, texture,
                 shape, and motion) in the spatio-temporal domain of
                 videos. Image feature locales are grown from tiles
                 (blocks of pixels) and can be non-disjoint and
                 non-connected. Instead of using regions, the set of
                 tiles belonging to the feature locale (called the
                 envelope) is used to represent the locality of the
                 feature. Intuitively, feature locales are significant
                 feature blobs. To exploit the temporal redundancy in
                 digital videos, two algorithms (intra-frame and
                 inter-frame) are used to grow locales efficiently.
                 Multiple motion tracking is achieved by tracking and
                 performing tile-based dominant motion estimation for
                 each locale separately (i.e., only member tiles are
                 used); hence, the difficulty of multiple non-dominating
                 motions is avoided and using tiles as the base unit
                 makes the method more robust to pixel-level noise.
                 Furthermore, video locales that move together through
                 time may be grouped together to approximate
                 multi-feature video objects.
                 
                 Being at a higher feature level than pixels and more
                 robust than regions, video locales are more suited for
                 content-based video processing. How video locales may
                 be used in content-based multimedia encoding and
                 retrieval such as outlined in MPEG-4 and MPEG-7 is
                 discussed. Tests on natural videos have shown very good
                 results.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Itskevitch,
  author =       "Julia Itskevitch",
  title =        "Automatic Hierarchical {E}-mail Classification using
                 Association Rules",
  supervisor =   "Jiawei Han",
  month =        jul,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-07-20",
  pages =        "1--125",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2001/JuliaItskevitchMSc.ps.gz",
  abstract =     "The explosive growth of on-line communication, in
                 particular e-mail communication, makes it necessary to
                 organize the information for faster and easier
                 processing and searching. Storing e-mail messages into
                 hierarchically organized folders, where each folder
                 corresponds to a separate topic, has proven to be very
                 useful.
                 
                 Previous approaches to this problem use Naive Bayes- or
                 TF-IDF-style classifiers that are based on the
                 unrealistic term independence assumption. These methods
                 are also context-insensitive in that the meaning of
                 words is independent of presence/absence of other words
                 in the same message. It was shown that text
                 classification methods that deviate from the
                 independence assumption and capture context achieve
                 higher accuracy. In this thesis, we address the problem
                 of term dependence by building an associative
                 classifier called Classification using Cohesion and
                 Multiple Association Rules, or COMAR in short. The
                 problem of context capturing is addressed by looking
                 for phrases in message corpora. Both rules and phrases
                 are generated using an efficient FP-growth-like
                 approach. Since the amount of rules and phrases
                 produced can be very large, we propose two new
                 measures, rule cohesion and phrase cohesion, that
                 possess the anti-monotone property which allows the
                 push of rule and phrase pruning deeply into the process
                 of their generation. This approach to pattern pruning
                 proves to be much more efficient than
                 ``generate-and-prune'' methods.
                 
                 Both unstructured text attributes and semi-structured
                 non-text attributes, such as senders and recipients,
                 are used for the classification. COMAR classification
                 algorithm uses multiple rules to predict several
                 highest probability topics for each message. Different
                 feature selection and rule ranking methods are
                 compared. Our studies show that the hierarchical
                 associative classifier that utilizes phrases, multiple
                 rules and deep rule pruning and uses biased confidence
                 or rule cohesion for rule ranking achieves higher
                 accuracy and is more efficient than other associative
                 classifiers and is also more accurate than Naive
                 Bayes.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Li,
  author =       "Tianyi Li",
  title =        "Web-Document Prediction and Presending Using
                 Association Rule Sequential Classifiers",
  supervisor =   "Qiang Yang",
  month =        jul,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-07-27",
  pages =        "77",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/TianyiIanLiMSc.ps.gz",
  abstract =     "An important data source for data mining is the
                 web-log data that traces the user's web browsing
                 actions. From the web logs, one can build prediction
                 models that predict with high accuracy the user's next
                 requests based on past behavior. To do this with the
                 traditional classification and association rule methods
                 will cause a number of serious problems due to the
                 extremely large data size and the rich domain knowledge
                 that must be applied. Most web log data are sequential
                 in nature and exhibit the ``most recent--most
                 important'' behavior. To overcome this difficulty, we
                 examine two important dimensions of building prediction
                 models, namely the type of antecedents of rules and the
                 criterion for selecting prediction rules. This thesis
                 proposes a better overall method for prediction model
                 representation and refinement. We show empirically on
                 realistic web log data that the proposed model
                 dramatically outperforms previous ones. How to apply
                 the learned prediction model to the task of presending
                 web documents is also demonstrated.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@liao,
  author =       "Nancy Yaqin Liao",
  title =        "Fault-Tolerant Repeat Pattern Mining on Biological
                 Data",
  supervisor =   "Dr. Jiawei Han",
  month =        aug,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       "SFU_CS_School",
  defended =     "2001-August-02",
  pages =        "82",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/NancyYaqinLiaoMSc.ps.gz",
  abstract =     "With the development of biotechnology, more and more
                 biological data is collected and available for
                 analysis. One example is the GenBank and Proteins data
                 from NCBI (National Center for Biotechnology
                 Information). There is huge amount of data available,
                 including DNA sequences, RNA sequences and protein
                 sequences of all different species. And not much is
                 known about this data. How can one extract the most
                 interesting and knowledgeable patterns from that data
                 which may guide us to more discoveries is an
                 interesting task.
                 
                 In this thesis, we study a quite important problem in
                 molecular biology, tandem repeat finding problem.
                 Furthermore, we refine the problem to find the complete
                 set of tandem repeat patterns and analyze the problem,
                 investigate the properties that the tandem repeat
                 patterns have, and propose two algorithms to solve it.
                 Interesting patterns we found by using our algorithms
                 TCD and LSD are proposed. Our performance studies show
                 that our LSD algorithm is efficient and scalable.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Lam,
  author =       "Joyce Man Wing Lam",
  title =        "Multi-dimensional Constrained Gradient Mining",
  supervisor =   "Dr. Jiawei Han",
  month =        aug,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-Aug-03",
  pages =        "103",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/JoyceWingManLamMSc.pdf",
  ps =           "ftp://fas.sfu.ca/pub/cs/theses/2001/JoyceWingManLamMSc.ps.gz",
  abstract =     "Since its emergence, data cubes have been well-adopted
                 by users as a tool to analyze data collected in
                 multi-dimensional way. However, often users are
                 overwhelmed by the massive amount of data presented in
                 a data cube, especially it is not uncommon for the
                 number of cells to reach the magnitude of thousand or
                 even million. This poses non-trivial challenges on
                 locating pairs of cells having significant difference
                 in measures within a data cube. Thus, it is crucial and
                 practical to investigate into the issue of efficient
                 cell comparisons with complex measures based on
                 user-specified constraints.
                 
                 This thesis introduces the concept of constrained
                 gradients. It proposes two algorithms,
                 All-Significant-Pairs and LiveSet-Driven, to mine
                 constrained gradients in data cubes by extracting pairs
                 of cells with significant difference in their meausres.
                 The two algorithms approach the same problem from
                 different angles. Experimental results show that
                 LiveSet-Driven algorithm, which employs
                 group-processing technique, is more efficient.
                 
                 To complete our study, we also investigate the problem
                 of constrained gradient mining in transaction
                 databases. Motivated by cross-selling, constrained
                 frequent pattern pairs with significant difference in
                 their measures are mined in transaction database. A
                 Top-k FP-tree data structure is proposed to deal with
                 complex measures in frequent pattern mining. Based on
                 FP-growth algorithm, two algorithms are presented to
                 find valid constrained frequent pattern pairs.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Ranaweera,
  author =       "Prabha Ranaweera",
  title =        "Quorum Based Total Order Group Communication System",
  supervisor =   "Dr. Tiko Kameda",
  month =        aug,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-08-08",
  pages =        "1--89",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/PrabhaRanaweeraMSc.ps.gz",
  abstract =     "Applications designed for distributed systems are
                 gaining popularity as they provide high availability
                 and high performance at a relatively low cost.
                 
                 However designing efficient, fault tolerant and well
                 coordinated, distributed applications is a difficult
                 task. Group communication systems provide a powerful
                 building block for efficient design and implementation
                 of distributed applications.
                 
                 The main services provided by a group communication
                 system are the reliable ordered delivery of messages
                 among the participants of the distributed system, and
                 the handling of membership changes. Reliable ordered
                 delivery of messages provide the guarantee that
                 messages that are exchanged among the participants of
                 the system are delivered to all the participants in the
                 correct agreed order. The membership services provide
                 the fault tolerance capabilities. They handle
                 membership changes that can occur in the system such as
                 processor failures, recoveries and join-ins of new
                 processors to the system.
                 
                 In this thesis we design and implement a new group
                 communication system that is based on the concept of
                 {\it quorum systems} and {\it coteries}. The concept of
                 quorum systems and coteries is widely used in the
                 design of distributed algorithms as a tool that
                 provides mutual exclusion and distributed coordination.
                 We put forward the design and implementation of the
                 Quorum Based Total Order (QBTO) group communication
                 system as the subject of this thesis. It is a new group
                 communication system that provides reliable total order
                 message delivery services and membership services for
                 fault tolerance that can be employed to serve efficient
                 development of distributed applications. In QBTO group
                 communication protocol, totally ordered delivery of
                 messages is achieved by the use of a globally unique
                 sequence number assigned to each message. This globally
                 unique sequence number is obtained with the use of a
                 coterie imposed on the group. The properties of a
                 coterie guarantee the uniqueness of the sequence
                 number.
                 
                 As a result of the extensive and comprehensive research
                 performed on this topic we conclude that it is feasible
                 to design an efficient group communication protocol
                 which provides reliable total order message delivery
                 services and membership services for distributed system
                 development using the concept of quorum systems and
                 coteries.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Cong,
  author =       "Shi Cong",
  title =        "{MINING} {THE} {TOP}-{K} {FREQUENT} {ITEMSET} {WITH}
                 {MINIMUM} {LENGTH} {M}",
  supervisor =   "Jiawei Han",
  month =        jul,
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-08-08",
  pages =        "74",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/ShiCongMSc.ps.gz",
  abstract =     "With the explosive growth of data stored in electronic
                 form, data mining has become essential in searching
                 nontrivial, implicit, previously unknown and
                 potentially useful information from a huge amount of
                 data. Association rule mining in large transactional
                 databases is an important topic in the field of data
                 mining. It takes a set of transactions, which is a list
                 of items, as input to find sets of objects tending to
                 associate with each other. Having been investigated
                 intensively during the past few years, it has been
                 shown that the major computational task is to identify
                 all of the frequent itemsets which satisfy a minimum
                 support threshold, min-sup. Association rules can then
                 be generated easily. In this work, I propose two
                 interesting frequent itemset mining algorithms, DIPT
                 and FIPT, which find the top-k frequent itemsets with
                 the minimum length m in the transaction database. The
                 novelty of these algorithms is that there is no need to
                 specify a min-sup threshold by users. I compared these
                 methods with other frequent itemset mining algorithms
                 and showed that this approach presents greater
                 advantages in terms of both flexibility and
                 efficiency.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Blackstock,
  author =       "Michael A. Blackstock",
  title =        "Markup for Transformation to Mobile Device Presentation
                 Markups using XML",
  supervisor =   "Dr. Wo-Shun Luk",
  month =        "nov",
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-11-30",
  pages =        "237",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/MichaelBlackstockMsc.pdf",
  abstract =     "This thesis provides some background on mobile devices 
                 and wireless networks, web infrastructure for mobile 
                 application development using HTML, and newer markups 
                 such as Wireless Markup Language (WML) and Voice XML.  
                 It describes a prototype XML application developed  
                 during our research called Mobility Markup (MM) that 
                 can be used for developing simple web applications for 
                 HTML, WML and VoiceXML from a single source.  This could 
                 save development time by reducing the need for authors 
                 to learn user interface and mobile device-specific 
                 markups. 

                 MM is compared and contrasted with other approaches for 
                 the development of mobile web applications.  While other 
                 approaches provide partial solutions that address web 
                 forms, conversion from HTML to WML, or application-
                 specific solutions, MM illustrates that a more general 
                 solution is possible.  MM separates application objects, 
                 purpose and presentation into sections within the markup 
                 and provides an abstraction for the purpose of an HTML 
                 web page, a WML deck or VoiceXML dialog to accomplish 
                 its goals.

                 To study the feasibility of MM a simple example is 
                 described in detail.  This example shows that an author 
                 can create general purpose content and convert it to 
                 useable HTML, WML and VoiceXML for virtually any 
                 application using XSLT transformations.  Two additional 
                 feasibility prototypes follow to provide a framework 
                 for a future implementation.

                 In addition to its original purpose, MM could have a 
                 wider range of applications including the facilitation 
                 of content localization, the development of content for 
                 the disabled, and possibly support for collaboration 
                 between people with different devices.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Huang,
  author =       "Haiming Huang",
  title =        "Lossless Semantic Compression for Relational Databases",
  supervisor =   "Dr. Jiawei Han",
  month =        "nov",
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-08-03",
  pages =        "61",
  abstract =     "With the widespread use of databases and data
                 warehousing technologies, the amount of data stored
                 in databases has increased tremendously. It becomes
                 attractive to compress data in database systems.
                 Relational data is quite different from text and
                 multimedia data, because many semantic structures
                 (e.g., data dependencies and correlations) exist
                 within relational data. However, traditional
                 compression methods, such as Lempel-Ziv, simply
                 treat input data as a large byte string and operate
                 at the byte level. Thus, they fail to exploit the
                 semantic structures in the relation.

                 Based on the observation that sets of some attribute
                 values occur frequently in a relational table, we
                 propose a semantic compression technique that
                 exploits frequent dependency patterns embedded in
                 the relational table. One advantage of this approach
                 is that compression/decompression is performed at
                 the tuple-level, which is desirable for integrating
                 the compression technique into database systems. We
                 show that it is hard to compute an optimal compression
                 solution. Therefore, an iterative greedy compression
                 framework is offered to solve this problem. This work
                 primarily focuses on the underneath component of the
                 compression framework, that is to efficiently find
                 dependency patterns in relational data to optimize the
                 compression ratio. The experimental results on several
                 real-life datasets demonstrate the effectiveness of our
                 approach, as well as the efficiency and scalability.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2001@Tang,
  author =       "Liu Tang",
  title =        "Top-Down FP-Growth for Association Rule Mining",
  supervisor =   "Dr. Ke Wang",
  month =        "dec",
  year =         "2001",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2001-12-06",
  pages =        "70",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2001/LiuTangMsc.ps.gz",
  abstract =     " Association rule mining has attracted lots of 
                 interests in data mining research. Most of previous 
                 studies adopt Apriori heuristic and use uniform 
                 minimum support threshold in pruning. 

                 In this paper, we propose a novel TD-FP-Growth (the 
                 shorthand for Top-Down FP-Growth) algorithm family to 
                 mine association rule more efficiently and effectively. 
                 TD-FP-Growth algorithm for frequent pattern mining 
                 beats both Apriori algorithm and newly suggested 
                 FP-growth algorithm. TD-FP-Growth algorithm for 
                 association rule mining explores association rule 
                 mining from two aspects: using multiple minimum 
                 supports for different classes and pushing a new 
                 constraint, acting constraint of confidence, into the 
                 mining process to reduce the search space and speed 
                 up the mining. 

                 Our performance shows that TD-FP-Growth algorithm 
                 family is the most efficient algorithm so far. The 
                 efficiency of TD-FP-Growth is achieved by: (1) it 
                 stores the compressed database in a highly compressed 
                 tree structure. (2) the mining of the rules involves 
                 only the FP-tree and a few header tables. (3) the 
                 introduction of a new anti-monotone constraint, acting 
                 constraint of confidence, and push it into the mining 
                 process.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@JChen,
  author =       "Justin Jiong Chen",
  title =        "A Design and Implementation of Index-Fabric
                 Algorithm",
  supervisor =   "Woshun Luk",
  month =        feb,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-02-01",
  pages =        "89",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2002/JustinJiongChenMSc.pdf",
  abstract =     "Index Fabric is an indexing algorithm specially
                 designed for optimizing query processing on
                 semistructured data such as XML. Compared with other
                 latest development of XML indexing techniques, Index
                 Fabric promises performance, flexibility and
                 efficiency. One difficulty facing all XML indexing
                 techniques for query optimization is the fact that XML
                 data is both data and document; here document is
                 treated as data with its internal data processing
                 structure. Therefore XML indexing need not only index
                 data but also the structure path that comes with the
                 data. Index Fabric solves the problem smartly by
                 converting and merging both the data and its structure
                 path into an intermediate string format and indexing on
                 it with the most efficient algorithm. The key of its
                 performance and efficiency relies on a special data
                 structure knows as Patricia trie, which is highly
                 optimized for indexing long and complex key.
                 
                 The goal of this project is to design and implement the
                 Index Fabric system. The idea is to follow the original
                 Index Fabric description to build a system and also
                 study its performance. There are a number of reasons
                 for implementing Index Fabric. Firstly, as an XML
                 indexing algorithm, Index Fabric optimizes XML querying
                 speed significantly and it has flexibility required for
                 optimizing XML query processing. Secondly, Index
                 Fabrics is rather new and its implementation detail is
                 not yet known, therefore these solutions can serve as a
                 testing bed for future algorithm. Thirdly, we shall
                 test the idea of speeding up the overall performance of
                 Index Fabric by running a main memory version.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Kulpinski,
  author =       "Dejan Kulpinski",
  title =        "{LLE} and Isomap Analysis of Spectra and Colour
                 Images",
  supervisor =   "Brian Funt",
  month =        mar,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-03-27",
  pages =        "85",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/DejanKulpinskiMSc.pdf",
  abstract =     "Locally Linear Embedding (LLE) [2] and Isomap [1]
                 techniques can be used to process and analyze high-
                 dimensional data domains, such as semantics, images,
                 and colour. These techniques allow creation of low-
                 dimensional embeddings of the original data that are
                 much easier to visualize and work with then the
                 initial, high-dimensional data. In particular, the
                 dimensionality of such embeddings is similar to that
                 obtained by classical techniques used for
                 dimensionality reduction, such as Principal Components
                 Analysis (PCA).
                 
                 The goal of this thesis is to show how the above
                 methods can be applied to the area of colour vision, in
                 particular to images, chromaticity histograms and
                 spectra. Using the Isomap technique, it was found that
                 the dimensionality of the spectral reflectances could
                 be as low as 3. For the chromaticity histograms, the
                 251 original histogram dimensions were transformed into
                 5-6 dimensional space. The chromaticity histogram
                 result is significantly better then the result obtained
                 by PCA on the same data set.
                 
                 In addition to providing an estimate of the underlying
                 dimensionality of the data, both the Isomap and LLE
                 techniques were used to produce low-dimensional
                 embeddings of the high-dimensional data for the purpose
                 of data visualization. These low-dimensional embeddings
                 were valuable in determining the non-linear
                 relationships that existed among the members of the
                 original data sets. For example, the relationship among
                 colour histograms in the embedded space was based on
                 both the chromaticity and image content while the
                 embedded spectra showed groupings based on the RGB
                 values.
                 
                 The possible role of the LLE and Isomap in image
                 classification, spectrarecons truction and RGB-to-
                 Spectra mapping is also examined.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Yan,
  author =       "Edward Mingjun Yan",
  title =        "Minimizing Bandwidth Requirement of Broadcasting 
                  Protocol in Video-on-Demand Services",
  supervisor =   "Prof. Tiko Kameda",
  month =        "apr",
  year =         "2002",
  org =          "SFU-CMPT",
  school =        SFU_CS_School,
  defended =     "2002-04-30",
  pages =        "95",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2002/EdwardYanMSc.pdf",
  abstract =     "In order to address the scalability problem of video-
                 on-demand (VOD) systems, several periodic broadcast 
                 schemes have been proposed, which partition a video 
                 into segments and repetitively broadcast each segment 
                 on a separate channel.  Most of the research literature 
                 focuses on minimizing the server broadcast bandwidth 
                 for a waiting time. The most efficient broadcasting 
                 schemes currently available require the same bandwidth 
                 on the client side as that on server side. In reality, 
                 however, the client side bandwidth requirement is often 
                 the limiting factor. 
                 
                 We propose a new broadcast scheme, named Generalized 
                 Fibonacci Broadcasting (GFB), to address the issue of 
                 minimizing the client-side bandwidth requirement. GFB 
                 allows the client to download data from c (a positive 
                 integer that can be selected) concurrent broadcasting 
                 channels, each with a bandwidth of b/k (k > 0), where 
                 b (bits/sec) is the display rate. We demonstrate that, 
                 for realistic sets of parameters, GFB is the most 
                 efficient among the currently known broadcasting 
                 schemes with client bandwidth limitation.  Furthermore, 
                 it gives a VOD service provider great flexibility and 
                 simplicity in implementing VOD services based on the 
                 current technologies.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2002@Walenstein,
  author =       "Andrew Walenstein",
  title =        "Cognitive Support in Software Engineering Tools: {A}
                 Distributed Cognition Framework",
  supervisor =   "Robert D. Cameron",
  month =        may,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-05-13",
  pages =        "402",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/AWalensteinPhD.pdf",
  abstract =     "Software development remains mentally challenging
                 despite the continual advancement of training,
                 techniques, and tools. Because completely automating
                 software development is currently impossible, it makes
                 sense to seriously consider how tools can improve the
                 mental activities of developers apart from automating
                 them away. Such mental assistance can be called
                 ``cognitive support''. Understanding and developing
                 cognitive support in software engineering tools is an
                 important research issue but, unfortunately, at the
                 moment our theoretical foundations for it are
                 inadequately developed. Furthermore, much of the
                 relevant research has occurred outside of the software
                 engineering community, and is therefore not easily
                 available to the researchers who typically develop
                 software engineering tools. Tool evaluation,
                 comparison, and development are consequently impaired.
                 The present work introduces a theoretical framework
                 intended to seed further systematic study of cognitive
                 support in the field of software engineering tools.
                 This theoretical framework, called RODS, imports ideas
                 and methods from a field of cognitive science called
                 ``distributed cognition''. The crucial concept in RODS
                 is that cognitive support can be understood and
                 explained in terms of the computational advantages that
                 are conferred when cognition is redistributed between
                 software developer and their tools and environment. The
                 name RODS, in fact, comes from the four cognitive
                 support principles the framework describes. With RODS
                 in hand, it is possible to interpret good design in
                 terms of how cognition is beneficially rearranged. To
                 make such analyses fruitful, a cognitive modeling
                 framework called HASTI is also proposed. The main
                 purpose of HASTI is to provide an analysis of ways of
                 modifying developer cognition using RODS. RODS and
                 HASTI can be used to convert previously tacit design
                 knowledge into explicit and reusable knowledge. RODS
                 and HASTI are evaluated analytically by using them to
                 reconstruct rationales for two exemplar reverse
                 engineering tools. A preliminary field study was also
                 conducted to determine their potential for being
                 inexpensively applied in realistic tool development
                 settings. These studies are used to draw implications
                 for research in software engineering and, more broadly,
                 for the design of computer tools in cognitive work
                 domains.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Afshar,
  author =       "Ramin Afshar",
  title =        "Mining Frequent Max and Closed Sequential Patterns",
  supervisor =   "Jiawei Han",
  month =        may,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-5-24",
  pages =        "68",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/RaminAfsharMSc.pdf",
  abstract =     "Although frequent sequential pattern mining has an
                 important role in many data mining tasks, however, it
                 often generates a large number of sequential patterns,
                 which reduces its efficiency and effectiveness. For
                 many applications mining all the frequent sequential
                 patterns is not necessary, and mining frequent Max, or
                 Closed sequential patterns will provide the same amount
                 of information. Comparing to frequent sequential
                 pattern mining, frequent Max, or Closed sequential
                 pattern mining generates less number of patterns, and
                 therefore improves the efficiency and effectiveness of
                 these tasks.
                 
                 This thesis first gives a formal definition for
                 frequent Max, and Closed sequential pattern mining
                 problem, and then proposes two efficient programs
                 MaxSequence, and ClosedSequence to solve these
                 problems. Finally it compares the results, and
                 performance of these programs with two brute force
                 programs designed to solve the same problems.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2002@Pei,
  author =       "Jian Pei",
  title =        "Pattern-growth Methods for Frequent Pattern Mining",
  supervisor =   "Dr. Jiawei Han",
  month =        jun,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-05-22",
  pages =        "165",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/JianPeiPhD.pdf",
  abstract =     "Mining frequent patterns from large databases plays an
                 essential role in many data mining tasks and has broad
                 applications. Most of the previously proposed methods
                 adopt apriori-like candidate-generation-and-test
                 approaches. However, those methods may encounter
                 serious challenges when mining datasets with prolific
                 patterns and/or long patterns.
                 
                 In this work, we develop a class of novel and efficient
                 pattern-growth methods for mining various frequent
                 patterns from large databases. Pattern-growth methods
                 adopt a divide-and-conquer approach to decompose both
                 the mining tasks and the databases. Then, they use a
                 pattern fragment growth method to avoid the costly
                 candidate-generation-and-test processing completely.
                 Moreover, effective data structures are proposed to
                 compress crucial information about frequent patterns
                 and avoid expensive, repeated database scans. A
                 comprehensive performance study shows that
                 pattern-growth methods, FP-growth and H-mine, are
                 efficient and scalable. They are faster than some
                 recently reported new frequent pattern mining methods.
                 
                 Interestingly, pattern growth methods are not only
                 efficient, but also effective. With pattern growth
                 methods, many interesting patterns can also be mined
                 efficiently, such as patterns with some tough
                 non-anti-monotonic constraints and sequential patterns.
                 These techniques have strong implications to many other
                 data mining tasks.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Swindells,
  author =       "Colin Edward Swindells",
  title =        "Use That There! Pointing to Establish Device
                 Identity",
  supervisor =   "Dr. Kori M. Inkpen and Dr. John C. Dill",
  month =        jun,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-06-01",
  pages =        "120",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/ColinSwindellsMSc.pdf",
  abstract =     "Computing devices within current work and play
                 environments are relatively static. As the number of
                 networked devices grows, and as people and their
                 devices become more dynamic, situations will commonly
                 arise where users will wish to use that device there
                 instead of navigating through traditional user
                 interface widgets such as lists and trees. Our method
                 of interacting with that device there is composed of
                 two main parts: identification of a target device, and
                 transfer of information to or from the target device.
                 By decoupling these processes, we can explore the most
                 effective way to support each part. This thesis
                 describes our process for identifying devices through a
                 pointing gesture using custom tags and a custom stylus
                 called the gesturePen. Implementation details for this
                 system are provided along with qualitative and
                 quantitative results from a formal user study. The
                 results of this work indicate that our gesturePen
                 method is an effective method for device
                 identification, and is well suited to dynamic computing
                 environments that are envisioned to be commonplace in
                 the near future.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Stark,
   author =       "Paul Stark",
   title =        "Fourier Volume Rendering of Irregular Data Sets",
   supervisor =   "Torsten M{\"o}ller",
   month =        "jun",
   year =         "2002",
   org =          "SFU-CMPT",
   school =       SFU_CS_School,
   defended =     "2002-6-6",
   pages =        "174",
   url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/PaulStrarkMSc.ps.gz",
   abstract =     "Examining irregularly sampled data sets usually
                  requires gridding that data set. However, examination
                  of a data set at one particular resolution may not be
                  adequate since either fine details will be lost, or
                  coarse details will be obscured. In either case, the
                  original data set has been lost. We present an
                  algorithm to create a regularly sampled data set from
                  an irregular one. This new data set is not only an
                  approximation to the original, but allows the original
                  points to be accurately recovered, while still
                  remaining relatively small. This result is accompanied
                  by an efficient `zooming' operation that allows the
                  user to increase the resolution while gaining new
                  details, all without re-gridding the data. The
                  technique is presented in $N$-dimensions, but is
                  particularly well suited to Fourier Volume Rendering,
                  which is the fastest known method of direct volume
                  rendering. Together, these techniques allow accurate
                  and efficient, multi-resolution exploration of volume
                  data.",
   gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Hsiao,
  author =       "Janet Hui-wen Hsiao",
  title =        "Dealing with Semantic Anomalies in a Connectionist
                 Network for Word Prediction",
  supervisor =   "Robert F. Hadley",
  month =        aug,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-08-07",
  pages =        "126",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/JanetHsiaoMSc.ps",
  abstract =     "Humans are able to recognize a grammatically correct
                 but semantically anomalous sentence. On the task of
                 predicting the range of possible next words in a
                 sentence, given the current word as the input, however,
                 many networks (e.g. Elman, 1990, 1993; Christiansen &
                 Chater, 1994; Hadley et al, 2001) that have been
                 proposed are capable of displaying a certain degree of
                 systematicity, but fail in recognizing anomalous
                 sentences.
                 
                 We believe that humans require both syntactic and
                 semantic information to predict the category of the
                 next word in a sentence. Based on an expansion of
                 Hadley's model (Hadley et al, 2001), we present a
                 competitive network, which employs two sub-networks
                 that discern coarse-grained and fine-grained categories
                 respectively, by being trained via different parameter
                 settings. Hence, one of the sub- networks will have a
                 greater capacity for recognizing the syntactic
                 structure of the preceding words, while the other will
                 have a greater capacity for recognizing the semantic
                 structure of the preceding pattern of words.
                 
                 Also, we employ a mechanism to switch attention between
                 the predictions from the two sub-networks, in order to
                 make the global network more closely approximate human
                 behavior. The results show that the network is capable
                 of exhibiting strong systematicity, as defined by
                 Hadley (Hadley, 1994a). In addition, it is able to
                 predict in compliance with the semantic constraints
                 implied in the training corpus, and deal with
                 grammatically correct but semantically anomalous
                 sentences. We can conclude that the network has
                 provided a more realistic model for human behavior on
                 the task of predicting the range of possible next words
                 in a sentence.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Yang,
  author =       "Yingchen Yang",
  title =        "Web Table Mining And Database Discovery",
  supervisor =   "Wo-Shun Luk",
  month =        aug,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-08-02",
  pages =        "143",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/YingchenEricYangMSc.pdf",
  abstract =     "Table mining, as a sub-field of Information Extraction
                 (IE), is concerned with recognizing and extracting
                 information from tables, which are embedded either in
                 plain texts or HTML texts. While early studies of table
                 mining focused on plain text table mining, web table
                 mining has lately received much attention. This is not
                 only because the web is a popular medium for publishing
                 information, but the table is also a document type in
                 HTML in which a great deal of information is found. The
                 focus of this thesis is to investigate how the
                 knowledge of the structural aspects of HTML tables may
                 improve the effectiveness of web table mining, in terms
                 of both increased accuracy and more specificity in the
                 extracted information, which is in a database form,
                 i.e. attribute-value pairs.
                 
                 We have made a systematic study into the structures of
                 both simple and complex tables, and proposed some
                 generic web table mining heuristics and algorithms that
                 can theoretically apply to any web site domain of
                 discourse, and practically proved to be successful in
                 fulfilling web table mining tasks across multiple
                 domains. Based on the analyses of the structures of web
                 tables, the studies in this thesis show that our web
                 table mining heuristics and algorithms perform more
                 effectively than other current approaches, as measured
                 by IE evaluation metrics, precision and recall, in the
                 whole web table mining process including web table
                 recognition, structure interpretation and
                 attribute-value pair extraction. We have also developed
                 heuristics to discover OLAP database by mining into
                 multi-dimensional tables, which usually possess
                 hierarchical attribute structures, and therefore are
                 much more complex than a relation as defined by a
                 relational database.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2002@Ovans,
  author =       "Russell Ovans",
  title =        "A Multiagent Solution to the Venue Equalization
                 Problem",
  supervisor =   "Dr. William S. Havens",
  month =        aug,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-08-01",
  pages =        "145",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/RussellOvansPhD.pdf",
  abstract =     "Within audio engineering the problem of developing
                 systems that automatically equalize themselves has been
                 an active area of research. Traditional approaches have
                 centered on the synthesis of a digital filter that
                 inverts the impulse response of the room, but this
                 technique does not scale to large acoustic spaces
                 measured at multiple listening locations. In this
                 thesis we view the problem as heuristic combinatorial
                 optimization of a search space defined by the settings
                 of conventional 1/3-octave graphic equalizers. The
                 methodology used to develop this system is that of
                 modeling the problem as a multiagent constraint
                 optimization system; i.e., a society of autonomous
                 rational agents that sense and control their
                 environment while seeking to maximize utility. Each
                 agent uses a microphone to sense the frequency response
                 at a given location, and adjusts the equalizer settings
                 in order to maximize its utility by finding a 'good'
                 response curve.
                 
                 Because there are multiple agents each trying to adjust
                 the same set of equalizers, a coordination mechanism is
                 needed to ensure that the system converges to stable
                 states that are fair (maximizes a social welfare
                 function) and efficient (no utility is wasted).
                 Previous multiagent systems research has considered
                 several models of coordination, including economics,
                 dynamical systems, social insects, and AI planning.
                 This thesis proposes a family of novel mechanisms
                 inspired by economic theories of taxation and voting
                 devised for the allocation of public goods. A set of
                 experiments conducted on random problem instances in a
                 simulated acoustic environment compared the efficiency
                 of these mechanisms. The one found to perform best at
                 maximizing the sum of agent utility while minimizing
                 the amount of equalization is a simple aggregate voting
                 system whereby agents submit bids ^V comprised of
                 changes in utility as a function of discrete
                 alternatives to the status quo ^V to a centralized
                 Auctioneer. Coupled with a heuristic equalizer control
                 strategy that seeks to find the best match between the
                 Auctioneer's social choice and the equalizers'
                 preferences, this algorithm is shown to rapidly
                 converge in O(n) iterations and room measurements,
                 where n is the number of equalizers. Utilizing one of
                 these public goods mechanisms, a prototype device was
                 built that successfully equalized a sound system in a
                 real-world setting, and resulted in dramatic
                 improvements in listener quality.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Kroon,
  author =       "Frederick W. Kroon",
  title =        "Linguistic Variation In Information Retrieval Using
                 Query Reformulation",
  supervisor =   "Fred Popowich and Paul McFetridge",
  month =        sep,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-09-03",
  pages =        "107",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2002/FrederickWKroonMSc.ps.gz",
  abstract =     "As time progresses, the amount of written
                 documentation available electronically is increasing
                 dramatically. The field of information retrieval
                 examines automated ways to wade through the reams of
                 electronically available information automatically and
                 efficiently. However, many attempts at providing such a
                 tool have been stymied by various problems relating to
                 the processing of natural languages. The focus of this
                 paper is one such problem - the problem of linguistic
                 variation.
                 
                 Linguistic variation is defined as the ways in which
                 two pieces of text can differ at the surface level,
                 while having the same (or similar) meaning. An overview
                 of the problem of linguistic variation as it relates to
                 the various subfields of information retrieval is
                 presented. Various types of linguistic variation are
                 examined, along with prior approaches to dealing with
                 each.
                 
                 One possible approach to dealing with the problem of
                 linguistic variation is to allow users to construct
                 more complex rules that account for linguistic
                 structure. While this approach may be suitable for
                 those with linguistic training, it is not feasible for
                 casual users. What is needed for such an approach to
                 work for casual users is a system that will construct
                 such rules from simple examples.
                 
                 The Variant Rule Generator (VRG) is a system designed
                 to cope with linguistic variation by generating text
                 retrieval rules that cover all variants from a single
                 text string. VRG essentially defines ``phrase
                 equivalence classes'' into which query phrases can
                 fall. VRG extracts content words from the query phrase
                 using extraction templates that are associated with
                 each phrase equivalence class. Finally, a rule covering
                 the whole equivalence class, instantiated with the
                 content words, is generated.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@SU,
  author =       "Ming-Yen Thomas Su",
  title =        "Item Selection by 'Hub-Authority' Profit Ranking",
  supervisor =   "Ke Wang",
  month =        sep,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-09-10",
  pages =        "51",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/ThomasSuMSc.pdf",
  abstract =     "A fundamental problem in business and other
                 applications is ranking items with respect to some
                 notion of profit based on historical transactions. The
                 difficulty is that the profit of one item not only
                 comes from its own sales, but also from its influence
                 on the sales of other items, that is, the
                 !'cross-selling effect!(. In this thesis, we draw an
                 analogy between this influence and the mutual
                 reinforcement of hub/authority web pages, and we
                 present a novel approach to the item-ranking problem
                 based on this analogy. The idea is ranking items by
                 profit in a way similar to ranking web pages by
                 authority, while taking into account the cross-selling
                 effect. We also address several issues unique to the
                 item ranking.
                 
                 This ranking approach can be applied to solve two
                 selection problems. In the size-constrained selection,
                 the maximum number of items that can be selected is
                 fixed. In the cost-constrained selection, there is no
                 maximum number of items to be selected, but there is
                 some cost associated with the selection of each item.
                 In both cases, the question is what items should be
                 selected to maximize the profit. An empirical study
                 shows that our approach finds profitable items in the
                 presence of cross-selling effect.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Fung,
  author =       "Benjamin C. M. Fung",
  title =        "Hierarchical Document Clustering Using Frequent
                 Itemsets",
  supervisor =   "Prof. Ke Wang",
  month =        sep,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-09-09",
  pages =        "1--63",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/BFungMSc.pdf",
  abstract =     "Most state-of-the art document clustering methods are
                 modifications of traditional clustering algorithms that
                 were originally designed for data tuples in relational
                 or transactional database. However, they become
                 impractical in real-world document clustering which
                 requires special handling for high dimensionality, high
                 volume, and ease of browsing. Furthermore, incorrect
                 estimation of the number of clusters often yields poor
                 clustering accuracy. In this thesis, we propose to use
                 the notion of frequent itemsets, which comes from
                 association rule mining, for document clustering. The
                 intuition of our clustering criterion is that there
                 exist some common words, called frequent itemsets, for
                 each cluster. We use such words to cluster documents
                 and a hierarchical topic tree is then constructed from
                 the clusters. Since we are using frequent itemsets as a
                 preliminary step, the dimension of each document is
                 therefore, drastically reduced, which in turn increases
                 efficiency and scalability.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Gallinger,
  author =       "Ioulia Julie Gallinger",
  title =        "Effect of gradient quantization on quality of
                 volume-rendered images",
  supervisor =   "Torsten M{\"o}ller",
  month =        oct,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-10-18",
  pages =        "134",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2002/IouliaGalingerMSc.pdf",
  abstract =     "Improvements to available computation technology and
                 memory storage have lead to increases in sizes of
                 volume datasets. A dataset with 512 samples in each
                 dimension requires 128 megabytes of storage at 8 bits
                 per sample for the tissue density information. In the
                 field of scientific visualisation, the data is rendered
                 for display, and gradient, or surface normal, at each
                 voxel is used to shade the surfaces, and may also be
                 used to determine the tissue opacity. The gradient has
                 three components, typically stored as floating point
                 values using 96 bits. For a 256x256x145 dataset, 108.75
                 Mb are required for the gradient information. In order
                 to visualise this dataset, the opacities (36.25 Mb
                 total), gradients (108.75 Mb total), and colours
                 (108.75 Mb total) must be computed. Lack of disk and
                 memory storage and long rendering times become
                 prohibitive to efficient use of such data, and data
                 compression becomes essential.
                 
                 The aim of this project is an investigation of the
                 effect of gradient compression on the quality of the
                 rendered images, using several methods of gradient
                 storage, extended to vary the compression rates. Images
                 of several datasets are rendered using ray-casting. The
                 resulting images are compared against a master image,
                 generated with uncompressed gradients. Several image
                 comparison metrics are used. Two of the datasets are
                 synthetically created, and two come from CT and MRI
                 data.
                 
                 Significant reduction in the number of bits per normal,
                 as compared to the 96 bits, is found possible. Several
                 methods and bit ranges are found promising. The choice
                 of a bit range and method depends on the situation at
                 hand. Considerations of available storage, available
                 time and quality requirements are necessary. Whether a
                 dataset is to be rendered repeatedly, or different
                 datasets are to be displayed each time may also affect
                 the final decision, as it is possible to realise
                 greater reduction in necessary storage, while enjoying
                 acceptable quality with some pre-processing for
                 repeatedly-rendered datasets.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Shu,
  author =       "Keith Shu",
  title =        "Digital Fashion: Wearing your heart on your sleeve",
  supervisor =   "Dr. Stella Atkins and Dr. Kori Inkpen",
  month =        "11",
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-11-25",
  pages =        "161",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/KeithSGShuMSc.pdf",
  abstract =     "Clothes and fashion open a secondary i.e. non-verbal
                 communication channel that allows individuals to make
                 connections with each other. Our work proposes the
                 concept of Digital Fashion which uses technology to
                 connect people in close proximity by enhancing a visual
                 secondary communication channel. Non-private profile
                 information known as shared knowledge is communicated
                 via an online poll system that activates poll questions
                 periodically. Our Digital Fashion implementation
                 utilizes a handheld computer connected to a wireless
                 radio network to drive a public wearable display worn
                 on the user's body. The wearable display system
                 consists of flexible electroluminescent wire and a
                 light controller responsive to wireless communications.
                 Shared knowledge is stored as answers to poll
                 questions. As poll questions become active, the
                 wearable display changes colours to reflect the
                 wearers' answer to the active poll question.
                 
                 Several user studies were conducted to evaluate this
                 technology with data from field observations,
                 questionnaires and interviews. The results from our
                 user studies revealed that technology could play a
                 useful role within social settings if designed
                 appropriately. We found the choice of fashion as a
                 secondary communication channel very appropriate
                 because of its unobtrusiveness. Our system did not
                 appear to hinder interactions but rather helped to
                 create richer social interactions.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2002@Chen,
  author =       "Cecilia Chao Chen",
  title =        "Illumination Invariant Image Enhancement",
  supervisor =   "Mark S. Drew",
  month =        dec,
  year =         "2002",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2002-11-26",
  pages =        "87",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2002/CeceliaChaoChenMSc.pdf",
  abstract =     "The grayscale illumination invariant image is formed
                 by projecting 2D log-chromaticity coordinates into a 1D
                 direction orthogonal to lighting change. The
                 lighting-change direction is determined by a
                 calibration of the imaging camera. This invariant is
                 very useful for many computer vision problems, such as
                 removal of shadows for images, hence helps with image
                 retrieval, object recognition, and so on.
                 
                 However, the invariant image quality, i.e., whether the
                 shadow is completely attenuated and how smooth is the
                 transition across the shadow boundary, is affected by
                 many issues. Some of these are: natural lights only
                 approximate theoretical Planckian lights; cameras with
                 real broad-band sensors scatter the lighting-change
                 direction; quantization process produces different
                 intensity patterns on the two sides of shadow
                 boundaries; and most images output from cameras are
                 nonlinear images, so that the chromaticity values are
                 not linearly related to the original color signals
                 sensed by camera sensors.
                 
                 Previous research has shown that invariant images can
                 be formed from real images with only approximately
                 Planckian lights, and the spectral sharpening method
                 can be used to transform broad-band camera sensors to
                 narrower ones. However, little has been done on dealing
                 with nonlinear images.
                 
                 In this thesis, I propose two optimization routines
                 that establish a $3\times 3$ matrix to apply to the
                 camera sensors that can best improve invariant image
                 quality. Observing that sharpened sensors can form
                 better invariant images, I initialize optimizations
                 with a spectral sharpening transform matrix. One
                 optimizer minimizes the difference of invariance
                 intensities formed from the same surfaces under
                 different lightings, and another one directly minimizes
                 the gradient difference of the lighting change
                 directions formed from chromaticity data. Experiment
                 results on both synthesized and real images show that
                 the optimization routines converge and the resulting
                 matrix does improve the invariant.
                 
                 To reduce the effect of quantization, a shadow
                 detection and a smoothing process are carried out on
                 the improved invariant images to diminish the
                 difference between the two sides of shadow boundaries,
                 thus further enhance the invariant images.
                 
                 For nonlinear images, I construct a linearization model
                 and determine the parameters for a particular camera
                 with function fitting. The fitted function is then used
                 to convert nonlinear images taken with this camera to
                 linearized images. Finally the above optimization
                 routines are carried out on linearized images to form
                 invariant images. Experiments show that invariant
                 images derived from linearized images achieve better
                 performance in shadow removal than those formed from
                 nonlinear images directly.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@She,
  author =       "Rong She",
  title =        "Association-Rule-Based Prediction of Outer Membrane
                 Proteins",
  supervisor =   "Ke Wang",
  month =        apr,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-04-10",
  pages =        "57",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/RongSheMSc.pdf",
  abstract =     "A class of medically important disease-causing
                 bacteria (collectively known as Gramnegative bacteria)
                 has been shown to have a rather distinct cell
                 structure, in which there exists an extra ``outer''
                 membrane in addition to the ``inner'' membrane that
                 presents in the cells of most other organisms. Proteins
                 resident in this outer membrane (outer membrane
                 proteins) are of primary research interest since such
                 proteins are exposed on the surface of these bacterial
                 cells and so are the prioritized targets to develop
                 drugs against. Determination of the biological patterns
                 that discriminate outer membrane proteins from
                 non-outer membrane proteins could also provide insights
                 into the biology of this important class of proteins.
                 
                 To date, it remains difficult to predict outer membrane
                 proteins with high precision. Existing protein
                 localization prediction algorithms either do not
                 predict outer membrane proteins at all, or they simply
                 concentrate on the overall accuracy or recall when
                 identifying outer membrane proteins. However, as the
                 study of a potential drug or vaccine takes great amount
                 of time and effort in the laboratory, it is more
                 appropriate that priority be given to getting a high
                 precision on outer membrane protein prediction.
                 
                 In this thesis, we address the problem of protein
                 localization classification with the performance
                 measured mainly on precision of the outer membrane
                 protein prediction. We apply the technique of
                 association-rule based classification and propose
                 several important optimization techniques in order to
                 speed up the rule-mining process. In addition, we
                 introduce the framework of building classifiers with
                 multiple levels, which we call the refined classifier,
                 in order to further improve the classification
                 performance on top of the single-level classifier. Our
                 experimental results show that our algorithms are
                 efficient and produce high precision while maintaining
                 the corresponding recall at a good level. Also, the
                 idea of refined classification indeed improves the
                 performance of the final classifier. Furthermore, our
                 classification rules turn out to be very helpful for
                 biologists to improve their understanding of functions
                 and structures of the outer membrane proteins.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Lewis,
  author =       "Benjamin C. Lewis",
  title =        "Design Trade-offs for Inclined {LEO} Satellite
                 Networks",
  supervisor =   "Joseph Peters",
  month =        apr,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-04-14",
  pages =        "x+57",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/BenjaminCLewisMSc.ps.gz",
  abstract =     "We study inclined low earth orbit (LEO) satellite
                 networks and examine the effects that the various
                 parameters in the design of these networks have on
                 intersatellite communications. The performance of a
                 network can be measured in different ways; we focus on
                 satellite to satellite path lengths, link lengths, the
                 time derivative of link lengths, and the angular
                 velocity requirements for antennas used for links
                 between neighbouring orbital planes. In particular, we
                 show that for networks at a given altitude, the most
                 important design parameter determining path lengths is
                 related to the skew of the network. The other relevant
                 factors are the inclination of the orbits, the number
                 of orbits, the number of satellites per orbit; we
                 determine the relative importance of these factors, and
                 of the interactions between them. We also show that if
                 the goal is to minimize the angular velocities of
                 satellite antennas used for inter-satellite
                 communications in these networks, the effect of any of
                 these design parameters depends very strongly on the
                 value of all the other parameters.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Law,
  author =       "Benjamin Law",
  title =        "Eye Movements in a Virtual Laparoscopic Training
                 Environment",
  supervisor =   "Dr. M. Stella Atkins",
  month =        "04",
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-03-10",
  pages =        "102",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/BenjaminLawMSc.pdf",
  abstract =     "Computer simulations are a promising alternative to
                 traditional surgery training methods especially in
                 minimally-invasive procedures such as laparoscopic
                 surgery. In minimally-invasive surgery (MIS), eye-hand
                 coordination is important because the surgical site is
                 viewed on a monitor with limited depth cues and a
                 restricted field of view. Thus, the emphasis has been
                 on training tools that effectively prepare novices for
                 the eye-hand co-ordination difficulties in MIS. How
                 people interact with the system is of interest to those
                 developing surgical training tools.
                 
                 This thesis compares the eye movements of novice and
                 experienced subjects who performed a task in a virtual
                 laparoscopic training environment. The eye movements in
                 skilled performance were found to be different from
                 those with less experience. In addition, the eye
                 movements of novices after training became more similar
                 to the experienced subjects.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2003@Orchard,
  author =       "Jeffery J. Orchard",
  title =        "Simultaneous Registration and Activation Detection:
                 Overcoming Activation-Induced Registration Errors in
                 Functional {MRI}",
  supervisor =   "M. Stella Atkins",
  month =        apr,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-04-17",
  pages =        "147",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/JefferyJOrchardPhD.pdf",
  abstract =     "In the processing of functional magnetic resonance
                 imaging (fMRI) data, motion correction is typically
                 performed before activation detection. However, on
                 high-eld MR scanners (3 T and higher), the strength of
                 the blood oxygen level dependent (BOLD) signal can
                 cause registration algorithms to produce motion
                 estimates that have stimulus-correlated errors. Motion
                 compensation using these biased motion estimates can
                 result in both false-positive and false-negative
                 regions of activation. By formulating the registration
                 and activation detection problems into a single least-
                 squares problem, both the motion estimates and
                 activation map can be solved for simulta- neously.
                 However, the solution is not unique and an additional
                 constraint is used to nd a solution that is
                 appropriate. This constrained optimization problem can
                 be solved e- ciently, and two equivalent methods are
                 proposed and demonstrated on both simulated and in vivo
                 datasets.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2003@Wang,
  author =       "Yong Wang",
  title =        "Routing Algorithms for Ring Networks",
  supervisor =   "Qian-ping Gu",
  month =        jul,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-06-03",
  pages =        "54",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/YongWangMSc.ps.gz",
  abstract =     "In this thesis, we study routing problems on ring
                 networks. The ring is a popular topology for
                 communication networks and has attracted much research
                 attention.
                 
                 A communication application on a ring network can be
                 regarded as a set of connection requests, each of which
                 is represented by a set of nodes to be connected in the
                 ring network. To implement a communication application,
                 we need to develop a routing algorithm to find a path
                 connecting all the nodes involved in each connection
                 request. One of the most important optimization
                 problems for the communication on ring networks is to
                 develop a routing algorithm such that the maximum
                 congestion (i.e., the maximum number of paths that use
                 any single link in the ring) is minimized. This problem
                 can be formulated as the {\em Minimum Congestion
                 Hypergraph Embedding in a Cycle (MCHEC)} problem with a
                 set of connection requests represented by a hypergraph.
                 A special case of the MCHEC problem, in which each
                 connection request involves exactly two nodes, is known
                 as the {\em Minimum Congestion Graph Embedding in a
                 Cycle} problem. A more general case, in which
                 connection requests may have non-uniform bandwidth
                 requirements, is known as the {\em Minimum Congestion
                 Weighted Hypergraph Embedding in a Cycle} problem. The
                 Minimum Congestion Graph Embedding in a Cycle problem
                 is solvable in polynomial time, and the other problems
                 are NP-hard.
                 
                 In this thesis, we focus on the MCHEC problem and propose
                 efficient algorithms in three categories. In the first
                 category is a 1.8-approximation algorithm that improves
                 the previous 2-approximation algorithms. In the second
                 category is an algorithm that computes optimal
                 solutions for the MCHEC problem. This algorithm runs in
                 polynomial time for subproblems with constant maximum
                 congestions, and is more efficient in terms of the time
                 complexity than the previous algorithm that solves the
                 same problem. The third category contains two heuristic
                 approaches. According to our simulation results, both
                 heuristics have lower time complexities and better
                 practical performance than a well known heuristic.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Bian,
  author =       "Zhengbing Bian",
  title =        "Wavelength Assignment Algorithms for {WDM} Optical
                 Networks",
  supervisor =   "Qian-ping Gu",
  month =        aug,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-07-30",
  pages =        "76",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/ZhengbingBianMSc.pdf",
  abstract =     "The explosive growth of the Internet and
                 bandwidth-intensive applications such as
                 video-on-demand and multimedia conferences require
                 high-bandwidth networks. The current high-speed
                 electronic networks cannot provide such capacity.
                 Optical networks offer much higher bandwidth than
                 traditional networks. When employed with the
                 \emph{wavelength division multiplexing} (WDM)
                 technology, they can provide the huge bandwidth needed.
                 The \emph{tree of rings} is a popular topology which
                 can often be found in WDM networks. In this thesis, we
                 first study \emph{wavelength assignment} (WA)
                 algorithms for trees of rings.
                 
                 A tree of rings is a graph obtained by interconnecting
                 rings in a tree structure such that any two rings
                 intersect in at most one node and any two nodes are
                 connected by exactly two edge-disjoint paths. The WA
                 problem is that given a set of paths on a graph, assign
                 wavelengths to the paths such that any two paths
                 sharing a common edge are assigned different
                 wavelengths and the number of wavelengths is minimized.
                 The WA problem on trees of rings is known to be
                 NP-hard. A trivial lower bound on the number of
                 wavelengths is the maximum number $L$ of paths on any
                 link. In this thesis, we propose a greedy approximation
                 algorithm which uses at most $3L$ wavelengths on a tree
                 of rings with node degree at most 8. This improves the
                 previous $4L$ upper bound. Our algorithm uses at most
                 $4L$ wavelengths for a tree of rings with node degree
                 greater than 8. We also show that $3L$ is the lower
                 bound for some instances of the WA problem on trees of
                 rings. In addition, we show that our algorithm achieves
                 approximation ratios of $2\frac{1}{16}$ and
                 $2\frac{3}{13}$ for trees of rings with node degrees at
                 most $4$ and $6$, respectively.
                 
                 Optical switches, which keep the data stream
                 transmitted in optical form from source to destination
                 to eliminate the electro-optic conversion bottleneck at
                 intermediate nodes, are key devices in realizing the
                 huge bandwidth of optical networks. One of the common
                 ways to build large optical switches is to use
                 \emph{directional couplers} (DCs). However, DCs suffer
                 from an intrinsic crosstalk problem. In this thesis, we
                 study the nonblocking properties of \emph{Benes
                 networks} and \emph{Banyan-type networks} with extra
                 stages under crosstalk constraints.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Lee,
  author =       "Christina Yi-Chien Lee",
  title =        "Capturing Users' Perceptions Of Clickability",
  supervisor =   "Arthur Kirkpatrick",
  month =        aug,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-08-06",
  pages =        "137",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/ChristinaYi-ChienLeeMSc.pdf",
  abstract =     "There are many clickable items on computer screens.
                 Each clickable item is a visual representation of a
                 command indicating its functionality. The problem is
                 that designers have their own conceptual models, which
                 means visual representations of commands are usually
                 very different from one designer to another. In
                 addition, users do not always perceive visual cues the
                 way that designers intended. In fact, two different
                 users may interpret the same visual cue in a completely
                 opposite way. A theory of how users know where to click
                 would ease communication from designers to users.
                 
                 The theory of affordance, defined differently by Gibson
                 and Norman, offers the possibility of such a theory.
                 Gibson (1977) defined affordance as the possible
                 actions available in the environment to an animal.
                 Norman (1988) defined affordance as appearance
                 suggesting possible uses of the object. While Gibson
                 was referring to the physical environment, Norman was
                 referring to the mental model. Whereas Gibson's
                 affordance is independent of individuals, Norman's
                 affordance may be dependent on an individual's
                 experience.
                 
                 Designers who are aware of Norman's definition have
                 applied it in their designs. However, this does not
                 guarantee users perceive commands even though they are
                 visible. On the other hand, users may perceive commands
                 even though the design has not met any design
                 guideline. This is because users have some expectations
                 where and how commands should be represented. To
                 resolve the gap between users and designers, we are
                 looking for a theory of {\"c}lickability{\",} which
                 includes but goes beyond the theory of affordance. We
                 first observed users' behaviour performing specied
                 tasks on real applications. These results were
                 ambiguous. Therefore we developed simple abstract
                 screens, apart from any real application. In those
                 abstract screens, cues are tested separately. Based on
                 the current data, intentions and context direct users'
                 responses. In particular, command location is the most
                 powerful factor of all. In conclusion, there are many
                 factors, which are closely interrelated, involved in
                 the design besides affordance. The theory of
                 {\"c}lickability{\" }must include all of them.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Lapierre,
  author =       "Soleil Lapierre",
  title =        "An Investigation of Fourier Domain Fluid Simulation",
  supervisor =   "Torsten M{\"o}ller",
  month =        aug,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-08-13",
  pages =        "68",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/SoleilLapierreMSc.pdf",
  abstract =     "Motivated by the reduced rendering cost of the Fourier
                 Volume Rendering method, we construct a Navier-Stokes
                 fluid flow simulation that operates entirely in the
                 frequency domain. We show results from a practical
                 implementation and compare with Jos Stam's spatial
                 domain and FFT-based simulations.
                 
                 We break down the simulation pipeline into its major
                 components and evaluate the cost of each component in
                 the spatial domain and the frequency domain. We present
                 an analytical as well as an experimental analysis, and
                 we use our experimental results to identify the most
                 efficient simulation pipelines for all grid sizes.
                 
                 We conclude that frequency domain implementation of the
                 Navier-Stokes flow equations for Euler modeling schemes
                 is prohibitively expensive in terms of compute time. We
                 also conclude that of the solutions we evaluated,
                 Stam's semi-Lagrangian simulation pipelines are the
                 best choices for real-time applications such as video
                 games, where perfect physical accuracy is not
                 required.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2003@Moise,
  author =       "Adrian Moise",
  title =        "Designing better user interfaces for radiology
                 workstations",
  supervisor =   "Dr. M. Stella Atkins",
  month =        aug,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-08-14",
  pages =        "203",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/AdrianMoisePhD.pdf",
  abstract =     "Since the 1980s, radiologists have started to
                 interpret digital radiographs using modern computer
                 systems, a process known as softcopy reading. For
                 softcopy reading, Hanging Protocols are used to
                 automatically arrange images for interpretation upon
                 opening a case, thus minimizing the need for physicians
                 to manipulate images. We have developed a strategy,
                 called HP++, which extends current hanging protocols
                 with support for 'scenario-based' interpretation,
                 matching the radiologist's workflow and ensuring a
                 chronological presentation of information. We
                 hypothesized that HP++ significantly reduces off-image
                 eye fixations, the interpretation time, and the
                 frequency and complexity of user input. We validated
                 our hypothesis with inexpensive usability studies based
                 on an abstraction of the radiologist's task,
                 transferred to novice subjects. For a radiology
                 look-alike task, we compared the performance of 20
                 graduate students using our HP++ based interaction
                 technique with their performance using a conventional
                 interaction technique. We observed a 15% reduction in
                 the average interpretation time using the staged
                 approach, with one third fewer interpretation errors,
                 two thirds fewer mouse clicks, and over 65% less eye
                 gaze over the workstation controls. User satisfaction
                 with the staged interface was significantly higher than
                 with the traditional interface. Preliminary external
                 validation of these results with physician subjects
                 indicate our usability results transfer to radiology
                 softcopy reading. We conclude that designing radiology
                 workstations with support for HP++ can improve the
                 performance of workstation users.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Mitchell,
  author =       "G. Daryn Mitchell",
  title =        "Orientation on Tabletop Displays",
  supervisor =   "Dr. Arthur (Ted) Kirkpatrick",
  month =        oct,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-10-03",
  pages =        "107",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/GDarynMitchellMSc.pdf",
  abstract =     "Tabletop computer displays suffer from an orientation
                 problem. Solutions based on various approaches have
                 been implemented, but there remain unanswered questions
                 about the ideal solution. Within a context of
                 interacting with documents on tabletop displays,
                 requirements for orientation control were found by
                 examining literature on how people use paper documents
                 and what manipulations they perform. It was determined
                 that control must provide quick, either-handed,
                 low-attention manipulation of individual objects on the
                 display. An evaluation of existing interaction
                 techniques for rotation was performed in light of these
                 criteria. The trade-off between two desirable
                 characteristics, integral translation and rotation, and
                 direct input, was examined. An evaluation of
                 mouse-based techniques confirmed that integral input
                 has the potential to be faster than the established
                 sequential-manipulation techniques due to the time
                 saved by overlapping manipulation of different degrees
                 of freedom. For a single task combining translation and
                 rotation the separable technique took twice as long
                 compared to two separate tasks of the same translation
                 or rotation, whereas the integral techniques took less
                 time for the combined action than the two separate
                 actions. However, the integral mouse-based techniques
                 were too slow in their actual manipulation, with the
                 scroll wheel taking four times as long to rotate than
                 the separable technique, and the new drag technique not
                 being used in an integral manner. It was concluded that
                 the current generally available technology such as the
                 mouse and single-point touch-sensitive overlays are
                 inadequate. Acceptable tabletop orientation control
                 will be dependent on the maturation of newer
                 technologies based on tangible interfaces or possibly
                 multi-finger input. Until then, interaction techniques
                 for manipulating documents on tabletop displays should
                 be chosen according to their ability to concurrently
                 control position and orientation, such as three
                 degree-of-freedom input devices on digitizing
                 tablets.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Zhang,
  author =       "Xiang Zhang",
  title =        "A Top-Down Approach for Mining Most Specific Frequent
                 Patterns in Biological Sequence Data",
  supervisor =   "Martin Ester",
  month =        sep,
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-10-29",
  pages =        "58",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/XiangZhangMSc.pdf",
  abstract =     "The emergence of automated high-throughput sequencing
                 technologies has resulted in a huge increase of the
                 amount of DNA and protein sequences available in public
                 databases. A promising approach for mining such
                 biological sequence data is mining frequent
                 subsequences. One way to limit the number of patterns
                 discovered is to determine only the most specific
                 frequent subsequences which subsume a large number of
                 more general patterns. In the biological domain, a
                 wealth of knowledge on the relationships between the
                 symbols of the underlying alphabets (in particular,
                 amino-acids) of the sequences has been acquired, which
                 can be represented in concept graphs. Using such
                 concept graphs, much longer frequent patterns can be
                 discovered which are more meaningful from a biological
                 point of view. In this paper, we introduce the problem
                 of mining most specific frequent patterns in biological
                 data in the presence of concept graphs. While the
                 well-known methods for frequent sequence mining
                 typically follow the paradigm of bottom-up pattern
                 generation, we present a novel top-down method (ToMMS)
                 for mining such patterns. ToMMS (1) always generates
                 more specific patterns before more general ones and (2)
                 performs only minimal generalizations of infrequent
                 candidate sequences. Due to these properties, the
                 number of patterns generated and tested is minimized.
                 Our experimental results demonstrate that ToMMS clearly
                 outperforms state-of-the-art methods from the
                 bioinformatics community as well as from the data
                 mining community for reasonably low minimum support
                 thresholds.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2003@Zhan,
  author =       "Meihua Zhan",
  title =        "Reliable Multicast Extension To {IEEE} 802.11 In
                 Ad-hoc Networks",
  supervisor =   "Joseph G. Peters",
  month =        "12",
  year =         "2003",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-12-02",
  pages =        "80",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/MeihuaZhanMSc.ps.gz",
  abstract =     "The IEEE 802.11 standard uses channel reservation
                 schemes and acknowledgements (ACKs) to provide reliable
                 Medium Access Control (MAC) layer unicast services. In
                 contrast, 802.11 does not guarantee the reliability of
                 MAC layer broadcast and multicast transmissions. This
                 lack of reliability extends to ad hoc routing
                 protocols, such as DSR and AODV, which depend on
                 broadcast packets to exchange routing information among
                 nodes.
                 
                 In this thesis, we introduce an efficient and reliable
                 MAC layer multicast/broadcast protocol called SAM
                 (Sequential Acknowledgement Multicast) protocol. We use
                 simulations to investigate the efficiency and
                 reliability of SAM and to compare the performance of
                 SAM to that of other broadcast/multicast protocols. The
                 foundation of SAM is that the multicast receivers send
                 back Clear to Send (CTS) and ACK frames in a predefined
                 order to avoid collisions at the transmitter. During
                 the retransmission
                 
                 phase of the protocol, the sender only needs to
                 retransmit to nodes that failed to send back a n ACK.
                 This method releases those nodes which have been
                 unnecessarily forced to keep silent in a mult i-hop ad
                 hoc network.
                 
                 SAM is efficient, reliable, and easy to implement. Most
                 importantly, it is compatible with the IEEE 802.11
                 standard." ,
  gen =          "0+",
}

@PhdThesis{U-SFraser-CMPT-PhD:2003@Fong,
  author =       "Philip W. L. Fong",
  title =        "Proof Linking: {A} Modular Verification Architecture
                 for Mobile Code Systems",
  supervisor =   "Robert D. Cameron",
  month =        jan,
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2003-12-22",
  pages =        "221",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2003/PhilipWLFongPhD.pdf",
  postscript =   "ftp://fas.sfu.ca/pub/cs/TH/2003/PhilipWLFongPhD.ps.gz",
  abstract =     "This dissertation presents a critical rethinking of
                 the Java bytecode verification architecture from the
                 perspective of a software engineer. In existing
                 commercial implementations of the Java Virtual Machine,
                 there is a tight coupling between the dynamic linking
                 process and the bytecode verifier. This leads to
                 delocalized and interleaving program plans, making the
                 verifier difficult to maintain and comprehend. A
                 modular mobile code verification architecture, called
                 Proof Linking, is proposed. By establishing explicit
                 verification interfaces in the form of proof
                 obligations and commitments, and by careful scheduling
                 of linking events, Proof Linking supports the
                 construction of bytecode verifier as a separate
                 engineering component, fully decoupled from Java's
                 dynamic linking process. This turns out to have two
                 additional benefits: (1) Modularization enables
                 distributed verification protocols, in which part of
                 the verification burden can be safely offloaded to
                 remote sites; (2) Alternative static analyses can now
                 be integrated into Java's dynamic linking process with
                 ease, thereby making it convenient to extend the
                 protection mechanism of Java. These benefits make Proof
                 Linking a competitive verification architecture for
                 mobile code systems. A prototype of the Proof Linking
                 Architecture has been implemented in an open source
                 Java Virtual Machine, the Aegis VM ({\tt
                 http://aegisvm.sourceforge.net}).
                 
                 On the theoretical side, the soundness of Proof Linking
                 was captured in three correctness conditions: Safety,
                 Monotonicity and Completion. Java instantiations of
                 Proof Linking with increasing complexity have been
                 shown to satisfy all the three correctness conditions.
                 The correctness proof had been formally verified by the
                 PVS proof checker.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@Vajihollahi,
  author =       "Mona Vajihollahi",
  title =        "High Level Specification and Validation of the
                 Business Process Execution Language for Web Services",
  supervisor =   "Dr. Uwe Glaesser",
  month =        "April",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-04-06",
  pages =        "151",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/MonaVajihollahiMSc.pdf",
  gen =          "0-+",
  abstract =     "The Business Process Execution Language for Web Services 
                 (BPEL) is an XML based formal language for the design of
                 networking protocols for automated business processes. 
                 Originally introduced by leading e-business vendors, including
                 IBM and Microsoft, BPEL is now a forthcoming industrial 
                 standard as the work on the language continues at 
                 OASIS\footnote{Organization for the Advancement of Structured 
                 Information Standards (OASIS), www.oasis-open.org} within the
                 technical committee on the Web Services Business Process 
                 Execution Language (WSBPEL TC).

                 We formally define an abstract executable semantics for the 
                 language in terms of a distributed abstract state machine 
                 (DASM). The DASM paradigm has proven to be a feasible, yet
                 robust, approach for modeling architectural and programming 
                 languages and has been used as the basis for industrial
                 standardization before.

                 The goal of this work is to support the design and 
                 standardization of BPEL by eliminating weak points in the 
                 language definition and validating key system attributes 
                 through experimental validation. The necessity of
                 formalisation in the standardization process is well 
                 recognized by the OASIS WSBPEL TC and is formulated as one of 
                 the basic issues by the technical committee. \emph{``There is 
                 a need for formalism. It will allow us to not only reason 
                 about the current specification and related issues, but also 
                 uncover issues that would otherwise go unnoticed. Empirical 
                 deduction is not sufficient.''}\footnote{Issue \#42, WSBPEL 
                 Issue List, WSBPEL TC at OASIS}.
 
                 We take a hierarchical refinement approach to model the 
                 language. Starting from an abstract ground model of the core 
                 attributes of the language, we perform step-wise refinements 
                 obtaining a hierarchy of ground models at different levels of 
                 abstraction which leads to the final executable model. The 
                 executable model is then used together with a graphical
                 visualization tool to experimentally validate the key
                 attributes of the language through simulation of abstract 
                 machine runs.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@Zhang, 
  author =       "Zhang, Yingjian", 
  title =        "Prediction of Financial Time Series with Hidden Markov 
                 Models", 
  supervisor =   "Dr. Anoop Sarkar and Dr. Andrey Pavlov and 
                 Dr. Oliver Schulte", 
  month =        "may", 
  year =         "2004", 
  org =          "SFU-CMPT", 
  school =       "SFU_CS_School", 
  defended =     "2004-05-27", 
  pages =        "90", 
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/YingjianRockyZhangMSc.pdf",
  abstract =     "In this thesis, we develop an extension of the Hidden 
                 Markov Model (HMM) that addresses two of the most important
                 challenges of financial time series modeling: non-stationary
                 and non-linearity. Specifically, we extend the HMM to 
                 include a novel exponentially weighted 
                 Expectation-Maximization (EM) algorithm to handle these two
                 challenges. We show that this extension allows the HMM 
                 algorithm to model not only sequence data but also dynamic 
                 financial time series. We show the update rules for the HMM
                 parameters can be written in a form of exponential moving
                 averages of the model variables so that we can take the 
                 advantage of existing technical analysis techniques. We 
                 further propose a double weighted EM algorithm that is able
                 to adjust training sensitivity automatically. Convergence
                 results for the proposed algorithms are proved using 
                 techniques from the EM Theorem. 
                  
                 Experimental results show that our models consistently beat 
                 the S\&P 500 Index over five 400-day testing periods from 
                 1994 to 2002, including both bull and bear markets. Our 
                 models also consistently outperform the top 5 S\&P 500 
                 mutual funds in terms of the Sharpe Ratio.",
} 


@MastersThesis{U-SFraser-CMPT-MSc:2004@SWu,
  key =          "SWu2004",
  author =       "Shufang Wu",
  title =        "Practical lossless broadcasting schemes for variable bit
                 rate videos in video-on-demand service",
  supervisor =   "Tsunehiko Kameda",
  month =        "06",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-06-16",
  gen =          "0->",
  pages =        "107",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/ShufangWuMSc.pdf",
  abstract =     "Broadcasting in video-on-demand services scales well in
                 transporting popular videos. A large number of broadcasting 
                 schemes have been proposed to minimize the server bandwidth
                 for a given waiting time, a small number of which deals
                 with variable bit rate (VBR) videos that are used in 
                 practice. We propose three new lossless VBR video 
                 broadcasting schemes to improve on existing schemes.  
                 Backward Segmentation using Equal Bandwidth and 
                 Prefix-Caching (BSEB-PC) Segments a video backwards from 
                 its end until a relatively small part is left, which is 
                 cached by users in advance. All segments except the initial 
                 one are repeatedly broadcast on given standard channels of
                 equal bandwidth. Experiments with real videos show that 
                 this scheme achieves server bandwidth requirement within 1% 
                 of the optimum.  Forward Segmentation using Equal Bandwidth
                 (FSEB) addresses the issue of minimizing server bandwidth
                 using channels of equal bandwidth, given an upper bound on
                 initial user wait time and user-bandwidth. Experimental 
                 results with real videos show that it achieves both better
                 server bandwidth and user bandwidth than any other known 
                 scheme in the same deployment environment (i.e., number of 
                 server broadcasting channels, the maximum number of channels
                 from which a user can receive concurrently, and user wait
                 time).  General Frame Level Segmentation (GFLS) deals with
                 the situation where there are frames in the video that
                 require future reference frames for decoding. This situation
                 is common in practice, but has not been considered in the 
                 literature.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@XWu,
  key =          "XWu2004",
  author =       "Xiaojing Wu",
  title =        "Efficient Java Interface Invocation Using IZone",
  supervisor =   "Dr. Robert D. Cameron",
  month =        "jun",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-06-29",
  gen =          "0+",
  pages =        "119",
  postscript =   "ftp://fas.sfu.ca/pub/cs/TH/2004/LindaXiaojingWuMSc.ps.gz",
  abstract =     "This thesis addresses the problem of improving the
                 efficiency of interface invocation in the Java Virtual
                 Machine (JVM). In current JVM implementations, interface
                 method invocation is not so efficient as virtual method
                 invocation, because of the need to support multiple
                 interface inheritance in Java. This leads to the mistaken
                 impression that Java interface invocation is inherently
                 inefficient. This thesis will show that, with proper
                 implementation, the performance of interface invocation can
                 be substantially improved.

                 A new approach -- IZone based interface invocation -- is
                 proposed in this thesis. IZone is a new data structure
                 associated with an interface type in the method area. It is
                 composed of several implementation lookup areas, one for
                 each subclass of the interface. IZone is populated as
                 subclasses are loaded and resolved, ensuring that the lookup
                 areas within it are arranged in the resolution order of the
                 corresponding subclasses. A fully constructed IZone contains
                 pointers to different subclasses' implementation of the
                 interface methods. Within a lookup area, the pointers are
                 arranged according to the corresponding methods' declaration
                 order by the interface. By taking advantage of class
                 resolution orders and method declaration orders, IZone
                 provides a quick access to the implementation of interface
                 methods. As the experimental results demonstrate, with
                 moderate space overhead, IZone-based interface invocation is
                 the fastest approach after lightweight optimizations, and
                 the second fastest after heavyweight optimizations. "
}


@PhdThesis{U-SFraser-CMPT-PhD:2004@Zahariev,
  author =       "Zahariev, Manuel",
  title =        "A (Acronyms)",
  supervisor =   "Veronica Dahl",
  month =        "June",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-04-02",
  gen =          "0->",
  pages =        "179",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/ManuelZaharievPhD.pdf",
  abstract =     "Acronyms are a significant and the most dynamic area of
                 the lexicon of many languages.  Building automated acronym
                 systems poses two problems: acquisition and disambiguation.
                 Acronym acquisition is based on the identification of 
                 anaphoric or cataphoric expressions which introduce the
                 meaning of an acronym in text; acronym disambiguation is a
                 word sense disambiguation task, with expansions of an acronym
                 being its possible senses.  It is proposed here that acronyms
                 are universal phenomena, occurring in all languages with a
                 written form, and that their formation is governed by 
                 linguistic preferences, based on regularities at the 
                 character, phoneme, word and phrase levels.  A universal
                 explanatory theory of acronyms is presented, which rests on a
                 set of testable hypotheses, and is manifested through a set of
                 violable, ordered rules. The theory is developed based on
                 examples from fifteen languages, with six different writing 
                 systems. A dynamic programming algorithm is implemented based 
                 on the explanatory theory of acronyms. The algorithm is
                 evaluated on lists of acronyms-expansion pairs in Russian, 
                 Spanish, Danish, German, English, French, Italian, Dutch, 
                 Portuguese, Finnish, and Swedish and achieves excellent
                 performance.  A two-pass greedy algorithm for automatic 
                 acronym acquisition is designed, which results in good
                 performance for specific domains. A hybrid, machine learning
                 algorithm . using features generated through dynamic
                 programming acronym-expansion matching . is proposed and
                 results in good performance on noisy, parsed, newspaper text.
                 A machine learning algorithm for acronym sense disambiguation 
                 is presented, which is trained and evaluated automatically on
                 information downloaded following search engine lookup. The
                 algorithm achieves good performance on deciding whether an 
                 acronym occurs with a certain sense in a given context, and
                 good accuracy when picking the correct sense for an acronym in
                 a given context.  All algorithms presented allow for 
                 efficient, readily usable implementations that can be
                 included as components in larger natural language frameworks. 
                 Technologies developed have applicability beyond acronym
                 acquisition and disambiguation, to aspects of the more
                 general problems of anaphora resolution and word sense 
                 disambiguation, within information extraction or natural
                 language understanding systems.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@FARAHBOD,
  key =          "Farahbod2004",
  author =       "Roozbeh Farahbod",
  title =        "Extending and Refining an Abstract Operational Semantics of
                 the Web Services Architecture for the Business Process 
                 Execution Language",
  supervisor =   "Uwe Gl\'{a}sser",
  month =        "jul",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-07-30",
  pages =        "199",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/RoozbehFarahbodMSc.pdf",
  abstract =     "The Business Process Execution Language for Web Services
                 (BPEL) is a forthcoming industrial standard for automated 
                 business processes, proposed by the 
                 OASIS\footnote{Organization for the Advancement of Structured
                 Information Standards} Web Services BPEL Technical 
                 Committee. BPEL is a service orchestration language which 
                 extends the underlying Web services interaction model and 
                 enables Web services to support long running business 
                 transactions.  We formally define an abstract operational 
                 semantics for BPEL based on the {\em abstract state machine}
                 (ASM) paradigm. Specifically, we model the dynamic 
                 properties of the key language constructs through the 
                 construction of a {\em BPEL Abstract Machine} in terms of 
                 partially ordered runs of distributed real-time ASMs. The 
                 goal of our work is to provide a well defined semantic
                 foundation for establishing the key language attributes by 
                 eliminating deficiencies hidden in the informal language 
                 definition. This work combines two well-defined ASM 
                 refinement techniques to complement our previous efforts on
                 the \emph{core} model of the BPEL Abstract Machine. First, 
                 we elaborate the \emph{core} model with regard to structural 
                 and behavioural aspects to make it more robust and flexible 
                 for further refinements. Specifically, we formalize the 
                 {\em process execution model} of BPEL and its decomposition 
                 into {\em execution lifecycles} of BPEL activities. We also 
                 introduce an \emph{agent interaction model} to facilitate 
                 the interaction between different Distributed Abstract State
                 Machine (DASM) agents of the BPEL Abstract Machine. We then
                 extend the \emph{core} model through two consecutive 
                 refinement steps to include \emph{data handling} and one of 
                 the most controversial issues in BPEL, \emph{fault and 
                 compensation handling}. The resulting abstract machine model
                 provides a comprehensive formalization of the BPEL dynamic 
                 semantics and the underlying Web services architecture."
}


@PhdThesis{U-SFraser-CMPT-PhD:2004@Tory,
  key =          "Tory2004",
  author =       "Melanie Tory",
  title =        "Combining 2D and 3D Views for Visualization of Spatial 
                 Data",
  supervisor =   "Torsten M{\"o}ller",
  month =        "jul",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-07-06",
  pages =        "",
  gen =          "0+",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/MelanieToryPhD.pdf",
  abstract =     "This research compares two-dimensional (2D), 
three-dimensional (3D), and 2D/3D combination displays (orientation icon, 
ExoVis, and in-place) for visualization of 3D spatial data. Both 2D and 3D 
views can be valuable for different reasons. 3D views can provide an 
overview of a 3D space, illustrate the 3D shape of objects, and support 3D 
navigation. 2D views can reduce occlusion of specific parts, show 
undistorted angles and distances, and enable precise positioning and 
navigation. Combining 2D and 3D views is valuable when benefits of 2D and 
3D are both relevant to the task.

First, three 2D/3D combination displays were compared in terms of physical 
integration of views, occlusion, deformation, flexibility, screen space 
requirements, and viewing angles. Orientation icons (i.e., 2D and 3D views 
separated into different windows) offered high flexibility, non-oblique 
viewing, and low occlusion and deformation, but required substantial screen 
space and had poor integration of 2D and 3D views. In-place displays (i.e., 
clip and cutting planes) were the opposite. ExoVis displays (i.e., 2D views 
surrounding a 3D view in the same scene) had better integration than 
orientation icons, but greater flexibility and less occlusion and 
deformation than in-place displays.

A theory describing when orientation icon, ExoVis, and in-place displays 
would be useful was then developed, and experiments that compared 2D 
displays, 3D displays, and 2D/3D combinations for mental registration, 
relative positioning, orientation, and volume of interest tasks were 
performed. In-place supported the easiest mental registration of 2D and 3D 
views, followed by ExoVis, and lastly orientation icon displays. 3D 
displays were effective for approximate navigation and positioning when 
appropriate cues (e.g., shadows) were present, but were not effective for 
precise navigation and positioning except in specific circumstances (e.g., 
with good viewing angles). For precise tasks, orientation icon and ExoVis 
displays were better than 2D or 3D displays alone. These displays had as 
good or better performance, inspired higher confidence, and allowed 
natural, integrated navigation. In-place displays were not effective for 3D 
orientation because they forced users to frequently switch back and forth 
between dimensions. Major factors contributing to display preference and 
usability were task characteristics, personal strategy, orientation cues, 
spatial proximity of views that were used together, occlusion, oblique 
viewing of 2D views, and methods used to interact with the display. Results 
of this thesis can be used to guide designers to choose the most 
appropriate display technique for a given task."
}

@MastersThesis{U-SFraser-CMPT-MSc:2004@Letourneau,
  key =          "Letourneau2004",
  author =       "Michael J. Letourneau",
  title =        "Heuristics for Generating Additive Spanners",
  supervisor =   "Dr. Arthur L. Liestman",
  month =        "aug",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-08-05",
  pages =        "113",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/MichaelLetourneauMSc.pdf",
  abstract =     "Given an undirected and unweighted graph $G$, the subgraph 
                 $S$ is an additive spanner of $G$ with delay $d$ if the 
                 distance between any two vertices in $S$ is no more than $d$
                 greater than their distance in $G$. It is known that the
                 problem of finding additive spanners of arbitrary graphs for
                 any fixed value of $d$ with a minimum number of edges is 
                 NP-hard.  Additive spanners are used as substructures for 
                 communication networks which are subject to design 
                 constraints such as minimizing the number of connections in
                 the network, or permitting only a maximum number of 
                 connections at any one node.

                 In this thesis, we consider the problem of constructing good
                 additive spanners. We say that a spanner is ``good'' if it 
                 contains few edges, but not necessarily a minimum number of 
                 them. We present several algorithms which, given a graph $G$
                 and a delay parameter $d$ as input, produce a graph $S$
                 which is an additive spanner of $G$ with delay $d$.

                 We evaluate each of these algorithms experimentally over a 
                 large set of input graphs, and for a series of delay values.
                 We compare the spanners produced by each algorithm against
                 each other, as well as against spanners produced by the
                 best-known constructions for those graph classes with known 
                 additive spanner constructions. We highlight several 
                 algorithms which consistently produce spanners which are good
                 with respect to the spanners produced by the other
                 algorithms, and which are nearly as good as or, in some
                 cases, better than the spanners produced by the 
                 constructions. Finally, we conclude with a discussion of 
                 future algorithmic approaches to the construction of 
                 additive spanners, as well as a list of possible 
                 applications for additive spanners beyond the realm of
                 communication networks."
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@Mukherjee,
  key =          "Mukherjee2004",
  author =       "Kaustav Mukherjee",
  title =        "Application of the Gabriel Graph to Instance Based
                 Learning Algorithms",
  supervisor =   "Dr. Binay Bhattacharya",
  month =        "aug",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       "SFU_CS_School",
  defended =     "2004-08-25",
  pages =        "52",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/KaustavMukherjeeMSc.pdf",
  abstract =     "Instance based learning (IBL) algorithms attempt to
                 classify a new unseen instance (test data) based on some 
                 ^proximal neighbour^ rule, i.e. by taking a majority vote 
                 among the class labels of its proximal instances in the
                 training or reference dataset. The k nearest neighbours
                 (k-NN) are commonly used as the proximal neighbours. We study
                 the use of the state of the art approximate technique of 
                 k-NN search on the best known IBL algorithms. The results
                 are impressive; substantial speed up in computation is 
                 achieved and on average the accuracy of classification is 
                 preserved.
                  
                 Geometric proximity graphs especially the Gabriel graph (a 
                 subgraph of the well known Delaunay triangulation) provides 
                 an elegant algorithmic alternative to k-NN based IBL
                 algorithms. The main reason for this is that the Gabriel 
                 graph preserves the original nearest neighbour decision 
                 boundary between data points of different classes very well.
                 However computing the Gabriel graph of a dataset in practice
                 is prohibitively expensive. Extending the idea of 
                 approximate k-NN search to approximate Gabriel neighbours 
                 search, it becomes feasible to compute the latter. We 
                 thin (reduce) the original reference dataset by computing its
                 approximate Gabriel graph and use it independently as a 
                 classifier as well as an input to the IBL algorithms. We 
                 achieve excellent empirical results; in terms of 
                 classification accuracy, reference set storage reduction and
                 consequently, query response time. The Gabriel thinning 
                 algorithm coupled with the IBL algorithms consistently 
                 outperforms the IBL algorithms used alone, with respect to 
                 storage requirements and maintains similar accuracy levels."
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@Evangelista,
  key =          "Evangelista2004",
  author =       "Eric Evangelista",
  title =        "A Knowledge-Level View of Consistent Query Answers",
  supervisor =   "Dr. James P. Delgrande",
  month =        "sep",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-09-09",
  pages =        "113",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/EricEvangelistaMSc.pdf",
  abstract =     "Of the numerous formal approaches that deal with
                 database inconsistencies with respect to integrity
                 constraints, all share the view that such constraints
                 are statements about the world the database models. An
                 alternative perspective, however, considers constraints
                 as statements about the knowledge the database has of
                 its domain. The result of this shift in perspective
                 allows us to regard integrity constraint violations as
                 a fragment of the incomplete knowledge the system has
                 of the world. We can then query the possibly
                 inconsistent database for {\em consistent query
                 answers}.\\ \indent We address the above considerations
                 with an epistemic query language $\mathcal{KL}$, where
                 the possible ways to {\em repair} a database that
                 violates its integrity constraints are characterized by
                 a set of possible worlds, an {\em epistemic state}
                 $e_{C}$. This culminates in a situation where only
                 consistent information is known. We ascertain this by
                 querying $e_C$ with $\mathcal{KL}$, providing a
                 knowledge-level formalization of consistent query
                 answers. At the outset, we show that $\mathcal{KL}$ is
                 an adequate language for querying databases by
                 specifying a class of {\em admissible} formulas for
                 which the set of answers to such queries are {\em safe}
                 and {\em domain independent}. After formulating {\em
                 database dependencies} in $\mathcal{KL}$, we prove that
                 they are members of this class. A Prolog-like sound and
                 complete query evaluator, $cqa$, for {\em admissible}
                 $\mathcal{KL}$ formulas is presented. Finally, we
                 completely characterize what is known in $e_C$ with a
                 set of first-order sentences.",
}

@PhdThesis{U-SFraser-CMPT-PhD:2004@Grbavec,
  key =          "Grbavec2004",
  author =       "Ann Marie Grbavec",
  title =        "Second-Order Generalization in Neural Networks",
  supervisor =   "Dr. Robert F. Hadley",
  month =        "oct",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-10-06",
  pages =        "183",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2004/AnnGrbavecPhD.ps.gz",
  abstract =     "One of the main strengths of neural networks is their 
                 ability to generalize beyond training data. However, recent 
                 research suggests that certain types of generalization, 
                 which humans appear to perform readily, are problematic for 
                 traditional neural networks. This thesis examines the 
                 foundations of such claims and considers possibilities for 
                 resolving several issues they raise.

                 These forms of generalization have attracted attention in 
                 large part due to their implications about the role of 
                 symbol-processing in cognition. They have been shown to be 
                 beyond the scope of the types of neural networks that have 
                 been considered to offer an alternative to classically 
                 symbolic representations and rules: back-propagating 
                 multilayer  perceptrons and simple recurrent networks. 
                 Interestingly, claims have been made that many classically 
                 symbolic machine learning techniques also fail to generalize
                 in this way. 

                 The tasks in question can be described, broadly, as 
                 generalizing relations to novel items. Previous formulations 
                 of the research problem have offered various specific but 
                 somewhat inconsistent criteria for characterizing this type 
                 of generalization. In this thesis, an analysis of previous 
                 formulations reveals how they are limited by unacknowledged 
                 assumptions about the role of representation, task form, and
                 learning in problem equivalence. A framework for specifying 
                 a generalization task is introduced to support a more lucid 
                 discussion of these issues. Applying the framework to sample 
                 tasks reveals an underlying distinction between ways in 
                 which a generalization may be reached. Based on the results
                 of the analysis, a more unified view of the problem space is
                 presented and related sub-problems are identified. 

                 Ways in which winner-take-all networks could play a role in 
                 the solution of two problem classes are outlined. 
                 Winner-take-all networks are considered to be more 
                 biologically plausible than the back-propagating networks
                 which have been considered previously and unsuccessfully for
                 the solution of such problems."

}

@MastersThesis{U-SFraser-CMPT-MSc:2004@ZhiwenLin,
  key =          "Zhiwen2004",
  author =       "Zhiwen Lin",
  title =        "Near-Optimal Heuristic Solutions for Truncated Harmonic
                 Windows Scheduling and Harmonic Group Windows Scheduling",
  supervisor =   "Dr. Tiko Kameda",
  month =        "oct",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-10-12",
  pages =        "125",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/ZhiwenLinMSc.pdf",
  abstract =     "Dividing a video into many segments and then broadcasting
                 each segment periodically has proved to be an efficient and 
                 cost-effective way of providing near Video-on-Demand 
                 services. Some of the known broadcasting schemes, such as 
                 Fixed-Delay Pagoda Broadcasting (FDPB), adopt the 
                 fixed-delay policy, which requires the user to wait for a 
                 fixed time before watching a video. Our first broadcasting 
                 scheme, the Generalized Fixed-Delay Pagoda Broadcasting 
                 (GFDPB), based on the fixed-delay policy, improves Bar-Noy 
                 et al.^s greedy algorithm for the Harmonic Windows
                 Scheduling Problem. GFDPB achieves the lowest maximum 
                 waiting time among all the known protocols using segments of
                 equal duration and channels of equal bandwidth. In addition,
                 its performance is very close to the theoretical
                 optimum. Second, we define the Harmonic Group Windows 
                 Scheduling (HGWS) problem and present a new broadcasting 
                 scheme to solve it, Harmonic Page-set Broadcasting (HPB), 
                 which provides the lowest average waiting time of all
                 currently known protocols by using the fewest channels for 
                 given server bandwidth. Finally, we present a hybrid 
                 broadcasting scheme, Preloading Page-Set Broadcasting 
                 (PPSB), which compromises between the average waiting time 
                 and the maximum waiting time of HPB. While still providing 
                 the shortest average waiting time of all known protocols 
                 using segments of equal duration and channels of equal 
                 bandwidth, PPSB achieves much shorter maximum waiting time
                 than HPB. Furthermore, PPSB provides a very desirable trade-
                 off between the average waiting time and the maximum waiting
                 for a given server bandwidth, while guaranteeing that its
                 maximum waiting time is only 1/3 longer than its average 
                 waiting time."
}

@MastersThesis{U-SFraser-CMPT-MSc:2004@Anderson,
  key =          "Anderson2004",
  author =       "Darryl Anderson",
  title =        "Image Object Search Combining Colour With Gabor
                 Wavelet Shape Descriptors",
  supervisor =   "Dr. Mark Drew",
  month =        "December",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-09-08",
  pages =        "102",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2004/DarrylAndersonMSc.pdf",
  abstract =     "An image and object search and retrieval algorithm is
                  devised that combines colour and spatial information. 
                  Spatial characteristics are described in terms of Wiskott's
                  jets formulation, based on a set of Gabor wavelet functions
                  at varying scales, orientations and locations. Colour 
                  information is first converted to a form more impervious to
                  illumination colour change, reduced to 2D, and encoded in a
                  histogram. The histogram, which is based on a new stretched
                  chromaticity space for which all bins are populated, is 
                  resized and compressed by way of a DCT. An image database 
                  is devised by replicating JPEG images by a set of 
                  transforms that include resizing, various cropping attacks,
                  JPEG quality changes, aspect ratio alteration, and
                  reducing colour to greyscale. Correlation of the complete 
                  encode vector is used as the similarity measure. For both 
                  searches with the original image as probe within the 
                  complete dataset, and with the altered images as probes
                  with the original dataset, the grayscale, the stretched, and
                  the resized images had near-perfect results. The most 
                  formidable challenge was found to be images that were 
                  cropped both horizontally as well as vertically. The 
                  algorithm's ability to identify objects, as opposed to just
                  images, is also tested. In searching for images in a set 
                  of 5 classifications, the jets were found to contribute 
                  most analytic power when objects with distinctive spatial 
                  characteristics were the target."
}


@MastersThesis{U-SFraser-CMPT-MSc:2004@Hwang,
  key =          "HWang2004",
  author =       "Cho Yee Joey Hwang",
  title =        "A Theoretical Comparison of Resolution Proof Systems for
                 {CSP} Algorithms",
  supervisor =   "Dr. David G. Mitchell",
  month =        "Dec",
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-12-02",
  pages =        "82",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2004/ChoYeeJoeyHwangMSc.pdf",
  abstract =     "Many problems from a variety of applications such as
                 graph coloring and circuit design can be modelled as
                 constraint satisfaction problems (CSPs). This provides
                 strong motivation to develop effective algorithms for
                 CSPs. In this thesis, we study two resolution-based
                 proof systems, NG-RES and C-RES, for finite-domain CSPs
                 which have a close connection to common CSP algorithms.
                 We give an almost complete characterization of the
                 relative power among the systems and their restricted
                 tree-like variants. We demonstrate an exponential
                 separation between NG-RES and C-RES, improving on the
                 previous super-polynomial separation, and present other
                 new separations and simulations. We also show that most
                 of the separations are nearly optimal. One immediate
                 consequence of our results is that simple backtracking
                 with 2-way branching is exponentially more powerful
                 than simple backtracking with d-way branching.",
}

@MastersThesis{U-SFraser-CMPT-MSc:2004@Storjohann,
  author =       "Storjohann, Rasmus",
  title =        "Growing brains \emph{in silico}: Integrating biochemistry,
                 genetics and neural activity in neurodevelopmental
                 simulations",
  supervisor =   "Dr. Robert F. Hadley",
  month =        dec,
  year =         "2004",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2004-12-14",
  pages =        "346",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2004/RasmusStorjohann.pdf",
  abstract =     "Biologists' understanding of the roles of genetics,
                 biochemistry and activity in neural function is rapidly
                 improving. All three interact in complex ways during 
                 development, recovery from injury and in learning and memory. 
                 The software system NeuroGene was written to simulate
                 neurodevelopmental processes. Simulated neurons develop 
                 within a 3D environment. Protein diffusion, decay and 
                 receptor-ligand binding are simulated. Simulations are 
                 controlled by genetic information encoded using a novel
                 programming language mimicking the control mechanisms of
                 biological genes. Simulated genes may be regulated by protein
                 concentrations, neural activity and cellular morphology. 
                 Genes control protein production, changes in cell morphology 
                 and neural properties, including learning. We successfully 
                 simulate the formation of topographic projection from the
                 retina to the tectum. We propose a novel model of topography
                 based on simulated growth cones. We also simulate 
                 activity-dependent refinement, through which diffuse 
                 connections are modified until each retinal cell connects to 
                 only a few target cells."
}


@PhdThesis{U-SFraser-CMPT-PhD:2005@Lu,
  author =       "Ye Lu",
  title =        "Automatic Object Extraction and Reconstruction in
                 Active Video",
  supervisor =   "Dr. Ze-Nian Li",
  month =        jan,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-01-10",
  pages =        "158",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2005/YeLuPhD.pdf",
  abstract =     "A new method of video object extraction is proposed to
                 accurately obtain the object of interest from actively
                 acquired videos. Traditional video object extraction
                 techniques often operate under the assumption of
                 homogeneous object motion and extract various parts of
                 the video that are motion consistent as objects. In
                 contrast, the proposed active video object extraction
                 (AVOE) paradigm assumes that the object of interest is
                 being actively tracked by a camera moving in 3D and
                 classifies the possible motions of the camera that
                 result in the 2D motion patterns as recovered from 2D
                 image sequences. Consequently, the AVOE method is able
                 to extract only objects of interest from active videos
                 while ignoring other less important objects. We
                 formalize the AVOE process using notions from Gestalt
                 psychology. We define a new Gestalt factor called
                 ``shift and hold'' which acts as a bridge between 2D
                 Gestalt groupings and 3D object perception.
                 
                 We also propose a novel cooperative method for
                 efficient dense 2D motion estimation as part of the
                 AVOE framework. Using motion fields recovered from
                 successive frames of the video, we propose a core
                 algorithm to perform 2D object extraction. In addition,
                 we also propose a linear programming based boundary
                 adjustment algorithm that takes into account the
                 strength and orientation of candidate boundary pixels
                 to refine object outlines extracted by the core
                 algorithm.
                 
                 More effective indexing and retrieval techniques can be
                 devised if the extracted objects are not limited only
                 to their 2D views but can be intelligently integrated
                 to form 3D object models. In this way, objects can be
                 searched and retrieved using their 3D shapes in
                 addition to the 2D image based features. In order to
                 address this need for 3D object models, we also
                 describe the design and implementation of an active
                 video object extraction and 3D reconstruction system as
                 part of this thesis.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2005@Chen,
  author =       "Hao (Leo) Chen",
  title =        "User Clustering and Traffic Prediction in a Trunked
                 Radio System",
  supervisor =   "Ljiljana Trajkovic",
  month =        feb,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-02-14",
  pages =        "91",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/HaoLeoChenMSc.pdf",
  ps =           "ftp://fas.sfu.ca/pub/cs/theses/2005/HaoLeoChenMSc.ps",
  abstract =     "Traditional statistical analysis of network data is
                 often employed to determine traffic distribution, to
                 summarize user's behavior patterns, or to predict
                 future network traffic . Mining of network data may be
                 used to discover hidden user groups, to detect payment
                 fraud, or to identify network abnormalities. In our
                 research we combine traditional traffic analysis with
                 data mining technique. We analyze three months of
                 continuous network log data from a deployed public
                 safety trunked radio network. After data cleaning and
                 traffic extraction, we identify clusters of talk groups
                 by applying AutoClass tool and K-means algorithm on
                 user's behavior patterns represented by the hourly
                 number of calls. We propose a traffic prediction model
                 by applying the classical SARIMA models on the clusters
                 of users. The predicted network traffic agrees with the
                 collected traffic data and the proposed cluster-based
                 prediction approach performs well compared to the
                 prediction based on the aggregate traffic.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@bastani,
  author =       "Behnam Bastani",
  title =        "Analysis of Colour Display Characteristics",
  supervisor =   "Dr. Brian Funt",
  month =        mar,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-03-04",
  pages =        "147",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/BehnamBastaniMSc.pdf",
  abstract =     "Predicting colours across multiple display devices
                 requires implementation of device characterization,
                 gamut mapping, and perceptual models. This thesis
                 studies characteristics of CRT, LCD monitors and
                 projectors. It compares existing models and introduces
                 a new model that improves existing calibration
                 algorithms.
                 
                 Gamut mapping assigns a mapping between two different
                 colour spaces. Previously, the focus of gamut mapping
                 has been between monitor and printer, which have
                 relatively different gamut shape. Implementation and
                 result of existing models are compared and a new model
                 is introduced that its output images are as good as the
                 best available models but runs in less time. DLP
                 projectors with a different technology require a more
                 complex calibration algorithm. A new approach for
                 calibrating DLP projectors is introduced with a
                 significantly better performance on predicting RGB data
                 given tristimulus values. At the end, a new calibration
                 method, using Support Vector Regression is
                 introduced.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2005@Liu,
  author =       "Jingyu Liu",
  title =        "Lexicon Caching in Full-Text Databases",
  supervisor =   "Tiko Kameda",
  month =        mar,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-02-07",
  pages =        "72",
  URL =          "ftp://fas.sfu.ca/pub/cs/theses/2005/JingyuLiuMSc.ps",
  abstract =     "Caching is a widely used technique to leverage access time 
                 difference between two adjacent levels of storage in the 
                 computer memory hierarchy, e.g., {\em cells in main memory} 
                 $\leftrightarrow$ {\em cells in the cpu cache}, and {\em 
                 blocks on disk} $\leftrightarrow$ {\em pages in main memory}. 
                 Especially in a database system, buffer management is an 
                 important layer to keep hot spot data in main memory so as to 
                 minimize slow disk I/O and thus improve system performance. 
                 In this thesis, we present a term-based method to cache 
                 lexicon terms in full-text databases, which aims at reducing
                 the size of the lexicon that must be kept in memory, while 
                 providing good performance for finding the requested terms. 
                 We empirically show that,under the assumption of Zipfs-like 
                 term access distribution, given the same amount of main 
                 memory, our term-based caching method achieves a much higher
                 hit ratio and much faster response time than traditional 
                 page-based buffering methods used in database systems.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Song,
  author =       "Jiaqing Song",
  title =        "Modeling and Performance Analysis of Public Safety
                 Wireless Networks",
  supervisor =   "Dr. Ljiljana Trajkovic",
  month =        apr,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-01-26",
  pages =        "80",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/JiaqingSongMSc.ps",
  abstract =     "Public safety wireless networks (PSWNs) play a vital
                 role in operations of emergency agencies such as police
                 and fire departments. In this thesis, we describe
                 analysis and modeling of traffic data collected from
                 the Emergency Communications for Southwestern British
                 Columbia (E-Comm) PSWN. We analyze network and agency
                 call traffic and find that lognormal distribution and
                 exponential distribution are adequate for modeling call
                 holding time and call inter-arrival time, respectively.
                 We also describe a newly developed wide area radio
                 network simulator, named WarnSim. We use WarnSim
                 simulations to validate the proposed traffic model,
                 evaluate the performance of the E-Comm network, and
                 predict network performance in cases of traffic
                 increase. We also provide recommendations on allocating
                 E-Comm network resources to deal with the increased
                 traffic volume.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@ghaffari,
  author =       "Roozbeh Ghaffari",
  title =        "Snake Contours in Three-Dimensions From Colour Stereo Image
                 Pairs",
  supervisor =   "Brian Funt and Ghassan Hamarneh",
  month =        apr,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-04-15",
  pages =        "65",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/RoozbehGhaffariMSc.pdf",
  abstract =     "Snakes (active contour models) are extended to segment 
                 regions of interest on the surface of 3D objects. Stereo 
                 images taken with calibrated cameras are used as input. For
                 this method the depth map for the whole image does not need to
                 be computed. Instead, 3D external forces are designed to keep
                 the contour on the surface of the object while moving it
                 toward the desired boundaries. Color information is used to 
                 improve the ability of snakes in detecting the boundaries, in 
                 contrast to the majority of previous methods which are based
                 on intensity information alone. The proposed method produces 
                 3D contours on the surface of the object with coordinates in
                 physical units, e.g.\ millimeters. These contours can be used
                 to view the structure and dimensions of any distinguishable 
                 region on the surface of an object. Examples include oral 
                 lesions and skin diseases. The whole process requires minimal
                 human interaction; however, user input can be used to improve
                 segmentation."
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Leung,
  author =       "Chi Hang (Philip) Leung",
  title =        "Understanding, Interpreting and Querying Web
                 Statistical Tables",
  supervisor =   "Dr. Wo-Shun Luk",
  month =        apr,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-04-18",
  pages =        "78",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/ChiHangPhilipLeungMSc.pdf",
  abstract =     "Extraction of information from tables published on the
                 Web is made less complicated because of easy
                 identification of the text inside a table cell. In this
                 thesis, we propose, and have implemented, a scheme
                 which not only understands the contents in a
                 statistical table, but is also able to convert them
                 into a multidimensional database which can then be fed
                 into an off-the-shelf system for querying and data
                 integration. By carefully interpreting the intention of
                 the table author via the visual cues embedded into the
                 HTML text, and the layout design of multidimensional
                 database modelling techniques, our system can
                 successfully classify the keywords into semantically
                 distinct dimension hierarchies, without any
                 domain-specific knowledge, or machine learning.
                 Experiments on a set of real-life statistical tables
                 have confirmed the validity of this approach.
                 Experiments on a set of real-life statistical tables
                 have confirmed the validity of this approach.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Zuluaga,
  author =       "Mauricio Zuluaga",
  title =        "{RAGE} {AMONGST} {THE} {MACHINES}: {A} biological
                 approach to reducing spatial interference in
                 multi-robot systems",
  supervisor =   "Richard Vaughan",
  month =        may,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-05-25",
  pages =        "101",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2005/MauricioZuluagaMSc.pdf",
  abstract =     "Interference is a common problem for animals, and can
                 be characterized as a competition for different types
                 of resources such as food, mates or territory. In the
                 case of multi-robot systems, similar problems arise.
                 Many species have evolved aggressive displays as a more
                 efficient alternative to physical combat to solve
                 conflicts over resources. This thesis considers a
                 transportation task in which a team of robots with no
                 centralized control frequently interfere with each
                 other. This thesis describes two new, principled
                 approaches to selecting an aggression level, based on a
                 robot's investment in a task. The methods are
                 economically rational. Simulation experiments in an
                 office-type environment and a smaller-scale real world
                 implementation show that under some special
                 circumstances, the methods are able to significantly
                 improve system performance compared to a similar
                 competition with a random outcome.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-PhD:2005@Zhang,
  author =       "Zhong Zhang",
  title =        "Applications of Visibility Space in Polygon Search
                 Problems",
  supervisor =   "Dr. Tiko Kameda",
  month =        aug,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-04-22",
  pages =        "153",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2005/ZhongZhangPhD.pdf",
  abstract =     "This thesis investigates several problems related to
                 searching a polygonal area for intruders.
                 
                 The mutual visibility between an arbitrary pair of
                 points on the boundary of a polygon is an important
                 piece of information we can make use of when searching
                 a polygon. We extensively employ the visibility diagram
                 that represents mutual visibility information for each
                 pair of boundary points.
                 
                 We first investigate the {\it two-guard room search}
                 problem, where two guards cooperate in finding an
                 intruder by maintaining mutual visibility. In terms of
                 the visibility diagram we characterize the class of
                 searchable rooms in a concise way. We also find all
                 doors in a polygon, if any, such that the resultant
                 rooms are searchable by two guards. The second problem
                 we tackle in this thesis is the polygon search problem
                 by a {\it boundary 1-searcher}, who moves along the
                 boundary of a polygon and can see the points on the
                 beam from a flashlight. We identify the patterns that
                 make a polygon non-searchable. The third problem we
                 investigate is to search a polygon with one hole by two
                 boundary 1-searchers. We solve this problem by
                 extending the visibility diagram.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Birke,
  author =       "Julia Birke",
  title =        "A Clustering Approach for the Unsupervised Recognition
                 of Nonliteral Language",
  supervisor =   "Dr. Anoop Sarkar",
  month =        aug,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-07-29",
  pages =        "186",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2005/JuliaBirkeMSc.pdf",
  abstract =     "In this thesis we present TroFi, a system for
                 separating literal and nonliteral usages of verbs
                 through unsupervised statistical word-sense
                 disambiguation and clustering techniques. TroFi
                 distinguishes itself by redefining the types of
                 nonliteral language handled and by depending purely on
                 sentential context rather than selectional constraint
                 violations and paths in semantic hierarchies. TroFi
                 uses literal and nonliteral seed sets acquired and
                 cleaned without human supervision to bootstrap
                 learning. We adapt a word-sense disambiguation
                 algorithm to our task and add learners, a voting
                 schema, SuperTags, and additional context. Detailed
                 experiments on hand-annotated data and the introduction
                 of active learning and iterative augmentation allow us
                 to build the TroFi Example Base, an expandable resource
                 of literal/nonliteral usage clusters for the NLP
                 community. We also describe some other possible
                 applications of TroFi and the TroFi Example Base. Our
                 basic algorithm outperforms the baseline by 24.4%.
                 Adding active learning increases this to over 35%." ,
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Meynert,
  author =       "Alison Maria Meynert",
  title =        "Common Evidence Network: An Integrated Approach to
                 Investigating Gene Relationships",
  supervisor =   "Arvind Gupta",
  month =        aug,
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-08-03",
  pages =        "113",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2005/AlisonMeynertMSc.pdf",
  abstract =     "A common evidence network is a data structure that
                 integrates evidence for relationships between genes
                 from disparate data sources and across data types. It
                 is an undirected weighted graph where nodes represent
                 genes and edge weights are quantitative measures of
                 confidence in the evidence linking two genes. We
                 describe methods for producing edge weights for two
                 evidence types: literature co-citation and similarity
                 of Gene Ontology annotations. A tool was developed for
                 identifying genes across multiple databases and
                 consolidating selected annotations. Using gene synonym
                 lists obtained from this tool, we extracted
                 co-citations of genes from annotated biomedical
                 abstracts as evidence. We developed a novel approach to
                 interpreting the similarity of Gene Ontology terms
                 annotated to genes. The method produces a score that
                 quantitatively describes the similarity of Gene
                 Ontology term annotations between two genes. We tested
                 both methods on a set of genes sharing a common
                 sequence feature.",
  gen =          "0+",
}


@PhdThesis{U-SFraser-CMPT-PhD:2005@Mandryk,
  author =       "R.L. Mandryk",
  title =        "Modeling User Emotion in Interactive Play Environments: A 
                 Fuzzy Physiological Approach",
  supervisor =   "T.W. Calvert, K.M. Inkpen",
  month =        "December",
  year =         "2005",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-12-13",
  pages =        "290",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2005/ReganMandrykPhD.pdf",
  abstract =     "Researchers are integrating emerging technologies into
                 interactive play environments, and established game markets
                 continue to expand, yet evaluating play environments is 
                 challenging.  While task performance metrics are commonly used
                 to objectively and quantitatively analyse productivity 
                 systems; with play systems, the quality of the experience, not
                 the performance of the participant is important. This research
                 presents three experiments that examine users' physiological
                 signals to continuously model user emotion during interaction
                 with play technologies. Modeled emotions are powerful because
                 they capture usability and playability, account for user
                 emotion, are quantitative and objective, and can be 
                 represented continuously.
                 
                 In Experiment One we explored how physiological signals 
                 respond to interaction with play technologies. We collected a 
                 variety of physiological measures while observing participants
                 playing a computer game in four difficulty conditions, 
                 providing a basis for experimental exploration of this domain.

                 In Experiment Two we investigated how physiological signals 
                 differ between play conditions, and how physiological signals 
                 co-vary with subjective reports. A different physiological 
                 response was observed when playing a computer game against a 
                 collocated friend versus a computer. When normalized, the 
                 physiological results mirrored subjective reports.

                 In Experiment Three we developed a method for modeling emotion
                 using physiological data. A fuzzy logic model transformed four
                 physiological signals into arousal and valence. A second fuzzy
                 logic model transformed arousal and valence into five 
                 emotions: boredom, challenge, excitement, frustration, and 
                 fun. The modeled emotions' means were evaluated with test
                 data, and exhibited the same trends as the reported emotions
                 for fun, boredom, and excitement, but modeled emotions
                 revealed differences between three play conditions, while
                 differences between reported emotions were not significant.

                 Mean emotion modeled from physiological data fills a 
                 knowledge gap for objective and quantitative evaluation of 
                 entertainment technologies. Using our technique, user emotion 
                 can be analyzed over an entire experience, revealing variance 
                 within and between conditions. This continuous representation
                 has a high evaluative bandwidth, and is important because the
                 process, not the outcome of playing determines success. The
                 continuous representation of modeled emotion is a powerful
                 evaluative tool, that when combined with other approaches,
                 forms a robust method for evaluating user interaction with 
                 play technologies.",
  gen =          "0+",
}

@MastersThesis{U-SFraser-CMPT-MSc:2006@Fouron,
  author =       "Anne G.Fouron",
  title =        "Development of Bayesian Network Models for Obstructive Sleep 
                 Apnea Syndrome Assesment",
  supervisor =   "Dr. Stella Atkins",
  month =        "January",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-12-19",
  pages =        "108",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/AnneFouronMSc.pdf",
  abstract =     "Bayesian Belief networks have been used for diagnosis in some
                 medical domains and in this thesis we provide a methodology 
                 for creating Bayesian Networks to predict Obstructive Sleep 
                 Apnea Syndrome severity. We build 3 Bayesian Network 
                 topologies: by knowledge engineering, Na%G%@e Bayes 
                 configuration and a third topology is created using results of
                 the Na%G%@e network.   All networks are trained on data
                 from 652 patients referred for an overnight polysomnogram.  
                 Data is derived from multiple data sources and includes a mix 
                 of continuous and discrete variables.   We investigate the 
                 impact of different topologies and discretizing continuous 
                 variables, adding nodes with large amounts of missing values, 
                 and removing nodes from networks.

                 Results show that performance is dependent on the interaction 
                 between topology and discretization.   Node removal increases 
                 sensitivity while node addition decreases it. ",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Pekerskaya,
  author =       "Irina Pekerskaya",
  title =        "Mining Changing Regions From Access Constrained Data Sets: A
                 Cluster-Embedded Decision Tree Approach",
  supervisor =   "Dr. Jian Pei",
  month =        "January",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-01-16",
  gen =          "0-+",
  pages =        "73",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/IrinaPekerskyaMSc.pdf",
  abstract =     "Change detection is important in many applications. Most of
                 the existing methods have to use at least one of the original
                 data sets to detect changing regions. However, in some
                 important applications, due to data access constraints such as
                 privacy concerns and limited data online availability, the
                 original data may not be available for change detection. In
                 this work, we tackle the problem by proposing a simple yet 
                 effective model-based approach. In the model construction
                 phase, original data sets are summarized using the novel
                 cluster-embedded decision trees as concise models. Once the 
                 models are built, the original data will not be accessed 
                 anymore. In the change detection phase, to compare any two
                 data sets, we compare the two corresponding cluster-embedded
                 decision trees.  Our systematic experimental results on both
                 real and synthetic data sets show that our approach can detect
                 changes accurately and effectively.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Shen,
  author =       "Wei Shen",
  title =        "BGP Route Flap Damping Algorithms",
  supervisor =   "Dr. Ljiljana Trajkovic and Dr. Uwe Glaesser",
  month =        "January",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       "SFU_CS_School",
  defended =     "2005-12-22",
  pages =        "129",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/WeiShenMSc.pdf",
  abstract =     "Route flap damping (RFD) is a mechanism used in Border
                 Gateway Protocol (BGP) to prevent persistent routing
                 oscillations in the Internet. It plays an important role in 
                 maintaining the stability of the Internet routing system. RFD
                 works by suppressing routes that flap persistently. Several
                 existing algorithms address the issue of identifying and
                 penalizing route flaps. In this thesis, we compare three such
                 algorithms: original RFD, selective RFD, and RFD+. We 
                 implement these algorithms in ns-2 and evaluate their 
                 performance. We also propose two possible improvements to the 
                 RFD algorithms.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Su,
  author =       "Ming (Mike) Su",
  title =        "Using Abstract State Machines to Model a Graphical User
                 Interface System",
  supervisor =   "Uwe Gl{\"a}sser",
  month =        "April",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-01-10",
  pages =        "131",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/MikeMingSuMSc.pdf",
  abstract =     "A graphical user interface (GUI) system is a visual
                 tool for the users to operate computer applications. In the 
                 software engineering world, verifying that the functions of a
                 GUI system satisfy the perspective of users is one important
                 goal. System modeling provides an opportunity to verify the
                 functionality of the system before implementing it.

                 In this thesis, we model the GUI system of the CoreASM 
                 language debugger based on the abstract state machine (ASM) 
                 paradigm, and give a formal specification to the GUI system. 
                 This GUI system model provides a formalmathematical foundation
                 to specify the architecture and the function form of the GUI
                 system and to specify the interactive actions between the 
                 users and the computer application (the CoreASM engine). The
                 design approach in this work incorporates both object-oriented
                 and task-oriented approaches. A process of level-wise
                 refinement is used to solve particular design problems.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Mahabadi,
  author =       "Ladan A. Mahabadi",
  title =        "A Pseudorandom Generator Construction based on
                 Randomness Extractors and Combinatorial Designs",
  supervisor =   "Dr. Valentine Kabanets",
  month =        "April",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-04-07",
  pages =        "68",
  gen =          "0+",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/LadanMahabadiMSc.pdf",
  abstract =     "Nisan and Wigderson in their seminal work introduced a new
                 (conditional) pseudorandom generator construction which since 
                 then has been extensively used in complexity theory and has 
                 led to extensive further research. Impagliazzo and Wigderson 
                 (1997), and Sudan, Trevisan, and Vadhan (2001) have shown how 
                 this construction can be utilized to prove conditional
                 derandomization results under weaker hardness assumptions. We
                 study the construction of pseudorandom generators, and use an
                 observation of Sudan $\etal$ to recast the 
                 Impagliazzo-Wigderson construction in terms of weak sources of
                 randomness; such a source is a distribution on binary strings
                 that is 'random' in the sense of having high 'entropy'. We
                 will then use an efficient algorithm of Gabizon $\etal$ to
                 extract almost all of the randomness present, obtaining a
                 pseudorandom generator that stretches $O(n)$ bits to
                 $\Omega(n2^n)$ bits.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Tang,
  author =       "Calvin Tang",
  title =        "Model Checking Abstract State Machines with Answer Set
                 Programming",
  supervisor =   "Eugenia Ternovska and Uwe Glasser",
  month =        "04",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-04-07",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/CalvinTangMSc.pdf",
  pages =        "125",
  abstract =     "Answer Set Programming (ASP) is a logic programming paradigm
                 that has been shown as a useful tool in various application 
                 areas due to its expressive modelling language. These 
                 application areas include Bounded Model Checking (BMC). BMC
                 is a verification technique that is recognized for its strong
                 ability of finding errors in computer systems. To apply BMC, 
                 a system needs to be modelled in a formal specification 
                 language, such as the widely used formalism of Abstract State
                 Machines (ASMs). In this thesis, we present BMC of ASMs based
                 on ASP. We show how to translate an ASM and a temporal
                 property into a logic program and solve the BMC problem for
                 the ASM by computing an answer set for the logic program. 
                 Experimental results for our method using the answer set 
                 solvers SMODELS and CMODELS are also given.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Bavarian,
  author =       "Maryam Bavarian",
  title =        "Design and Analysis of Biological Sequences using Constraint
                 Handling Rules",
  supervisor =   "Veronica Dahl",
  month =        "April",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-04-05",
  pages =        "88",
  gen =          "0+",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/MaryamBavarianMSc.pdf",
  abstract =     "The need for processing biological information is rapidly
                 growing, owing to the masses of new information in digital
                 form being produced at this time. Old methodologies for
                 processing it can no longer keep up with this rate of growth. 
                 We present a novel methodology for solving an important
                 bioinformatics problem, which has been proved to be 
                 computationally hard: that of finding a RNA sequence which 
                 folds into a given structure.  Previous solutions to this
                 problem divide the whole structure into smaller substructures
                 and apply some techniques to resolve it for smaller parts,
                 which causes them to be slow while working with longer RNAs. 
                 We prove that by using a set of simple CHR rules we are able
                 to solve this problem and obtain an approximate but still
                 useful solution more efficiently. We expect the results we
                 present to be applicable, among other things, to in vitro
                 genetics and to drug design.",
}


@PhdThesis{U-SFraser-CMPT-PhD:2006@Voll,
  author =       "Kimberly Voll",
  title =        "A Methodology of Error Detection: Improving Speech
                 Recognition in Radiology",
  supervisor =   "Veronica Dahl",
  month =        "April",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-04-07",
  pages =        "208",
  gen =          "0+",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/KimberlyVollPhD.pdf",
  abstract =     "Automated speech recognition (ASR) in radiology report
                 dictation demands highly accurate and robust recognition
                 software.  Despite vendor claims, current implementations are
                 sub-optimal, leading to poor accuracy, and time and money 
                 wasted on proofreading.  Thus, other methods must be 
                 considered for increasing the reliability and performance of 
                 ASR before it is a viable alternative to human transcription. 
                 One such method is post-ASR error detection, used to recover 
                 from the inaccuracy of speech recognition. This thesis
                 proposes that detecting and highlighting errors, or areas of
                 low confidence, in a machine-transcribed report allows the
                 radiologist to proofread more efficiently.  This, in turn, 
                 restores the benefits of ASR in radiology, including efficient
                 report handling and resource utilization.

                 To this end, an objective classification of error-detection 
                 methods for ASR is established.  Under this classification, a 
                 new theory of error detection in ASR is derived from the 
                 hybrid application of multiple error-detection heuristics.  
                 This theory is contingent upon the type of recognition errors
                 and the complementary coverage of the heuristics.  Inspired by
                 these principles, a hybrid error-detection application is 
                 developed as proof of concept.   The algorithm relies on four 
                 separate artificial-intelligence heuristics together covering 
                 semantic, syntactic, and structural error types, and 
                 developed with the help of 2700 anonymised reports obtained 
                 from a local radiology clinic.  Two heuristics involve
                 statistical modeling: pointwise mutual information and 
                 co-occurrence analysis.  The remaining two are non-statistical
                 techniques: a property-based, constraint-handling-rules 
                 grammar, and a conceptual distance metric relying on the 
                 ontological knowledge in the Unified Medical Language System. 
                 When the hybrid algorithm is applied to thirty real-world
                 radiology reports, the results are encouraging: up to a 24\% 
                 increase in the recall performance and an 8\% increase in the
                 precision performance over the best single technique.  In
                 addition, the resulting algorithm is efficient and modular.

                 Also investigated is the development necessary to turn the 
                 hybrid algorithm into a real-world application suitable for 
                 clinical deployment.  Finally, as part of an investigation of 
                 future directions for this research, the greater context of 
                 these contributions is demonstrated, including two
                 applications of the hybrid method in cognitive science and
                 machine learning.",
}

@MastersThesis{U-SFraser-CMPT-MSc:2006@Cao,
  author =       "Xiao Rui Cao",
  title =        "A Framework for Benchmarking Haptic Systems",
  supervisor =   "Ted Kirkpatrick",
  month =        "04",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  type =         "Master's Final Project Thesis",
  defended =     "2005-04-10",
  pages =        "59",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/XiaoRuiCaoMSc.pdf",
  abstract =     "As more and more haptic rendering algorithms are developed,
                 the need for evaluation and comparison is becoming more 
                 pressing. However, evaluating and comparing haptic rendering 
                 algorithms presents two challenges. First, haptic systems 
                 provide bidirectional communication between humans and the
                 computer. Therefore, the outputs of such systems are highly
                 reliant on human inputs, which are hard to reproduce 
                 consistently. Second, haptic systems are real-time systems. 
                 Testing real-time systems itself is difficult because of their
                 timing constraints. Our solution to these challenges is to
                 build a simulation-based tool where human inputs and the real
                 haptic device are replaced by input simulation models, and a 
                 haptic device simulator, respectively. The main purpose of 
                 this tool is to provide repeatable inputs for haptic systems 
                 testing and enable qualitative evaluation among them. The tool
                 also provides replicable test cases for haptic system
                 debugging and regression tests.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Memon,
  author =       "Mashaal Anwar Memon",
  title =        "Specification Language Design Concepts: Aggregation and
                 Extensibility in CoreASM",
  supervisor =   "Dr. Uwe Gl{\"a}sser",
  month =        "April",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-04-21",
  pages =        "140",
  gen =          "0->",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/MashaalAnwarMemonMSc.pdf",
  abstract =     "Abstract State Machines (ASMs) are a proven methodology
                 for the precise high-level specification of formal 
                 requirements in early phases of software design. Many 
                 extensions to ASMs have been proposed and used widely, 
                 including Distributed ASMs, Turbo ASMs, Gurevich's partial
                 updates, and syntactically convenient rule forms. This, 
                 coupled with the fact that ASMs do not bind the user to any 
                 predetermined data types or operators, allows for extreme 
                 flexibility in exploration of the problem space. Striving to
                 provide this same level of freedom with executable ASMs, the
                 CoreASM engine and language have been designed with syntactic 
                 and semantic extensibility in mind. We formally specify 
                 extensibility mechanisms that allow for language augmentation
                 with arbitrary data structures supporting simultaneous 
                 incremental modification, new operators, and additional
                 language syntax. Our work is a major step toward providing an
                 environment suitable for both further experimentation with
                 ASMs and for the machine-aided creation of robust software 
                 specifications.",
}

@MastersThesis{U-SFraser-CMPT-MSc:2006@Jain,
  author =       "Varun Jain",
  title =        "Robust Correspondence and Retrieval of Articulated
                 Shapes",
  supervisor =   "Dr. Richard (Hao) Zhang",
  month =        may,
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-05-26",
  pages =        "92",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/VarunJainMSc.pdf",
  abstract =     "We consider the problem of shape correspondence and
                 retrieval. Although our focus is on articulated shapes,
                 the methods developed are applicable to any shape
                 specified as a contour, in the 2D case, or a surface
                 mesh, in 3D. We propose separate methods for 2D and 3D
                 shape correspondence and retrieval, but the basic idea
                 for both is to characterize shapes using intrinsic
                 measures, defined by geodesic distances between points,
                 to achieve robustness against bending in articulated
                 shapes. In 2D, we design a local, geodesic-based shape
                 descriptor, inspired by the well-known shape context
                 for image correspondence. For 3D shapes, we first
                 transform them into the spectral domain based on
                 geodesic affinities to normalize bending and other
                 common geometric transformations and compute
                 correspondence and retrieval in the new domain. Various
                 techniques to ensure robustness of results and
                 efficiency are proposed. We present numerous
                 experimental results to demonstrate the effectiveness
                 of our approaches.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Clements,
  author =       "Andrew Clements",
  title =        "Minimum Ratio Contours For Meshes",
  supervisor =   "Richard Zhang",
  month =        "June",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2005-05-31",
  pages =        "74",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/AndrewClementsMSc.pdf",
  abstract =     "We present a novel minimum ratio contour (MRC) algorithm,
                 for discretely optimizing contours on the surface of triangle 
                 meshes. We compute the contour having the minimal ratio 
                 between a numerator and a denominator energy. The numerator 
                 energy measures the bending and salience (feature adaptation) 
                 of a contour, while the denominator energy measures contour 
                 length. Given an initial contour, the optimal contour within a
                 prescribed search domain is sought. The search domain is 
                 modeled by a weighted acyclic edge graph, where nodes in the 
                 graph correspond to directed edges in the mesh. The acyclicity
                 of this graph allows for an efficient computation of the MRC. 
                 To further improve the result, the algorithm may be run on a 
                 refined mesh to allow for smoother contours that can cut
                 across mesh faces. Results are demonstrated for postprocessing
                 in mesh segmentation. We also speculate on possible global 
                 optimization methods for computing a MRC.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Zhang,
  author =       "Yinan Zhang",
  title =        "Spatial Interference Reduction for Multi-robot Systems
                 Using Rational And Team-based Aggression",
  supervisor =   "Richard Vaughan",
  month =        "July",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-07-31",
  pages =        "71",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/YinanZhangMSc.pdf",
  abstract =     "A team of robots with no centralized control performing a
                 transportation task in a confined environment frequently 
                 interfere with each other.  Previous work has shown that 
                 stereotyped robot-robot competition, inspired by aggressive
                 displays in animals, can be used to effectively reduce such
                 interference and improve overall system performance.  Two
                 principled approaches to determining aggression for robots are
                 described in this thesis.  The first, global investment, is 
                 based on the concept of `economical investment'.  The second, 
                 team-based aggression, extends and improves upon the 
                 economical investment scheme by increasing the coordination
                 between robots.  Simulation results show that both approaches
                 improve the system efficiency compared to the approach of 
                 setting robots' aggression at random, and team-based 
                 aggression provides the best performance yet observed.  The 
                 thesis also introduces a new scheme for implementing
                 aggression functions using a simple network model.  Further,
                 the effects of interference-reduction methods over a range of 
                 population sizes are studied.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Liu,
  author =       "Daphne Hao Liu",
  title =        "A Consistency-Based System for Knowledge Base Merging",
  supervisor =   "Jim Delgrande",
  month =        "August",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-08-11",
  pages =        "95",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/DaphneHaoLiuMSc.pdf",
  abstract =     "The ability to change one's beliefs consistently is essential
                 for sound reasoning in a world where the new information one
                 acquires may invalidate or augment one's current beliefs.  
                 Belief revision is the process wherein an agent modifies its 
                 beliefs to incorporate the new information received, and 
                 knowledge base merging the process wherein the agent is given
                 two or more knowledge bases to merge.

                 We present a binary decision diagram (BDD) - based 
                 implementation of Delgrande and Schaub's consistency-based 
                 belief change framework. Our system focuses on knowledge base
                 merging with the possible incorporation of integrity
                 constraints, using a BDD solver for consistency checking. We 
                 show that the result of merging finite knowledge bases can be 
                 represented as a finite formula, and that merging can be 
                 streamlined algorithmically by restricting attention to a 
                 subset of the vocabulary of the propositional formulas
                 involved.  Experimental results and comparisons with related
                 systems are also given.",
  gen =          "0-+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Ishida,
  author =       "Mayu Ishida",
  title =        "Reasoning about Actions: A Model-Theoretic Approach",
  supervisor =   "Eugenia (Evgenia) Ternovska",
  month =        "August",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-08-02",
  pages =        "76",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2006/MayuIshidaMSc.ps",
  abstract =     "A knowledge-based agent reasons with its knowledge and
                 answers queries while performing various tasks. We consider
                 the case where we describe the agent's knowledge in a 
                 propositional fragment of the situation calculus and queries
                 in a fragment of ID-logic, the extension of first-order logic 
                 with inductive definitions. This fragment of ID-logic is
                 equivalently as expressive as the alternation-free 
                 $\mu$-calculus. We formulate the agent's reasoning process as
                 the following question: does the representation $T$ of the 
                 agent's knowledge logically entail the query $\phi$ (i.e., 
                 $T \vDash \phi$)? We provide an efficient algorithm for this 
                 task, using a model-theoretic approach: we construct from $T$
                 a canonical model $\str{F}^T$ of the agent's knowledge and ask
                 whether $\str{F}^T$ satisfies $\phi$. Using this approach, the
                 agent can answer the query in time linear with respect to both
                 the size of $T$ and the size of $\phi$.",
  gen =          "0+",

}


@PhdThesis{U-SFraser-CMPT-PhD:2006@Jiang,
  author =       "Yuelong Jiang",
  title =        "Finding Interesting Rules From Large Data Sets",
  supervisor =   "Ke Wang",
  month =        "August",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-08-15",
  pages =        "142",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/YuelongJiangPhD.pdf",
  abstract =     "A main goal in data mining is finding those interesting 
                 rules data, which may help the user to do something to her 
                 advantage. In order to explore really interesting rules, 
                 numerous measures of interestingness and corresponding 
                 constraints have been developed based on the structures of 
                 rules and the statistic information of data. Given measures
                 and their constraints in a large data set, one challenging
                 task is to design efficient algorithms by analyzing the 
                 property of the measure constraints to quickly find all 
                 qualified rules. Another important aspect is the consideration
                 of the domain knowledge from the user, which will make many
                 statistically significant rules not interesting if the 
                 background information is given. In that situation, the 
                 subjective measures, which depend on the class of users and 
                 their domain knowledge, will be more pivotal to find
                 interesting rules.

                 In this work, we develop a class of methods to address the
                 efficiency and effectiveness issues in finding interesting 
                 rules. We firstly introduce a novel approach to speed up the 
                 rule mining by pruning search space for a wide class of 
                 measure constraints that are hard to be exploited previously. 
                 Then, we discuss two new models on subjective measures, which
                 consider not only the real interestingness of the rules from
                 the user's standpoint, but also the efficiency of the
                 algorithms. The extensive experiments show that the models are
                 more tractable and scalable than existing approaches.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2006@Lu,
  author =       "Cheng Lu",
  title =        "Removing Shadows from Color Images",
  supervisor =   "Mark S. Drew",
  month =        "August",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-07-21",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/ChengLuPhD.pdf",
  abstract =     "This thesis is concerned with the derivation of a shadow-free image
representation. We propose several methods of automatically detecting and
removing shadows in a color image.  Our methods stem from the illuminant
invariant theory which requires a camera calibration step to find the
direction of changes of illumination. In our work, instead of a camera
calibration we aim at finding this direction from evidence in the colour
image itself.  Specifically, we recognize that producing a 1-d projection
in the correct invariant direction which is orthogonal to the direction
of changes of illumination, will result in a 1-d distribution of pixel
values that have smaller entropy than projecting in the wrong direction.
Hence we seek that projection which minimizes entropy, and from there
go on to remove shadows from the color images. To be able to develop an
effective description of the entropy-minimization task, we go over to
the quadratic entropy, rather than Shannon's definition.  Replacing the
observed pixels with a kernel density probability distribution, the
quadratic entropy can be written as a very simple formulation, and can
be evaluated using the efficient Fast Gauss Transform.  The entropy,
written in this enbodiment, is more insensitive to quantization than is
the usual definition.  The resulting algorithm is quite reliable, and
the shadow removal step produces good shadow-free colour image results
whenever strong shadow edges are present in the image.  In almost every
case studied, entropy has a strong minimum for the invariant direction,
revealing a new property of image formation.

To recover a 3-d, full colour shadow-free image representation, we
define a thresholding operation to identify the shadow edge. Edges are
in-painted across the shadow edge, and re-integrating yields a colour
image, equal to the original save for the fact that it is shadow-free.

Shadow detection per se is an important step in image analysis. We
propose a method of detecting not just strong shadow edges but indeed
entire shadow regions in the image taken under ambient light, given extra
information from a flash image registered with the first.  We argue that
the difference in a log domain of the flash image and the ambient image
gives a very simple feature space consisting of two components --- one in
an illuminant-change 3-vector direction, and one along the gray axis. This
space provides excellent separation of the shadow and nonshadow areas.

We also propose a method for efficient ambient illuminant estimation
using the flash image. We verify that the chromaticities corresponding
to illuminants with different temperatures fall into different color
temperature groups along a line on a plane in the log geometric-mean
chromaticity space.  Remarkably, our algorithm is truly practical as
it can estimate the color of the ambient light even without any prior
knowledge about surface reflectance, flash light, or camera sensors. In
addition, we propose a novel white balance method which uses the white
patch under the estimated illuminant as reference white color for
balancing images.

Finally, for consumer-grade digital cameras, due to the different
illumination conditions, the flash and non-flash images usually have
different camera settings when they are taken. We propose a method
which can parametrically adjust the two images so as to compensate for
the difference in camera settings. The difference between compensated
images reflects only the difference in illumination.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Lai,
  author =       "Lily Yi-Ting Lai",
  title =        "Influential Marketing: A New Direct Marketing Strategy 
                 Addressing the Existence of Voluntary Buyers",
  supervisor =   "Ke Wang",
  month =        sep,
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-Sept-01",
  pages =        "60",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/LilyLaiMSc.pdf",
  abstract =     "The traditional direct marketing paradigm implicitly assumes
                 that there is no possibility of a customer purchasing the 
                 product unless he receives the direct promotion. In real 
                 business environments, however, there are 'voluntary buyers'
                 who will make the purchase even without marketing contact. 
                 While no direct promotion is needed for voluntary buyers, the
                 traditional response-driven paradigm tends to target such 
                 customers.

                 In this thesis, the traditional paradigm is examined in 
                 detail. We argue that it cannot maximize the net profit. 
                 Therefore, we introduce a new direct marketing strategy, 
                 called 'influential marketing'.  To achieve the maximum net 
                 profit, influential marketing targets only the customers who 
                 can be positively influenced by the campaign. Nevertheless, 
                 targeting such customers is not a trivial task. We present a 
                 novel and practical solution to this problem which requires no
                 major changes to standard practices. The evaluation of our 
                 approach on real data provides promising results.",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@ZhiHaoLin,
  author =       "Zhi Hao Lin",
  title =        "An Examination on Convergence Time of the Iterated Prisoner's
                  Dilemma Game Using the Pavlov Strategy in the Different Types
                  of the Interaction Graphs",
  supervisor =   "Petra Berenbrink",
  month =        "Sept",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-09-21",
  pages =        "85",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/ZhiHaoLinMSc.pdf",
  abstract =     "We embed Iterated Prisoner's Dilemma problem into a
                 graph system. Each vertex in the graph corresponds to a player
                 in the IPD game. At each round, an edge in the graph is picked
                 at random, and two players, who are connected by this edge, 
                 will play one round of PD game using Pavlov strategy. After 
                 that, another edge will be chosen and corresponding players
                 will play again. This game cycle repeats infinitely, until all
                 of the players in the system choose cooperation as their 
                 strategy. We call this state as 'convergence state', and the 
                 number of rounds used to reach convergence state as 
                 'convergence time'. Our interest is the relationship between 
                 the sizes of various types of graphs and their convergence 
                 time. This project explored this relationship by computer
                 simulations. In addition, it also provided observations on 
                 other related issues.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2006@Kwiatkowska,
  author =       "Bogumila (Mila) Kwiatkowska",
  title =        "Integrating Knowledge-Driven and Data-Driven Approaches in 
                 the Derivation of Clinical Prediction Rules",
  supervisor =   "M. Stella Atkins",
  month =        "November",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-08-31",
  pages =        "221",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/MilaKwiatkowskaPhD.pdf",
  abstract =     "Clinical prediction rules play an important role in medical
                 practice. They expedite diagnosis and treatment for the 
                 serious cases and limit unnecessary tests for low-probability 
                 cases. However, the creation process for prediction rules is 
                 costly, lengthy, and involves several steps: initial clinical
                 trials, rule generation and refinement, validation, and 
                 evaluation in clinical settings. With the current development
                 of efficient data mining algorithms and growing accessibility 
                 to a vast amount of medical data, the creation of clinical 
                 rules can be supported by automated or semi-automated rule
                 induction from the existing data sources. A data-driven method
                 based on the reuse of previously collected medical records and
                 clinical trial statistics is very cost-effective; however, it
                 requires well defined and intelligent methods for data, 
                 information, and knowledge integration.

                 This thesis presents a new framework for the integration of 
                 domain knowledge into purely data-driven techniques for the 
                 derivation of clinical prediction rules. We concentrate on two
                 aspects: knowledge representation for the predictors and 
                 prediction rules, and knowledge-based evaluation for the 
                 automatically induced models. We propose a new integrative 
                 framework, a semio-fuzzy approach that has its theoretical 
                 foundations in semiotics and fuzzy logic. Semiotics provides 
                 representation for the measurements and interpretation of the 
                 medical predictors. Fuzzy logic provides explicit 
                 representation for the impression of the measurements and 
                 prediction rules. The integrative framework is applied to the
                 construction of a knowledge repository for existing facts and 
                 rules, detection of medical outliers, handling missing values,
                 handling imbalanced data, and feature selection. Several
                 machine learning techniques are considered, based on model 
                 comprehensibility, interpretability, and practical utility in 
                 clinical settings.

                 This new semio-fuzzy framework is applied towards the creation
                 of prediction rules for the diagnosis of obstructive sleep 
                 apnea, a serious and under-diagnosed respiratory disorder, and
                 tested on heterogeneous clinical data sets. The induced 
                 decision trees and logistic regression models are evaluated in
                 context of the existing clinical prediction rules published in
                 medical literature. We describe how the induced rules may 
                 confirm, contradict, and expand the expert-created rules.",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Li,
  author =       "John Yung San Li",
  title =        "Nonobtuse Meshes with Guaranteed Angle Bounds",
  supervisor =   "Richard Zhang",
  month =        "dec",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-12-11",
  pages =        "87",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/JohnYungSanLiMSc.pdf",
  abstract =     "High-quality mesh representations of 3D objects are useful in
                 many applications ranging from computer graphics to 
                 mathematical simulation. We present a novel method to generate
                 triangular meshes with a guaranteed face angle bound of
                 [30 degrees, 90 degrees]. Our strategy is to first
                 remesh a 2-manifold, open or closed, mesh into a rough
                 approximate mesh that respects the proposed angle bounds. This
                 is achieved by a novel extension to the Marching Cubes
                 algorithm. Next, we perform an iterative constrained 
                 optimization, along with constrained Laplacian smoothing, to 
                 arrive at a close approximation of the input mesh. A
                 constrained mesh decimation algorithm is then carried out to 
                 produce a hierarchy of coarse meshes where the angle bounds
                 are maintained. We demonstrate the quality of our work through
                 several examples.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2006@Hormozdiari,
  author =       "Fereydoun Hormozdiari",
  title =        "Protein Protein Interaction Network Comparison and
                 Emulation",
  supervisor =   "Dr. Cenk Sahinalp",
  month =        "December",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-12-12",
  pages =        "49",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/FereydounHormozdiariMSc.pdf",
  abstract =     "The (asymptotic) degree distribution of the best known scale
                 free network models are all similar and are independent of the
                 seed graph used. Hence it has been tempting to assume that
                 networks generated by these models are similar in general. In 
                 this thesis it is shown that several key topological features
                 of such networks depend heavily on the specific model and seed
                 graph used. Furthermore, it is shown that starting with right
                 seed graph, the duplication model captures many topological 
                 features of publicly available PPI networks very well.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Huang,
  author =       "Xiaorong Huang",
  title =        "Fitting Protein Chains to Lattice Using Integer Programming
                 Approach",
  supervisor =   "Arvind Gupta",
  month =        "January",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-12-18",
  pages =        "125",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/XiaorongHuangMSc.pdf",
  abstract =     "Fitting Protein chains to Lattice (FPL) problem can be 
                 formulated as follows. Given the 3D-coordinates of all C-alpha
                 atoms in a protein fold, find the optimal lattice 
                 approximation (self-avoiding path in the lattice) of the fold.
                 The FPL problem is proved to be NP-complete for cubic lattice
                 with side length 3.8 angstroms when the coordinate root 
                 mean-square deviation (cRMS) is used as a similarity measure 
                 between original and approximated fold. We design three 
                 Integer Programming(IP) formulations for FPL problem, and 
                 generate a serial of algorithms which combine dynamic
                 programming and backtracking techniques aiming to reduce the
                 search space, while guaranteeing to find optimal solutions.
                 Experiments show that optimal lattice approximations in cubic
                 lattices with side length 3.8 angstroms using cRMS measurement
                 can be found in feasible time by ILOG CPLEX 9.1 for all
                 proteins in a randomly selected group of proteins (longest of
                 length 1014 residues). ",
  gen =          "0+",
}


@PhdThesis{U-SFraser-CMPT-PhD:2006@HaoJiang,
  author =       "Hao Jiang",
  title =        "Successive Convexification for Consistent Labeling",
  supervisor =   "Ze-Nian Li and Mark S. Drew",
  month =        "August",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-07-20",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/HaoJiangPhD.pdf",
  abstract =     "In this thesis, a novel successive convexification scheme is proposed
for solving consistent labeling problems with convex regularization
terms. Many computer vision problems can be modeled as such consistent
labeling problems. The main optimization term, the labeling cost,
however, is typically non-convex, which makes the problem difficult. As
well, the large search space, i.e., formally the large label set,
makes such applications thorny and inefficient to solve using
traditional schemes. The proposed scheme successively convexifies
the labeling cost surfaces by replacing them with their lower convex
hulls, each time starting from the original cost surfaces but within
shrinking trust regions, with a careful scheme for choosing new search
regions. This enables the new scheme to solve a sequence of much easier
convex programming problems and almost always find the correct labeling.
The proposed scheme can be applied to labeling problems with any convex
regularization terms.  In particular, problems with L1-norm regularization
terms can be solved with sequential linear programming; and problems with
L2-norm regularization terms with sequential quadratic programming. To
zero in on the targets in the search space, the method uses a set of
basis labels to approximate the cost surface for each site, and this
essentially decouples the size of the relaxed convex problem from the
number of labels. The proposed scheme also has other useful properties
making it well-suited to very large label-set problems, e.g. searching
within an entire image. The proposed successive convexification scheme
has been applied to many challenging computer vision problems: the task
of robustly locating objects in cluttered environments, dense motion
estimation with occlusion inference, appearance-adaptive object tracking
with boundary refinement, and finally the challenging problem of human
posture and action detection both in still images and in video. Compared
with traditional methods, the proposed scheme is shown to have a clear
advantage in these applications.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@WingXie,
  author =       "Wing Xie",
  title =        "Obstructions to Trigraph Homomorphisms",
  supervisor =   "Pavol Hell",
  month =        "December",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-12-18",
  pages =        "88",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2006/WingXieMSc.pdf",
  abstract =     "Many graph partition problems seek a partition into parts 
                 with certain internal constraints on each part, and similar
                 external constraints between the parts. Such problems have 
                 been traditionally modeled using matrices, as the so-called 
                 M-partition problems. More recently, they have also been 
                 modeled as trigraph homomorphism problems. This thesis 
                 consists of two parts. In the first part, we survey the 
                 literature dealing with both general and restricted versions
                 of these problems. Most existing results attempt to classify
                 these problems as NPcomplete or polynomial time solvable. In
                 the second part of the thesis, we investigate which of these
                 problems can be characterized by a finite set of forbidden 
                 induced subgraphs. We develop new tools and use them to find
                 all such partition problems with up to five parts. We also
                 observe that these problems are automatically polynomial 
                 time solvable.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2006@Wang,
  author =       "Yang-Wendy Wang",
  title =        "Sentence Ordering For Multi-Document Summarization in
                 Response to Multiple Queries",
  supervisor =   "Fred Popowich and Anoop Sarkar",
  month =        "October",
  year =         "2006",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2006-10-11",
  pages =        "79",
  abstract =     "The growing access to large amounts of text data opens more
                 opportunities in information processing. Given a list of
                 complex questions and a set of relevant documents, the task 
                 of producing an informative and coherent summary of those 
                 documents in response to the questions has attracted a great 
                 deal of attention recently. However, the problem of 
                 organizing information for summarization so that the 
                 generated summary is coherent has received relatively little
                 attention. Several approaches have been proposed for 
                 sentence ordering in single-document and generic multiple-
                 document summarization, but no single method has been found
                 to address sentence ordering in query-based summarization. 
                 In this thesis, we propose and implement an algorithm that
                 combines constraints from query order and topical
                 relatedness in human produced summaries of multiple 
                 documents in response to multiple questions. To test the 
                 effectiveness of the constraints, we construct a new query-
                 based corpus from the human produced summaries for the 
                 Document Understanding Conference(DUC) 2006 evaluation. We
                 then conduct experiments, using an automatic evaluation 
                 method based on Kendall's Tao, to evaluate and compare the
                 effectiveness of our approaches to others. Our results show 
                 that both query order and topical relatedness improve the 
                 ordering performance when compared to a baseline method, 
                 and a combination of these two constraints achieves even 
                 better results.",
  gen =          "0-+",
}


@PhdThesis{U-SFraser-CMPT-PhD:2007@Dan-Wang,
  author =       "Dan Wang",
  title =        "Continuous Data Collection in Wireless Sensor Networks",
  supervisor =   "Funda Ergun",
  month =        "January",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-01-18",
  pages =        "1-102",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/DanWangPhD.zip",
  abstract =     "Recently, it has come to be generally believed by academia 
                 end industry alike that the sensor network will have a key 
                 role to extend the reachability of the next generation 
                 Internet. A key characteristic of this network is that there 
                 is no single node in the network that is powerful enough to 
                 perform the assigned tasks. An application should be served 
                 via the cooperation of several nodes or even the entire 
                 network. The network serves as an information base, and is 
                 data driven, as opposed to a provider for the point-to-point
                 connection.

                 The main challenge of this network is huge information 
                 organization, including information storage, searching and 
                 retrieval, especially in a continuous way. There are many 
                 specific and interrelated problems. We list a few examples. 
                 First, data accuracy: the correctness of the sensor network to
                 represent the properties of the sensor field. Second, data 
                 search and retrieval delay; while low delay is always 
                 preferred, various applications have different delay 
                 constraints. Third, overhead; low transmission overhead is 
                 often the main consideration in system design, as it is 
                 directly related to the usage of energy, the most severely 
                 limited resource for sensors.

                 In this thesis, we first discuss load balanced sensor coverage
                 ,which provides a lower layer support for long run sensor data
                 collection. We then concentrate on how to balance the 
                 parameters in data collection of the sensor networks, so that
                 the user queries and applications can be satisfied with 
                 reasonable delay and low overhead. Based on different 
                 application specifics, we try to use a smaller number of
                 sensors, less number of transmissions by exploring historical
                 and topological information, coding techniques and data
                 distribution information. Our analysis and experimental 
                 results show that our architecture and algorithms provide both
                 theoretical and practical insights for sensor network design 
                 and deployment.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Zhou,
  author =       "Bin Zhou",
  title =        "Mining page farms and its application in link spam 
                 detection",
  supervisor =   "Jian Pei",
  month =        "March",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-03-05",
  pages =        "84",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/BinZhouMSc.pdf",
  abstract =     "Ranking pages on the Web is an essential task in Web search. 
                 For a Web page $p$, what other pages are the major 
                 contributors to the ranking score of $p$? How are the 
                 contributions made? Understanding the general relations of Web
                 pages and their environments is important with a few 
                 interesting applications such as Web spam detection. In this
                 thesis, we study the novel problem of page farm mining and its
                 application in link spam detection. A page farm is the set of
                 Web pages contributing to (a major portion of) the PageRank
                 score of a target page. We show that extracting page farms is
                 computationally expensive, and propose heuristic methods. We 
                 propose the concept of link spamicity based on page farms to 
                 evaluate the degree of a Web page being link spam. Using a 
                 real sample of more than $3$ million Web pages, we analyze the
                 statistics of page farms. We examine the effectiveness of our
                 spamicity-based link spam detection methods using a newly
                 available real data set of spam pages. The empirical study
                 results strongly indicate that our methods are effective.",
  gen =          "0->",

}


@MastersThesis{U-SFraser-CMPT-MSc:2007@gattani,
  author =       "Akshay Kishore Gattani",
  title =        "Automated Natural Language Headline Generation Using
                 Discriminative Machine Learning Models",
  supervisor =   "Anoop Sarkar and Martin Ester",
  month =        apr,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-04-01",
  pages =        "58",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/AkshayGattaniMSc.pdf",
  abstract =     "Headline or short summary generation is an important problem
                 in Text Summarization and has several practical applications. 
                 We present a discriminative learning framework and a rich
                 feature set for the headline generation task. Secondly, we 
                 present a novel Bleu measure based scheme for evaluation of 
                 headline generation models, which does not require human 
                 produced references. We achieve this by building a test corpus
                 using the Google news service.  We propose two stacked 
                 log-linear models for both headline word selection (Content
                 Selection) and for ordering words into a grammatical and 
                 coherent headline (Headline Synthesis).  For decoding a beam
                 search algorithm is used that combines the two log-linear
                 models to produce a list of k-best human readable headlines 
                 from a news story. Systematic training and experimental 
                 results on the Google-news test dataset demonstrate the
                 success and effectiveness of our approach.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Li,
  author =       "Xiaoxing Ginger Li",
  title =        "Towards Expression-Invariant Face Recognition using 
                 Multiple Adaptive Attributes",
  supervisor =   "Richard(Hao) Zhang",
  month =        apr,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-04-01",
  pages =        "76",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/XiaoxingGingerLiMSc.pdf",
  abstract =     "The performances of most existing face recognition 
                 systems suffer from facial expressions.  Unfortunately, there
                 is not yet a satisfactory solution. Therefore, the main focus
                 of this thesis is on expression-invariant face recognition 
                 algorithms.

                 In this thesis, we first propose a 2D face recognition 
                 algorithm by separately modeling geometry and texture
                 information in a face image. The effect of expression is 
                 removed from each of these two attributes independently. We
                 then re-combine them to construct a robust face identifier.
                 Then, we extend our algorithm to recognize 3D faces using 
                 multiple geometric attributes in a face mesh, taking advantage
                 of the invariance of 3D geometry under poses and
                 illuminations. In order to adapt to expression variations, 
                 training is performed for each geometric attribute as well as
                 the weighting scheme for combining multiple attributes.
                 Using our proposed algorithm, the recognition ratio exceeds 
                 96% for the challenging GavaDB database.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Rova,
  author =       "Andrew Rova",
  title =        "Eigen-Css Shape Matching and Recognizing Fish in Underwater
                 Video",
  supervisor =   "Mark Drew and Greg Mori",
  month =        apr,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-04-01",
  pages =        "80",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/AndrewRovaMSc.pdf",
  abstract =     "This thesis presents work on shape matching and object 
                 recognition. First, we describe Eigen-CSS, a faster and more 
                 accurate approach to representing and matching the curvature
                 scale space (CSS) features of shape silhouette contours. 
                 Phase-correlated marginal-sum features and PCA eigenspace 
                 decomposition via SVD differentiate our technique from earlier
                 work. Next, we describe a deformable template object 
                 recognition method for classifying fish species in underwater 
                 video. The efficient combination of shape contexts with 
                 larger-scale spatial structure information allows acceptable 
                 estimation of point correspondences between template and test 
                 images despite missing or inaccurate edge information. Fast 
                 distance transforms and tree-structured dynamic programming 
                 allow the efficient computation of globally optimal 
                 correspondences, and multi-class support vector machines 
                 (SVMs) are used for classification. The two methods, Eigen-CSS
                 shape matching and deformable template matching followed by 
                 texture-based recognition, are contrasted as complementary 
                 techniques that respectively suit the unique characteristics 
                 of two substantially different computer vision problems.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@MasoudHarati,
  author =       "Masoud Harati",
  title =        "IGAUNA(IMPROVEDGLOBALSEQUENCE ALIGNMENTUSINGNON-EXACT
                ANCHORS",
  supervisor =   "ArvindGupta",
  month =        "May",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-May-01",
  pages =        "75",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/MasoudHaratiMSc.pdf",
  abstract =     "Withthesequencingoftheentiregenomeformanyspecies, 
                 bioinformaticiansareincreasinglyrelyingonefficientwhole
                genomealignmenttoolsexhibiting bothhighsensitivity and
                 specifcity.WeintroduceandanalyzeIGAUNA(ImprovedGlobal 
                 AlignmentUsingNon-exact Anchors),anewglobalalignment
                 algorithmbasedonGAUNA,which incomparisonwiththeother
                state-of-the-artalgorithms,almostalwaysproducesasgood
                or betteralignmentswithhighsensitivity andspecificity 
                 usinglesstimeandspace.Whilethosetoolseitherfindexact
                orclose-to-exact matches as anchors,IGAUNA makes use of
                 suffixtrees to find both types ofanchorsdependingonthe
                 instance:Exactanchorsforverysimilarsequencesand
                 otherwisenon-exactanchorsthatareobtainedfroma
                 complicatedsetoftechniques.Inparticular,IGAUNAcan
                rapidlyalignsequencescontainingmillionsofbasesona
                 standardPCwhereothersimilarprogramsarecurrently
                 incapableofaccomplishingsuchatask.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2005@Ma,
  author =       "George Zi Sheng Ma",
  title =        "Model Checking Support for CoreASM: Model Checking 
                 Distributed Abstract State Machines Using SPIN",
  supervisor =   "Uwe Glaesser",
  month =        "May",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-05-08",
  pages =        "109",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/GeorgeZiShengMaMSc.pdf",
  abstract =     "We present an approach to model checking Abstract State
                 Machines, in the context of a larger project called CoreASM,
                 which aims to provide a comprehensive and extensible tool 
                 environment for the design, validation, and verification of 
                 systems using the Abstract State Machine (ASM) formal 
                 methodology. Model checking is an automated and efficient 
                 formal verification technique that allows us to 
                 algorithmically prove properties about state transition 
                 systems.  This thesis describes the design and implementation
                 of model checking support for CoreASM, thereby enabling formal
                 verification of ASMs. We specify extensions to CoreASM 
                 required to support model checking, as well as present a novel
                 procedure for transforming CoreASM specifications into Promela
                 models, which can be checked by the Spin model checker. We 
                 also present the results of applying our ASM model checking
                 tool to several non-trivial software specifications.",
  gen =          "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2007@Fung,
  author =       "Benjamin C. M. Fung",
  title =        "Privacy-Preserving Data Publishing",
  supervisor =   "Ke Wang",
  month =        "May",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-05-17",
  pages =        "",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/BenjaminFungPhD.pdf",
  abstract =     "The success of data mining relies on the availability of high
                 quality data. To ensure quality data mining, effective 
                 information sharing between organizations becomes a vital
                 requirement in today's society. Since data mining often 
                 involves person-specific and sensitive information like
                 medical records, the public has expressed a deep concern about
                 their privacy. Privacy-preserving data publishing is a study 
                 of eliminating privacy threats while, at the same time, 
                 preserving useful information in the released data for data 
                 mining. It is different from the study of privacy-preserving 
                 data mining which performs some actual data mining task. This
                 thesis identifies a collection of privacy threats in real-life
                 data publishing, and presents a unified solution to address
                 these threats.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Ye,
  author =       "Jiang Ye",
  title =        "A Review of Artificial Intelligence Techniques 
                 Applied to Protein Structure Prediction",
  supervisor =   "Veronica Dahl and Kay Wiese",
  month =        may,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-05-01",
  pages =        "89",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/JiangYeMSc.pdf",
  abstract =     "Protein structure prediction (PSP) is a significant, yet 
                 difficult problem that attracts attention from both biology
                 and computing worlds. The problem is to predict protein native
                 structure from primary sequence using computational means. It
                 remains largely unsolved due to the fact that no comprehensive
                 theory of protein folding is available and a global search in
                 the conformational space is intractable. This is why AI 
                 techniques have been effective in tackling some aspects of 
                 this problem.  This survey report reviews biologically 
                 initiated AI techniques that have been applied to PSP problem.
                 We focus on evolutionary computation and ANNs. Evolutionary 
                 computation is used as a population-based search technique, 
                 mainly in ab initio prediction approach. ANNs are most
                 successful in secondary structure prediction by learning 
                 meaningful relations between primary sequence and secondary 
                 structures from datasets. The report also reviews a new
                 generative encoding scheme L-systems to capture protein 
                 structure on lattice models.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Zeng,
 author =       "Xinghuo Zeng",
 title =        "Efficient Maximal Frequent Itemset Mining By Pattern-Aware
                Dynamic Scheduling",
 supervisor =   "Jian Pei",
 month =        "May",
 year =         "2007",
 org =          "SFU-CMPT",
 school =       SFU_CS_School,
 defended =     "2007-05-17",
 pages =        "77",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/XinghuoZengMSc.pdf",
 abstract =     "While frequent pattern mining is fundamental for
many data mining
tasks, mining maximal frequent itemsets efficiently is important in
both theory and applications of frequent itemset mining. The
fundamental challenge is how to search a large space of item
combinations. Most of the existing methods search an enumeration
tree of item combinations in a depth-first manner.

In this thesis, we develop a new technique for more efficient
maximal frequent itemset mining. Different from the classical
depth-first search, our method uses a novel probing and reordering
search method. It uses the patterns found so far to schedule its
future search so that many search subspaces can be pruned. Three
optimization techniques, namely reduced counting, pattern expansion
and head growth, are developed to improve the performance. As
indicated by a systematic empirical study using the benchmark data
sets, our new approach outperforms the currently fastest maximal
frequent itemset mining algorithm FPMax* clearly.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Best,
  author =       "Micah J. Best",
  title =        "Graphs with Monotone Connected Mixed Search Number of at Most
                 Two",
  supervisor =   "Arvind Gupta",
  month =        "August",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-07-01",
  pages =        "86",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2007/MicahBestMSc.pdf",
  abstract =     "Graph searching is used to model a variety of problems and
                 has close connections to variations of path-decomposition. 
                 This work explores Monotone Connected Mixed Search. 
                 Metaphorically, we consider this problem in terms of searchers
                 exploring a network of tunnels and rooms to locate an 
                 opponent. In one turn this opponent moves arbitrarily fast
                 while the searchers may only move to adjacent rooms. The 
                 objective is, given an arbitrary graph, to determine the
                 minimum number of searchers for which there exist a valid
                 series of moves that searches the graph. We show that the 
                 family of graphs requiring at most k searchers is closed under
                 graph contraction.
                 
                 We exploit the close ties between the contraction ordering and
                 the minor ordering to produce a number of structural 
                 decomposition techniques and show that there are 172 
                 obstructions in the contraction order for the set of graphs
                 requiring at most two searchers.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2007@Chen,
  author =       "Liang Chen",
  title =        "Solving Linear Systems of Equations over Cyclotomic
                 Fields",
  supervisor =   "Michael Monagan",
  month =        aug,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-08-07",
  pages =        "",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2007/LiangChenMSc.pdf",
  abstract =     "Let $A\in\Q^{n\times n}[z]$ be a matrix of polynomials
                 and $b\in\Q^n[z]$ be a vector of polynomials. Let $m(z)
                 = \Phi_k[z]$ be the $k^{th}$ cyclotomic polynomial. We
                 want to find the solution vector $x\in\Q^n[z]$ such
                 that the equation $Ax \equiv b \bmod{m(z)}$ holds. One
                 may obtain $x$ using Gaussian elimination, however, it
                 is inefficient because of the large rational numbers
                 that appear in the coefficients of the polynomials in
                 the matrix during the elimination. In this thesis, we
                 present two modular algorithms namely, Chinese
                 remaindering and linear $p$-adic lifting. We have
                 implemented both algorithms in Maple and have
                 determined the time complexity of both algorithms. We
                 present timing comparison tables on two sets of data,
                 firstly, systems with random generated coefficients and
                 secondly real systems given to us by Vahid Dabbaghian
                 which arise from computational group theory. The
                 results show that both of our algorithms are much
                 faster than Gaussian elimination.",
}

@PhdThesis{U-SFraser-CMPT-PhD:2007@Tsang,
  author =       "Herbert H. Tsang",
  title =        "SARNA-Predict: A Permutation-based Simulated Annealing
                 Algorithm for RNA Secondary Structure Prediction",
  supervisor =   "Kay C. Wiese",
  month =        "Aug",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-08-14",
  pages =        "148",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2007/HerbertHTsangPhD.pdf",
  abstract =     "This dissertation describes and presents SARNA-Predict, a
                 novel algorithm for Ribonucleic Acid (RNA) secondary structure
                 prediction based on Simulated Annealing (SA). SA is known to 
                 be effective in solving many different types of minimization 
                 problems and for finding the global minimum in the solution 
                 space. Based on free energy minimization techniques, 
                 SARNA-Predict heuristically searches for the structure with a 
                 free energy close to the minimum free energy delta G for a 
                 strand of RNA, within given constraints. Furthermore, 
                 SARNA-Predict has also been extended to predict RNA secondary
                 structures with pseudoknots.  Although dynamic programming 
                 algorithms are guaranteed to give the minimum free energy
                 structure, the lowest free energy structure is not always the
                 correct native structure.  This is mostly due to the 
                 imperfections in the currently available thermodynamic models.
                 Since SARNA-Predict can incorporate different thermodynamic 
                 models (INN-HB, efn2 and HotKnots) during the free energy 
                 evaluation, this feature makes SARNA-Predict superior to other
                 algorithms such as mfold. mfold can only predict 
                 pseudoknots-free structures and cannot readily be extended to 
                 use other thermodynamic models.  SARNA-Predict encodes RNA 
                 secondary structures as a permutation of helices that are
                 pre-computed. A novel swap mutation operator and different
                 annealing schedules were incorporated into this original 
                 algorithm for RNA Secondary Structure Prediction.  An
                 evaluation of the performance of the new algorithm in terms of
                 prediction accuracy is made via comparison with several 
                 state-of-the-art prediction algorithms. We measured the
                 sensitivity and specificity using nine prediction algorithms. 
                 Four of these are dynamic programming algorithms: mfold, 
                 Pseudoknot (pknotsRE), NUPACK, and pknotsRG-mfe.  The other
                 five are heuristic algorithms: P-RnaPredict, SARNA-Predict,
                 HotKnots, ILM, and STAR algorithms. An evaluation for the 
                 performance of the new algorithm in terms of prediction 
                 accuracy was verified with native structures. Experiments on 
                 thirty three individual known structures from eleven RNA 
                 classes (tRNA, viral RNA, anti-genomic HDV, telomerase RNA, 
                 tmRNA, rRNA, RNaseP, 5S rRNA, Group I intron 23S rRNA, Group I
                 intron 16S rRNA, and 16S rRNA) were performed. The results 
                 presented in this dissertation demonstrate that SARNA-Predict 
                 can out-perform other state-of-the-art algorithms in terms of
                 prediction accuracy.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Bagheri,
  author =       "Majid Bagheri",
  title =        "Efficient k-Coverage Algorithms forWireless Sensor Networks
                 and Their Applications to Early Detection of Forest Fires",
  supervisor =   "Mohamed Hefeeda",
  month =        aug,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-08-01",
  pages =        "64",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/MajidBagheriMSc.pdf",
  abstract =     "Achieving k-coverage in wireless sensor networks has been 
                 shown before to be NP-hard. We propose an efficient
                 approximation algorithm which achieves a solution of size 
                 within a logarithmic factor of the optimal. A key feature of 
                 our algorithm is that it can be implemented in a distributed
                 manner with local information and low message complexity. We
                 design and implement a fully distributed version of our 
                 algorithm. Simulation results show that our distributed
                 algorithm converges faster and consumes much less energy than
                 previous algorithms.  We use our algorithms in designing a 
                 wireless sensor network for early detection of forest fires. 
                 Our design is based on the Fire Weather Index (FWI) System 
                 developed by the Canadian Forest Service. Our experimental 
                 results show the efficiency and accuracy of the proposed
                 system.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Villecroze,
  author =       "Susan Villecroze",
  title =        "Qualitative Study of User Annotations in Satellite 
                 Scheduling",
  supervisor =   "Ted Kirkpatrick",
  month =        "August",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-07-26",
  pages =        "64",
  abstract =     "Past research in scheduling has focused on algorithmic issues
                 and has not addressed many important human-computer
                 interaction issues. For tasks that require a higher level of
                 abstraction and decision, annotation tools could provide an
                 aid. This study investigated how people used annotations to
                 solve problems presented on printed schedules. A user study
                 involving 5 participants was conducted.  Participants were
                 presented with a pre-computed satellite schedule and given a
                 practical problem to solve. Video observations, interview
                 answers, and markings on the schedule and source documents
                 provided data for analysis. Results show that while making
                 trade-offs on different priorities, every participant used and
                 benefited from the use of annotations. Participants did not
                 always use specific annotations because of the connotations of
                 the annotation appearance. The results suggest that support is
                 needed for marking priority changes, deleted activities,
                 interesting regions, and adding text on the schedule.",
  gen =          "0+",
}


@MastersThesis{U-SFraser-CMPT-MSc:2007@Haas,
  author =       "Wolfgang Haas",
  title =        "A Dynamic Resource Scheduling Framework applied to
                 Random Datasets in the Search and Rescue Domain",
  supervisor =   "Dr. Bill Havens",
  month =        sep,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-09-13",
  pages =        "84",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/WolfgangHaasMSc.pdf",
  abstract =     "Dynamic scheduling refers to a class of scheduling
                 problems in which dynamic events, such as delaying of a
                 task, occur throughout execution. We develop a
                 framework for dynamic resource scheduling implemented
                 in Java with a random problem generator, a dynamic
                 simulator and a scheduler. The problem generator is
                 used to generate benchmark datasets that are read by
                 the simulator, whose purpose is to notify the scheduler
                 of the dynamic events when they occur. We perform a
                 case-study on the CoastWatch problem which is an over
                 subscribed dynamic resource scheduling problem in which
                 we assign unit resources to tasks subject to temporal
                 and precedence constraints. Tabu search is implemented
                 as a uniform platform to test various heuristics and
                 neighbourhoods. We evaluate their performance on the
                 generated benchmark dataset and also measure schedule
                 disruption.",
}


@PhdThesis{U-SFraser-CMPT-PhD:2007@Luo,
  author =       "Wei Luo",
  title =        "Mind Change Optimal Learning: Theory and Applications",
  supervisor =   "Oliver Schulte",
  month =        "October",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-10-15",
  pages =        "110",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/WeiLuoPhD.pdf",
  abstract =     "Learning theories play a significant role to machine learning
                 as computability and complexity theories to software 
                 engineering.  Gold~Rs language learning paradigm is one 
                 cornerstone of modern learning theories. The aim of this 
                 thesis is to establish an inductive principle in Gold~Rs
                 language learning paradigm to guide the design of machine
                 learning algorithms.

                 We follow the common practice of using the number of mind
                 changes to measure complexity of Gold~Rs language learning 
                 problems, and study efficient learning with respect to mind 
                 changes. Our starting point is the idea that a learner that is
                 efficient with respect to mind changes minimizes mind changes
                 not only globally in the entire learning problem, but also 
                 locally in subproblems after receiving some evidence. 
                 Formalizing this idea leads to the notion of mind change 
                 optimality. We characterize mind change complexity of language
                 collections with Cantor~Rs classic concept of accumulation 
                 order. We show that the characteristic property of mind 
                 change optimal learners is that they output conjectures 
                 (languages) with maximal accumulation order. Therefore, we
                 obtain an inductive principle in Gold~Rs language learning 
                 paradigm based on the simple topological concept accumulation
                 order.

                 We illustrate the theory by describing strongly mind change
                 optimal learners for various problems such as identifying 
                 linear subspaces, ne-variable patterns, fixed-length patterns,
                 and Bayes net structure.

                 The new inductive principle enables the analysis of the 
                 practical problem of learning Bayes net structure in the rich
                 theoretical framework of Gold~Rs learning paradigm. Applying
                 the inductive principle of mind change optimality leads to a 
                 unique fastest mind change optimal Bayes net learner. This
                 learner conjectures a graph if it is a unique minimal 
                 ~Sindependence map~T, and outputs ~Sno guess~T otherwise. As
                 exact implementation of the fast mind change optimal learner
                 for learning Bayes net structure is NP-hard, mind change 
                 optimality can be approximated with a hybrid criterion for 
                 learning Bayes net structure. The criterion combines search 
                 based on a scoring function with information from statistical
                 tests. We show how to adapt local search algorithms to 
                 incorporate the new criterion. Simulation studies provide
                 evidence that one such new algorithm leads to improved 
                 structure on small to medium samples.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Tan,
 author =     "Yan Tan",
 title =      "Improving mouse pointing with eye-gaze targeting: application in radiology",
 supervisor = "M. Stella Atkins",
 month =      "October",
 year =       "2009",
 org =        "SFU-CMPT",
 school =     SFU_CS_School,
 defended =   "2009-09-15",
 pages =      "70",
 url =        "ftp://fas.sfu.ca/pub/cs/theses/2009/YanTanMSc.pdf",
 abstract =   "In current radiology workstations, a scroll mouse is typically 
              used as the primary input device for navigating image slices 
              and conducting operations on an image. Radiological analysis and
              diagnosis rely on careful observation and annotation of medical 
              images. During analysis of 3D MRI and CT volumes thousands of 
              mouse clicks are performed every day, which can cause wrist 
              fatigue. This thesis presents a dynamic Control-to-Display (C-D)
              gain mouse movement method, controlled by an eye-gaze tracker as
              the target predictor. By adjusting the C-D gain according to the
              distance to the target, the target width in motor space is 
              electively enlarged, thus reducing the index of diculty of the 
              mouse movement. Results indicate that using eye-gaze to predict
              the target position, the dynamic C-D gain method can improve 
              pointing performance and increase the accuracy over traditional 
              mouse movement.",
 gen =        "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2007@Glen,
  author =       "Edward Glen",
  title =        "jViz.Rna - A Tool for Visual Comparison and Analysis of RNA
                 Secondary Structures",
  supervisor =   "Kay C. Wiese",
  month =        "December",
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-09-18",
  pages =        "132",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2007/EdwardGlenMSc.pdf",
  abstract =     "RNA is a single stranded biomolecule which can fold back on
                 itself, forming hydrogen bonds.  Many projects exist which 
                 attempt to predict the 2 dimensional structure that will form 
                 from a given nucleotide sequence.  Visualization of these 
                 predictions helps researchers to better understand the 
                 behaviour and results of their prediction algorithms.

                 jViz.Rna is a tool designed to assist in the analysis of 
                 structure predictions with three unique features: analysis of 
                 multiple structures using seven different visualization 
                 methods, dual graphs of RNA structures which highlight the 
                 topology of the RNA, and finally, the ability to overlay a 
                 predicted structure on top of the native structure of a given 
                 RNA in all visualizations apart from dual graphs which use a
                 different comparison method.

                 jViz.Rna is available through http://jviz.cs.sfu.ca.  It has 
                 been successfully employed on Linux, Mac OS 10.4, and Windows
                 with Java 1.5 or greater.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2007@Tisdall,
  author =       "Matthew Dylan Tisdall",
  title =        "Development and Validation of Algorithms for MRI 
                 Signal Component Estimation",
  supervisor =   "M. Stella Atkins",
  month =        dec,
  year =         "2007",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-12-07",
  pages =        "123",
  gen =          "0->",
  url =          "http://ir.lib.sfu.ca/handle/1892/9730",
  abstract =     "The MRI analysis pipeline consists of a data-acquisition
                 stage defined by a protocol, an estimation stage defined by
                 a function, and an analysis stage -- normally performed by 
                 a adiologist. MRI data is acquired as a 3D or 4D grid of
                 complex-valued measurements. In some protocols more than
                 one set of measurements are fused into a vector of complex 
                 values. However, radiologists normally desire a real-valued
                 3D or 4D dataset representing a feature of interest. To
                 convert from the measurements to the real-valued feature 
                 an estimator must be applied.  

                 This thesis studies the 
                 development and evaluation of estimators. We approach the
                 problem not as one of general image processing, but as one
                 specific to MRI and based in the physics of the measurement 
                 process. The estimators proposed are based on the physics of 
                 MRI and protocols used clinically. We also show how 
                 estimators can be evaluated by testing suitability for 
                 radiological tasks.

                 We present statistical models for protocols and features of 
                 interest that arise in MRI. Since the models contain nuisance
                 parameters many estimators are available from the statistical
                 theory. Additionally, we consider how adding a constraint of
                 regularity in the phase coordinate of the complex data 
                 affects the estimators. We demonstrate how phase regularity
                 can be integrated into the model using estimation with local
                 models and avoiding a costly unwrapping step.

                 To choose among the variety of estimators available for a 
                 model, we suggest task-based quality metrics. In particular,
                 for estimators whose output is destined to be viewed by a 
                 radiologist, we demonstrate human observer studies and models
                 of human perception that can quantify the quality of an 
                 estimator. For features of interest that are analyzed
                 quantitatively, we study the trade-offs between bias and 
                 variance that are available.

                 We find that choosing an estimator specific to the feature of
                 interest and protocol can produce substantially improved 
                 output. Additionally, we find that our human observer results
                 are not predicted by SNR, challenging the use of SNR for 
                 quantifying estimator suitability. We conclude that 
                 MRI-specific estimation and evaluation provide substantial
                 advantages over general-purpose approaches.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@Cheng,
  author =       "Xu Cheng",
  title =        "On the Characteristics and Enhancement of Internet Short
                 Video Sharing",
  supervisor =   "Jiangchuan Liu",
  month =        "April",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-4-2",
  pages =        "78",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/XuChengMSc.pdf",
  abstract =     "In this thesis, using long-term data traces, we present an 
                 in-depth and systematic measurement study on the
                 characteristics of YouTube, the most successful site providing
                 a new generation of short video sharing service. We find that
                 YouTube videos have noticeable differences compared with 
                 traditional videos, making it difficult to use conventional 
                 strategies (e.g., peer-to-peer) to reduce the server workload.
                 However, the social network presented among YouTube videos 
                 opens new opportunities. We design a novel social network 
                 based peer-to-peer short video sharing system, in which peers
                 are responsible for re-distributing the videos that they have
                 cached. We address a series of key design issues to realize 
                 the system, including an efficient indexing scheme, a bi-layer
                 overlay leveraging social networking, a source rate allocation
                 and a pre-fetching strategy to guarantee the playback quality.
                 We perform extensive simulations, which show that the system
                 greatly reduces the server workload and improves the playback
                 quality.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@SimonXinCheng,
  author =       "Simon Xin Cheng",
  title =        "Locating Relay Nodes for P2P VOIP Applications: A 
                 PlanetLab-based Experimental Study",
  supervisor =   "Jiangchuan Liu",
  month =        "April",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-03-28",
  pages =        "62",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/XinChengMSc.pdf",
  abstract =     "In Peer-to-Peer VoIP applications, we can use relay nodes to
                 detour default IP paths for recovering from low quality 
                 sessions (LQSs). Locating quality relay nodes (QRNs) rapidly 
                 and inexpensively however is a very challenging task. In this 
                 project, we present a PlanetLab-based experimental study on 
                 locating relay nodes. We collect the 1-hop QRNs of all LQSs in
                 a network snapshot and evaluate a series of state-of-the-art 
                 QRN selection approaches. These evaluated schemas are either 
                 ineffective or involving excessive probes. We observe that 
                 LQSs with high location similarity tend to share QRNs. 
                 Inspired by this, we propose a geographic landmark (GLM)
                 based system that uses delays from peers to landmarks and 
                 geographic constraints to select QRNs. Evaluations demonstrate
                 that GLM outperforms the existing schemes in terms of cost 
                 efficiency.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2008@Stacho,
  author =      "Juraj Stacho",
  title =       "Complexity of Generalized Colourings of Chordal Graphs",
  supervisor =  "Pavol Hell",
  month =       "April",
  year =        "2008",
  org =         "SFU-CMPT",
  school =      SFU_CS_School,
  defended =    "2008-03-25",
  pages =       "168",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/JurajStachoPhD.pdf",
  abstract =    "The generalized graph colouring problem (GCOL) for a fixed
                integer $k$, and fixed classes of graphs 
                $P_1,\ldots, P_k$ (usually describing some 
                common graph properties), is to decide, for a given graph $G$, 
                whether the vertex set of $G$ can be partitioned into sets 
                $V_1,\ldots, V_k$ such that, for each $i$, the induced 
                subgraph of $G$ on $V_i$ belongs to $P_i$.  It 
                can be seen that GCOL generalizes many natural colouring and 
                partitioning problems on graphs.

                In this thesis, we focus on generalized colouring problems in
                chordal graphs.  The structure of chordal graphs is known to
                allow solving many difficult combinatorial problems, such as
                the graph colouring, maximum clique and others, in polynomial,
                and in many cases in linear time.  Our study of generalized
                colouring problems focuses on those problems in which the sets
                $P_i$ are characterized by a single forbidden induced
                subgraph.  We show, that for $k=2$, all such problems where
                the forbidden graphs have at most three vertices are polynomial
                time solvable in chordal graphs, whereas, it is known that
                almost all of them are $NP$-complete in general. On
                the other hand, we show infinite families of such problems
                which are $NP$-complete in chordal graphs.  By combining a
                polynomial algorithm and an $NP$-completeness proof, we answer
                a question of Broersma, Fomin, Ne&#353;et&#345;il and Woeginger
                about the complexity of the so-called subcolouring problem on
                chordal graphs.  Additionally, we explain, how some of these
                results generalize to particular subclasses of chordal graphs,
                and we show a complete forbidden subgraph characterization for
                the so-called monopolar partitions of chordal graphs.

                Finally, in the last part of the thesis, we focus on a
                different type of colouring problem -- injective colouring.
                We describe several algorithmic and (in-)approximability
                results for injective colourings in the class of chordal
                graphs and its subclasses.  In the process, we correct a
                result of Agnarsson et al. on inapproximability of the
                chromatic number of the square of a split graph.",
  gen =          "0->",
}

@MastersThesis{U-SFraser-CMPT-MSc:2008@Fulek,
  author =       "Radoslav Fulek",
  title =        "Intersecting convex sets by rays",
  supervisor =   "Dr. Gbor Tardos",
  month =        jun,
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-06-23",
  pages =        "50",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/RadoslavFulekMSc.pdf",
  abstract =     "What is the smallest number tau = tau_d(n) such that
                 for any collection C of n pairwise disjoint compact
                 convex sets in R^d, there is a point such that any ray
                 (half-line) emanating from it meets at most tau sets of
                 the collection? In this thesis we show an upper and
                 several lower bounds on the value tau_d(n),and thereby
                 we completely answer the above question for R^2,and
                 partially for higher dimensions.We show the order of
                 magnitude for an analog of tau_2(n) for collections of
                 fat sets with bounded diameter. We conclude the thesis
                 with some algorithmic solutions for finding a point p
                 that minimizes the maximum number of sets in C we are
                 able to intersect by a ray emanating from p in the
                 plane, and for finding a point that basically witnesses
                 our upper bound on tau_d(n) in any dimension. However,
                 the latter works only for restricted sets of objects.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@Colak,
  author =       "Recep Colak",
  title =        "Towards Finding The Complete Modulome: Density
                 Constrained Biclustering",
  supervisor =   "Martin Ester",
  month =        jun,
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-06-23",
  pages =        "95",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/RecepColakMSc.pdf",
  abstract =     "Large-scale gene expression experiments and
                 interaction networks have become major data sources for
                 discovery in systems biology. In several types of
                 interaction networks, as is widely established, active
                 modules, i.e. functional, simultaneously active groups
                 of genes, are best encoded as highly interconnected
                 regions that are co-expressed and show signi cant
                 changes in an accompanying set of gene expression
                 experiments. Accordingly, inferring an organism's
                 active modulome, the entirety of active modules,
                 translates to identifying these dense and co-expressed
                 regions, which is NP-hard.
                 
                 We provide a novel algorithm, DCB-Miner, that addresses
                 the corresponding computationally hard problem by means
                 of a carefully designed search strategy, which has been
                 speci cally adapted to the topological peculiarities of
                 protein interaction networks. Our algorithm outperforms
                 all prior related approaches on standard datasets from
                 H. sapiens and S. cerevisiae in a Gene Ontology-based
                 competition and nds modules that convey particularly
                 interesting novel biological meaning.",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@Schell,
  author =       "David George Schell",
  title =        "Matrix Partitions of Digraphs",
  supervisor =   "Pavol Hell",
  month =        "June",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-06-30",
  pages =        "73",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2008/DavidGeorgeSchellMSc.pdf",
  abstract =     "The matrix partition problem has been of recent interest
                 in graph theory. Matrix partitions generalize the study
                 of graph colourings and homomorphisms. Many well-known
                 graph partition problems can be stated in terms of 
                 matrices. For example skew partitions, split partitions,
                 homogeneous sets, clique-cutsets, stable-cutsets and
                 k-colourings can all be modeled as matrix partitions.
                 For each matrix partition problem there is an equivalent 
                 trigraph H-colouring problem.  We show a 'dichotomy' for
                 the class of list H-colouring problems where H is a 
                 so-called trigraph path. For each trigraph path H we
                 show that the list H-colouring problem is either
                 NP-complete or polynomial time solvable. For each 
                 trigraph path H we associate a digraph H-minus such that
                 the list (H-minus)-colouring problem is polynomial time 
                 solvable if the list H-colouring problem is polynomial
                 time solvable, and is NP-complete otherwise.",
}


@PhdThesis{U-SFraser-CMPT-PhD:2008@Bian,
  author =       "Zhengbing Bian",
  title =        "Algorithms for Wavelength Assignment and Call Control in
                 Optical Networks",
  supervisor =   "Qian-ping Gu",
  month =        "August",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2000-08-05",
  pages =        "167",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/ZhengbingBianPhD.pdf",
  abstract =     "Routing and channel assignment is a fundamental problem
                 in computer/communication networks. In wavelength 
                 division multiplexing (WDM) optical networks, the 
                 problem is called routing and wavelength assignment or
                 routing and path coloring (RPC) problem: given a set of
                 connection requests, find a routing path to connect each 
                 request and assign each path a wavelength channel (often
                 called a color) subject to certain constraints. One
                 constraint is the distinct channel assignment: the colors
                 (channels) of the paths in the same optical fiber must be
                 distinct. Another common constraint is the channel
                 continuity: a path is assigned a single color.  When a
                 path may be assigned different colors on different 
                 fibers, the RPC problem is known as the routing and call
                 control (RCC) problem.  When the routing paths are given
                 as part of the problem input, the RPC and RCC problems
                 are called the path coloring and call control problems,
                 respectively. Major optimization goals for the above 
                 problems include to minimize the number of colors for
                 realizing a given set of requests and to maximize the 
                 number of accommodated requests using a given number 
                 of colors. Those optimization problems are NP-hard for
                 most network topologies, even for simple networks like
                 rings and trees of depth one. In this thesis, we make 
                 the following contributions:

                 (1) We give better approximation algorithms which use 
                 at most 3L (L is the maximum number of paths in a fiber)
                 colors for the minimum path coloring problem in trees 
                 of rings. The 3L upper bound is tight since there are 
                 instances requiring 3L colors. We also give better 
                 approximation algorithms for the maximum RPC problem in 
                 rings. 
                 (2) We develop better algorithms for the minimum and 
                 maximum RPC problems on multi-fiber networks. 
                 (3) We develop better algorithms for the call control
                 problem on simple topologies.
                 (4) We develop carving-decomposition based exact
                 algorithms for the maximum edge-disjoint paths problem 
                 in general topologies. We develop and implement tools for 
                 computing optimal branch/carving decompositions of planar
                 graphs to provide a base for the 
                 branch/carving-decomposition based algorithms.  These
                 tools are of independent interests.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@Lau,
  author =       "Man Ki Lau",
  title =        "Energy-preserving Maintenance of k-centers on Large 
                 Wireless Sensor Networks",
  supervisor =   "Jian Pei",
  month =        "November",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2007-10-25",
  pages =        "73",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/ManKiLauMSc.pdf",
  abstract =     "Recently, large wireless sensor networks have been
                 used in many applications. Analyzing data detected
                 by numerous sensors is one of the prominent issues in
                 these applications. However, the power consumption of
                 sensors is the major bottleneck of wireless sensor
                 network lifetime. Energy-preserving data collection on 
                 large sensor networks becomes an important problem.
                 In this thesis, we focus on continuously maintaining 
                 k-centers of sensor readings in a large sensor network.
                 The goal is to preserve energy in sensors while the
                 quality of k-centers is retained. We also want to 
                 distribute the clustering task into sensors, so that
                 raw data and many intermediate results do not need to
                 be transmitted to the server. We propose the reading
                 reporting tree as the data collection and analysis 
                 framework in large sensor networks. We also introduced
                 a uniform sampling method, a reporting threshold 
                 method and a lazy approach to achieve good quality
                 approximation of k-centers.",
  gen =          "0-+",
}


@PhdThesis{U-SFraser-CMPT-PhD:2008@Xu,
  author =       "Yabo Xu",
  title =        "New Models and Techniques on Privacy-Preserving
                 Infomration Sharing",
  supervisor =   "Ke Wang and Jian Pei",
  month =        "12",
  year =         "2008",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-12-05",
  pages =        "150",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2008/Yabo-ArberZuPhD.pdf",
  abstract =     "Due to the wide deployment of Internet and 
                 information technology, the ever-growing privacy 
                 concern has been a major obstacle for information
                 sharing. This thesis work thus centres on developing
                 new models and techniques to deal with emerging 
                 privacy issues in various contexts of information 
                 sharing and exchange.  Specifically, along with the 
                 main theme, this thesis work can be divided into three
                 research problems, summarized as follows.  The first
                 problem is privacy-preserving data mining spanning 
                 multiple private data sources. The goal of this 
                 research is to enable the computation as the data
                 collected in a central place, but preserves the
                 privacy of participating sites. This problem has 
                 been studied in the context of classification with 
                 multiple private data sources integrated with join 
                 semantics.  The second problem is privacy-preserving
                 data publishing. This research aims to address the
                 scenario where a data owner wishes to publish the data
                 while preserving individual privacy. This topic has 
                 been extensively studied in the context of relational 
                 data, but much less is known for transaction data. 
                 We propose one way to address this issue in this 
                 thesis.  The third problem is privacy enhancing 
                 online personalized service. This research starts
                 from an end s point of view, and studies how to
                 submit a piece of personal data to exchange for 
                 service without compromising individual privacy. Our
                 contribution on this topic is a framework under which
                 individual users can strike a balance between service
                 quality and privacy protection.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Finkbeiner,
  author =       "Bernhard Finkbeiner",
  title =        "Fast Computed Tomography and Volume Rendering Using
                 the Body-Centered Cubic Lattice",
  supervisor =   "Torsten M{\"o}ller",
  month =        "March",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-03-17",
  pages =        "89",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/BernhardFinkbeinerMSc.pdf",
  abstract =     "Two main tasks in the field of volumetric image 
                 processing are acquisition and visualization of 3D 
                 data. The main challenge is to reduce processing costs,
                 while maintaining high accuracy. To achieve these goals
                 for volume rendering (visualization), we demonstrate
                 that non-separable box splines for body-centered cubic
                 (BCC) lattices can be adapted to fast evaluation on 
                 graphics hardware. Thus, the BCC lattice can be used for
                 interactive volume rendering leading to better image
                 quality than comparable methods. Leveraging this result, 
                 we study volumetric reconstruction methods based on the
                 Expectation Maximization (EM) algorithm. We show the 
                 equivalence of the standard implementation of the EM-
                 based reconstruction with an implementation based on
                 hardware-accelerated volume rendering for nearest-
                 neighbor interpolation to achieve fast reconstruction 
                 times.  Accuracy is improved by adapting the EM algorithm 
                 for the BCC lattice, leading to superior accuracy, more
                 compact data representation, and better noise reduction
                 compared to the Cartesian one.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Giabbanelli,
  author =       "Philippe J. Giabbanelli",
  title =        "Self-improving immunization policies for complex 
                 networks",
  supervisor =   "Joseph G. Peters",
  month =        "March",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-03-27",
  pages =        "128",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/PhilippeGiabbanelliMSc.pdf",
  abstract =     "Viruses are objects of research in both computer 
                 networks and epidemiology. The two types of viruses
                 are very close in terms of modeling: in both cases,
                 the spread of viruses can be modeled as a broadcast
                 in a graph with a decentralized scheme. In order to
                 design efficient immunization strategies, we have 
                 to be aware of the properties of the graphs in 
                 which viruses spread. Thus, we first review the 
                 properties found ! in many real-world graphs, such
                 as small-world and scale-free, and the deterministic
                 models that exhibit them. As a virus is an 
                 independent entity, the modeling should take into 
                 consideration parameters related to agents, such as
                 their heuristics and their memory. We perform a 
                 2^k factorial design to identify the contribution of
                 the parameters of the agents and the properties of 
                 the topology. To benefit from the potential of 
                 agents to immunize dynamic networks, we specify a 
                 multi-agent system: the agents observe their 
                 environment, exchange their knowledge with minimal
                 communication cost and fast consensus, and thus have
                 a model of the dynamics that allows them to cope 
                 with changes. We present an algebraic framework that
                 allows such exchange of knowledge while providing 
                 rigorous characterization. Among futures works, we
                 discuss deterministic models for scale-free networks
                 from vertex contractions, and reconfiguration of 
                 agent communications.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2009@Karimi,
  author =       "Mohammad Mahdi Karimi",
  title =        "Minimum Cost Homomorphisms to Digraphs",
  supervisor =   "Arvind Gupta, Pavol Hell",
  month =        "March",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-2-3",
  pages =        "113",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/MohammadMahdiKarimiPhD.pdf",
  abstract =     "For digraphs <em>D</em> and <em>H</em>, a homomorphism of <em>D</em> to <em>H</em>
                 is a mapping &fnof;:\ <em>V</em>(<em>D</em>) &rarr; <em>V</em>(<em>H</em>) such that <em>uv</em> &isin; <em>A</em>(<em>D</em>)
                 implies &fnof;(<em>u</em>)&fnof;(<em>v</em>) &isin; <em>A</em>(<em>H</em>). Suppose <em>D</em> and <em>H</em> are two
                 digraphs, and c&subi;(<em>u</em>), <em>u</em> &isin; <em>V</em>(<em>D</em>), <em>i</em> &isin; <em>V</em>(<em>H</em>), are
                 nonnegative integer costs. The cost of the homomorphism
                 &fnof; of <em>D</em> to <em>H</em> is <span style='font-size:1.25em;'>&sum;<sub><em>u</em>&isin;<em>V</em>(<em>D</em>)<sup>c<sub>&fnof;(<em>u</em>)</sub></sup></sub>(<em>u</em>)</span>. The
                 minimum cost homomorphism for a fixed digraph <em>H</em>, 
                 denoted by MinHOM(<em>H</em>), asks whether or not an input
                 digraph <em>D</em>, with nonnegative integer costs <em>c</em><sub><em>i</em></sub>(<em>u</em>),
                 <em>u</em> &isin; <em>V</em>(<em>D</em>), <em>i</em> &isin; <em>V</em>(<em>H</em>), admits a homomorphism &fnof; to
                 <em>H</em> and if it admits one, find a homomorphism of minimum
                 cost. Our interest is in proving a dichotomy for minimum
                 cost homomorphism problem: we would like to prove that 
                 for each digraph <em>H</em>, MinHOM(<em>H</em>) is polynomial-time 
                 solvable, or NP-hard. Gutin, Rafiey, and Yeo conjectured
                 that such a classification exists: MinHOM(<em>H</em>) is 
                 polynomial time solvable if <em>H</em> admits a <em>k</em>-Min-Max
                 ordering for some <em>k</em> &ge; 1, and it is NP-hard 
                 otherwise.

                 For undirected graphs, the complexity of the problem is 
                 well understood; for digraphs, the situation appears to
                 be more complex, and only partial results are known. In
                 this thesis, we seek to verify this conjecture for 
                 'large' classes of digraphs including reflexive 
                 digraphs, locally in-semicomplete digraphs, as well as
                 some classes of particular interest such as quasi-
                 transitive digraphs. For all classes, we exhibit a 
                 forbidden induced subgraph characterization of digraphs
                 with <em>k</em>-Min-Max ordering; our characterizations imply
                 a polynomial time test for the existence of a 
                 <em>k</em>-Min-Max ordering. Given these characterizations, we
                 show that for a digraph <em>H</em> which does not admit a 
                 <em>k</em>-Min-Max ordering, the minimum cost homomorphism 
                 problem is NP-hard. This leads us to a full dichotomy
                 classification of the complexity of minimum cost 
                 homomorphism problems for the aforementioned classes 
                 of digraphs.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Guo,
  author =       "Zhenshan Guo",
  title =        "Partial Aggregation and Query Processing of OLAP Cubes",
  supervisor =   "Wo-shun Luk",
  month =        "April",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2008-10-7",
  pages =        "54",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/ZhenshanGuoMSc.pdf",
  abstract =     "This work presents a novel and flexible PPA (Partial 
                 Pre-Aggregation) construction and query processing 
                 technique in OLAP (On-Line Analytical Processing) applications - 
                 SplitCube, which greatly reduces the cube size, shortens the 
                 cube building time and maintains an acceptable query performance
                 at the time. Furthermore, we devise two enhanced query 
                 processing techniques. They can further improve the query
                 performance or reduce cube building time further and keep query
                 response time at an acceptable level. The result analysis shows
                 more insights in cube construction and query processing 
                 procedure and illustrates the advantages and disadvantages of
                 each algorithm. Finally, we give guidelines in how to choose the
                 right algorithm in different user cases.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2008@Valadkhan,
  author =       "Payam Valadkhan",
  title =        "Extremal Oriented Graphs and Eerdos-Hajnal Conjecture",
  supervisor =   "Gabor Tardos",
  month =        "June",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-06-19?",
  pages =        "35",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/PayamValadkhanMSc.pdf",
  abstract =     "For a family L (finite or infinite) of oriented graphs, a new
                 parameter called the compressibility number of L and denoted 
                 by z(L) is defined. The main motivation is the application of
                 this parameter in a special case of Turan-type extremal 
                 problems for digraphs, in which it replaces the role of 
                 chromatic number parameter in the classic extremal problems. 
                 Determining this parameter, in the most explicit possible 
                 form, for oriented graphs with bounded oriented and/or acyclic
                 chromatic number (planar graph in particular) leads us to the 
                 infamous Erdos-Hajnal conjecture.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Tien,
  author =      "Geoffrey Tien",
  title =       "Building Interactive Eyegaze Menus for Surgery",
  supervisor =  "M. Stella Atkins",
  month =       "March",
  year =        "2009",
  org =         "SFU-CMPT",
  school =      SFU_CS_School,
  defended =    "2009-03-23",
  pages =       "72",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/GeoffreyTienMSc.pdf",
  abstract =    "A real-time hands-free eyegaze menu selection interface was implemented
                using a commercially available eyetracking system, with selections 
                activated by eyegaze fixations and glances on menu widgets. A pilot
                study tested three different spatial layouts of the menu widgets and
                employed a highly accurate two-stage selection mechanism. Improvements
                based on the pilot results were incorporated into a second revision of
                the interface with a more streamlined selection mechanism which allowed
                us to test users' selection accuracy. Another study was conducted on the
                revised interface and received a positive response from our participants
                in addition to a faster selection while maintaining high selection accuracy.
                A software framework was created for building eyetracking applications which 
                provides basic eyetracking and interaction features in a modular format. The
                framework is also expandable to include more features by adding customized
                modules. Two new applications built using the framework were evaluated to 
                demonstrate its flexibility.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Chuang,
  author =       "Johnson Chuang",
  title =        "Energy Aware Colour Mapping for Visualization",
  supervisor =   "Torsten M{\"o}ller and Daniel Weiskopf",
  month =        "June",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-6-23",
  pages =        "70",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/JohnsonChuangMSc.pdf",
  abstract =     "We present a design technique for colours that lower the
                 energy consumption of the display device. Our approach relies 
                 on a screen space variant energy model. Guided by perceptual 
                 principles, we present three variations of our approach for 
                 finding low energy, distinguishable, iso-lightness colours. 
                 The first is based on a set of discrete user-named 
                 (categorical) colours, which are ordered according to energy
                 consumption. The second optimizes for colours in the 
                 continuous CIELAB colour space. The third is hybrid, 
                 optimizing for colours in select CIELAB colour subspaces that
                 are associated with colour names. We quantitatively compare
                 our colours with a traditional choice of colours, 
                 demonstrating that approximately 45 percent of the display
                 energy is saved. The colour sets are applied to 2D 
                 visualization of nominal data and volume rendering of 3D 
                 scalar fields. A new colour blending method for volume 
                 rendering which preserves hues further improves colour 
                 distinguishability.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2009@Bastani,
  author =       "Behnam Bastani",
  title =        "Spectral Analysis of Output Devices, from Printing to
                 Predicting",
  supervisor =   "Dr. Brian Funt",
  month =        "Jun",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-07-29",
  pages =        "142",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/BehnamBastaniPhD.pdf",
  abstract =     "The focus of this thesis is to develop and introduce 
                 algorithms that extend traditional colour reproduction from
                 three dimensions to higher dimensions in order to minimize 
                 metamerism. The thesis introduces models that can 
                 accurately predict interactions between the primaries for 
                 non-linear output devices in spectral colour space. 
                 Experiments were designed and performed to aid in 
                 understanding how optimized the spectral characteristics of
                 existing printer inks and display primaries are, and how the
                 inks and primaries should be designed so that the accuracy
                 of the reproduction is optimized. The time and space 
                 computational complexity of the reproduction algorithms 
                 grows exponentially with the number of input dimensions. The
                 algorithms for finding the best combinations of inks or 
                 primaries matching a given input reflectance become more 
                 challenging when the inks interact with each other 
                 non-linearly, as is usually the case in printers. A number
                 of different methods are introduced in this thesis to handle 
                 gamut mapping and the colour reproduction process in higher 
                 dimensions. An ink-separation algorithm is introduced to 
                 find the ink combination yielding a chosen gamut-mapped 
                 spectral reflectance. Experiments with real inks for 
                 spectral colour reproduction were performed to compare the
                 results of the reproduction against trichromatic colour 
                 reproduction on a 9-ink printer system. Finally, a new
                 application of reflectance analysis in higher dimensions
                 is introduced.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Choi,
  author =       "Yongchul (Kenneth) Choi",
  title =        "SPATIAL OLAP QUERY ENGINE: PROCESSING AGGREGATE QUERIES ON 
                 SPATIAL OLAP DATA",
  supervisor =   "Wo-Shun Luk",
  month =        "July",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-07-17",
  pages =        "74",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/YongchulKennethChoiMSc.pdf",
  abstract =     "A spatial OLAP can be characterised as a practical union of 
                 OLAP analysis and geographic mapping. A spatial OLAP query
                 has a spatial confinement along with the conventional 
                 non-spatial predicate. An existing framework we opt for is
                 to convert a spatial OLAP query into a set of queries for a
                 general-purpose ROLAP engine. However, little has been done
                 at the query optimization level, once the queries are 
                 submitted to the query engine.  This thesis introduces three
                 query engines on an experimental MOLAP system. The first is
                 the implementation of the framework in the MOLAP context. 
                 The second increases the efficiency by adopting a novel
                 merging technique to screen out many useless queries. The
                 third does all aggregation on the fly, which outperforms the
                 first two query engines by a wide margin under many 
                 circumstances. Detailed experimental performance data are 
                 presented, using a real-life database with 1/3 million of 
                 spatial objects.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Huang,
  author =       "Jiawei Huang",
  title =        "Saliency Detection and Feature Matching for Image Trimming 
                 and Tracking in Active Video",
  supervisor =   "Ze-Nian Li",
  month =        "Aug",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-08-08",
  pages =        "69",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/JiaweiHuangMSc.pdf",
  abstract =     "We develop a new automatic Object of Interest detection
                 method for image trimming and a novel tracking technique in 
                 active videos.  Both applications consist of salient region
                 detection and feature matching. We deploy a color-saliency
                 weighted Probability-of-Boundary (cPoB) map to detect salient
                 regions. Scale Space Image Pyramid (SSIP) feature matching is
                 proposed for image trimming. An image pyramid is created to 
                 imitate the view point change for stable keypoint selection. 
                 Successive Classification Maximum Similarities (SCMS) feature
                 matching is used for tracking. A strong classifier trained by
                 AdaBoost is utilized for keypoint classification and 
                 subsequent Linear Programming rejects outliers. The object-
                 centered property of Active Video is highly beneficial because
                 it captures the essence of Human Visual Attention and
                 facilitates self-initialization in tracking. Experiments
                 demonstrate the importance of saliency detection and feature
                 matching and confirm that our approach can automatically 
                 detect salient regions in images and track reliably in 
                 videos.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Ma,
  author =       "William Pak Tun Ma",
  title =        "Motion Estimation for Functional Medical Imaging Studies
                 Using a Stereo Video Head Pose Tracking System",
  supervisor =   "Ghassan Hamarneh and Greg Mori",
  month =        "Aug",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-07-17",
  pages =        "70",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/WilliamPakTunMaMSc.pdf",
  abstract =     "Patient motion is unavoidable during long medical imaging
                 scan times. In particular, motion artifacts in functional and
                 molecular brain imaging (e.g., dynamic positron emission 
                 tomography in dPET) are known to corrupt the data leading to
                 inaccurate analysis and diagnosis. Most existing motion 
                 correction solutions either rely on attaching external markers
                 or on data-driven image registration algorithms. In this work,
                 we propose a new motion correction approach. It alleviates the
                 need for inconvenient external markers and relaxes the 
                 dependence on the fragile similarity metrics that are 
                 generally incapable of capturing the complex spatio-temporal 
                 tracer dynamics in dPET. We develop a hybrid, multi-sensor
                 method that uses a marker-free video tracker, along with 
                 image-based registration. The balance between the two is
                 automatically adapted to confide in the more certain 
                 measurement. Our quantitative results demonstrate improved 
                 motion estimation and kinetic parameter extraction when using 
                 our hybrid method.",
  gen =          "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2009@Hsu,
  author =       "Cheng-Hsin Hsu",
  title =        "Efficient Mobile Multimedia Streaming",
  supervisor =   "Mohamed Hefeeda",
  month =        "November",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-11-24",
  pages =        "1-224",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/Cheng-HsinHsuPhD.pdf",
  abstract =     "Modern mobile devices have evolved into small computers that
                 can render multimedia streaming content anywhere and anytime. 
                 These devices can extend the viewing time of users and provide
                 more business opportunities for service providers. Mobile
                 devices, however, make a challenging platform for providing
                 high-quality multimedia services. The goal of this thesis is to
                 identify these challenges from various aspects, and propose 
                 efficient and systematic solutions to solve them. In particular,
                 we study mobile video broadcast networks in which a base station
                 concurrently transmits multiple video streams over a shared air
                 medium to many mobile devices. We propose algorithms to optimize
                 various quality-of-service metrics, including streaming quality,
                 bandwidth efficiency, energy saving, and channel switching 
                 delay. We analytically analyze the proposed algorithms, and we
                 evaluate them using numerical methods and simulations. In 
                 addition, we implement the algorithms in a real testbed to show
                 their practicality and efficiency. Our analytical, simulation,
                 and experimental results indicate that the proposed algorithms
                 can: (i) maximize energy saving of mobile devices, (ii) 
                 maximize bandwidth efficiency of the wireless network, (iii) 
                 minimize channel switching delays on mobile devices, and (iv)
                 efficiently support heterogeneous mobile devices. Last, we give
                 network operators guidelines on choosing solutions suitable for
                 their mobile broadcast networks, which allow them to provide 
                 millions of mobile users much better viewing experiences, 
                 attract more subscribers, and thus increase the revenues.",
  gen =          "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Nguyen,
 author =     "Nhi Hoang Nguyen",
 title =      "A hybrid approach to segmenting hair in dermoscopic images using
              a universal kernel",
 supervisor = "M. Stella Atkins",
 month =      "November",
 year =       "2009",
 org =        "SFU-CMPT",
 school =     SFU_CS_School,
 defended =   "2009-10-26",
 pages =      "70",
 url =        "ftp://fas.sfu.ca/pub/cs/theses/2009/NhiHoangNguyenMSc.pdf",
 abstract =   "Hair occlusion often causes automated melanoma diagnostic 
              systems to fail. We present a new method to segment hair in 
              dermoscopic images. First, all possible dark and light hairs are
              amplified without prejudice with a universal matched filtering
              kernel. We then process the filter response with a novel tracing
              algorithm to get a raw hair mask. This raw mask is skeletonized
              to contain only the centerlines of all the possible hairs. Then
              the centerlines are verified by applying a model checker on the 
              response and the original images. If a centerline indeed
              corresponds to a hair, the hair is reconstructed; otherwise it is
              rejected. The result is a clean hair mask which can be used to 
              disocclude hair. Application on real dermoscopic images yields
              good results for thick hair of varying colours. The algorithm
              also performs well on skin images with a mixture of both dark and
              light hair.",
 gen =        "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Liu,
  author =       "Yi Liu",
  title =        "Video Streaming over Cooperative Wireless Networks",
  supervisor =   "Mohamed Hefeeda",
  month =        "December",
  year =         "2009",
  org =          "SFU-CMPT",
  school =       SFU_CS_School,
  defended =     "2009-12-10",
  pages =        "70",
  url =          "ftp://fas.sfu.ca/pub/cs/theses/2009/YiLiuMSc.pdf",
  abstract =     "We study the problem of broadcasting video streams over
                 a Wireless Metropolitan Area Network (WMAN) to many mobile
                 devices. We propose a cooperative network in which several
                 elected mobile devices share received video data over a 
                 Wireless Local Area Network (WLAN). The proposed system 
                 significantly reduces the energy consumption and the 
                 channel switching delay concurrently. We design a 
                 distributed leader election algorithm for the cooperative
                 system and analytically show that the proposed system 
                 outperforms current systems in terms of energy consumption
                 and channel switching delay. Our experimental results from
                 a real mobile video streaming testbed show that the 
                 proposed cooperative system is promising because it 
                 achieves high energy saving, significantly reduces channel
                 switching delay and uniformly distributes load on all 
                 mobile devices. Furthermore, we complement our empirical
                 evaluation with a trace driven simulator to rigorously 
                 show the viability of the proposed cooperative system.",
  gen =          "0->",
}



@PhdThesis{U-SFraser-CMPT-PhD:2008@haffari,
  author =      "Gholamreza Haffari",
  title =       "Machine Learning approaches for dealing with
                limited bilingual training data in statistical 
                machine translation",
  supervisor =  "Anoop Sarkar",
  month =       "December",
  year =        "2009",
  org =         "SFU-CMPT",
  school =      SFU_CS_School,
  defended =    "2009-08-17",
  pages =       "138",
  url =         "ftp://fas.sfu.ca/pub/cs/theses/2009/GholamrezaHaffariPhD.pdf",
  abstract =    "Statistical Machine Translation (SMT) models
                learn how to translate by examining a bilingual 
                parallel corpus containing sentences aligned 
                with their human-produced translations. 
                However, high quality translation output is 
                dependent on the availability of massive 
                amounts of parallel text in the source and 
                target languages. There are a large number of 
                languages that are considered low-density, 
                either because the population speaking the 
                language is not very large, or even if 
                millions of people speak the language, 
                insufficient online resources are available in
                that language. This thesis covers machine 
                learning approaches for dealing with such 
                situations in statistical machine translation 
                where the amount of available bilingual data
                is limited. The problem of learning from 
                insufficient labeled training data has been
                dealt with in machine learning community under 
                two general frameworks: (i) Semi-supervised 
                Learning, and (ii) Active Learning. The complex
                nature of machine translation task poses severe
                challenges to most of the algorithms developed
                in machine learning community for these two 
                learning scenarios. In this thesis, I develop 
                semi-supervised learning as well as active
                learning algorithms to deal with the shortage 
                of bilingual training data for Statistical 
                Machine Translation task, specific to cases 
                where there is shortage of bilingual training 
                data. This dissertation provides two approaches,
                unified in what is called the bootstrapping 
                framework, to this problem.",
  gen =         "0->",
}


@PhdThesis{U-SFraser-CMPT-PhD:2009@YudongLiu,
  author =      "Yudong Liu",
  title =       "Semantic Role Labeling using Lexicalized Tree Adjoining Grammars",
  supervisor =  "Anoop Sarkar",
  month =       "December",
  year =        "2009",
  org =         "SFU-CMPT",
  school =      SFU_CS_School,
  defended =    "2009-11-30",
  pages =       "116",
  url =         "ftp://fas.sfu.ca/pub/cs/theses/2009/YudongLiuPhD.pdf",
  abstract =    "The predicate-argument structure (PAS) of a
                natural language sentence is a useful representation that can be used
                for a deeper analysis of the underlying meaning of the sentence or
                directly used in various natural language processing (NLP)
                applications. The task of semantic role labeling (SRL) is to identify
                the predicate-argument structures and label the relations between the
                predicate and each of its arguments. Researchers have been studying
                SRL as a machine learning problem in the past six years, after large-
                scale semantically annotated corpora such as FrameNet and PropBank
                were released to the research community. Lexicalized Tree Adjoining
                Grammars (LTAGs), a tree rewriting formalism, are often a convenient
                representation for capturing locality of predicate-argument
                relations. Our work in this thesis is focused on the development and
                learning of the state of the art discriminative SRL systems with
                LTAGs. Our contributions to this field include: We apply to the SRL
                task a variant of the LTAG formalism called LTAG-spinal and the
                associated LTAG-spinal Treebank (the formalism and the Treebank were
                created by Libin Shen). Predicate-argument relations that are either
                implicit or absent from the original Penn Treebank are made explicit
                and accessible in the LTAG-spinal Treebank, which we show to be a
                useful resource for SRL. We propose the use of the LTAGs as an
                important additional source of features for the SRL task. Our
                experiments show that, compared with the best-known set of features
                that are used in state of the art SRL systems, LTAG-based features
                can improve SRL performance significantly. We treat multiple LTAG
                derivation trees as latent features for SRL and introduce a novel
                learning framework -- Latent Support Vector Machines (LSVMs) to the
                SRL task using these latent features. This method significantly
                outperforms state of the art SRL systems. In addition, we adapt an
                SRL framework to a real-world ternary relation extraction task in the
                biomedical domain. Our experiments show that the use of SRL related
                features significantly improves performance over the system using
                only shallow word-based features.",
  gen =         "0->",
}

@PhdThesis{U-SFraser-CMPT-PhD:2009@Shi,
  author =      "Lilong Shi",
  title =       "Novel Colour Constancy Algorithms for Digital Colour Imagery",
  supervisor =  "Dr. Brian Funt",
  month =       "December",
  year =        "2009",
  org =         "SFU-CMPT",
  school =      SFU_CS_School,
  defended =    "2009-12-15",
  pages =       "116",
  url =          "ftp://fas.sfu.ca/pub/cs/TH/2009/LilongShiPhD.pdf",
  abstract =    "Colour constancy algorithms differ in their
                derivation, implementation, performance and assumptions. The focus of
                the research presented in this thesis is to discover colour constancy
                solutions to recover surface colours, or equivalently, to estimate the
                illumination, of single light source in a given scene.
                Several colour constancy models will be proposed. These methods have
                different methodologies and constraints. For example, a method can be
                constrained on a particular model surface material, on blackbody
                radiation light source, on dichromatic model, and on spatial variation
                of the illumination and the reflectance. The methods to be discussed
                include, for instance, a method of identifying achromatic surfaces,
                which can then be used as known references for estimating the scene
                illumination. A second method examines the colour of human skin and
                its dependence on its hemoglobin content, melanin content, and the
                illuminating light. The corresponding basis of these three factors can
                be represented linearly in logarithm space, where the colour of the
                light can then be estimated. A third method, uses the fact that the
                colours reflected by an inhomogeneous dielectric material lie on a
                plane spanned by the colour of the specular component reflected from
                the air-surface interface and the colour reflected from the body of
                the material. Once these planes are detected by a Hough transform,
                their intersection line represents the scene illumination. A fourth
                method is based on the independence and difference in the rate of
                spatial variation of the luminance and the surface reflectance in a
                given scene, from which image features can be separated via
                non-negative matrix factorization to reveal the true surface
                reflectance. A fifth method is based on learning the correspondence
                between an s colour content and its illumination via
                thin-plate-spline interpolation so that the chromaticity of the light
                can be calculated. Finally, a quaternion-based curvature measure
                approach is developed that can be used as a complement to colour
                constancy methods that use information from spatial edges. In this
                thesis, these various methods are proposed to overcome drawbacks in
                existing approaches for better performance and improved robustness and
                efficiency. ",
  gen =         "0->",
}


@MastersThesis{U-SFraser-CMPT-MSc:2009@Belli,
  author =     "Fernando Belli",
  title =      "Improving Access to Data in Legacy Health Information Systems",
  supervisor = "Stella Atkins",
  month =      "December",
  year =       "2009",
  org =        "SFU-CMPT",
  school =     SFU_CS_School,
  defended =   "2009-12-07",
  pages =      "42",
  url =        "ftp://fas.sfu.ca/pub/cs/TH/2009/FernandoBelliMSc.pdf",
  abstract =   "In today's world, fast and on time access to information has become a necessity
               due to the increasing demand to make informed decisions based not only on estimations
               and historical information but also on current data. Decision makers expect to have
               access to the information in a timely way. This is a particular problem in the Fraser
               Health Authority where data needed by decision-makers is not easily accessible due to
               lack of functionality in their legacy Health Information Systems (HIS). Our solution
               involves extracting the data from the HIS into a SQL database every 15 minutes, that is
               then used to create a set of XML files. These XML files are processed and displayed to
               the end users using a client-side browser application in a secure way. This solution
               adheres to the policies of the Fraser Health Authority, is scalable for read-only access,
               and enables end-users to access current information.",
  gen =        "0->",
}
